Th is page contains all content from the legacy PDF  notes; gradient descent chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
In the previous chapter, we showed how to describe an interesting objective
function for machine learning, but we need a way to ﬁnd the optimal
, particularly when the objective function is not amenable to
analytical optimization. For example, this can be the case when  involves a
more complex loss function, or more general forms of regularization. It can also be
the case when there are simply too many parameters to learn for it to be
computationally feasible.
There is an enormous and fascinating literature on the mathematical and
algorithmic foundations of optimization, but for this class, we will consider one of
the simplest methods, called gradient descent.
Intuitively, in one or two dimensions, we can easily think of  as deﬁning a
surface over ; that same idea extends to higher dimensions. Now, our objective is
to ﬁnd the  value at the lowest point on that surface. One way to think about
gradient descent is that you start at some arbitrary point on the surface, look to see
in which direction the “hill” goes down most steeply, take a small step in that
direction, determine the direction of steepest descent from where you are, take
another small step, etc.
Below, we explicitly give gradient descent algorithms for one and multidimensional
objective functions (Section 3.1 and Section 3.2). We then illustrate the application of
gradient descent to a loss function which is not merely mean squared loss
(Section 3.3). And we present an important method known as stochastic gradient
descent (Section 3.4), which is especially useful when datasets are too large for
descent in a single batch, and has some important behaviors of its own.
3.1 Gradient descent in one dimension
We start by considering gradient descent in one dimension. Assume , and
that we know both  and its ﬁrst derivative with respect to , . Here is
pseudo-code for gradient descent on an arbitrary function . Along with  and its
gradient  (which, in the case of a scalar , is the same as its derivative ), we
have to specify some hyper-parameters. These hyper-parameters include the initial
value for parameter , a step-size hyper-parameter , and an accuracy hyper-
parameter .3  Gradient Descent
Note
Θ∗=argminΘJ(Θ)
J(Θ)
J(Θ)
Θ
Θ
Θ∈R
J(Θ) ΘJ′(Θ)
f f
∇Θf Θ f′
Θ η
ϵYou might want to consider
studying optimization some day!
It’s one of the fundamental tools
enabling machine learning, and it’s
a beautiful and deep ﬁeld. 3  Gradient Descent  The hyper-parameter  is often called learning rate when gradient descent is applied
in machine learning. For simplicity,  may be taken as a constant, as is the case in
the pseudo-code below; and we’ll see adaptive (non-constant) step-sizes soon.
What’s important to notice though, is that even when  is constant, the actual
magnitude of the change to  may not be constant, as that change depends on the
magnitude of the gradient itself too.
procedure 1D-G RADIENT -D ESCENT( )
repeat
until 
return 
end procedure
Note that this algorithm terminates when the derivative of the function  is
sufﬁciently small. There are many other reasonable ways to decide to terminate,
including:
Stop after a ﬁxed number of iterations , i.e., when . Practically, this is the
most common choice.
Stop when the change in the value of the parameter  is sufﬁciently small,
i.e., when .
❓ Study Question
Consider all of the potential stopping criteria for 1D-Gradient-Descent, both
in the algorithm as it appears and listed separately later. Can you think of ways
that any two of the criteria relate to each other?
Theorem 3.1 Choose any small distance . If we assume that  has a minimum, is
sufﬁciently “smooth” and convex, and if the learning rate  is sufﬁciently small, gradient
descent will reach a point within  of a global optimum point .
However, we must be careful when choosing the learning rate to prevent slow
convergence, non-converging oscillation around the minimum, or divergence.
The following plot illustrates a convex function , starting gradient
descent at  with a step-size of . It is very well-behaved!η
η
η
Θ
1: Θinit,η,f,f′,ϵ
2:Θ(0)←Θinit
3:t←0
4:
5:t←t+1
6: Θ(t)=Θ(t−1)−ηf′(Θ(t−1))
7: |f′(Θ(t))|<ϵ
8: Θ(t)
9:
f
T t=T
Θ
Θ(t)−Θ(t−1)<ϵ
∣∣~ϵ>0 f
η
~ϵ Θ
f(x)=(x−2)2
xinit=4.0 1/2 − 1 1 2 3 4 5 624
xf (x )
If  is non-convex, where gradient descent converges to depends on . First, let’s
establish some deﬁnitions. Let  be a real-valued function deﬁned over some
domain . A point  is called a global minimum point of  if  for
all other . A point  is instead called a local minimum point of a function
 if there exists some constant  such that for all  within the interval deﬁned
by  , where  is some distance metric, e.g.,
 A global minimum point is also a local minimum point, but a
local minimum point does not have to be a global minimum point.
❓ Study Question
What happens in this example with very small ? With very big ?
If  is non-convex (and sufﬁciently smooth), one expects that gradient descent (run
long enough with small enough learning rate) will get very close to a point at which
the gradient is zero, though we cannot guarantee that it will converge to a global
minimum point.
There are two notable exceptions to this common sense expectation: First, gradient
descent can get stagnated while approaching a point  which is not a local
minimum or maximum, but satisﬁes . For example, for , starting
gradient descent from the initial guess , while using learning rate 
will lead to  converging to zero as . Second, there are functions (even
convex ones) with no minimum points, like , for which gradient
descent with a positive learning rate converges to .
The plot below shows two different , and how gradient descent started from
each point heads toward two different local optimum points.f xinit
f
D x0∈D ff(x0)≤f(x)
x∈D x0∈D
f ϵ>0 x
d(x,x0)<ϵ,f(x0)≤f(x)d
d(x,x0)=||x−x0||.
η η
f
x
f′(x)=0 f(x)=x3
xinit=1 η<1/3
x(k)k→∞
f(x)=exp(−x)
+∞
xinit − 2 − 1 1 2 3 44681 0
xf (x )
3.2 Multiple dimensions
The extension to the case of multi-dimensional  is straightforward. Let’s assume
, so .
The gradient of  with respect to  is
The algorithm remains the same, except that the update step in line 5 becomes
and any termination criteria that depended on the dimensionality of  would have
to change. The easiest thing is to keep the test in line 6 as ,
which is sensible no matter the dimensionality of .
❓ Study Question
Which termination criteria from the 1D case were deﬁned in a way that assumes
 is one dimensional?
3.3 Application to regressionΘ
Θ∈Rmf:Rm→R
f Θ
∇Θf=⎡
⎢⎣∂f/∂Θ1
⋮
∂f/∂Θm⎤
⎥⎦
Θ(t)=Θ(t−1)−η∇Θf(Θ(t−1))
Θ
f(Θ(t))−f(Θ(t−1))<ϵ
∣∣Θ
Θ Recall from the previous chapter that choosing a loss function is the ﬁrst step in
formulating a machine-learning problem as an optimization problem, and for
regression we studied the mean square loss, which captures losws as
. This leads to the ordinary least squares objective
We use the gradient of the objective with respect to the parameters,
to obtain an analytical solution to the linear regression problem. Gradient descent
could also be applied to numerically compute a solution, using the update rule
Now, let’s add in the regularization term, to get the ridge-regression objective:
 
Recall that in ordinary least squares, we ﬁnessed handling  by adding an extra
dimension of all 1’s. In ridge regression, we really do need to separate the
parameter vector  from the offset , and so, from the perspective of our general-
purpose gradient descent method, our whole parameter set  is deﬁned to be
. We will go ahead and ﬁnd the gradients separately for each one:
Note that  will be of shape  and  will be a scalar since we
have separated  from  here.
❓ Study Question(guess−actual)2
J(θ)=1
nn
∑
i=1(θTx(i)−y(i))2
.
∇θJ=2
nXT
d×n(Xθ−Y)
n×1,        (3.1)
θ(t)=θ(t−1)−η2
nn
∑
i=1([θ(t−1)]T
x(i)−y(i))x(i).
3.3.1 Ridge regression
Jridge(θ,θ0)=1
nn
∑
i=1(θTx(i)+θ0−y(i))2
+λ∥θ∥2.
θ0
θ θ0
Θ
Θ=(θ,θ0)
∇θJridge(θ,θ0)=2
nn
∑
i=1(θTx(i)+θ0−y(i))x(i)+2λθ
∂Jridge(θ,θ0)
∂θ0=2
nn
∑
i=1(θTx(i)+θ0−y(i)).
∇θJridge d×1 ∂Jridge/∂θ0
θ0θ Convince yourself that the dimensions of all these quantities are correct, under
the assumption that  is . How does  relate to  as discussed for  in the
previous section?
❓ Study Question
Compute  by ﬁnding the vector of partial derivatives
. What is the shape of ?
❓ Study Question
Compute  by ﬁnding the vector of partial derivatives
.
❓ Study Question
Use these last two results to verify our derivation above.
Putting everything together, our gradient descent algorithm for ridge regression
becomes
procedure RR-G RADIENT -D ESCENT( )
repeat
until 
return 
end procedure
❓ Study Question
Is it okay that  doesn’t appear in line 8?
❓ Study Question
Is it okay that the 2’s from the gradient deﬁnitions don’t appear in the
algorithm?θd×1 d m Θ
∇θ||θ||2
(∂||θ||2/∂θ1,…,∂||θ||2/∂θd) ∇θ||θ||2
∇θJridge(θTx+θ0,y)
(∂Jridge(θTx+θ0,y)/∂θ1,…,∂Jridge(θTx+θ0,y)/∂θd)
1: θinit,θ0init,η,ϵ
2:θ(0)←θinit
3:θ(0)
0←θ0init
4:t←0
5:
6:t←t+1
7:θ(t)=θ(t−1)−η(1
n∑n
i=1(θ(t−1)Tx(i)+θ0(t−1)−y(i))x(i)+λθ(t−1))
8:θ(t)
0=θ(t−1)
0−η(1
n∑n
i=1(θ(t−1)Tx(i)+θ0(t−1)−y(i)))
9: Jridge(θ(t),θ(t)
0)−Jridge(θ(t−1),θ(t−1)
0)<ϵ
∣∣10: θ(t),θ(t)
0
11:
λBeware double superscripts!  is
the transpose of the vector .[θ]T
θ 3.4 Stochastic gradient descent
When the form of the gradient is a sum, rather than take one big(ish) step in the
direction of the gradient, we can, instead, randomly select one term of the sum, and
take a very small step in that direction. This seems sort of crazy, but remember that
all the little steps would average out to the same direction as the big step if you
were to stay in one place. Of course, you’re not staying in that place, so you move,
in expectation, in the direction of the gradient.
Most objective functions in machine learning can end up being written as an
average over data points, in which case, stochastic gradient descent (sgd) is
implemented by picking a data point randomly out of the data set, computing the
gradient as if there were only that one point in the data set, and taking a small step
in the negative direction.
Let’s assume our objective has the form
where  is the number of data points used in the objective (and this may be
different from the number of points available in the whole data set).
Here is pseudocode for applying sgd to such an objective ; it assumes we know the
form of  for all  in :
procedure STOCHAS TIC -G RADIENT -D ESCENT( )
for  do
randomly select 
end for
end procedure
Note that now instead of a ﬁxed value of ,  is indexed by the iteration of the
algorithm, . Choosing a good stopping criterion can be a little trickier for sgd than
traditional gradient descent. Here we’ve just chosen to stop after a ﬁxed number of
iterations .
For sgd to converge to a local optimum point as  increases, the learning rate has to
decrease as a function of time. The next result shows one learning rate sequence that
works.
Theorem 3.2 If  is convex, and  is a sequence satisfyingf(Θ)=1
nn
∑
i=1fi(Θ),
n
f
∇Θfii1…n
1: Θinit,η,f,∇Θf1,...,∇Θfn,T
2:Θ(0)←Θinit
3:t←1
4: i∈{1,2,…,n}
5: Θ(t)=Θ(t−1)−η(t)∇Θfi(Θ(t−1))
6:
7:
ηη
t
T
t
f η(t)
∞
∑
t=1η(t)=∞and∞
∑
t=1η(t)2<∞,Sometimes you will see that the
objective being written as a sum,
instead of an average. In the “sum”
convention, the  normalizing
constant is getting “absorbed” into
individual .1
n
fi
f(Θ)=n
∑
i=1fi(Θ). then SGD converges with probability one* to the optimal .*
Why these two conditions? The intuition is that the ﬁrst condition, on , is
needed to allow for the possibility of an unbounded potential range of exploration,
while the second condition, on , ensures that the learning rates get smaller
and smaller as  increases.
One “legal” way of setting the learning rate is to make  but people often
use rules that decrease more slowly, and so don’t strictly satisfy the criteria for
convergence.
❓ Study Question
If you start a long way from the optimum, would making  decrease more
slowly tend to make you move more quickly or more slowly to the optimum?
There are multiple intuitions for why sgd might be a better choice algorithmically
than regular gd (which is sometimes called batch gd (bgd)):
bgd typically requires computing some quantity over every data point in a data
set. sgd may perform well after visiting only some of the data. This behavior
can be useful for very large data sets – in runtime and memory savings.
If your  is actually non-convex, but has many shallow local optimum points
that might trap bgd, then taking samples from the gradient at some point 
might “bounce” you around the landscape and away from the local optimum
points.
Sometimes, optimizing  really well is not what we want to do, because it
might overﬁt the training set; so, in fact, although sgd might not get lower
training error than bgd, it might result in lower test error.Θ
∑η(t)
∑η(t)2
t
η(t)=1/t
η(t)
f
Θ
f Th is page contains all content from the legacy PDF  notes; classiﬁcation chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
4.1 Classiﬁcation
Classiﬁcation is a machine learning problem seeking to map from inputs  to
outputs in an unordered set.
Examples of classiﬁcation output sets could be  if we’re
trying to ﬁgure out what type of fruit we have, or  if
we’re working in an emergency room and trying to give the best medical care to a
new patient. We focus on an essential simple case, binary classiﬁcation, where we aim
to ﬁnd a mapping from  to two outputs. While we should think of the outputs as
not having an order, it’s often convenient to encode them as . As before, let
the letter  (for hypothesis) represent a classiﬁer, so the classiﬁcation process looks
like:
Like regression, classiﬁcation is a supervised learning problem, in which we are given
a training data set of the form
We will assume that each  is a  column vector. The intended use of this data
is that, when given an input , the learned hypothesis should generate output 
.
What makes a classiﬁer useful? As in regression, we want it to work well on new
data, making good predictions on examples it hasn’t seen. But we don’t know
exactly what data this classiﬁer might be tested on when we use it in the real world.
So, we have to assume a connection between the training data and testing data;
typically, they are drawn independently from the same probability distribution.
In classiﬁcation, we will often use 0-1 loss for evaluation (as discussed in
Section 1.3). For that choice, we can write the training error and the testing error. In
particular, given a training set  and a classiﬁer , we deﬁne the training error of 
to be4  Classiﬁcation
Note
Rd
{apples,oranges,pears}
{heartattack,noheartattack}
Rd
{+1,0}
h
x→ →y.h
Dtrain={(x(1),y(1)),…,(x(n),y(n))}.
x(i)d×1
x(i)y(i)
Dn h hTh is is in contrast to a continuo us
real-value d output, as we saw for
linear regression. 4  Classiﬁcation 
3 For now, we will try to ﬁnd a classiﬁer with small training error (later, with some
added criteria) and hope it generalizes well to new data, and has a small test error
on  new examples that were not used in the process of ﬁnding the classiﬁer.
We begin by introducing the hypothesis class of linear classiﬁers (Section 4.2) and
then deﬁne an optimization framework to learn linear logistic classiﬁers (Section 4.3).
4.2 Linear classiﬁers
We start with the hypothesis class of linear classiﬁers. They are (relatively) easy to
understand, simple in a mathematical sense, powerful on their own, and the basis
for many other more sophisticated methods. Following their deﬁnition, we present
a simple learning algorithm for classiﬁers.
A linear classiﬁer in  dimensions is deﬁned by a vector of parameters  and
scalar . So, the hypothesis class  of linear classiﬁers in  dimensions is
parameterized by the set of all vectors in . We’ll assume that  is a 
column vector.
Given particular values for  and , the classiﬁer is deﬁned by
Remember that we can think of  as specifying a -dimensional hyperplane
(compare the above with Equation 2.3). But this time, rather than being interested in
that hyperplane’s values at particular points , we will focus on the separator that it
induces. The separator is the set of  values such that . This is also a
hyperplane, but in  dimensions! We can interpret  as a vector that is
perpendicular to the separator. (We will also say that  is normal to the separator.)
Below is an embedded demo illustrating the separator and normal vector. Open
demo in full screen.Etrain(h)=1
nn
∑
i=1{ .1h(x(i))≠y(i)
0otherwise(4.1)
Etest(h)=1
n′n+n′
∑
i=n+1{1h(x(i))≠y(i)
0otherwise
n′
4.2.1 Linear classiﬁers: deﬁnition
d θ∈Rd
θ0∈R H d
Rd+1θd×1
θθ0
h(x;θ,θ0)=step(θTx+θ0)={ .+1if θTx+θ0>0
0 otherwise
θ,θ0 d
x
x θTx+θ0=0
d−1 θ
θ
Demo: Linear classiﬁer separator θ₁:
0.5θ₂:
0.5θ₀:
0.0Toggle z=0
Surface
Built with ❤  by Shen²  | Report a Bug
Features (x ₁ , x ₂ ) & z = θ ₁ x ₁  + θ ₂ x ₂  + θ ₀−2−1012−505Separ ator
Normal v ecto
Prediction: P
Prediction: NFeature space (x ₁ , x ₂
x ₁x ₂  
For example, in two dimensions ( ) the separator has dimension 1, which
means it is a line, and the two components of  give the orientation of
the separator, as illustrated in the following example.
Let  be the linear classiﬁer deﬁned by . Th e diagram below shows the 
vector (in green) and the separator it deﬁnes:d=2
θ=[θ1,θ2]T
4.2.2 Linear classiﬁers: examples
Example:
h θ=[],θ0=11
−1θ θTx +θ 0 = 0
x 1x 2
θ θ 2
θ 1
W hat is ? We can solve for it by plug ging a point on the line into the equa tion for the
line. It is often convenient to choose a point on one of the axes, e.g., in this case,
, for which , giving .
In this example, the separator divides , the space our  points live in, into two
half-spaces. The one that is on the same side as the normal vector is the positive half-
space, and we classify all points in that space as positive. The half-space on the
other side is negative and all points in it are classiﬁed as negative.
Note that we will call a separator a linear separator of a data set if all of the data with
one label falls on one side of the separator and all of the data with the other label
falls on the other side of the separator. For instance, the separator in the next
example is a linear separator for the illustrated data. If there exists a linear separator
on a dataset, we call this dataset linearly separable.
Let  be the linear classiﬁer deﬁned by .
Th e diagram below shows several points classiﬁed by . In particular, let  and
.θ0
x=[0,1]TθT[]+θ0=00
1θ0=1
Rdx(i)
Example:
h θ=[],θ0=3−1
1.5
h x(1)=[]3
2
x(2)=[]4
−1
( [] ) Th us ,  and  are given positive (label +1) and negative (label 0) classiﬁcations,
respectively.
❓ Study Question
What is the green vector normal to the separator? Specify it as a column vector.
❓ Study Question
What change would you have to make to  if you wanted to have the
separating hyperplane in the same place, but to classify all the points labeled ‘+’
in the diagram as negative and all the points labeled ‘-’ in the diagram as
positive?
4.3 Linear logistic classiﬁers
Given a data set and the hypothesis class of linear classiﬁers, our goal will be to ﬁnd
the linear classiﬁer that optimizes an objective function relating its predictions to
the training data. To make this problem computationally reasonable, we will need
to take care in how we formulate the optimization problem to achieve this goal.
For classiﬁcation, it is natural to make predictions in  and use the 0-1 loss
function, , as introduced in Chapter 1:h(x(1);θ,θ0)=step([ ][]+3)=step(3)=+1
h(x(2);θ,θ0)=step([ ][]+3)=step(−2.5)=0−11.53
2
−11.54
−1
x(1)x(2)
θ,θ0
{+1,0}
L01
L01(g,a)={ .0if g=a
1otherwise However, even for simple linear classiﬁers, it is very difﬁcult to ﬁnd values for 
that minimize simple 0-1 training error
This problem is NP-hard, which probably implies that solving the most difﬁcult
instances of this problem would require computation time exponential in the number
of training examples, .
What makes this a difﬁcult optimization problem is its lack of “smoothness”:
There can be two hypotheses,  and , where one is closer in
parameter space to the optimal parameter values , but they make the
same number of misclassiﬁcations so they have the same  value.
All predictions are categorical: the classiﬁer can’t express a degree of certainty
about whether a particular input  should have an associated value .
For these reasons, if we are considering a hypothesis  that makes ﬁve incorrect
predictions, it is difﬁcult to see how we might change  so that it will perform
better, which makes it difﬁcult to design an algorithm that searches in a sensible
way through the space of hypotheses for a good one. For these reasons, we
investigate another hypothesis class: linear logistic classiﬁers, providing their
deﬁnition, then an approach for learning such classiﬁers using optimization.
The hypotheses in a linear logistic classiﬁer (LLC) are parameterized by a -
dimensional vector  and a scalar , just as is the case for linear classiﬁers.
However, instead of making predictions in , LLC hypotheses generate real-
valued outputs in the interval . An LLC has the form
This looks familiar! What’s new?
The logistic function, also known as the sigmoid function, is deﬁned as
and is plotted below, as a function of its input . Its output can be interpreted as a
probability, because for any value of  the output is in .θ,θ0
J(θ,θ0)=1
nn
∑
i=1L01(step(θTx(i)+θ0),y(i)).
n
(θ,θ0) (θ′,θ′
0)
(θ∗,θ∗
0)
J
x y
θ,θ0
θ,θ0
4.3.1 Linear logistic classiﬁers: deﬁnition
d
θ θ0
{+1,0}
(0,1)
h(x;θ,θ0)=σ(θTx+θ0).
σ(z)=1
1+e−z,
z
z (0,1)Th e “probably” here is not becaus e
we’re too lazy to look it up , but
actua lly becaus e of a fun damental
un solved problem in computer-
science theory, known as “P
vs. NP.” − 4 − 2 2 40. 51
zσ (z )
❓ Study Question
Convince yourself the output of  is always in the interval . Why can’t it
equal 0 or equal 1? For what value of  does ?
What does an LLC look like? Let’s consider the simple case where , so our
input points simply lie along the  axis. Classiﬁers in this case have dimension ,
meaning that they are points. The plot below shows LLCs for three different
parameter settings: , , and 
− 4 − 2 2 40. 51
xσ (θTx +θ 0 )
❓ Study Question
Which plot is which? What governs the steepness of the curve? What governs
the  value where the output is equal to 0.5?
But wait! Remember that the deﬁnition of a classiﬁer is that it’s a mapping from
 or to some other discrete set. So, then, it seems like an LLC is actually
not a classiﬁer!
Given an LLC, with an output value in , what should we do if we are forced to
make a prediction in ? A default answer is to predict  ifσ (0,1)
zσ(z)=0.5
4.3.2 Linear logistic classiﬁer: examples
d=1
x 0
σ(10x+1)σ(−2x+1)σ(2x−3).
x
Rd→{+1,0}
(0,1)
{+1,0} +1  and  otherwise. The value  is sometimes called a prediction
threshold.
In fact, for different problem settings, we might prefer to pick a different prediction
threshold. The ﬁeld of decision theory considers how to make this choice. For
example, if the consequences of predicting  when the answer should be  are
much worse than the consequences of predicting  when the answer should be 
, then we might set the prediction threshold to be greater than .
❓ Study Question
Using a prediction threshold of 0.5, for what values of  do each of the LLCs
shown in the ﬁgure above predict ?
When , then our inputs  lie in a two-dimensional space with axes  and ,
and the output of the LLC is a surface, as shown below, for .
❓ Study Question
Convince yourself that the set of points for which , that is, the
``boundary’’ between positive and negative predictions with prediction
threshold , is a line in  space. What particular line is it for the case in
the ﬁgure above? How would the plot change for , but now with
? For ?
Optimization is a key approach to solving machine learning problems; this also
applies to learning linear logistic classiﬁers (LLCs) by deﬁning an appropriate loss
function for optimization. A ﬁrst attempt might be to use the simple 0-1 lossσ(θTx+θ0)>0.5 0 0.5
+1 −1
−1 +1
0.5
x
+1
d=2 x x1x2
θ=(1,1),θ0=2
σ(θTx+θ0)=0.5
0.5 (x1,x2)
θ=(1,1)
θ0=−2θ=(−1,−1),θ0=2
4.3.3 Learning linear logistic classiﬁers function  that gives a value of 0 for a correct prediction, and a 1 for an incorrect
prediction. As noted earlier, however, this gives rise to an objective function that is
very difﬁcult to optimize, and so we pursue another strategy for deﬁning our
objective.
For learning LLCs, we’d have a class of hypotheses whose outputs are in , but
for which we have training data with  values in . How can we deﬁne an
appropriate loss function? We start by changing our interpretation of the output to
be the probability that the input should map to output value 1 (we might also say that
this is the probability that the input is in class 1 or that the input is ‘positive.’)
❓ Study Question
If  is the probability that  belongs to class , what is the probability that
 belongs to the class , assuming there are only these two classes?
Intuitively, we would like to have low loss if we assign a high probability to the correct
class. We’ll deﬁne a loss function, called negative log-likelihood (NLL), that does just
this. In addition, it has the cool property that it extends nicely to the case where we
would like to classify our inputs into more than two classes.
In order to simplify the description, we assume that (or transform our data so that)
the labels in the training data are .
We would like to pick the parameters of our classiﬁer to maximize the probability
assigned by the LLC to the correct  values, as speciﬁed in the training set. Letting
guess , that probability is
under the assumption that our predictions are independent. This can be cleverly
rewritten, when , as
❓ Study Question
Be sure you can see why these two expressions are the same.
The big product above is kind of hard to deal with in practice, though. So what can
we do? Because the log function is monotonic, the  that maximize the quantityL01
(0,1)
y {+1,0}
h(x) x +1
x −1
y∈{0,1}
y
g(i)=σ(θTx(i)+θ0)
n
∏
i=1{ ,g(i)if y(i)=1
1−g(i)otherwise
y(i)∈{0,1}
n
∏
i=1g(i)y(i)
(1−g(i))1−y(i).
θ,θ0Remember to be sur e your   value s
have this form if you try to learn an
LLC us ing NLL!y
Th at crazy hug e  represents
taking the produc t over a bun ch of
factors jus t as hug e  represents
taking the sum  over a bun ch of
terms.Π
Σ above will be the same as the  that maximize its log, which is the following:
Finally, we can turn the maximization problem above into a minimization problem
by taking the negative of the above expression, and writing in terms of minimizing
a loss
where  is the negative log-likelihood loss function:
This loss function is also sometimes referred to as the log loss or cross entropy. and it
won’t make any real difference. If we ask you for numbers, use log base .
What is the objective function for linear logistic classiﬁcation? We can ﬁnally put
all these pieces together and develop an objective function for optimizing
regularized negative log-likelihood for a linear logistic classiﬁer. In fact, this process
is usually called “logistic regression,” so we’ll call our objective , and deﬁne it as
❓ Study Question
Consider the case of linearly separable data. What will the  values that
optimize this objective be like if ? What will they be like if  is very big?
Try to work out an example in one dimension with two data points.
What role does regularization play for classiﬁers? This objective function has the
same structure as the one we used for regression, Equation 2.2, where the ﬁrst term
(in parentheses) is the average loss, and the second term is for regularization.
Regularization is needed for building classiﬁers that can generalize well (just as was
the case for regression). The parameter  governs the trade-off between the two
terms as illustrated in the following example.
Suppose we wish to obtain a linear logistic classiﬁer for this one-dimensional
dataset:θ,θ0
n
∑
i=1(y(i)logg(i)+(1−y(i))log(1−g(i))).
n
∑
i=1Lnll(g(i),y(i))
Lnll
Lnll(guess,actual)=−(actual⋅log(guess)+(1−actual)⋅log(1−guess)).
e
Jlr
Jlr(θ,θ0;D)=(1
nn
∑
i=1Lnll(σ(θTx(i)+θ0),y(i)))+λ∥θ∥2. (4.2)
θ
λ=0 λ
λ Clearly, this can be ﬁt very nicely by a hypothesis , but what is the best
value for ? Evidently, when there is no regularization ( ), the objective
function  will approach zero for large values of , as shown in the plot on the
left, below. However, would the best hypothesis really have an inﬁnite (or very
large) value for ? Such a hypothesis would suggest that the data indicate strong
certainty that a sharp transition between  and  occurs exactly at ,
despite the actual data having a wide gap around .
In absence of other beliefs about the solution, we might prefer that our linear
logistic classiﬁer not be overly certain about its predictions, and so we might prefer
a smaller  over a large  By not being overconﬁdent, we might expect a somewhat
smaller  to perform better on future examples drawn from this same distribution.
This preference can be realized using a nonzero value of the regularization trade-off
parameter, as illustrated in the plot on the right, above, with .
Another nice way of thinking about regularization is that we would like to prevent
our hypothesis from being too dependent on the particular training data that we
were given: we would like for it to be the case that if the training data were changed
slightly, the hypothesis would not change by much.
4.4 Gradient descent for logistic regression
Now that we have a hypothesis class (LLC) and a loss function (NLL), we need to
take some data and ﬁnd parameters! Sadly, there is no lovely analytical solution like
the one we obtained for regression, in Section 2.7.2. Good thing we studied gradient
h(x)=σ(θx)
θ λ=0
Jlr(θ) θ
θ
y=0y=1 x=0
x=0
θ θ.
θ
λ=0.2 descent! We can perform gradient descent on the  objective, as we’ll see next. We
can also apply stochastic gradient descent to this problem.
Luckily,  has enough nice properties that gradient descent and stochastic
gradient descent should generally “work”. We’ll soon see some more challenging
optimization problems though – in the context of neural networks, in Section 6.7.
First we need derivatives with respect to both  (the scalar component) and  (the
vector component) of . Explicitly, they are:
Note that  will be of shape  and  will be a scalar since we have
separated  from  here.
Putting everything together, our gradient descent algorithm for logistic regression
becomes:
❓ Study Question
Convince yourself that the dimensions of all these quantities are correct, under
the assumption that  is .
❓ Study Question
Compute  by ﬁnding the vector of partial derivatives .
What is the shape of ?
❓ Study Question
Compute  by ﬁnding the vector of partial derivatives
.
❓ Study Question
Use these last two results to verify our derivation above.
Algorithm 4.1 LR-Gradient-Descent( )
repeatJlr
Jlr
θ0 θ
Θ
∇θJlr(θ,θ0)=1
nn
∑
i=1(g(i)−y(i))x(i)+2λθ
∂Jlr(θ,θ0)
∂θ0=1
nn
∑
i=1(g(i)−y(i)).
∇θJlr d×1∂Jlr
∂θ0
θ0θ
θd×1
∇θ∥θ∥2(∂∥θ∥2
∂θ1,…,∂∥θ∥2
∂θd)
∇θ∥θ∥2
∇θLnll(σ(θTx+θ0),y)
(∂Lnll(σ(θTx+θ0),y)
∂θ1,…,∂Lnll(σ(θTx+θ0),y)
∂θd)
θinit,θ0init,η,ϵ
1:θ(0)←θinit
2:θ(0)
0←θ0init
3:t←0
4: until 
return 
Logistic regression, implemented using batch or stochastic gradient descent, is a
useful and fundamental machine learning technique. We will also see later that it
corresponds to a one-layer neural network with a sigmoidal activation function,
and so is an important step toward understanding neural networks.
Much like the squared-error loss function that we saw for linear regression, the NLL
loss function for linear logistic regression is a convex function of the parameters 
and  (below is a proof if you’re interested). This means that running gradient
descent with a reasonable set of hyperparameters will behave nicely.
4.5 Handling multiple classes
So far, we have focused on the binary classiﬁcation case, with only two possible
classes. But what can we do if we have multiple possible classes (e.g., we want to
predict the genre of a movie)? There are two basic strategies:
Train multiple binary classiﬁers using different subsets of our data and
combine their outputs to make a class prediction.
Directly train a multi-class classiﬁer using a hypothesis class that is a
generalization of logistic regression, using a one-hot output encoding and NLL
loss.
The method based on NLL is in wider use, especially in the context of neural
networks, and is explored here. In the following, we will assume that we have a
data set  in which the inputs  but the outputs  are drawn from a set of
 classes . Next, we extend the idea of NLL directly to multi-class
classiﬁcation with  classes, where the training label is represented with what is
called a one-hot vector , where  if the example is of class 
and  otherwise. Now, we have a problem of mapping an input  that is in
 into a -dimensional output. Furthermore, we would like this output to be
interpretable as a discrete probability distribution over the possible classes, which5:t←t+1
6:θ(t)←θ(t−1)−η(1
n∑n
i=1(σ(θ(t−1)Tx(i)+θ(t−1)
0)−y(i))x(i)+2λθ(t−1))
7:θ(t)
0←θ(t−1)
0−η(1
n∑n
i=1(σ(θ(t−1)Tx(i)+θ(t−1)
0)−y(i)))
8:Jlr(θ(t),θ(t)
0)−Jlr(θ(t−1),θ(t−1)
0)<ϵ
∣∣9: θ(t),θ(t)
0
4.4.1 Convexity of the NLL Loss Function
θ
θ0
Proof of convexity of the NLL loss function
D x(i)∈Rdy(i)
K {c1,…,cK}
K
y=[ ]Ty1,…,yK yk=1 k
yk=0 x(i)
RdK means the elements of the output vector have to be non-negative (greater than or
equal to 0) and sum to 1.
We will do this in two steps. First, we will map our input  into a vector value
 by letting  be a whole  matrix of parameters, and  be a 
vector, so that
Next, we have to extend our use of the sigmoid function to the multi-dimensional
softmax function, that takes a whole vector  and generates
which can be interpreted as a probability distribution over  items. To make the
ﬁnal prediction of the class label, we can then look at  ﬁnd the most likely
probability over these  entries in  (i.e. ﬁnd the largest entry in ) and return the
corresponding index as the “one-hot” element of  in our prediction.
❓ Study Question
Convince yourself that the vector of  values will be non-negative and sum to 1.
Putting these steps together, our hypotheses will be
Now, we retain the goal of maximizing the probability that our hypothesis assigns
to the correct output  for each input . We can write this probability, letting 
stand for our “guess”, , for a single example  as .
❓ Study Question
How many elements that are not equal to 1 will there be in this product?
The negative log of the probability that we are making a correct guess is, then, for
one-hot vector  and probability distribution vector ,
We’ll call this nllm for negative log likelihood multiclass. It is also worth noting that the
NLLM loss function is also convex; however, we will omit the proof.x(i)
z(i)∈RKθ d×K θ0K×1
z=θTx+θ0.
z∈RK
g=softmax(z)= .⎡
⎢⎣exp(z1)/∑iexp(zi)
⋮
exp(zK)/∑iexp(zi)⎤
⎥⎦
K
g,
K g, g,
1
g
h(x;θ,θ0)=softmax(θTx+θ0).
yk x g
h(x) (x,y)∏K
k=1gyk
k
y g
Lnllm(g,y)=−K
∑
k=1yk⋅log(gk).Let’s check dimensions!  is
 and  is , and  is
, so  is  and we’re
good!θT
K×dxd×1θ0
K×1zK×1 ❓ Study Question
Be sure you see that is  is minimized when the guess assigns high
probability to the true class.
❓ Study Question
Show that  for  is the same as .
4.6 Prediction accuracy and validation
In order to formulate classiﬁcation with a smooth objective function that we can
optimize robustly using gradient descent, we changed the output from discrete
classes to probability values and the loss function from 0-1 loss to NLL. However,
when time comes to actually make a prediction we usually have to make a hard
choice: buy stock in Acme or not? And, we get rewarded if we guessed right,
independent of how sure or not we were when we made the guess.
The performance of a classiﬁer is often characterized by its accuracy, which is the
percentage of a data set that it predicts correctly in the case of 0-1 loss. We can see
that accuracy of hypothesis  on data  is the fraction of the data set that does not
incur any loss:
where  is the ﬁnal guess for one class or the other that we make from ,
e.g., after thresholding. It’s noteworthy here that we use a different loss function for
optimization than for evaluation. This is a compromise we make for computational
ease and efﬁciency.Lnllm
LnllmK=2 Lnll
h D
A(h;D)=1−1
nn
∑
i=1L01(g(i),y(i)),
g(i)h(x(i)) Th is page contains all content from the legacy PDF  notes; featur es chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
Linear regression and classiﬁcation are powerful tools, but in the real world, data
often exhibit non-linear behavior that cannot immediately be captured by the linear
models which we have built so far. For example, suppose the true behavior of a
system (with ) looks like this wavelet:
Such behavior is actually ubiquitous in physical systems, e.g., in the vibrations of
the surface of a drum, or scattering of light through an aperture. However, no single
hyperplane would be a very good ﬁt to such peaked responses!
A richer class of hypotheses can be obtained by performing a non-linear feature
transformation  before doing the regression. That is,  is a linear
function of , but  is a non-linear function of  if  is a non-linear
function of .
There are many different ways to construct . Some are relatively systematic and
domain independent. Others are directly related to the semantics (meaning) of the
original features, and we construct them deliberately with our application (goal) in
mind.
5.1 Gaining intuition about feature
transformations
In this section, we explore the effects of non-linear feature transformations on
simple classiﬁcation problems, to gain intuition.
Let’s look at an example data set that starts in 1-D:5  Feature Representation
Note
d=2
ϕ(x) θTx+θ0
xθTϕ(x)+θ0 x,ϕ
x
ϕ 5  Feature Representation  x
0
These points are not linearly separable, but consider the transformation
. Plotting this transformed data (in two-dimensional space, since
there are now two features), we see that it is now separable. There are lots of
possible separators; we have just shown one of them here.
xx2
s e p a r a t o r
A linear separator in  space is a nonlinear separator in the original space! Let’s see
how this plays out in our simple example. Consider the separator 
(which corresponds to  and  in our transformed space), which
labels the half-plane  as positive. What separator does it correspond to in
the original 1-D space? We have to ask the question: which  values have the
property that . The answer is  and , so those two points constitute
our separator, back in the original space. Similarly, by evaluating where 
and where , we can ﬁnd the regions of 1D space that are labeled positive
and negative (respectively) by this separator.
Exampleϕ(x)=[x,x2]T
Example
ϕ
x2−1=0
θ=[0,1]Tθ0=−1
x2−1>0
x
x2−1=0 +1 −1
x2−1>0
x2−1<0
Example x
01 - 1
5.2 Systematic feature construction
Here are two different ways to systematically construct features in a problem
independent way.
If the features in your problem are already naturally numerical, one systematic
strategy for constructing a new feature space is to use a polynomial basis. The idea is
that, if you are using the th-order basis (where  is a positive integer), you include
a feature for every possible product of  different dimensions in your original input.
Here is a table illustrating the th order polynomial basis for different values of ,
calling out the cases when  and :
Order in general ( )
0
1
2
3
⋮ ⋮ ⋮
This transformation can be used in combination with linear regression or logistic
regression (or any other regression or classiﬁcation model). When we’re using a
linear regression or classiﬁcation model, the key insight is that a linear regressor or
separator in the transformed space is a non-linear regressor or separator in the
original space.
To give a regression example, the wavelet pictured at the start of this chapter can be
ﬁt much better using a polynomial feature representation up to order ,
compared to just using a simple hyperplane in the original (single-dimensional)
feature space:
5.2.1 Polynomial basisk k
k
k k
d=1d>1
d=1 d>1
[1] [1]
[1,x]T[1,x1,…,xd]T
[1,x,x2]T[1,x1,…,xd,x2
1,x1x2,…]T
[1,x,x2,x3]T[1,x1,…,xd,x2
1,x1x2,…,x3
1,x1x2
2,x1x2x3,…]T
k=8 The raw data (with  random samples) is plotted on the left, and the
regression result (curved surface) is on the right.
Now let’s look at a classiﬁcation example and see how polynomial feature
transformation may help us.
One well-known example is the “exclusive or” (xor) data set, the drosophila of
machine-learning data sets:
Clearly, this data set is not linearly separable. So, what if we try to solve the xor
classiﬁcation problem using a polynomial basis as the feature transformation? We
can just take our two-dimensional data and transform it into a higher-dimensional
data set, by applying some feature transformation . Now, we have a classiﬁcation
problem as usual.
Let’s try it for  on our xor problem. The feature transformation is
❓  Study Question
If we train a classiﬁer after performing this feature transformation, would we
lose any expressive power if we let  (i.e., trained without offset instead of
with offset)?
We might run a classiﬁcation learning algorithm and ﬁnd a separator with
coefﬁcients  and . This corresponds to
n=1000
Example
ϕ
k=2
ϕ([x1,x2]T)=[1,x1,x2,x2
1,x1x2,x2
2]T.
θ0=0
θ=[0,0,0,0,4,0]Tθ0=0
2 2D. Melanogaster is a species of
fruit ﬂy, used as a simple system in
which to study genetics, since 1910. and is plotted below, with the gray shaded region classiﬁed as negative and the
white region classiﬁed as positive:
❓  Study Question
Be sure you understand why this high-dimensional hyperplane is a separator,
and how it corresponds to the ﬁgure.
For fun, we show some more plots below. Here is another result for a linear
classiﬁer on xor generated with logistic regression and gradient descent, using a
random initial starting point and second-order polynomial basis:
Here is a harder data set. Logistic regression with gradient descent failed to
separate it with a second, third, or fourth-order basis feature representation, but0+0x1+0x2+0x2
1+4x1x2+0x2
2+0=0
Example
Example
 succeeded with a ﬁfth-order basis. Shown below are some results after 
gradient descent iterations (from random starting points) for bases of order 2
(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).
❓  Study Question
Percy Eptron has a domain with four numeric input features, . He
decides to use a representation of the form
where  means the vector  concatenated with the vector .
What is the dimension of Percy’s representation? Under what assumptions
about the original features is this a reasonable choice?
Another cool idea is to use the training data itself to construct a feature space. The
idea works as follows. For any particular point  in the input space , we can∼1000
Example
(x1,…,x4)
ϕ(x)=PolyBasis((x1,x2),3)⌢PolyBasis((x3,x4),3)
a⌢b a b
5.2.2 (Optional) Radial basis functions
p X construct a feature  which takes any element  and returns a scalar value
that is related to how far  is from the  we started with.
Let’s start with the basic case, in which . Then we can deﬁne
This function is maximized when  and decreases exponentially as  becomes
more distant from .
The parameter  governs how quickly the feature value decays as we move away
from the center point . For large values of , the  values are nearly 0 almost
everywhere except right near ; for small values of , the features have a high value
over a larger part of the space.
Now, given a dataset  containing  points, we can make a feature transformation
 that maps points in our original space, , into points in a new space, . It is
deﬁned as follows:
So, we represent a new datapoint  in terms of how far it is from each of the
datapoints in our training set.
This idea can be generalized in several ways and is the fundamental concept
underlying kernel methods, that are not directly covered in this class but we
recommend you read about some time. This idea of describing objects in terms of
their similarity to a set of reference objects is very powerful and can be applied to
cases where  is not a simple vector space, but where the inputs are graphs or
strings or other types of objects, as long as there is a distance metric deﬁned on the
input space.
5.3 (Optional) Hand-constructing features for real
domains
In many machine-learning applications, we are given descriptions of the inputs
with many different types of attributes, including numbers, words, and discrete
features. An important factor in the success of an ML application is the way that the
features are chosen to be encoded by the human who is framing the learning
problem.
Getting a good encoding of discrete features is particularly important. You want to
create “opportunities” for the ML system to ﬁnd the underlying patterns. Although
there are machine-learning methods that have special mechanisms for handling
discrete inputs, most of the methods we consider in this class will assume the inputfp x∈X
x p
X=Rd
fp(x)=e−β∥p−x∥2.
p=x x
p
β
p βfp
p β
D n
ϕ RdRn
ϕ(x)=[fx(1)(x),fx(2)(x),…,fx(n)(x)]T.
x
X
5.3.1 Discrete features vectors  are in . So, we have to ﬁgure out some reasonable strategies for turning
discrete values into (vectors of) real numbers.
We’ll start by listing some encoding strategies, and then work through some
examples. Let’s assume we have some feature in our raw data that can take on one
of  discrete values.
Numeric: Assign each of these values a number, say . We
might want to then do some further processing, as described in Section 1.3.3.
This is a sensible strategy only when the discrete values really do signify some
sort of numeric quantity, so that these numerical values are meaningful.
Thermometer code: If your discrete values have a natural ordering, from
, but not a natural mapping into real numbers, a good strategy is to use
a vector of length  binary variables, where we convert discrete input value
 into a vector in which the ﬁrst  values are  and the rest are .
This does not necessarily imply anything about the spacing or numerical
quantities of the inputs, but does convey something about ordering.
Factored code: If your discrete values can sensibly be decomposed into two
parts (say the “maker” and “model” of a car), then it’s best to treat those as two
separate features, and choose an appropriate encoding of each one from this
list.
One-hot code: If there is no obvious numeric, ordering, or factorial structure,
then the best strategy is to use a vector of length , where we convert discrete
input value  into a vector in which all values are , except for the 
th, which is .
Binary code: It might be tempting for the computer scientists among us to use
some binary code, which would let us represent  values using a vector of
length . This is a bad idea! Decoding a binary code takes a lot of work, and
by encoding your inputs this way, you’d be forcing your system to learn the
decoding algorithm.
As an example, imagine that we want to encode blood types, that are drawn from
the set . There is no obvious linear
numeric scaling or even ordering to this set. But there is a reasonable factoring, into
two features:  and . And, in fact, we can further reasonably
factor the ﬁrst group into , . So, here are two plausible
encodings of the whole set:
Use a 6-D vector, with two components of the vector each encoding the
corresponding factor using a one-hot encoding.
Use a 3-D vector, with one dimension for each factor, encoding its presence as
 and absence as  (this is sometimes better than ). In this case, 
would be  and  would be .x Rd
k
1.0/k,2.0/k,…,1.0
1,…,k
k
0<j≤k j 1.0 0.0
k
0<j≤k 0.0 j
1.0
k
logk
{A+,A−,B+,B−,AB+,AB−,O+,O−}
{A,B,AB,O} {+,−}
{A,notA}{B,notB}
1.0 −1.0 0.0 AB+
[1.0,1.0,1.0]TO− [−1.0,−1.0,−1.0]T ❓  Study Question
How would you encode  in both of these approaches?
The problem of taking a text (such as a tweet or a product review, or even this
document!) and encoding it as an input for a machine-learning algorithm is
interesting and complicated. Much later in the class, we’ll study sequential input
models, where, rather than having to encode a text as a ﬁxed-length feature vector,
we feed it into a hypothesis word by word (or even character by character!).
There are some simple encodings that work well for basic applications. One of them
is the bag of words (bow) model, which can be used to encode documents. The idea is
to let  be the number of words in our vocabulary (either computed from the
training set or some other body of text or dictionary). We will then make a binary
vector (with values  and ) of length , where element  has value  if word 
occurs in the document, and  otherwise.
If some feature is already encoded as a numeric value (heart rate, stock price,
distance, etc.) then we should generally keep it as a numeric value. An exception
might be a situation in which we know there are natural “breakpoints” in the
semantics: for example, encoding someone’s age in the US, we might make an
explicit distinction between under and over 18 (or 21), depending on what kind of
thing we are trying to predict. It might make sense to divide into discrete bins
(possibly spacing them closer together for the very young) and to use a one-hot
encoding for some sorts of medical situations in which we don’t expect a linear (or
even monotonic) relationship between age and some physiological features.
❓  Study Question
Consider using a polynomial basis of order  as a feature transformation  on
our data. Would increasing  tend to increase or decrease structural error? What
about estimation error?A+
5.3.2 Text
d
1.0 0.0 d j 1.0 j
0.0
5.3.3 Numeric values
k ϕ
k Th is page contains all content from the legacy PDF  notes; neur al networks chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
You’ve probably been hearing a lot about “neural networks.” Now that we have
several useful machine-learning concepts (hypothesis classes, classiﬁcation,
regression, gradient descent, regularization, etc.), we are well equipped to
understand neural networks in detail.
This is, in some sense, the “third wave” of neural nets. The basic idea is founded on
the 1943 model of neurons of McCulloch and Pitts and the learning ideas of Hebb.
There was a great deal of excitement, but not a lot of practical success: there were
good training methods (e.g., perceptron) for linear functions, and interesting
examples of non-linear functions, but no good way to train non-linear functions
from data. Interest died out for a while, but was re-kindled in the 1980s when
several people came up with a way to train neural networks with “back-
propagation,” which is a particular style of implementing gradient descent, that we
will study here.
As with many good ideas in science, the basic idea for how to train non-linear
neural networks with gradient descent was independently developed by more than
one researcher.
By the mid-90s, the enthusiasm waned again, because although we could train non-
linear networks, the training tended to be slow and was plagued by a problem of
getting stuck in local optima. Support vector machines (SVMs) that use
regularization of high-dimensional hypotheses by seeking to maximize the margin,
alongside kernel methods that provide an efﬁcient and beautiful way of using
feature transformations to non-linearly transform data into a higher-dimensional
space, provided reliable learning methods with guaranteed convergence and no
local optima.
However, during the SVM enthusiasm, several groups kept working on neural
networks, and their work, in combination with an increase in available data and
computation, has made neural networks rise again. They have become much more
reliable and capable, and are now the method of choice in many applications. There
are many, many variations of neural networks, which we can’t even begin to survey.
We will study the core “feed-forward” networks with “back-propagation” training,
and then, in later chapters, address some of the major advances beyond this core.
We can view neural networks from several different perspectives:6  Neural Networks
Note
The number of neural network
variants increases daily, as may be
seen on arxiv.org.
6  Ne ural Ne tworks 6  Neural Networks  View 1: An application of stochastic gradient descent for classiﬁcation and
regression with a potentially very rich hypothesis class.
View 2: A brain-inspired network of neuron-like computing elements that learn
distributed representations.
View 3: A method for building applications that make predictions based on huge
amounts of data in very complex domains.
We will mostly take view 1, with the understanding that the techniques we develop
will enable the applications in view 3. View 2 was a major motivation for the early
development of neural networks, but the techniques we will study do not seem to
actually account for the biological learning processes in brains.
6.1 Basic element
The basic element of a neural network is a “neuron,” pictured schematically below.
We will also sometimes refer to a neuron as a “unit” or “node.”
x 1
.. .
xmf ( · ) aw 1
wm
w 0z
i n p u tp r e - a c t i v a t i o no u t p u t
a c t i v a t i o n f u n c t i o n
It is a (generally non-linear) function of an input vector  to a single output
value .
It is parameterized by a vector of weights  and an offset or
threshold .
We also specify an activation function . In general, this is chosen to be a
non-linear function, which means the neuron is non-linear. In the case that the
activation function is the identity ( ) or another linear function, then the
neuron is a linear function of ). The activation can theoretically be any function,
though we will only be able to work with it if it is differentiable.
The function represented by the neuron is expressed as:x∈Rm
a∈R
(w1,…,wm)∈Rm
w0∈R
f:R→R
f(x)=x
x
a=f(z)=f((m
∑
j=1xjwj)+w0)=f(wTx+w0).Some prominent researchers are, in
fact, working hard to ﬁnd
analogues of these methods in the
brain.
Sorry for changing our notation
here. We were using  as the
dimension of the input, but we are
trying to be consistent here with
many other accounts of neural
networks. It is impossible to be
consistent with all of them though
—there are many different ways of
telling this story.d
This should remind you of our 
and  for linear models.θ
θ0
6  Ne ural Ne tworks Before thinking about a whole network, we can consider how to train a single unit.
Given a loss function  and a dataset ,
we can do (stochastic) gradient descent, adjusting the weights  to minimize
where  is the output of our single-unit neural net for a given input.
We have already studied two special cases of the neuron: linear logistic classiﬁers
(LLCs) with NLL loss and regressors with quadratic loss! The activation function for
the LLC is  and for linear regression it is simply .
❓ Study Question
Just for a single neuron, imagine for some reason, that we decide to use
activation function  and loss function
. Derive a gradient descent update for 
and .
6.2 Networks
Now, we’ll put multiple neurons together into a network. A neural network in
general takes in an input  and generates an output . It is constructed
out of multiple neurons; the inputs of each neuron might be elements of  and/or
outputs of other neurons. The outputs of the neural network are generated by 
output units.
In this chapter, we will only consider feed-forward networks. In a feed-forward
network, you can think of the network as deﬁning a function-call graph that is
acyclic: that is, the input to a neuron can never depend on that neuron’s output.
Data ﬂows one way, from the inputs to the outputs, and the function computed by
the network is just a composition of the functions computed by the individual
neurons.
Although the graph structure of a feed-forward neural network can really be
anything (as long as it satisﬁes the feed-forward constraint), for simplicity in
software and analysis, we usually organize them into layers. A layer is a group of
neurons that are essentially “in parallel”: their inputs are the outputs of neurons in
the previous layer, and their outputs are the inputs to the neurons in the next layer.
We’ll start by describing a single layer, and then go on to the case of multiple layers.L(guess,actual) {(x(1),y(1)),…,(x(n),y(n))}
w,w0
J(w,w0)=∑
iL(NN(x(i);w,w0),y(i)),
NN
f(x)=σ(x) f(x)=x
f(z)=ez
L(guess,actual)=(guess−actual)2w
w0
x∈Rma∈Rn
x
n
6.2.1 Single layer
6  Ne ural Ne tworks A layer is a set of units that, as we have just described, are not connected to each
other. The layer is called fully connected if, as in the diagram below, all of the inputs
(i.e.,  in this case) are connected to every unit in the layer. A layer has
input  and output (also known as activation) .



.. .
x 1
x 2
.. .
xmf
f
f
.. .
fa 1
a 2
a 3
.. .
anW, W 0
Since each unit has a vector of weights and a single offset, we can think of the
weights of the whole layer as a matrix, , and the collection of all the offsets as a
vector . If we have  inputs,  units, and  outputs, then
 is an  matrix,
 is an  column vector,
, the input, is an  column vector,
, the pre-activation, is an  column vector,
, the activation, is an  column vector,
and the output vector is
The activation function  is applied element-wise to the pre-activation values .
A single neural network generally combines multiple layers, most typically by
feeding the outputs of one layer into the inputs of another layer.x1,x2,…xm
x∈Rma∈Rn
W
W0 m n n
Wm×n
W0n×1
X m×1
Z=WTX+W0 n×1
A n×1
A=f(Z)=f(WTX+W0).
f Z
6.2.2 Many layers
6  Ne ural Ne tworks We have to start by establishing some nomenclature. We will use  to name a layer,
and let  be the number of inputs to the layer and  be the number of outputs
from the layer. Then,  and  are of shape  and , respectively.
Note that the input to layer  is the output from layer , so we have ,
and as a result  is of shape , or equivalently . Let  be the
activation function of layer . Then, the pre-activation outputs are the  vector
and the activation outputs are simply the  vector
Here’s a diagram of a many-layered network, with two blocks for each layer, one
representing the linear part of the operation and one representing the non-linear
activation function. We will use this structural decomposition to organize our
algorithmic thinking and implementation.
W1
W1
0f1W2
W2
0f2 · · ·WL
WL
0fLX =A0Z1A1Z2A2AL − 1ZLAL
l a y e r 1 l a y e r 2 l a y e rL
6.3 Choices of activation function
There are many possible choices for the activation function. We will start by
thinking about whether it’s really necessary to have an  at all.
What happens if we let  be the identity? Then, in a network with  layers (we’ll
leave out  for simplicity, but keeping it wouldn’t change the form of this
argument),
So, multiplying out the weight matrices, we ﬁnd that
which is a linear function of ! Having all those layers did not change the
representational capacity of the network: the non-linearity of the activation function
is crucial.
❓ Study Question
Convince yourself that any function representable by any number of linear
layers (where  is the identity function) can be represented by a single layer.l
mlnl
WlWl
0ml×nlnl×1
l l−1 ml=nl−1
Al−1ml×1 nl−1×1fl
l nl×1
Zl=WlTAl−1+Wl
0
nl×1
Al=fl(Zl).
f
f L
W0
AL=WLTAL−1=WLTWL−1T⋯W1TX.
AL=WtotalX,
X
fIt is technically possible to have
different activation functions
within the same layer, but, again,
for convenience in speciﬁcation
and implementation, we generally
have the same activation function
within a layer.
6  Ne ural Ne tworks Now that we are convinced we need a non-linear activation, let’s examine a few
common choices. These are shown mathematically below, followed by plots of these
functions.
Step function:
Rectiﬁed linear unit (ReLU):
Sigmoid function: Also known as a logistic function. This can sometimes be
interpreted as probability, because for any value of  the output is in :
Hyperbolic tangent: Always in the range :
Softmax function: Takes a whole vector  and generates as output a vector
 with the property that , which means we can interpret it as
a probability distribution over  items:
− 2 − 1 1 2
− 0. 50. 511. 5
zs t e p (z )
− 2 − 1 1 2
− 0. 50. 511. 5
zR e L U (z )
− 4 − 2 2 4
− 1− 0. 50. 51
zσ (z )
− 4 − 2 2 4
− 1− 0. 50. 51
zt a n h (z )step(z)={0if z<0
1otherwise
ReLU(z)={ =max(0,z)0if z<0
zotherwise
z (0,1)
σ(z)=1
1+e−z
(−1,1)
tanh(z)=ez−e−z
ez+e−z
Z∈Rn
A∈(0,1)n∑n
i=1Ai=1
n
softmax(z)=⎡
⎢⎣exp(z1)/∑iexp(zi)
⋮
exp(zn)/∑iexp(zi)⎤
⎥⎦
6  Ne ural Ne tworks The original idea for neural networks involved using the step function as an
activation, but because the derivative of the step function is zero everywhere except
at the discontinuity (and there it is undeﬁned), gradient-descent methods won’t be
useful in ﬁnding a good setting of the weights, and so we won’t consider the step
function further. Step functions have been replaced, in a sense, by the sigmoid,
ReLU, and tanh activation functions.
❓ Study Question
Consider sigmoid, ReLU, and tanh activations. Which one is most like a step
function? Is there an additional parameter you could add to a sigmoid that
would make it be more like a step function?
❓ Study Question
What is the derivative of the ReLU function? Are there some values of the input
for which the derivative vanishes?
ReLUs are especially common in internal (“hidden”) layers, sigmoid activations are
common for the output for binary classiﬁcation, and softmax activations are
common for the output for multi-class classiﬁcation (see Section 4.3.3 for an
explanation).
6.4 Loss functions and activation functions
At layer  which is the output layer, we need to specify a loss function, and
possibly an activation function as well. Different loss functions make different
assumptions about the range of values they will get as input and, as we have seen,
different activation functions will produce output values in different ranges. When
you are designing a neural network, it’s important to make these things ﬁt together
well. In particular, we will think about matching loss functions with the activation
function in the last layer, . Here is a table of loss functions and activations that
make sense for them:
Loss task
squared linear regression
nll sigmoid binary classiﬁcation
nllm softmax multi-class classiﬁcation
We explored squared loss in Chapter 2 and (nll and nllm) in Chapter 4.L,
fL
fL
6  Ne ural Ne tworks 6.5 Error back-propagation
We will train neural networks using gradient descent methods. It’s possible to use
batch gradient descent, in which we sum up the gradient over all the points (as in
Section 3.2 of Chapter 3) or stochastic gradient descent (SGD), in which we take a
small step with respect to the gradient considering a single point at a time (as in
Section 3.4 of Chapter 3).
Our notation is going to get pretty hairy pretty quickly. To keep it as simple as we
can, we’ll focus on computing the contribution of one data point  to the gradient
of the loss with respect to the weights, for SGD; you can simply sum up these
gradients over all the data points if you wish to do batch descent.
So, to do SGD for a training example , we need to compute
, where  represents all weights  in all the layers
. This seems terrifying, but is actually quite easy to do using the chain
rule.
Remember that we are always computing the gradient of the loss function with
respect to the weights for a particular value of . That tells us how much we want
to change the weights, in order to reduce the loss incurred on this particular
training example.
To get some intuition for how these derivations work, we’ll ﬁrst suppose everything
in our neural network is one-dimensional. In particular, we’ll assume there are
 inputs and  outputs at every layer. So layer  looks like:
In the equation above, we’re using the lowercase letters  to
emphasize that all of these quantities are scalars just for the moment. We’ll look at
the more general matrix case below.
To use SGD, then, we want to compute  and
 for each layer  and each data point . Below we’ll write
“loss” as an abbreviation for . Then our ﬁrst quantity of interest is
. The chain rule gives us the following.
First, let’s look at the case :x(i)
(x,y)
∇WL(NN(x;W),y)W Wl,Wl
0
l=(1,…,L)
(x,y)
6.5.1 First, suppose everything is one-dimensional
ml=1 nl=1 l
al=fl(zl),zl=wlal−1+wl
0.
al,zl,wl,al−1,wl
0
∂L(NN(x;W),y)/∂wl
∂L(NN(x;W),y)/∂wl
0 l (x,y)
L(NN(x;W),y)
∂loss/∂wl
l=L
∂loss
∂wL=∂loss
∂aL⋅∂aL
∂zL⋅∂zL
∂wL
=∂loss
∂aL⋅(fL)′(zL)⋅aL−1.Remember the chain rule! If
 and , so that
, thena=f(b)b=g(c)
a=f(g(c))
da
dc=da
db⋅db
dc
=f′(b)g′(c)
=f′(g(c))g′(c)
Check your understanding: why
do we need exactly these quantities
for SGD?
6  Ne ural Ne tworks Now we can look at the case of general :
Note that every multiplication above is scalar multiplication because every term in
every product above is a scalar. And though we solved for all the other terms in the
product, we haven’t solved for  because the derivative will depend on
which loss function you choose. Once you choose a loss function though, you
should be able to compute this derivative.
❓ Study Question
Suppose you choose squared loss. What is ?
❓ Study Question
Check the derivations above yourself. You should use the chain rule and also
solve for the individual derivatives that arise in the chain rule.
❓ Study Question
Check that the ﬁnal layer ( ) case is a special case of the general layer  case
above.
❓ Study Question
Derive  for yourself, for both the ﬁnal layer ( ) and
general .
❓ Study Question
Does the  case remind you of anything from earlier in this course?
❓ Study Question
Write out the full SGD algorithm for this neural network.l
∂loss
∂wl=∂loss
∂aL⋅∂aL
∂zL⋅∂zL
∂aL−1⋅∂aL−1
∂zL−1⋯∂zl+1
∂al⋅∂al
∂zl⋅∂zl
∂wl
=∂loss
∂aL⋅(fL)′(zL)⋅wL⋅(fL−1)′(zL−1)⋯⋅wl+1⋅(fl)′(zl)⋅al−1
=∂loss
∂zl⋅al−1.
∂loss/∂aL
∂loss/∂aL
l=L l
∂L(NN(x;W),y)/∂wl
0 l=L
l
L=1
6  Ne ural Ne tworks It’s pretty typical to run the chain rule from left to right like we did above. But, for
where we’re going next, it will be useful to notice that it’s completely equivalent to
write it in the other direction. So we can rewrite our result from above as follows:
Next we’re going to do everything that we did above, but this time we’ll allow any
number of inputs  and outputs  at every layer. First, we’ll tell you the results
that correspond to our derivations above. Then we’ll talk about why they make
sense. And ﬁnally we’ll derive them carefully.
OK, let’s start with the results! Again, below we’ll be using “loss” as an
abbreviation for . Then,
where
or equivalently,
First, compare each equation to its one-dimensional counterpart, and make sure
you see the similarities. That is, compare the general weight derivatives in
Equation 6.4 to the one-dimensional case in Equation 6.1. Compare the intermediate
derivative of loss with respect to the pre-activations  in Equation 6.5 to the one-
dimensional case in Equation 6.2. And ﬁnally compare the version where we’ve
substituted in some of the derivatives in Equation 6.6 to Equation 6.3. Hopefully∂loss
∂wl=al−1⋅∂loss
∂zl(6.1)
∂loss
∂zl=∂al
∂zl⋅∂zl+1
∂al⋯∂aL−1
∂zL−1⋅∂zL
∂aL−1⋅∂aL
∂zL⋅∂loss
∂aL(6.2)
=∂al
∂zl⋅wl+1⋯∂aL−1
∂zL−1⋅wL⋅∂aL
∂zL⋅∂loss
∂aL. (6.3)
6.5.2 The general case
mlnl
L(NN(x;W),y)
∂loss
∂Wl
ml×nl=Al−1
ml×1(∂loss
∂Zl)T
1×nl        
    (6.4)
∂loss
∂Zl=∂Al
∂Zl⋅∂Zl+1
∂Al⋯⋅∂AL−1
∂ZL−1⋅∂ZL
∂AL−1⋅∂AL
∂ZL⋅∂loss
∂AL(6.5)
∂loss
∂Zl=∂Al
∂Zl⋅Wl+1⋯⋅∂AL−1
∂ZL−1⋅WL⋅∂AL
∂ZL⋅∂loss
∂AL. (6.6)
ZlEven though we have reordered
the gradients for notational
convenience, when actually
computing the product in
Equation 6.3, it is computationally
much cheaper to run the
multiplications from right-to-left
than from left-to-right. Convince
yourself of this, by reasoning
through the cost of the matrix
multiplications in each case.
There are lots of weights in a
neural network, which means we
need to compute a lot of gradients.
Luckily, as we can see, the
gradients associated with weights
in earlier layers depend on the
same terms as the gradients
associated with weights in later
layers. This means we can reuse
terms and save ourselves some
computation!
6  Ne ural Ne tworks you see how the forms are very analogous. But in the matrix case, we now have to
be careful about the matrix dimensions. We’ll check these matrix dimensions below. Let’s start by talking through each of the terms in the matrix version of these
equations. Recall that loss is a scalar, and  is a matrix of size . You can
read about the conventions in the course for derivatives starting in this chapter in
Appendix A. By these conventions (not the only possible conventions!), we have
that  will be a matrix of size  whose  entry is the scalar
. In some sense, we’re just doing a bunch of traditional scalar
derivatives, and the matrix notation lets us write them all simultaneously and
succinctly. In particular, for SGD, we need to ﬁnd the derivative of the loss with
respect to every scalar component of the weights because these are our model’s
parameters and therefore are the things we want to update in SGD.
The next quantity we see in Equation 6.4 is , which we recall has size  (or
equivalently  since it represents the outputs of the  layer). Finally, we
see . Again, loss is a scalar, and  is a  vector. So by the
conventions in Appendix A, we have that  has size . The transpose
then has size . Now you should be able to check that the dimensions all make
sense in Equation 6.4; in particular, you can check that inner dimensions agree in
the matrix multiplication and that, after the multiplication, we should be left with
something that has the dimensions on the lefthand side.
Now let’s look at Equation 6.6. We’re computing  so that we can use it in
Equation 6.4. The weights are familiar. The one part that remains is terms of the
form . Checking out Appendix A, we see that this term should be a matrix
of size  since  and  both have size . The  entry of this matrix
is . This scalar derivative is something that you can compute when you
know your activation function. If you’re not using a softmax activation function, 
typically is a function only of , which means that  should equal 0
whenever , and that .
❓ Study Question
Compute the dimensions of every term in Equation 6.5 and Equation 6.6 using
Appendix A. After you’ve done that, check that all the matrix multiplications
work; that is, check that the inner dimensions agree and that the lefthand side
and righthand side of these equations have the same dimensions.
❓ Study Question
If I use the identity activation function, what is  for any ? What is the
full matrix ?Wlml×nl
∂loss/∂Wlml×nl(i,j)
∂loss/∂Wl
i,j
Al−1ml×1
nl−1×1 l−1
∂loss/∂ZlZlnl×1
∂loss/∂Zlnl×1
1×nl
∂loss/∂Zl
∂Al/∂Zl
nl×nlAlZlnl×1 (i,j)
∂Al
j/∂Zl
i
Al
j
Zl
j∂Al
j/∂Zl
i
i≠j ∂Al
j/∂Zl
j=(fl)′(Zl
j)
∂Al
j/∂Zl
jj
∂Al/∂Zl
6  Ne ural Ne tworks You can use everything above without deriving it yourself. But if you want to ﬁnd
the gradients of loss with respect to  (which we need for SGD!), then you’ll want
to know how to actually do these derivations. So next we’ll work out the
derivations.
The key trick is to just break every equation down into its scalar meaning. For
instance, the  element of  is . If you think about it for a
moment (and it might help to go back to the one-dimensional case), the loss is a
function of the elements of , and the elements of  are a function of the .
There are  elements of , so we can use the chain rule to write
To ﬁgure this out, let’s remember that . We can write one
element of the  vector, then, as . It follows that
 will be zero except when  (check you agree!). So we can rewrite
Equation 6.7 as
Finally, then, we match entries of the matrices on both sides of the equation above
to recover Equation 6.4.
❓ Study Question
Check that Equation 6.8 and Equation 6.4 say the same thing.
❓ Study Question
Convince yourself that  by comparing the entries of the
matrices on both sides on the equality sign.
❓ Study Question
Convince yourself that Equation 6.5 is true.
❓ Study Question
Apply the same reasoning to ﬁnd the gradients of  with respect to .
6.5.3 Derivations for the general caseWl
0
(i,j) ∂loss/∂Wl∂loss/∂Wl
i,j
ZlZlWl
i,j
nlZl
∂loss
∂Wl
i,j=nl
∑
k=1∂loss
∂Zl
k∂Zl
k
∂Wl
i,j. (6.7)
Zl=(Wl)⊤Al−1+Wl
0
ZlZl
b=∑ml
a=1Wl
a,bAl−1
a+(Wl
0)b
∂Zl
k/∂Wl
i,jk=j
∂loss
∂Wl
i,j=∂loss
∂Zl
j∂Zl
j
∂Wl
i,j=∂loss
∂Zl
jAl−1
i. (6.8)
∂Zl/∂Al−1=Wl
loss Wl
0
6  Ne ural Ne tworks This general process of computing the gradients of the loss with respect to the
weights is called error back-propagation.
The idea is that we ﬁrst do a forward pass to compute all the  and  values at all the
layers, and ﬁnally the actual loss. Then, we can work backward and compute the
gradient of the loss with respect to the weights in each layer, starting at layer  and
going back to layer 1.
W1
W1
0f1W2
W2
0f2 · · ·WL
WL
0fLL o s sX =A0Z1A1Z2A2AL − 1ZLALy
∂ l o s s
∂AL∂ l o s s
∂ZL∂ l o s s
∂AL − 1∂ l o s s
∂A2∂ l o s s
∂Z2∂ l o s s
∂A1∂ l o s s
∂Z1
If we view our neural network as a sequential composition of modules (in our work
so far, it has been an alternation between a linear transformation with a weight
matrix, and a component-wise application of a non-linear activation function), then
we can deﬁne a simple API for a module that will let us compute the forward and
backward passes, as well as do the necessary weight updates for gradient descent.
Each module has to provide the following “methods.” We are already using letters
 with particular meanings, so here we will use  as the vector input to the
module and  as the vector output:
forward: 
backward: 
weight grad:  only needed for modules that have weights
In homework we will ask you to implement these modules for neural network
components, and then use them to construct a network and train it as described in
the next section.
6.6 Training
Here we go! Here’s how to do stochastic gradient descent training on a feed-
forward neural network. After this pseudo-code, we motivate the choice of
initialization in lines 2 and 3. The actual computation of the gradient values (e.g.,
) is not directly deﬁned in this code, because we want to make the
structure of the computation clear.
❓ Study Question
6.5.4 Reﬂecting on backpropagationaz
L
a,x,y,z u
v
u→v
u,v,∂L/∂v→∂L/∂u
u,∂L/∂v→∂L/∂W
W
∂loss/∂ALNotice that the backward pass does
not output , even though the
forward pass maps from  to . In
the backward pass, we are always
directly computing and ``passing
around’’ gradients of the loss.∂v/∂u
uv
6  Ne ural Ne tworks What is ?
❓ Study Question
Which terms in the code below depend on ?
procedure SGD-NEURAL-NET( )
for  to  do
end for
for  to  do
//forward pass to compute 
for  to  do
end for
for  down to  do//error back-propagation
//SGD update
end for
end for
end procedure
Initializing  is important; if you do it badly there is a good chance the neural
network training won’t work well. First, it is important to initialize the weights to
random values. We want different parts of the network to tend to “address”
different aspects of the problem; if they all start at the same weights, the symmetry
will often keep the values from moving in useful directions. Second, many of our
activation functions have (near) zero slope when the pre-activation  values have
large magnitude, so we generally want to keep the initial weights small so we will
be in a situation where the gradients are non-zero, so that gradient descent will
have some useful signal about which way to go.
One good general-purpose strategy is to choose each weight at random from a
Gaussian (normal) distribution with mean 0 and standard deviation  where
 is the number of inputs to the unit.
❓ Study Question∂Zl/∂Wl
fL
1: Dn,T,L,(m1,…,mL),(f1,…,fL),Loss
2:l←1L
3:Wl
ij∼Gaussian(0,1/ml)
4:Wl
0j∼Gaussian(0,1)
5:
6:t←1T
7:i←randomsamplefrom {1,…,n}
8:A0←x(i)AL
9: l←1L
10: Zl←WlTAl−1+Wl
0
11: Al←fl(Zl)
12:
13: loss←Loss(AL, y(i))
14: l←L 1
15:∂loss
∂Al←{∂Zl+1
∂Al⋅∂loss
∂Zl+1if l<L,
∂loss
∂AL otherwise
16:∂loss
∂Zl←∂Al
∂Zl⋅∂loss
∂Al
17:∂loss
∂Wl←Al−1(∂loss
∂Zl)⊤
18:∂loss
∂Wl
0←∂loss
∂Zl
19: Wl←Wl−η(t)∂loss
∂Wl
20: Wl
0←Wl
0−η(t)∂loss
∂Wl
0
21:
22:
23:
W
z
(1/m)
m
6  Ne ural Ne tworks If the input  to this unit is a vector of 1’s, what would the expected pre-
activation  value be with these initial weights?
We write this choice (where  means “is drawn randomly from the distribution”) as
It will often turn out (especially for fancier activations and loss functions) that
computing  is easier than computing  and  So, we may instead ask for
an implementation of a loss function to provide a backward method that computes
 directly.
6.7 Optimizing neural network parameters
Because neural networks are just parametric functions, we can optimize loss with
respect to the parameters using standard gradient-descent software, but we can take
advantage of the structure of the loss function and the hypothesis class to improve
optimization. As we have seen, the modular function-composition structure of a
neural network hypothesis makes it easy to organize the computation of the
gradient. As we have also seen earlier, the structure of the loss function as a sum
over terms, one per training data point, allows us to consider stochastic gradient
methods. In this section we’ll consider some alternative strategies for organizing
training, and also for making it easier to handle the step-size parameter.
Assume that we have an objective of the form
where  is the function computed by a neural network, and  stands for all the
weight matrices and vectors in the network.
Recall that, when we perform batch (or the vanilla) gradient descent, we use the
update rule
which is equivalent to
So, we sum up the gradient of loss at each training point, with respect to , and
then take a step in the negative direction of the gradient.x
z
∼
Wl
ij∼Gaussian(0,1
ml).
∂loss
∂ZL∂loss
∂AL∂AL
∂ZL.
∂loss/∂ZL
6.7.1 Batches
J(W)=1
nn
∑
i=1L(h(x(i);W),y(i)),
h W
Wt=Wt−1−η∇WJ(Wt−1),
Wt=Wt−1−ηn
∑
i=1∇WL(h(x(i);Wt−1),y(i)).
W
6  Ne ural Ne tworks In stochastic gradient descent, we repeatedly pick a point  at random from
the data set, and execute a weight update on that point alone:
As long as we pick points uniformly at random from the data set, and decrease  at
an appropriate rate, we are guaranteed, with high probability, to converge to at least
a local optimum.
These two methods have offsetting virtues. The batch method takes steps in the
exact gradient direction but requires a lot of computation before even a single step
can be taken, especially if the data set is large. The stochastic method begins moving
right away, and can sometimes make very good progress before looking at even a
substantial fraction of the whole data set, but if there is a lot of variability in the
data, it might require a very small  to effectively average over the individual steps
moving in “competing” directions.
An effective strategy is to “average” between batch and stochastic gradient descent
by using mini-batches. For a mini-batch of size , we select  distinct data points
uniformly at random from the data set and do the update based just on their
contributions to the gradient
Most neural network software packages are set up to do mini-batches.
❓ Study Question
For what value of  is mini-batch gradient descent equivalent to stochastic
gradient descent? To batch gradient descent?
Picking  unique data points at random from a large data-set is potentially
computationally difﬁcult. An alternative strategy, if you have an efﬁcient procedure
for randomly shufﬂing the data set (or randomly shufﬂing a list of indices into the
data set) is to operate in a loop, roughly as follows:
procedure MINI -B ATCH -SGD(NN, data, K)
while not done do
RANDOM -S HUFFLE (data)
for  to  do
BATCH -G RADIENT -U PDATE(NN, data[(i-1)K : iK])
end for
end while
end procedure(x(i),y(i))
Wt=Wt−1−η∇WL(h(x(i);Wt−1),y(i)).
η
η
K K
Wt=Wt−1−η
KK
∑
i=1∇WL(h(x(i);Wt−1),y(i)).
K
K
1:
2:n←length(data)
3:
4:
5: i←1⌈n
K⌉
6:
7:
8:
9:In line 4 of the algorithm above, 
is known as the ceiling function; it
returns the smallest integer greater
than or equal to its input. E.g.,
 and .⌈⋅⌉
⌈2.5⌉=3 ⌈3⌉=3
6  Ne ural Ne tworks Picking a value for  is difﬁcult and time-consuming. If it’s too small, then
convergence is slow and if it’s too large, then we risk divergence or slow
convergence due to oscillation. This problem is even more pronounced in stochastic
or mini-batch mode, because we know we need to decrease the step size for the
formal guarantees to hold.
It’s also true that, within a single neural network, we may well want to have
different step sizes. As our networks become deep (with increasing numbers of
layers) we can ﬁnd that magnitude of the gradient of the loss with respect the
weights in the last layer, , may be substantially different from the
gradient of the loss with respect to the weights in the ﬁrst layer . If you
look carefully at Equation 6.6, you can see that the output gradient is multiplied by
all the weight matrices of the network and is “fed back” through all the derivatives
of all the activation functions. This can lead to a problem of exploding or vanishing
gradients, in which the back-propagated gradient is much too big or small to be
used in an update rule with the same step size.
So, we can consider having an independent step-size parameter for each weight, and
updating it based on a local view of how the gradient updates have been going.
Some common strategies for this include momentum (“averaging” recent gradient
updates), Adadelta (take larger steps in parts of the space where  is nearly ﬂat),
and Adam (which combines these two previous ideas). Details of these approaches
are described in Section B.1.
6.8 Regularization
So far, we have only considered optimizing loss on the training data as our objective
for neural network training. But, as we have discussed before, there is a risk of
overﬁtting if we do this. The pragmatic fact is that, in current deep neural networks,
which tend to be very large and to be trained with a large amount of data,
overﬁtting is not a huge problem. This runs counter to our current theoretical
understanding and the study of this question is a hot area of research. Nonetheless,
there are several strategies for regularizing a neural network, and they can
sometimes be important.
One group of strategies can, interestingly, be shown to have similar effects to each
other: early stopping, weight decay, and adding noise to the training data.
Early stopping is the easiest to implement and is in fairly common use. The idea is
to train on your training set, but at every epoch (a pass through the whole training
6.7.2 Adaptive step-sizeη
∂loss/∂WL
∂loss/∂W1
J(W)
6.8.1 Methods related to ridge regressionThis section is very strongly
inﬂuenced by Sebastian Ruder’s
excellent blog posts on the topic:
{ruder.io/optimizing-gradient-
descent}
Result is due to Bishop, described
in his textbook and here.
Warning: If you use your
validation set in this way – i.e., to
6  Ne ural Ne tworks set, or possibly more frequently), evaluate the loss of the current  on a validation
set. It will generally be the case that the loss on the training set goes down fairly
consistently with each iteration, the loss on the validation set will initially decrease,
but then begin to increase again. Once you see that the validation loss is
systematically increasing, you can stop training and return the weights that had the
lowest validation error.
Another common strategy is to simply penalize the norm of all the weights, as we
did in ridge regression. This method is known as weight decay, because when we
take the gradient of the objective
we end up with an update of the form
This rule has the form of ﬁrst “decaying”  by a factor of  and then
taking a gradient step.
Finally, the same effect can be achieved by perturbing the  values of the training
data by adding a small amount of zero-mean normally distributed noise before each
gradient computation. It makes intuitive sense that it would be more difﬁcult for
the network to overﬁt to particular training data if they are changed slightly on
each training step.
Dropout is a regularization method that was designed to work with deep neural
networks. The idea behind it is, rather than perturbing the data every time we train,
we’ll perturb the network! We’ll do this by randomly, on each training step,
selecting a set of units in each layer and prohibiting them from participating. Thus,
all of the units will have to take a kind of “collective” responsibility for getting the
answer right, and will not be able to rely on any small subset of the weights to do
all the necessary computation. This tends also to make the network more robust to
data perturbations.
During the training phase, for each training example, for each unit, randomly with
probability  temporarily set . There will be no contribution to the output and
no gradient update for the associated unit.
When we are done training and want to use the network to make predictions, we
multiply all weights by  to achieve the same average activation levels.W
J(W)=n
∑
i=1L(NN(x(i)),y(i);W)+λ∥W∥2
Wt=Wt−1−η((∇WL(NN(x(i)),y(i);Wt−1))+2λWt−1)
=Wt−1(1−2λη)−η(∇WL(NN(x(i)),y(i);Wt−1)).
Wt−1 (1−2λη)
x(i)
6.8.2 Dropout
p aℓ
j=0
pset the number of epochs (or any
other hyperparameter associated
with your learning algorithm) –
then error on the validation set no
longer provides a “pure” estimate
of error on the test set (i.e.,
generalization error). This is
because information about the
validation set has “leaked” into the
design of your algorithm. See also
the discussion on Validation and
Cross-Validation in Chapter 2.
6  Ne ural Ne tworks Implementing dropout is easy! In the forward pass during training, we let
where  denotes component-wise product and  is a vector of ’s and ’s drawn
randomly with probability . The backwards pass depends on , so we do not need
to make any further changes to the algorithm.
It is common to set  to , but this is something one might experiment with to get
good results on your problem and data.
Another strategy that seems to help with regularization and robustness in training
is batch normalization.
It was originally developed to address a problem of covariate shift: that is, if you
consider the second layer of a two-layer neural network, the distribution of its input
values is changing over time as the ﬁrst layer’s weights change. Learning when the
input distribution is changing is extra difﬁcult: you have to change your weights to
improve your predictions, but also just to compensate for a change in your inputs
(imagine, for instance, that the magnitude of the inputs to your layer is increasing
over time—then your weights will have to decrease, just to keep your predictions
the same).
So, when training with mini-batches, the idea is to standardize the input values for
each mini-batch, just in the way that we did it in Section 5.3.3 of Chapter 5,
subtracting off the mean and dividing by the standard deviation of each input
dimension. This means that the scale of the inputs to each layer remains the same,
no matter how the weights in previous layers change. However, this somewhat
complicates matters, because the computation of the weight updates will need to
take into account that we are performing this transformation. In the modular view,
batch normalization can be seen as a module that is applied to , interposed after
the product with  and before input to .
Although batch-norm was originally justiﬁed based on the problem of covariate
shift, it’s not clear that that is actually why it seems to improve performance. Batch
normalization can also end up having a regularizing effect for similar reasons that
adding noise and dropout do: each mini-batch of data ends up being mildly
perturbed, which prevents the network from exploiting very particular values of
the data points. For those interested, the equations for batch normalization,
including a derivation of the forward pass and backward pass, are described in
Section B.2.aℓ=f(zℓ)∗dℓ
∗ dℓ0 1
p aℓ
p0.5
6.8.3 Batch normalization
zl
WlflFor more details see
arxiv.org/abs/1502.03167.
We follow here the suggestion from
the original paper of applying
batch normalization before the
activation function. Since then it
has been shown that, in some
cases, applying it after works a bit
better. But there aren’t any deﬁnite
ﬁndings on which works better
and when.
6  Ne ural Ne tworks 6  Ne ural Ne tworks Th is page contains all content from the legacy PDF  notes; convolutional neur al networks
chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
So far, we have studied what are called fully connected neural networks, in which all
of the units at one layer are connected to all of the units in the next layer. This is a
good arrangement when we don’t know anything about what kind of mapping
from inputs to outputs we will be asking the network to learn to approximate. But if
we do know something about our problem, it is better to build it into the structure
of our neural network. Doing so can save computation time and signiﬁcantly
diminish the amount of training data required to arrive at a solution that
generalizes robustly.
One very important application domain of neural networks, where the methods
have achieved an enormous amount of success in recent years, is signal processing.
Signals might be spatial (in two-dimensional camera images or three-dimensional
depth or CAT scans) or temporal (speech or music). If we know that we are
addressing a signal-processing problem, we can take advantage of invariant
properties of that problem. In this chapter, we will focus on two-dimensional spatial
problems (images) but use one-dimensional ones as a simple example. In a later
chapter, we will address temporal problems.
Imagine that you are given the problem of designing and training a neural network
that takes an image as input, and outputs a classiﬁcation, which is positive if the
image contains a cat and negative if it does not. An image is described as a two-
dimensional array of pixels, each of which may be represented by three integer
values, encoding intensity levels in red, green, and blue color channels.
There are two important pieces of prior structural knowledge we can bring to bear
on this problem:
Spatial locality: The set of pixels we will have to take into consideration to ﬁnd
a cat will be near one another in the image.
Translation invariance: The pattern of pixels that characterizes a cat is the
same no matter where in the image the cat occurs.
We will design neural network structures that take advantage of these properties.
7.1 Filters7  Convolutional Neural Networks
Note
A pixel is a “pictur e element.”
So , for example, we won’t have to
consider some combination of
pixels in the four  corners of the
image, in order to see if they
encode cat-ness.Cats don’t look different if they’re
on the left or the right side of the
image. 7  Convolutional Neural Networks  We begin by discussing image ﬁlters.
An image ﬁlter is a function that takes in a local spatial neighborhood of pixel
values and detects the presence of some pattern in that data.
Let’s consider a very simple case to start, in which we have a 1-dimensional binary
“image” and a ﬁlter  of size two. The ﬁlter is a vector of two numbers, which we
will move along the image, taking the dot product between the ﬁlter values and the
image values at each step, and aggregating the outputs to produce a new image.
Let  be the original image, of size ; then pixel  of the the output image is
speciﬁed by
To ensure that the output image is also of dimension , we will generally “pad” the
input image with 0 values if we need to access pixels that are beyond the bounds of
the input image. This process of applying the ﬁlter to the image to create a new
image is called “convolution.”
If you are already familiar with what a convolution is, you might notice that this
deﬁnition corresponds to what is often called a correlation and not to a convolution.
Indeed, correlation and convolution refer to different operations in signal
processing. However, in the neural networks literature, most libraries implement
the correlation (as described in this chapter) but call it convolution. The distinction
is not signiﬁcant; in principle, if convolution is required to solve the problem, the
network could learn the necessary weights. For a discussion of the difference
between convolution and correlation and the conventions used in the literature you
can read Section 9.1 in this excellent book: Deep Learning.
Here is a concrete example. Let the ﬁlter . Then given the image in
the ﬁrst line below, we can convolve it with ﬁlter  to obtain the second image.
You can think of this ﬁlter as a detector for “left edges” in the original image—to see
this, look at the places where there is a  in the output image, and see what pattern
exists at that position in the input image. Another interesting ﬁlter is
. The third image (the last line below) shows the result of
convolving the ﬁrst image with , where we see that the output pixel 
corresponds to when the center of  is aligned at input pixel .
❓  Study Question
Convince yourself that ﬁlter  can be understood as a detector for isolated
positive pixels in the binary image.F
X d i
Yi=F⋅(Xi−1,Xi).
d
F1=(−1,+1)
F1
1
F2=(−1,+1,−1)
F2 i
F2 i
F2Unfortun ately in
AI/ M L/CS/ M ath, the word
``ﬁlter’’ gets us ed in many ways: in
addition to the one we describe
here, it can describe a temporal
process (in fact, our  moving
averages are a kind of ﬁlter) and
even a somewhat esoteric algebraic
struc tur e.
And ﬁlters are also sometimes
called convolutional kernels. 0 0 1 1 1 0 1 0 0 0 I m a g e :
F 1 : - 1 + 1
0 0 1 0 0 - 1 1 - 1 0 0 A f t e r c o n v o l u t i o n ( w i t hF 1 ) :
0 - 1 0 - 1 0 - 2 1 - 1 0 0 A f t e r c o n v o l u t i o n ( w i t hF 2 ) :F 2 - 1 + 1 - 1
Two-dimensional versions of ﬁlters like these are thought to be found in the visual
cortex of all mammalian brains. Similar patterns arise from statistical analysis of
natural images. Computer vision people used to spend a lot of time hand-designing
ﬁlter banks. A ﬁlter bank is a set of sets of ﬁlters, arranged as shown in the diagram
below.
I m a g e
All of the ﬁlters in the ﬁrst group are applied to the original image; if there are 
such ﬁlters, then the result is  new images, which are called channels. Now imagine
stacking all these new images up so that we have a cube of data, indexed by the
original row and column indices of the image, as well as by the channel. The next
set of ﬁlters in the ﬁlter bank will generally be three-dimensional: each one will be
applied to a sub-range of the row and column indices of the image and to all of the
channels.
These 3D chunks of data are called tensors. The algebra of tensors is fun, and a lot
like matrix algebra, but we won’t go into it in any detail.
Here is a more complex example of two-dimensional ﬁltering. We have two 
ﬁlters in the ﬁrst layer,  and . You can think of each one as “looking” for three
pixels in a row,  vertically and  horizontally. Assuming our input image is
, then the result of ﬁltering with these two ﬁlters is an  tensor. Nowk
k
3×3
f1f2
f1 f2
n×n n×n×2Th ere are now many us eful neur al-
network software packages, suc h
as TensorFlow and PyTorch that
make operations on tensors easy. we apply a tensor ﬁlter (hard to draw!) that “looks for” a combination of two
horizontal and two vertical bars (now represented by individual pixels in the two
channels), resulting in a single ﬁnal  image.
When we have a color image as input, we treat it as having three channels, and
hence as an  tensor.
f 2
f 1t e n s o r
ﬁ l t e r
We are going to design neural networks that have this structure. Each “bank” of the
ﬁlter bank will correspond to a neural-network layer. The numbers in the individual
ﬁlters will be the “weights” (plus a single additive bias or offset value for each
ﬁlter) of the network, that we will train using gradient descent. What makes this
interesting and powerful (and somewhat confusing at ﬁrst) is that the same weights
are used many many times in the computation of each layer. This weight sharing
means that we can express a transformation on a large image with relatively few
parameters; it also means we’ll have to take care in ﬁguring out exactly how to train
it!
We will deﬁne a ﬁlter layer  formally with:
number of ﬁlters ;
size of one ﬁlter is  plus  bias value (for this one ﬁlter);
stride  is the spacing at which we apply the ﬁlter to the image; in all of our
examples so far, we have used a stride of 1, but if we were to “skip” and apply
the ﬁlter only at odd-numbered indices of the image, then it would have a
stride of two (and produce a resulting image of half the size);
input tensor size 
padding:  is how many extra pixels – typically with value 0 – we add around
the edges of the input. For an input of size , our new
effective input size with padding becomes
.n×n
n×n×3
l
ml
kl×kl×ml−11
sl
nl−1×nl−1×ml−1
pl
nl−1×nl−1×ml−1
(nl−1+2⋅pl)×(nl−1+2⋅pl)×ml−1For simplicity, we are assum ing
that all images and ﬁlters are
squa re (having the same num ber of
rows and colum ns). Th at is in no
way necessary, but is us ua lly ﬁne
and deﬁnitely simpliﬁes our
notation. This layer will produce an output tensor of size , where
. The weights are the values deﬁning the ﬁlter:
there will be  different  tensors of weight values; plus each ﬁlter
may have a bias term, which means there is one more weight value per ﬁlter. A ﬁlter
with a bias operates just like the ﬁlter examples above, except we add the bias to the
output. For instance, if we incorporated a bias term of 0.5 into the ﬁlter  above,
the output would be  instead of
.
This may seem complicated, but we get a rich class of mappings that exploit image
structure and have many fewer weights than a fully connected layer would.
❓  Study Question
How many weights are in a convolutional layer speciﬁed as above?
❓  Study Question
If we used a fully-connected layer with the same size inputs and outputs, how
many weights would it have?
7.2 Max pooling
It is typical (both in engineering and in natrure) to structure ﬁlter banks into a
pyramid, in which the image sizes get smaller in successive layers of processing. The
idea is that we ﬁnd local patterns, like bits of edges in the early layers, and then
look for patterns in those patterns, etc. This means that, effectively, we are looking
for patterns in larger pieces of the image as we apply successive ﬁlters. Having a
stride greater than one makes the images smaller, but does not necessarily
aggregate information over that spatial range.
Another common layer type, which accomplishes this aggregation, is max pooling. A
max pooling layer operates like a ﬁlter, but has no weights. You can think of it as
purely functional, like a ReLU in a fully connected network. It has a ﬁlter size, as in a
ﬁlter layer, but simply returns the maximum value in its ﬁeld.
Usually, we apply max pooling with the following traits:
, so that the resulting image is smaller than the input image; and
, so that the whole image is covered.
As a result of applying a max pooling layer, we don’t keep track of the precise
location of a pattern. This helps our ﬁlters to learn to recognize patternsnl×nl×ml
nl=⌈(nl−1+2⋅pl−(kl−1))/sl⌉
mlkl×kl×ml−1
F2
(−0.5,0.5,−0.5,0.5,−1.5,1.5,−0.5,0.5)
(−1,0,−1,0,−2,1,−1,0)
stride>1
k≥strideRecall that  is the fun ction; it
retur ns the smallest integer greater
than or equa l to its input. E.g.,
 and .⌈⋅⌉
⌈2.5⌉=3 ⌈3⌉=3
We sometimes us e the term
receptive ﬁeld or jus t ﬁeld to mean
the area of an input image that a
ﬁlter is being applied to. independent of their location.
Consider a max pooling layer where both the strides and  are set to be 2. This
would map a  image to a  image. Note that max pooling
layers do not have additional bias or offset values.
❓  Study Question
Maximilian Poole thinks it would be a good idea to add two max pooling layers
of size , one right after the other, to their network. What single layer would be
equivalent?
One potential concern about max-pooling layers is that they actually don’t
completely preserve translation invariance. If you do max-pooling with a stride
other than 1 (or just pool over the whole image size), then shifting the pattern you
are hoping to detect within the image by a small amount can change the output of
the max-pooling layer substantially, just because there are discontinuities induced
by the way the max-pooling window matches up with its input image. Here is an
interesting paper that illustrates this phenomenon clearly and suggests that one
should ﬁrst do max-pooling with a stride of 1, then do “downsampling” by
averaging over a window of outputs.
7.3 Typical architecture
Here is the form of a typical convolutional network:
At the end of each ﬁlter layer, we typically apply a ReLU activation function. There
may be multiple ﬁlter plus ReLU layers. Then we have a max pooling layer. Then
we have some more ﬁlter + ReLU layers. Then we have max pooling again. Once
the output is down to a relatively small size, there is typically a last fully-connected
layer, leading into an activation function such as softmax that produces the ﬁnal
output. The exact design of these structures is an art—there is not currently any
clear theoretical (or even systematic empirical) understanding of how these various
design choices affect overall performance of the network.
The critical point for us is that this is all just a big neural network, which takes an
input and computes an output. The mapping is a differentiable function of the
weights, which means we can adjust the weights to decrease the loss by performingk
64×64×3 32×32×3
k
Th e “depth” dimension in the
layers shown as cub oids
corresponds to the num ber of
channels in the output tensor.
(Figur e sour ce: M athworks)
Well, technically the derivative
does not exist at every point, both
becaus e of the ReLU and the max gradient descent, and we can compute the relevant gradients using back-
propagation!
7.4 Backpropagation in a simple CNN
Let’s work through a very simple example of how back-propagation can work on a
convolutional network. The architecture is shown below. Assume we have a one-
dimensional single-channel image  of size , and a single ﬁlter  of size
 (where we omit the ﬁlter bias) for the ﬁrst convolutional operation
denoted “conv” in the ﬁgure below. Then we pass the intermediate result 
through a ReLU layer to obtain the activation , and ﬁnally through a fully-
connected layer with weights , denoted “fc” below, with no additional
activation function, resulting in the output .
X =A000
p a d wit h 0 ’ s 
( t o g e t o u t p u t 
o f s a m e s h a p e )W1
Z1A1Z2=A2W2c o n v R e L U f c
For simplicity assume  is odd, let the input image , and assume we are
using squared loss. Then we can describe the forward pass as follows:
❓  Study Question
Assuming a stride of  for a ﬁlter of size , how much padding do we need to
add to the top and bottom of the image? We see one zero at the top and bottom
in the ﬁgure just above; what ﬁlter size is implicitly being shown in the ﬁgure?
(Recall the padding is for the sake of getting an output the same size as the
input.)X n×1×1 W1
k×1×1
Z1
A1
W2
A2
k X=A0
Z1
i=W1TA0
[i−⌊k/2⌋:i+⌊k/2⌋]
A1=ReLU(Z1)
A2=Z2=W2TA1
Lsquare(A2,y)=(A2−y)2
1, k
7.4.1 Weight updatepooling operations, but we ignore
that fact. How do we update the weights in ﬁlter ?
 is the  matrix such that . So, for
example, if , which corresponds to column 10 in this matrix, which
illustrates the dependence of pixel 10 of the output image on the weights, and
if , then the elements in column 10 will be .
 is the  diagonal matrix such that
, an  vector
Multiplying these components yields the desired gradient, of shape .
One last point is how to handle back-propagation through a max-pooling operation.
Let’s study this via a simple example. Imagine
where  and  are each computed by some network. Consider doing back-
propagation through the maximum. First consider the case where . Then the
error value at  is propagated back entirely to the network computing the value .
The weights in the network computing  will ultimately be adjusted, and the
network computing  will be untouched.
❓  Study Question
What is  ?W1
∂loss
∂W1=∂Z1
∂W1∂A1
∂Z1∂loss
∂A1
∂Z1/∂W1k×n ∂Z1
i/∂W1
j=Xi−⌊k/2⌋+j−1
i=10
k=5 X8,X9,X10,X11,X12
∂A1/∂Z1n×n
∂A1
i/∂Z1
i={1if Z1
i>0
0otherwise
∂loss/∂A1=(∂loss/∂A2)(∂A2/∂A1)=2(A2−y)W2n×1
k×1
7.4.2 Max pooling
y=max(a1,a2),
a1a2
a1>a2
y a1
a1
a2
∇(x,y)max(x,y) Th is page contains all content from the legacy PDF  notes; autoencoders chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
In previous chapters, we have largely focused on classiﬁcation and regression
problems, where we use supervised learning with training samples that have both
features/inputs and corresponding outputs or labels, to learn hypotheses or models
that can then be used to predict labels for new data.
In contrast to supervised learning paradigm, we can also have an unsupervised
learning setting, where we only have features but no corresponding outputs or
labels for our dataset. On natural question aries then: if there are no labels, what are
we learning?
One canonical example of unsupervised learning is clustering, which is discussed in
Section 12.3. In clustering, the goal is to develop algorithms that can reason about
“similarity” among data points’s features, and group the data points into clusters.
Autoencoders are another family of unsupervised learning algorithms, in this case
seeking to obtain insights about our data by learning compressed versions of the
original data, or, in other words, by ﬁnding a good lower-dimensional feature
representations of the same data set. Such insights might help us to discover and
characterize underlying factors of variation in data, which can aid in scientiﬁc
discovery; to compress data for efﬁcient storage or communication; or to pre-
process our data prior to supervised learning, perhaps to reduce the amount of data
that is needed to learn a good classiﬁer or regressor.
8.1 Autoencoder structure
Assume that we have input data , where . We seek to
learn an autoencoder that will output a new dataset , where
 with . We can think about  as the new representation of data point
. For example, in Figure 8.1 we show the learned representations of a dataset of
MNIST digits with . We see, after inspecting the individual data points, that
unsupervised learning has found a compressed (or latent) representation where
images of the same digit are close to each other, potentially greatly aiding
subsequent clustering or classiﬁcation tasks.8  Representation Learning (Autoencoders)
Note
D={x(1),…,x(n)}x(i)∈Rd
Dout={a(1),…,a(n)}
a(i)∈Rkk<d a(i)
x(i)
k=2 8  Representation Learning (Autoencoders)  Formally, an autoencoder consists of two functions, a vector-valued encoder
 that deterministically maps the data to the representation space 
, and a decoder  that maps the representation space back into the
original data space.
In general, the encoder and decoder functions might be any functions appropriate
to the domain. Here, we are particularly interested in neural network embodiments
of encoders and decoders. The basic architecture of one such autoencoder,
consisting of only a single layer neural network in each of the encoder and decoder,
is shown in Figure 8.2; note that bias terms  and  into the summation nodes
exist, but are omitted for clarity in the ﬁgure. In this example, the original -
dimensional input is compressed into  dimensions via the encoder
 with  and , and where the
non-linearity  is applied to each dimension of the vector. To recover (an
approximation to) the original instance, we then apply the decoder
, where  denotes a different non-linearity
(activation function). In general, both the decoder and the encoder could involve
multiple layers, as opposed to the single layer shown here. Learning seeks
parameters  and  such that the reconstructed instances,
, are close to the original input .
8.2 Autoencoder Learning
g:Rd→Rka∈Rk
h:Rk→Rd
W1
0W2
0
d
k=3
g(x;W1,W1
0)=f1(W1Tx+W1
0)W1∈Rd×kW1
0∈Rk
f1
h(a;W2,W2
0)=f2(W2Ta+W2
0)f2
W1,W1
0W2,W2
0
h(g(x(i);W1,W1
0);W2,W2
0) x(i)
Figur e 8.1: Compression of digits
dataset into two dimensions. Th e
input , an image of a
handwritten digit, is shown at the
new low-dimensional
representation .x(i)
(a1,a2)
Figur e 8.2: Autoencoder struc tur e,
showing the encoder (left half,
light green), and the decoder (right
half, light blue ), encoding inputs 
to the representation , and
decoding the representation to
produc e , the reconstruc tion. In
this speciﬁc example, the
representation ( , , ) only has
three dimensions.x
a
~x
a1a2a3 We learn the weights in an autoencoder using the same tools that we previously
used for supervised learning, namely (stochastic) gradient descent of a multi-layer
neural network to minimize a loss function. All that remains is to specify the loss
function , which tells us how to measure the discrepancy between the
reconstruction  and the original input . For
example, for continuous-valued  it might make sense to use squared loss, i.e.,
.
Learning then seeks to optimize the parameters of  and  so as to minimize the
reconstruction error, measured according to this loss function:
8.3 Evaluating an autoencoder
What makes a good learned representation in an autoencoder? Notice that, without
further constraints, it is always possible to perfectly reconstruct the input. For
example, we could let  and  and  be the identity functions. In this case, we
would not obtain any compression of the data.
To learn something useful, we must create a bottleneck by making  to be smaller
(often much smaller) than . This forces the learning algorithm to seek
transformations that describe the original data using as simple a description as
possible. Thinking back to the digits dataset, for example, an example of a
compressed representation might be the digit label (i.e., 0–9), rotation, and stroke
thickness. Of course, there is no guarantee that the learning algorithm will discover
precisely this representation. After learning, we can inspect the learned
representations, such as by artiﬁcially increasing or decreasing one of the
dimensions (e.g., ) and seeing how it affects the output , to try to better
understand what it has learned.
As with clustering, autoencoders can be a preliminary step toward building other
models, such as a regressor or classiﬁer. For example, once a good encoder has been
learned, the decoder might be replaced with another neural network that is then
trained with supervised learning (perhaps using a smaller dataset that does include
labels).
8.4 Linear encoders and decoders
We close by mentioning that even linear encoders and decoders can be very
powerful. In this case, rather than minimizing the above objective with gradient
descent, a technique called principal components analysis (PCA) can be used to obtainL(~x,x)
~x=h(g(x;W1,W1
0);W2,W2
0) x
x
LSE(~x,x)=∑d
j=1(xj−~xj)2
hg
min
W1,W1
0,W2,W2
0n
∑
i=1LSE(h(g(x(i);W1,W1
0);W2,W2
0),x(i))
k=dhg
k
d
a1 h(a)Alternatively, you could think of
this as multi-task learning, where
the goal is to predict each
dimension of . On e can mix-and-
match loss fun ctions as appropriate
for each dimension’s data type.x a closed-form solution to the optimization problem using a singular value
decomposition (SVD). Just as a multilayer neural network with nonlinear
activations for regression (learned by gradient descent) can be thought of as a
nonlinear generalization of a linear regressor (ﬁt by matrix algebraic operations),
the neural network based autoencoders discussed above (and learned with gradient
descent) can be thought of as a generalization of linear PCA (as solved with matrix
algebra by SVD).
8.5 Advanced encoders and decoders
Advanced neural networks built on encoder-decoder architectures have become
increasingly powerful. One prominent example is generative networks, designed to
create new outputs that resemble—but differ from—existing training examples. A
notable type, variational autoencoders, learns a compressed representation
capturing statistical properties (such as mean and variance) of training data. These
latent representations can then be sampled to generate novel outputs using the
decoder.
Another inﬂuential encoder-decoder architecture is the Transformer, covered in
Chapter 9. Transformers consist of multiple encoder and decoder layers combined
with self-attention mechanisms, which excel at predicting sequential data, such as
words and sentences in natural language processing (NLP).
Central to autoencoders and Transformers is the idea of learning representations.
Autoencoders compress data into efﬁcient, informative representations, while NLP
models encode language—words, phrases, sentences—into numerical forms. This
numerical encoding leads us to the concept of vector embeddings.
8.6 Embeddings
In NLP, words are represented as vectors, commonly known as word embeddings. A
key property of good embeddings is that their numerical closeness mirrors semantic
similarity. For instance, semantically related words such as “dog” and “cat” should
have vectors close together, while unrelated words like “cat” and “table” should be
farther apart.
Similarity between embeddings is frequently measured using the inner product:
The inner product indicates how aligned two vectors are: highly positive values
imply strong similarity, negative values indicate opposition, and values near zero
suggest no similarity (up to a scaling factor related to the magnitude).aTb=a⋅b A groundbreaking embedding method, word2vec (2012), signiﬁcantly advanced NLP
by producing embeddings where vector arithmetic corresponded to real-world
semantic relationships. For instance:
Such embeddings revealed meaningful semantic relationships like analogies across
diverse vocabulary (e.g., uncle – man + woman ≈ aunt).
Importantly, embeddings don’t need exact coordinates—it’s their relative
positioning within the vector space that matters. Embeddings are considered
effective if they facilitate downstream NLP tasks, such as predicting missing words,
classifying texts, or language translation.
For example, effective embeddings allow models to accurately predict a missing
word in a sentence:
After the rain, the grass was ____.
Or a model could be built that tries to correctly predict words in the middle of
sentences:
The child fell __ __ during the long car ride
This task exempliﬁes self-supervision, a training approach where models generate
labels directly from the data itself, eliminating the need for manual labeling.
Training neural networks through self-supervision involves optimizing their ability
to predict words accurately from large text corpora (e.g., Wikipedia). Through such
optimization, embeddings capture subtle semantic and syntactic nuances, greatly
enhancing NLP capabilities.
The idea of good embeddings will play a central role when we discuss attention
mechanisms in Chapter 9, where embeddings dynamically adjust based on context
(via the so-called attention mechanism), enabling a more nuanced understanding of
language.embeddingparis−embeddingfrance+embeddingitaly≈embeddingrome We are actively overhauling the Transformers chapter from the legacy PDF  notes to
enhance clarity and presentation. Please feel free to raise issue s or reque st more
explanation on speciﬁc topics.
Transformers are a very recent family of architectures that were originally
introduced in the ﬁeld of natural language processing (NLP) in 2017, as an
approach to process and understand human language. Since then, they have
revolutionized not only NLP but also other domains such as image processing and
multi-modal generative AI. Their scalability and parallelizability have made them
the backbone of large-scale foundation models, such as GPT, BERT, and Vision
Transformers (ViT), powering many state-of-the-art applications.
Like CNNs, transformers factorize signal processing into stages, each involving
independently and identically processed chunks. Transformers have many intricate
components; however, we’ll focus on their most crucial innovation: a new type of
layer called the attention layer. Attention layers enable transformers to effectively
mix information across chunks, allowing the entire transformer pipeline to model
long-range dependencies among these chunks. To help make Transformers more
digestible, in this chapter, we will ﬁrst succinctly motivate and describe them in an
overview Section 9.1. Then, we will dive into the details following the ﬂow of data –
ﬁrst describing how to represent inputs Section 9.2, and then describe the attention
mechanism Section 9.3, and ﬁnally we then assemble all these ideas together to
arrive at the full transformer architecture in Section 9.5.
9.1 Transformers Overview
Transformers are powerful neural architectures designed primarily for sequential
data, such as text. At their core, transformers are typically auto-regressive, meaning
they generate sequences by predicting each token sequentially, conditioned on
previously generated tokens. This auto-regressive property ensures that the
transformer model inherently captures temporal dependencies, making them
especially suited for language modeling tasks like text generation and completion.
Suppose our training data contains this sentence: “To date, the cleverest thinker was
Issac.” The transformer model will learn to predict the next token in the sequence,
given the previous tokens. For example, when predicting the token “cleverest,” the
model will condition its prediction on the tokens “To,” “date,” and “the.” This9  Transformers
Caution
Hum an langua ge is inherently
seque ntial in natur e (e.g.,
characters form words, words form
sentences, and sentences form
paragraphs and docum ents). Prior
to the advent of the transformers
architectur e, recur rent neur al
networks (RNNs) brieﬂy
dominated the ﬁeld for their ability
to process seque ntial information.
However, RNNs, like many other
architectur es, processed seque ntial
information in an
iterative/seque ntial fashion,
whereby each item of a seque nce
was individua lly processed one
after another. Transformers offer
many advantages over RNNs,
includ ing their ability to process all
items in a seque nce in a parallel
fashion (as do CNNs). 9  Transformers  process continues until the entire sequence is generated.
The animation above illustrates the auto-regressive nature of transformers.
Below is another example. Suppose the sentence is the 2nd law of robotics: “A robot
must obey the orders given it by human beings…” The training objective of a
transformer would be to make each token’s prediction, conditioning on previously
generated tokens, forming a step-by-step probability distribution over the
vocabulary.
The transformer architecture processes inputs by applying multiple identical
building blocks stacked in layers. Each block performs a transformation that
progressively reﬁnes the internal representation of the data.
Speciﬁcally, each block consists of two primary sub-layers: an attention layer
Section 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6.
Attention layers mix information across different positions (or "chunks") in the
sequence, allowing the model to effectively capture dependencies regardless of
distance. Meanwhile, the feed-forward network signiﬁcantly enhances the
expressiveness of these representations by applying non-linear transformations
independently to each position.
A notable strength of transformers is their capacity for parallel processing.
Transformers process entire sequences simultaneously rather than sequentially
token-by-token. This parallelization signiﬁcantly boosts computational efﬁciency
and makes it feasible to train larger and deeper models.
In this overview, we emphasize the auto-regressive nature of transformers, their
layered approach to transforming representations, the parallel processing advantage, and the critical role of the feed-forward layers in enhancing their
expressive power.
There are additional essential components and enhancements—such as causal
attention mechanisms and positional encoding—that further empower
transformers. We’ll explore these "bells and whistles" in greater depth in subsequent
discussions.
9.2 Embedding and Representations
We start by describing how language is commonly represented, then we provide a
brief explanation of why it can be useful to predict subsequent items (e.g.,
words/tokens) in a sequence.
As a reminder, two key components of any ML system are: (1) the representation of
the data; and (2) the actual modelling to perform some desired task. Computers, by
default, have no natural way of representing human language. Modern computers
are based on the Von Neumann architecture and are essentially very powerful
calculators, with no natural understanding of what any particular string of
characters means to us humans. Considering the rich complexities of language (e.g.,
humor, sarcasm, social and cultural references and implications, slang, homonyms,
etc), you can imagine the innate difﬁculties of appropriately representing
languages, along with the challenges for computers to then model and
“understand” language.
The ﬁeld of NLP aims to represent words with vectors of ﬂoating-point numbers
(aka word embeddings) such that they capture semantic meaning. More precisely,
the degree to which any two words are related in the ‘real-world’ to us humans
should be reﬂected by their corresponding vectors (in terms of their numeric
values). So, words such as ‘dog’ and ‘cat’ should be represented by vectors that are
more similar to one another than, say, ‘cat’ and ‘table’ are.
To measure how similar any two word embeddings are (in terms of their numeric
values) it is common to use some similarity as the metric, e.g. the dot-product
similarity we saw in Chapter 8.
Thus, one can imagine plotting every word embedding in -dimensional space and
observing natural clusters to form, whereby similar words (e.g., synonyms) are
located near each other. The problem of determining how to parse (aka tokenize)
individual words is known as tokenization. This is an entire topic of its own, so we
will not dive into the full details here. However, the high-level idea of tokenization
is straightforward: the individual inputs of data that are represented and processed
by a model are referred to as tokens. And, instead of processing each word as a
whole, words are typically split into smaller, meaningful pieces (akin to syllables).
For example, the word “evaluation” may be input into a model as 3 individuald How can we deﬁne an optimal
vocabulary of suc h tokens? How
many distinct tokens should we
have in our  vocabulary? How
should we handle digits or other
pun ctua tion? How does this work
for non-English langua ges, in
particular, script-based langua ges
where word boun daries are less
obvious  (e.g., Chinese or
Japanese)? All of these are open tokens (eval + ua + tion). Thus, when we refer to tokens, know that we’re referring
to these sub-word units. For any given application/model, all of the language data
must be predeﬁned by a ﬁnite vocabulary of valid tokens (typically on the order of
40,000 distinct tokens).
9.3 Query, Key, Value, and Attention Output
Attention mechanisms efﬁciently process global information by selectively focusing
on the most relevant parts of the input. Given an input sentence, each token is
processed sequentially to predict subsequent tokens. As more context (previous
tokens) accumulates, this context ideally becomes increasingly beneﬁcial—provided
the model can appropriately utilize it. Transformers employ a mechanism known as
attention, which enables models to identify and prioritize contextually relevant
tokens.
For example, consider the partial sentence: “Anqi forgot ___“. At this point, the
model has processed tokens”Anqi” and “forgot,” and aims to predict the next
token. Numerous valid completions exist, such as articles (“the,” “an”),
prepositions (“to,” “about”), or possessive pronouns (“her,” “his,” “their”). A well-
trained model should assign higher probabilities to contextually relevant tokens,
such as “her,” based on the feminine-associated name “Anqi.” Attention
mechanisms guide the model to selectively focus on these relevant contextual cues
using query, key, and value vectors.
Our goal is for each input token to learn how much attention it should give to every
other token in the sequence. To achieve this, each token is assigned a unique query
vector used to “probe” or assess other tokens—including itself—to determine
relevance.
A token’s query vector  is computed by multiplying the input token  (a -
dimensional vector) by a learnable query weight matrix  (of dimension ,
 is a hyperparameter typically chosen such that ):
Thus, for a sequence of  tokens, we generate  distinct query vectors.
To complement query vectors, we introduce key vectors, which tokens use to
“answer” queries about their relevance. Speciﬁcally, when evaluating token , its
query vector  is compared to each token’s key vector  to determine the attention
weight. Each key vector  is computed similarly using a learnable key weight
matrix :
9.3.1 Query Vectorsqi xid
Wq d×dk
dk dk<d
qi=WT
qxi
n n
9.3.2 Key Vectors
x3
q3 kj
ki
Wk
TNLP research problems receiving
increased attention lately. The attention mechanism calculates similarity using the dot product, which
efﬁciently measures vector similarity:
The vector  (softmax’d attention scores) quantiﬁes how much attention token 
should pay to each token in the sequence, normalized so that elements sum to 1.
Normalizing by  prevents large dot-product magnitudes, stabilizing training.
To incorporate meaningful contributions from attended tokens, we use value
vectors (), providing distinct representations for contribution to attention outputs.
Each token’s value vector is computed with another learnable matrix :
Finally, attention outputs are computed as weighted sums of value vectors, using
the softmax’d attention scores:
This vector  represents token ’s enriched embedding, incorporating context
from across the sequence, weighted by learned attention.
9.4 Self-attention Layer
Self-attention is an attention mechanism where the keys, values, and queries are all
generated from the same input.
At a very high level, typical transformer with self-attention layers maps
. In particular, the transformer takes in  tokens, each having feature
dimension  and through many layers of transformation (most important of which
are self-attention layers); the transformer ﬁnally outputs a sequence of  tokens,
each of which -dimensional still.
With a self-attention layer, there can be multiple attention head. We start with
understanding a single head.
A single self-attention head is largely the same as our discussion in Section 9.3. The
main additional info introduced in this part is a compact matrix form. The layerki=WT
kxi
ai=softmax([qT
ik1,qT
ik2,…,qT
ikn]
√dk)T
∈R1×n
ai qi
√dk
9.3.3 Value Vectors
vi
Wv
vi=WT
vxi
9.3.4 Attention Output
zi=n
∑
j=1aijvj∈Rdk
zi xi
Rn×d⟶ Rn×dn
d,
n
d
9.4.1 A Single Self-attention Head takes in  tokens, each having feature dimension . Thus, all tokens can be
collectively written as , where the -th row of  stores the -th token,
denoted as . For each token , self-attention computes (via learned
projection matrices, discussed in Section 9.3), a query , key , and
value , and overall, we will have  queries,  keys, and  values; all of
these vectors live in the same dimension in practice, and we often denote all three
embedding dimension via a uniﬁed .
The self-attention output is calculated as a weighted sum:
where  is the th element in .
So far, we’ve discussed self-attention focusing on a single token input-output.
Actually, we can calculate all outputs  ( ) at the same time using a
matrix form. For clearness, we ﬁrst introduce the  query matrix,
 key matrix, and  value matrix:
It should be straightforward to understand that the , ,  matrices simply stack
, , and  in a row-wise manner, respectively. Now, the the full attention matrix
 is:
which often time is shorten as:
Note that the Softmax operation is applied in a row-wise manner. The th row  of
this matrix corresponds to the softmax’d attention scores computed for query 
over all keys (i.e., ). The full output of the self-attention layer can then be written
compactly as:n d
X∈Rn×di X i
xi∈R1×dxi
qi∈Rdqki∈Rdk
vi∈Rdv n n n
dk
zi=n
∑
j=1aijvj∈Rdk
aijj ai
zii=1,2,…,n
Q∈Rn×dk
K∈Rn×dk V∈Rn×dk
Q= ∈Rn×d,K= ∈Rn×d,V= ∈Rn×dv⎡
⎢⎣q⊤
1
q⊤
2
⋮
q⊤
n⎤
⎥⎦⎡
⎢⎣k⊤
1
k⊤
2
⋮
k⊤
n⎤
⎥⎦⎡
⎢⎣v⊤
1
v⊤
2
⋮
v⊤
n⎤
⎥⎦
QKV
qikivi
A∈Rn×n
A=⎡
⎢⎣softmax([ ]/√dk)
softmax([ ]/√dk)
⋮
softmax([ ]/√dk)q⊤
1k1q⊤
1k2⋯q⊤
1kn
q⊤
2k1q⊤
2k2⋯q⊤
2kn
q⊤
nk1q⊤
nk2⋯q⊤
nkn⎤
⎥⎦(9.1)
=softmax(QK⊤
√dk) A=softmax1
√dk⎛
⎜⎝⎡
⎢⎣q⊤
1k1q⊤
1k2⋯q⊤
1kn
q⊤
2k1q⊤
2k2⋯q⊤
2kn
⋮ ⋮ ⋱ ⋮
q⊤
nk1q⊤
nk2⋯q⊤
nkn⎤
⎥⎦⎞
⎟⎠
iA
qi
αi
⊤ where  is the matrix of value vectors stacked row-wise, and  is
the output, whose th row corresponds to the attention output for the th query (i.e.,
).
You will also see this compact notation Attention in the literature, which is an
operation of three arguments , , and  (and we add an emphasis that the
softmax is performed on each row):
Human language can be very nuanced. There are many properties of language that
collectively contribute to a human’s understanding of any given sentence. For
example, words have different tenses (past, present, future, etc), genders,
abbreviations, slang references, implied words or meanings, cultural references,
situational relevance, etc. While the attention mechanism allows us to appropriately
focus on tokens in the input sentence, it’s unreasonable to expect a single set of
 matrices to fully represent – and for a model to capture – the meaning of
a sentence with all of its complexities.
To address this limitation, the idea of multi-head attention is introduced. Instead of
relying on just one attention head (i.e., a single set of  matrices), the
model uses multiple attention heads, each with its own independently learned set
of  matrices. This allows each head to attend to different parts of the
input tokens and to model different types of semantic relationships. For instance,
one head might focus on syntactic structure and another on verb tense or sentiment.
These different “perspectives” are then concatenated and projected to produce a
richer, more expressive representation of the input.
Now, we introduce the formal math notations. Let us denote the number of head as
. For the th head, the input  is linearly projected into query, key, and
value matrices using the projection matrices , , and
 (recall that usually ):
The output of the -th head is : . After
computing all  heads, we concatenate their outputs and apply a ﬁnal linearZ= =AV∈Rn×dk⎡
⎢⎣z⊤
1
z⊤
2
⋮
z⊤
n⎤
⎥⎦
V∈Rn×dk Z∈Rn×dk
i i
zi
QKV
Attention(Q,K,V)=softmaxrow(QK⊤
√dk)V
9.4.2 Multi-head Self-attention
{Q,K,V}
{Q,K,V}
{Q,K,V}
H h X∈Rn×d
Wh
q∈Rd×dqWh
k∈Rd×dk
Wh
v∈Rd×dv dq=dk=dv
Qh=XWh
q
Kh=XWh
k
Vh=XWh
v
i ZhZh=Attention(Qh,Kh,Vh)∈Rn×dk
h projection:
where the concatenation operation concatenates  horizontally, yielding a matrix
of size , and  is a ﬁnal linear projection matrix.
9.5 Transformers Architecture Details
An extremely observant reader might have been suspicious of a small but very
important detail that we have not yet discussed: the attention mechanism, as
introduced so far, does not encode the order of the input tokens. For instance, when
computing softmax’d attention scores and building token representations, the
model is fundamentally permutation-equivariant — the same set of tokens, even if
scrambled into a different order, would result in identical outputs permuted in the
same order — Formally, when we ﬁx  and switch the input  with 
, then the output  and  will be switched. However, natural language is not a bag
of words: meaning is tied closely to word order.
To address this, transformers incorporate positional embeddings — additional
information that encodes the position of each token in the sequence. These
embeddings are added to the input token embeddings before any attention layers
are applied, effectively injecting ordering information into the model.
There are two main strategies for positional embeddings: (i) learned positional
embeddings, where a trainable vector  is assigned to each position (i.e.,
token index) . These vectors are learned alongside all other model
parameters and allow the model to discover how best to encode position for a given
task, (ii) ﬁxed positional embeddings, such as sinusoidal positional embedding
proposed in the original Transformer paper:
where  is the token index, while  is the dimension index.
Namely, this sinusoidal positional embedding uses sine for the even dimension and
cosine for the odd dimension. Regardless of learnable or ﬁxed positional
embedding, it will enter the computation of attention at the input place:
  where  is the th original input token, and  is its positional
embedding. The  will now be what we really feed into the attention layer, so that
the input to the attention mechanism now carries information about both what the
token is and where it appears in the sequence.MultiHead(X)=Concat(Z1,…,ZH)(WO)T
Zh
n×HdkWO∈Rd×Hdk
9.5.1 Positional Embeddings
{Wq,Wk,Wv} xixj
zizj
pi∈Rd
i=0,1,2,...,n
p(i,2k)=sin(i
100002k/d)
p(i,2k+1)=cos(i
100002k/d)
i=1,2,..,n k=1,2,...,d
x∗
i=xi+pi ,xii pi
x∗
i This simple additive design enables attention layers to leverage both semantic
content and ordering structure when deciding where to focus. In practice, this
addition occurs at the very ﬁrst layer of the transformer stack, and all subsequent
layers operate on position-aware representations. This is a key design choice that
allows transformers to work effectively with sequences of text, audio, or even image
patches (as in Vision Transformers).
More generally, a mask may be applied to limit which tokens are used in the
attention computation. For example, one common mask limits the attention
computation to tokens that occur previously in time to the one being used for the
query. This prevents the attention mechanism from “looking ahead” in scenarios
where the transformer is being used to generate one token at a time. This causal
masking is done by introducing a mask matrix  that restricts attention to
only current and previous positions. A typical causal mask is a lower-triangular
matrix:
and we now have the masked attention matrix:
The softmax is performed to each row independently. The attention output is still
. Essentially, the lower-triangular property of  ensures that the self-
attention operation for the -th query only considers tokens . Note that we
should apply the masking before performing softmax, so that the attention matrix
can be properly normalized (i.e., each row sum to 1).
Each self-attention stage is trained to have key, value, and query embeddings that
lead it to pay speciﬁc attention to some particular feature of the input. We generally
want to pay attention to many different kinds of features in the input; for example,
in translation one feature might be be the verbs, and another might be objects or
subjects. A transformer utilizes multiple instances of self-attention, each known as
an “attention head,” to allow combinations of attention paid to many different
features.
9.5.2 Causal Self-attentionM∈Rn×n
M=⎡
⎢⎣0−∞−∞⋯ −∞
00−∞⋯ −∞
00 0 ⋯ −∞
⋮⋮ ⋮⋱ ⋮
00 0 ⋯ 0⎤
⎥⎦
A=softmax1
√dk+M⎛
⎜⎝⎡
⎢⎣q⊤
1k1q⊤
1k2⋯q⊤
1kn
q⊤
2k1q⊤
2k2⋯q⊤
2kn
⋮ ⋮ ⋱ ⋮
q⊤
nk1q⊤
nk2⋯q⊤
nkn⎤
⎥⎦⎞
⎟⎠
Y=AV M
j 0,1,...,j  This page contains all content from the legacy PDF notes; markov decision processes
chapter.
As we phase out the PDF, this page may receive updates not reﬂected in the static PDF.
Consider a robot learning to navigate through a maze, a game-playing AI
developing strategies through self-play, or a self-driving car making driving
decisions in real-time. These problems share a common challenge: the agent must
make a sequence of decisions where each choice affects future possibilities and
rewards. Unlike static prediction tasks where we learn a one-time mapping from
inputs to outputs, these problems require reasoning about the consequences of
actions over time.
This sequential and dynamical nature demands mathematical tools beyond the
more static supervised or unsupervised learning approaches. The most general
framework for such problems is reinforcement learning (RL), where an agent learns to
take actions in an unknown environment to maximize cumulative rewards over
time.
In this chapter, we’ll ﬁrst study Markov decision processes (MDPs), which provide the
mathematical foundation for understanding and solving sequential decision
making problems like RL. MDPs formalize the interaction between an agent and its
environment, capturing the key elements of states, actions, rewards, and transitions.
10.1 Deﬁnition and value functions
Formally, a Markov decision process is  where  is the state space, 
is the action space, and:
 is a transition model, where
specifying a conditional probability distribution;
 is a reward function, where  speciﬁes an immediate
reward for taking action  when in state ; and
 is a discount factor, which we’ll discuss in Section 10.1.2.
In this class, we assume the rewards are deterministic functions. Further, in this
MDP chapter, we assume the state space and action space are discrete and ﬁnite.10  Markov Decision Processes
Note
⟨S,A,T,R,γ⟩ S A
T:S×A×S→R
T(s,a,s′)=Pr(St=s′|St−1=s,At−1=a),
R:S×A→R R(s,a)
a s
γ∈[0,1]Th e notation  us es a capital
letter  to stand for a random
variable, and small letter  to stand
for a concrete value . So   here is a
random variable that can take on
elements of  as value s.St=s′
S
s
St
S10  Markov Decision Processes  The following description of a simple machine as Markov decision process provides a
concrete example of an MDP.
The machine has three possible operations (actions): wash, paint, and eject (each with
a corresponding button). Objects are put into the machine, and each time you push a
button, something is done to the object. However, it’s an old machine, so it’s not very
reliable. The machine has a camera inside that can clearly detect what is going on with the
object and will output the state of the object: dirty, clean, painted, or ejected.
For each action, this is what is done to the object:
Wash
If you perform the wash operation on any object—whether it’s dirty, clean, or
painted—it will end up clean with probability 0.9 and dirty otherwise.
Paint
If you perform the paint operation on a clean object, it will become nicely painted
with probability 0.8. With probability 0.1, the paint misses but the object stays clean,
and with probability 0.1, the machine dumps rusty dust all over the object, making it
dirty.
If you perform the paint operation on a painted object, it stays painted with
probability 1.0.
If you perform the paint operation on a dirty object, it stays dirty with
probability 1.0.
Eject
If you perform an eject operation on any object, the object comes out of the
machine and the process is terminated. The object remains ejected regardless of
any further actions.
These descriptions specify the transition model , and the transition function for each
action can be depicted as a state machine diagram. For example, here is the diagram for
wash:
ExampleT d i r t y c l e a n
p a i n t e d e j e c t e d0 . 1
0 . 90 . 9
0 . 1
0 . 1 0 . 9 1 . 0
You get reward +10 for ejecting a painted object, reward 0 for ejecting a non-painted
object, reward 0 for any action on an “ejected” object, and reward -3 otherwise. The MDP
description would be completed by also specifying a discount factor.
A policy is a function  that speciﬁes what action to take in each state. The policy is
what we will want to learn; it is akin to the strategy that a player employs to win a
given game. Below, we take just the initial steps towards this eventual goal. We
describe how to evaluate how good a policy is, ﬁrst in the ﬁnite horizon case
Section 10.1.1 when the total number of transition steps is ﬁnite. In the ﬁnite
horizon case, we typically denote the policy as , where  is a non-negative
integer denoting the number of steps remaining and  is the current state. Then
we consider the inﬁnite horizon case Section 10.1.2, when you don’t know when the
game will be over.
The goal of a policy is to maximize the expected total reward, averaged over the
stochastic transitions that the domain makes. Let’s ﬁrst consider the case where
there is a ﬁnite horizon , indicating the total number of steps of interaction that the
agent will have with the MDP.
We seek to measure the goodness of a policy. We do so by deﬁning for a given
horizon  and MDP policy , the “horizon  value” of a state, . We do this by
induction on the horizon, which is the number of steps left to go.
The base case is when there are no steps remaining, in which case, no matter what
state we’re in, the value is 0, so
Then, the value of a policy in state  at horizon  is equal to the reward it will
get in state  plus the next state’s expected horizon  value, discounted by a factor π
πh(s) h
s∈S
10.1.1 Finite-horizon value functions
h
h πh h Vπ
h(s)
Vπ
0(s)=0. (10.1)
s h+1
s h γ . So, starting with horizons 1 and 2, and then moving to the general case, we have:
The sum over  is an expectation: it considers all possible next states , and
computes an average of their -horizon values, weighted by the probability
that the transition function from state  with the action chosen by the policy 
assigns to arriving in state , and discounted by .
❓  Study Question
What is the value of
for any given state–action pair ?
❓  Study Question
Convince yourself that the deﬁnitions in Equation 10.1 and Equation 10.3 are
special cases of the more general formulation in Equation 10.4.
Then we can say that a policy  is better than policy  for horizon  if and only if
for all ,  and there exists at least one  such that
.
More typically, the actual ﬁnite horizon is not known, i.e., when you don’t know
when the game will be over! This is called the inﬁnite horizon version of the problem.
How does one evaluate the goodness of a policy in the inﬁnite horizon case?
If we tried to simply take our deﬁnitions above and use them for an inﬁnite
horizon, we could get in trouble. Imagine we get a reward of 1 at each step under
one policy and a reward of 2 at each step under a different policy. Then the reward
as the number of steps grows in each case keeps growing to become inﬁnite in the
limit of more and more steps. Even though it seems intuitive that the second policy
should be better, we can’t justify that by saying .Vπ
1(s)=R(s,π1(s))+0 (10.2)
Vπ
2(s)=R(s,π2(s))+γ∑
s′T(s,π2(s),s′)Vπ
1(s′) (10.3)
⋮
Vπ
h(s)=R(s,πh(s))+γ∑
s′T(s,πh(s),s′)Vπ
h−1(s′) (10.4)
s′s′
(h−1)
s πh(s)
s′γ
∑
s′T(s,a,s′)
(s,a)
π ¯π h
s∈SVπ
h(s)≥V¯π
h(s) s∈S
Vπ
h(s)>V¯π
h(s)
10.1.2 Inﬁnite-horizon value functions
∞<∞ One standard approach to deal with this problem is to consider the discounted
inﬁnite horizon. We will generalize from the ﬁnite-horizon case by adding a
discount factor.
In the ﬁnite-horizon case, we valued a policy based on an expected ﬁnite-horizon
value:
where  is the reward received at time .
What is ? This mathematical notation indicates an expectation, i.e., an average taken
over all the random possibilities which may occur for the argument. Here, the expectation
is taken over the conditional probability , where  is the random variable
for the reward, subject to the policy being  and the state being . Since  is a function,
this notation is shorthand for conditioning on all of the random variables implied by
policy  and the stochastic transitions of the MDP.
A very important point is that  is always deterministic (in this class) for any given
 and . Here  represents the set of all possible  at time step ; this  is a
random variable because the state we’re in at step  is itself a random variable, due to
prior stochastic state transitions up to but not including at step  and prior (deterministic)
actions dictated by policy 
Now, for the inﬁnite-horizon case, we select a discount factor , and
evaluate a policy based on its expected inﬁnite horizon value:
Note that the  indices here are not the number of steps to go, but actually the
number of steps forward from the starting state (there is no sensible notion of “steps
to go” in the inﬁnite horizon case).
Equation 10.5 and Equation 10.6 are a conceptual stepping stone. Our main objective is to
get to Equation 10.8, which can also be viewed as including  in Equation 10.4, with the
appropriate deﬁnition of the inﬁnite-horizon value.
There are two good intuitive motivations for discounting. One is related to
economic theory and the present value of money: you’d generally rather have some
money today than that same amount of money next week (because you could use it
now or invest it). The other is to think of the whole process terminating, with
probability  on each step of the interaction. (At every step, your expected
future lifetime, given that you have survived until now, is .) This value is
the expected amount of reward the agent would gain under this terminating model.E[h−1
∑
t=0γtRt∣π,s0], (10.5)
Rt t
Note
E[⋅]
Pr(Rt=r∣π,s0) Rt
π s0 π
π
R(s,a)
s a Rt R(st,a) t Rt
t
t
π.
0≤γ≤1
E[∞
∑
t=0γtRt∣π,s0]=E[R0+γR1+γ2R2+…∣π,s0]. (10.6)
t
Note
γ
1−γ
1/(1−γ) ❓  Study Question
Verify this fact: if, on every day you wake up, there is a probability of  that
today will be your last day, then your expected lifetime is  days.
Let us now evaluate a policy in terms of the expected discounted inﬁnite-horizon
value that the agent will get in the MDP if it executes that policy. We deﬁne the
inﬁnite-horizon value of a state  under policy  as
Because the expectation of a linear combination of random variables is the linear
combination of the expectations, we have
The equation deﬁned in Equation 10.8 is known as the Bellman Equation, which
breaks down the value function into the immediate reward and the (discounted)
future value function. You could write down one of these equations for each of the
 states. There are  unknowns . These are linear equations, and
standard software (e.g., using Gaussian elimination or other linear algebraic
methods) will, in most cases, enable us to ﬁnd the value of each state under this
policy.
10.2 Finding policies for MDPs
Given an MDP, our goal is typically to ﬁnd a policy that is optimal in the sense that
it gets as much total reward as possible, in expectation over the stochastic
transitions that the domain makes. We build on what we have learned about
evaluating the goodness of a policy (Section 10.1.2), and ﬁnd optimal policies for the
ﬁnite horizon case (Section 10.2.1), then the inﬁnite horizon case (Section 10.2.2).
How can we go about ﬁnding an optimal policy for an MDP? We could imagine
enumerating all possible policies and calculating their value functions as in the
previous section and picking the best one – but that’s too much work!
The ﬁrst observation to make is that, in a ﬁnite-horizon problem, the best action to
take depends on the current state, but also on the horizon: imagine that you are in a
situation where you could reach a state with reward 5 in one step or a state with
reward 100 in two steps. If you have at least two steps to go, then you’d move1−γ
1/(1−γ)
s π
Vπ
∞(s)=E[R0+γR1+γ2R2+⋯∣π,S0=s]
=E[R0+γ(R1+γ(R2+γ…)))∣π,S0=s].(10.7)
Vπ
∞(s)=E[R0∣π,S0=s]+γE[R1+γ(R2+γ…)))∣π,S0=s]
=R(s,π(s))+γ∑
s′T(s,π(s),s′)Vπ
∞(s′). (10.8)
n=|S| n Vπ(s)
10.2.1 Finding optimal ﬁnite-horizon policies toward the reward 100 state, but if you only have one step left to go, you should go
in the direction that will allow you to gain 5!For the ﬁnite-horizon case, we deﬁne  to be the expected value of
starting in state ,
executing action , and
continuing for  more steps executing an optimal policy for the
appropriate horizon on each step.
Similar to our deﬁnition of  for evaluating a policy, we deﬁne the  function
recursively according to the horizon. The only difference is that, on each step with
horizon , rather than selecting an action speciﬁed by a given policy, we select the
value of  that will maximize the expected  value of the next state.
where  denotes the next time-step state/action pair. We can solve for the
values of  with a simple recursive algorithm called ﬁnite-horizon value iteration
that just computes  starting from horizon 0 and working backward to the desired
horizon . Given , an optimal  can be found as follows:
which gives the immediate best action(s) to take when there are  steps left; then
 gives the best action(s) when there are  steps left, and so on. In the
case where there are multiple best actions, we typically can break ties randomly.
Additionally, it is worth noting that in order for such an optimal policy to be
computed, we assume that the reward function  is bounded on the set of all
possible (state, action) pairs. Furthermore, we will assume that the set of all possible
actions is ﬁnite.
❓  Study Question
The optimal value function is unique, but the optimal policy is not. Think of a
situation in which there is more than one optimal policy.Q∗
h(s,a)
s
a
h−1
V∗
h Q∗
h
h
a Q∗
h
Q∗
0(s,a)=0
Q∗
1(s,a)=R(s,a)+0
Q∗
2(s,a)=R(s,a)+γ∑
s′T(s,a,s′)max
a′Q∗
1(s′,a′)
⋮
Q∗
h(s,a)=R(s,a)+γ∑
s′T(s,a,s′)max
a′Q∗
h−1(s′,a′)
(s′,a′)
Q∗
h
Q∗
h
h Q∗
h π∗
h
π∗
h(s)=argmax
aQ∗
h(s,a).
h
π∗
h−1(s) (h−1)
R(s,a)
10.2.2 Finding optimal inﬁnite-horizon policiesWe can also deﬁne the action-value
fun ction for a ﬁxed policy ,
denoted by . Th is qua ntity
represents the expected sum  of
discoun ted rewards obtained by
taking action  in state  and
thereafter following the policy 
over the remaining horizon of
 steps.
Similar to ,  satisﬁes
the Bellman recur sion/equa tions
introduc ed earlier. In fact, for a
deterministic policy :
However, since our  primary goal in
dealing with action value s is
typically to identify an optimal
policy, we will not dwell
extensively on ( ). Instead,
we will place more emphasis on
the optimal action-value  fun ctions
.π
Qπ
h(s,a)
a s
π
h−1
Vπ
h(s)Qπ
h(s,a)
π
Qπ
h(s,π(s))=Vπ
h(s).
Qπ
h(s,a)
Q∗
h(s,a) In contrast to the ﬁnite-horizon case, the best way of behaving in an inﬁnite-horizon
discounted MDP is not time-dependent. That is, the decisions you make at time
 looking forward to inﬁnity, will be the same decisions that you make at time
 for any positive , also looking forward to inﬁnity.
An important theorem about MDPs is: in the inﬁnite-horizon case, there exists a
stationary optimal policy  (there may be more than one) such that for all 
and all other policies , we have
There are many methods for ﬁnding an optimal policy for an MDP. We have already
seen the ﬁnite-horizon value iteration case. Here we will study a very popular and
useful method for the inﬁnite-horizon case, inﬁnite-horizon value iteration. It is also
important to us, because it is the basis of many reinforcement-learning methods.
We will again assume that the reward function  is bounded on the set of all
possible (state, action) pairs and additionally that the number of actions in the
action space is ﬁnite. Deﬁne  to be the expected inﬁnite-horizon value of
being in state , executing action , and executing an optimal policy  thereafter.
Using similar reasoning to the recursive deﬁnition of  we can express this value
recursively as
This is also a set of equations, one for each  pair. This time, though, they are
not linear (due to the  operation), and so they are not easy to solve. But there is
a theorem that says they have a unique solution!
Once we know the optimal action-value function , then we can extract an
optimal policy  as
We can iteratively solve for the  values with the inﬁnite-horizon value iteration
algorithm, shown below:
Algorithm 10.1 Infinite-Horizon-Value-Iteration
Require: , , , , , 
Initialization:
for each  and  do
end for
while not converged do
for each  and  do
end for
if  then
return 
end ift=0
t=T T
π∗s∈S
π
Vπ(s)≤Vπ∗(s).
R(s,a)
Q∗
∞(s,a)
s a π∗
Vπ,
Q∗
∞(s,a)=R(s,a)+γ∑
s′T(s,a,s′)max
a′Q∗
∞(s′,a′).
(s,a)
max
Q∗
∞(s,a)
π∗
π∗(s)=argmax
aQ∗
∞(s,a)
Q∗
SATRγϵ
1:
2: s∈S a∈A
3:Qold(s,a)←0
4:
5:
6: s∈S a∈A
7: Qnew(s,a)←R(s,a)+γ∑
s′T(s,a,s′)max
a′Qold(s′,a′)
8:
9: max
s,a|Qold(s,a)−Qnew(s,a)|<ϵ
10: Qnew
11: end while
There are a lot of nice theoretical results about inﬁnite-horizon value iteration. For
some given (not necessarily optimal)  function, deﬁne .
After executing inﬁnite-horizon value iteration with convergence hyper-
parameter ,
There is a value of  such that
As the algorithm executes,  decreases monotonically on each
iteration.
The algorithm can be executed asynchronously, in parallel: as long as all 
pairs are updated inﬁnitely often in an inﬁnite run, it still converges to the
optimal value.12:Qold←Qnew
13:
Theory
Q πQ(s)=argmaxaQ(s,a)
ϵ
∥VπQnew−Vπ∗∥max<ϵ.
ϵ
∥Qold−Qnew∥max<ϵ⟹πQnew=π∗
∥VπQnew−Vπ∗∥max
(s,a) Th is page contains all content from the legacy PDF  notes; reinforcement learning chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
Reinforcement learning (RL) is a type of machine learning where an agent learns to
make decisions by interacting with an environment. Unlike other learning
paradigms, RL has several distinctive characteristics:
The agent interacts directly with an environment, receiving feedback in the
form of rewards or penalties
The agent can choose actions that inﬂuences what information it gains from the
environment
The agent updates its decision-making strategy incrementally as it gains more
experience
In a reinforcement learning problem, the interaction between the agent and
environment follows a speciﬁc pattern:
L e a r n e r
E n v i r o n m e n tr e w a r ds t a t e a c t i o n
The interaction cycle proceeds as follows:
1. Agent observes the current state 
2. Agent selects and executes an action 
3. Agent receives a reward  from the environment
4. Agent observes the new state 
5. Agent selects and executes a new action 
6. Agent receives a new reward 
7. This cycle continues…
Similar to MDP Chapter 10, in an RL problem, the agent’s goal is to learn a policy - a
mapping from states to actions - that maximizes its expected cumulative reward
over time. This policy guides the agent’s decision-making process, helping it choose
actions that lead to the most favorable outcomes.11  Reinforcement Learning
Note
s(i)
a(i)
r(i)
s(i+1)
a(i+1)
r(i+1) 11  Reinforcement Learning  11.1 Reinforcement learning algorithms overview
Approaches to reinforcement learning differ signiﬁcantly according to what kind of
hypothesis or model is being learned. Roughly speaking, RL methods can be
categorized into model-free methods and model-based methods. The main
distinction is that model-based methods explicitly learn the transition and reward
models to assist the end-goal of learning a policy; model-free methods do not. We
will start our discussion with the model-free methods, and introduce two of the
arguably most popular types of algorithms, Q-learning Section 11.1.2 and policy
gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,
we brieﬂy consider “bandit” problems Section 11.5, which differ from our MDP
learning context by having probabilistic rewards.
Model-free methods are methods that do not explicitly learn transition and reward
models. Depending on what is explicitly being learned, model-free methods are
sometimes further categorized into value-based methods (where the goal is to
learn/estimate a value function) and policy-based methods (where the goal is to
directly learn an optimal policy). It’s important to note that such categorization is
approximate and the boundaries are blurry. In fact, current RL research tends to
combine the learning of value functions, policies, and transition and reward models
all into a complex learning algorithm, in an attempt to combine the strengths of
each approach.
Q-learning is a frequently used class of RL algorithms that concentrates on learning
(estimating) the state-action value function, i.e., the  function. Speciﬁcally, recall
the MDP value-iteration update:
The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,
where we do not know the transition function  or reward function , and instead
rely on samples to perform the updates.
procedure Q-L EARNING( )
for all  do
end for
while  do
11.1.1 Model-free methods
11.1.2 Q-learningQ
Q(s,a)=R(s,a)+γ∑
s′T(s,a,s′)max
a′Q(s′,a′)
T R
1: S,A,γ,α,s0,max_iter
2:i←0
3: s∈S,a∈A
4: Qold(s,a)←0
5:
6:s←s0
7: i<max_iterTh e thing that most stud ents seem
to get confus ed about is when we
do value  iteration and when we do
Q- learning. Value  iteration
assum es you know  and  and
jus t need to compute . In Q-
learning, we don’t know or even
directly estimate  and : we
estimate  directly from
experience!T R
Q
T R
Q end while
return 
end procedure
With the pseudo‑code provided for Q‑Learning, there are a few key things to note.
First, we must determine which state to initialize the learning from. In the context of
a game, this initial state may be well deﬁned. In the context of a robot navigating an
environment, one may consider sampling the initial state at random. In any case,
the initial state is necessary to determine the trajectory the agent will experience as
it navigates the environment.
Second, different contexts will inﬂuence how we want to choose when to stop
iterating through the while loop. Again, in some games there may be a clear
terminating state based on the rules of how it is played. On the other hand, a robot
may be allowed to explore an environment ad inﬁnitum. In such a case, one may
consider either setting a ﬁxed number of transitions (as done explicitly in the
pseudo‑code) to take; or we may want to stop iterating in the example once the
values in the Q‑table are not changing, after the algorithm has been running for a
while.
Finally, a single trajectory through the environment may not be sufﬁcient to
adequately explore all state‑action pairs. In these instances, it becomes necessary to
run through a number of iterations of the Q‑Learning algorithm, potentially with
different choices of initial state .
Of course, we would then want to modify Q‑Learning such that the Q table is not
reset with each call.
Now, let’s dig into what is happening in Q‑Learning. Here,  represents the
learning rate, which needs to decay for convergence purposes, but in practice is often
set to a constant. It’s also worth mentioning that Q-learning assumes a discrete state
and action space where states and actions take on discrete values like  etc.
In contrast, a continuous state space would allow the state to take values from, say,
a continuous range of numbers; for example, the state could be any real number in
the interval . Similarly, a continuous action space would allow the action to be
drawn from, e.g., a continuous range of numbers. There are now many extensions
developed based on Q-learning that can handle continuous state and action spaces
(we’ll look at one soon), and therefore the algorithm above is also sometimes
referred to more speciﬁcally as tabular Q-learning.
In the Q-learning update rule8: a←select_action(s,Qold(s,a))
9: (r,s′)←execute(a)
10: Qnew(s,a)←(1−α)Qold(s,a)+α(r+γmaxa′Qold(s′,a′))
11: s←s′
12: i←i+1
13: Qold←Qnew
14:
15: Qnew
16:
s0
α∈(0,1]
1,2,3,…
[1,3]
Q[s,a]←(1−α)Q[s,a]+α(r+γmax
a′Q[s′,a′]) (11.1)Th is notion of run ning a num ber of
instances of Q‑ Learning is often
referred to as experiencing
multiple episodes. the term  is often referred to as the one-step look-ahead target.
The update can be viewed as a combination of two different iterative processes that
we have already seen: the combination of an old estimate with the target using a
running average with a learning rate 
Equation 11.1 can also be equivalently rewritten as
which allows us to interpret Q-learning in yet another way: we make an update (or
correction) based on the temporal difference between the target and the current
estimated value 
The Q-learning algorithm above includes a procedure called select_action, that,
given the current state  and current  function, has to decide which action to take.
If the  value is estimated very accurately and the agent is deployed to “behave” in
the world (as opposed to “learn” in the world), then generally we would want to
choose the apparently optimal action .
But, during learning, the  value estimates won’t be very good and exploration is
important. However, exploring completely at random is also usually not the best
strategy while learning, because it is good to focus your attention on the parts of the
state space that are likely to be visited when executing a good policy (not a bad or
random one).
A typical action-selection strategy that attempts to address this exploration versus
exploitation dilemma is the so-called -greedy strategy:
with probability , choose ;
with probability , choose the action  uniformly at random.
where the  probability of choosing a random action helps the agent to explore and
try out actions that might not seem so desirable at the moment.
Q-learning has the surprising property that it is guaranteed to converge to the actual
optimal  function! The conditions speciﬁed in the theorem are: visit every state-
action pair inﬁnitely often, and the learning rate  satisﬁes a scheduling condition.
This implies that for exploration strategy speciﬁcally, any strategy is okay as long as
it tries every state-action inﬁnitely often on an inﬁnite run (so that it doesn’t
converge prematurely to a bad action choice).
Q-learning can be very inefﬁcient. Imagine a robot that has a choice between
moving to the left and getting a reward of 1, then returning to its initial state, or
moving to the right and walking down a 10-step hallway in order to get a reward of
1000, then returning to its initial state.(r+γmaxa′Q[s′,a′])
α.
Q[s,a]←Q[s,a]+α((r+γmax
a′Q[s′,a′])−Q[s,a]), (11.2)
Q[s,a].
s Q
Q
argmaxa∈AQ(s,a)
Q
ϵ
1−ϵ argmaxa∈AQ(s,a)
ϵ a∈A
ϵ
Q
α ro b o t 1 2 3 4 5 6 7 8 9 1 0+ 1 0 0 0 + 1
- 1
The ﬁrst time the robot moves to the right and goes down the hallway, it will
update the  value just for state 9 on the hallway and action ``right’’ to have a high
value, but it won’t yet understand that moving to the right in the earlier steps was a
good choice. The next time it moves down the hallway it updates the value of the
state before the last one, and so on. After 10 trips down the hallway, it now can see
that it is better to move to the right than to the left.
More concretely, consider the vector of Q values , representing
the Q values for moving right at each of the positions . Position index 0
is the starting position of the robot as pictured above.
Then, for  and , Equation 11.2 becomes
Starting with Q values of 0,
Since the only nonzero reward from moving right is , after our
robot makes it down the hallway once, our new Q vector is
After making its way down the hallway again,
updates:
Similarly,Q
Q(i=0,…,9;right)
i=0,…,9
α=1 γ=0.9
Q(i,right)=R(i,right)+0.9max
aQ(i+1,a).
Q(0)(i=0,…,9;right)=[ ]. 0000000000
R(9,right)=1000
Q(1)(i=0,…,9;right)=[ ]. 0000000001000
Q(8,right)=0+0.9Q(9,right)=900
Q(2)(i=0,…,9;right)=[ ]. 000000009001000
Q(3)(i=0,…,9;right)=[ ], 00000008109001000
Q(4)(i=0,…,9;right)=[ ], 0000007298109001000We are violating our  us ua l
notational conventions here, and
writing  to mean the Q value
fun ction that results after the robot
run s all the way to the end of the
hallway, when executing the policy
that always moves to the right.Qi and the robot ﬁnally sees the value of moving right from position 0.
❓  Study Question
Determine the Q value functions that result from always executing the “move
left” policy.
11.2 Function approximation: Deep Q learning
In our Q-learning algorithm above, we essentially keep track of each  value in a
table, indexed by  and . What do we do if  and/or  are large (or continuous)?
We can use a function approximator like a neural network to store Q values. For
example, we could design a neural network that takes inputs  and , and outputs
. We can treat this as a regression problem, optimizing this loss:
where  is now the output of the neural network.
There are several different architectural choices for using a neural network to
approximate  values:
One network for each action , that takes  as input and produces  as
output;
One single network that takes  as input and produces a vector ,
consisting of the  values for each action; or
One single network that takes  concatenated into a vector (if  is discrete,
we would probably use a one-hot encoding, unless it had some useful internal
structure) and produces  as output.
The ﬁrst two choices are only suitable for discrete (and not too big) action sets. The
last choice can be applied for continuous actions, but then it is difﬁcult to ﬁnd
.
There are not many theoretical guarantees about Q-learning with function
approximation and, indeed, it can sometimes be fairly unstable (learning to perform
well for a while, and then suddenly getting worse, for example). But neural
network Q-learning has also had some signiﬁcant successes.…
Q(10)(i=0,…,9;right)=[ ], 387.4420.5478.3531.4590.5656.17298109001000
Q
s a S A
s a
Q(s,a)
(Q(s,a)−(r+γmax
a′Q(s′,a′)))2
Q(s,a)
Q
a s Q(s,a)
s Q(s,⋅)
Q
s,a a
Q(s,a)
argmaxa∈AQ(s,a)Here, we can see the
exploration/exploitation dilemma
in action: from the perspective of
, it will seem that getting the
immediate reward of  is a better
strategy without exploring the long
hallway.s0=0
1
Th is is the so-called squa red
Bellman error; as the name
sug gests, it’s closely related to the
Bellman equa tion we saw in MDPs
in Chapter Chapter 10. Roug hly
speaking, this error measur es how
muc h the Bellman equa lity is
violated.
For continuo us  action spaces, it is
popular to us e a class of methods
called actor-critic methods, which
combine policy and value -fun ction
learning. We won’t get into them in
detail here, thoug h. One form of instability that we do know how to guard against is catastrophic
forgetting. In standard supervised learning, we expect that the training  values
were drawn independently from some distribution.
But when a learning agent, such as a robot, is moving through an environment, the
sequence of states it encounters will be temporally correlated. For example, the
robot might spend 12 hours in a dark environment and then 12 in a light one. This
can mean that while it is in the dark, the neural-network weight-updates will make
the  function "forget" the value function for when it’s light.
One way to handle this is to use experience replay, where we save our 
experiences in a replay buffer. Whenever we take a step in the world, we add the
 to the replay buffer and use it to do a Q-learning update. Then we also
randomly select some number of tuples from the replay buffer, and do Q-learning
updates based on them as well. In general, it may help to keep a sliding window of
just the 1000 most recent experiences in the replay buffer. (A larger buffer will be
necessary for situations when the optimal policy might visit a large part of the state
space, but we like to keep the buffer size small for memory reasons and also so that
we don’t focus on parts of the state space that are irrelevant for the optimal policy.)
The idea is that it will help us propagate reward values through our state space
more efﬁciently if we do these updates. We can see it as doing something like value
iteration, but using samples of experience rather than a known model.
An alternative strategy for learning the  function that is somewhat more robust
than the standard -learning algorithm is a method called ﬁtted Q.
procedure FITTED -Q-L EARNING( )
//e.g.,  can be drawn randomly from 
initialize neural-network representation of 
while True do
 experience from executing -greedy policy based on  for  steps
 represented as tuples 
for each tuple  do
end for
re-initialize neural-network representation of 
end while
end procedure
Here, we alternate between using the policy induced by the current  function to
gather a batch of data , adding it to our overall data set , and then using
supervised neural-network training to learn a representation of the  value
function on the whole data set. This method does not mix the dynamic-x
Q
(s,a,s′,r)
(s,a,s′,r)
11.2.1 Fitted Q-learning
Q
Q
1: A,s0,γ,α,ϵ,m
2:s←s0 s0 S
3:D←∅
4: Q
5:
6: Dnew← ϵ Q m
7: D←D∪Dnew (s,a,s′,r)
8: Dsupervised←∅
9: (s,a,s′,r)∈D
10: x←(s,a)
11: y←r+γmaxa′∈AQ(s′,a′)
12: Dsupervised←Dsupervised∪{(x,y)}
13:
14: Q
15: Q←supervised-NN-regression(Dsupervised)
16:
17:
Q
Dnew D
QAnd, in fact, we routinely shufﬂe
their order in the data ﬁle, anyway. programming phase (computing new  values based on old ones) with the function
approximation phase (supervised training of the neural network) and avoids
catastrophic forgetting. The regression training in line 10 typically uses squared
error as a loss function and would be trained until the ﬁt is good (possibly
measured on held-out data).
11.3 Policy gradient
A different model-free strategy is to search directly for a good policy. The strategy
here is to deﬁne a functional form  for the policy, where  represents the
parameters we learn from experience. We choose  to be differentiable, and often
deﬁne
, a conditional probability distribution over our possible actions.
Now, we can train the policy parameters using gradient descent:
When  has relatively low dimension, we can compute a numeric estimate of
the gradient by running the policy multiple times for different values of , and
computing the resulting rewards.
When  has higher dimensions (e.g., it represents the set of parameters in a
complicated neural network), there are more clever algorithms, e.g., one called
REINFORCE, but they can often be difﬁcult to get to work reliably.
Policy search is a good choice when the policy has a simple known form, but the
MDP would be much more complicated to estimate.
11.4 Model-based RL
The conceptually simplest approach to RL is to model  and  from the data we
have gotten so far, and then use those models, together with an algorithm for
solving MDPs (such as value iteration) to ﬁnd a policy that is near-optimal given
the current models.
Assume that we have had some set of interactions with the environment, which can
be characterized as a set of tuples of the form .
Because the transition function  speciﬁes probabilities, multiple
observations of  may be needed to model the transition function. One
approach to building a model  for the true  is to estimate it using
a simple counting strategy:Q
f(s;θ)=a θ
f
f(s,a;θ)=Pr(a|s)
θ
θ
θ
R T
(s(t),a(t),s(t+1),r(t))
T(s,a,s′)
(s,a,s′)
^T(s,a,s′) T(s,a,s′)
^T(s,a,s′)=#(s,a,s′)+1
#(s,a)+|S|.Th is means the chance of choosing
an action depends on which state
the agent is in. Sup pose, e.g., a
robot is trying to get to a goal and
can go left or right. An
un conditional policy can say: I go
left 99% of the time; a conditional
policy can consider the robot’s
state, and say: if I’m to the right of
the goal, I go left 99% of the time. Here,  represents the number of times in our data set we have the
situation where , , , and  represents the number of
times in our data set we have the situation where , .
Adding 1 and  to the numerator and denominator, respectively, is a form of
smoothing called the Laplace correction. It ensures that we never estimate that a
probability is 0, and keeps us from dividing by 0. As the amount of data we gather
increases, the inﬂuence of this correction fades away.
In contrast, the reward function  is a deterministic function, such that
knowing the reward  for a given  is sufﬁcient to fully determine the function
at that point. Our model  can simply be a record of observed rewards, such that
.
Given empirical models  and  for the transition and reward functions, we can
now solve the MDP  to ﬁnd an optimal policy using value iteration, or
use a search algorithm to ﬁnd an action to take for a particular state.
This approach is effective for problems with small state and action spaces, where it
is not too hard to get enough experience to model  and  well; but it is difﬁcult to
generalize this method to handle continuous (or very large discrete) state spaces,
and is a topic of current research.
11.5 Bandit problems
Bandit problems are a subset of reinforcement learning problems. A basic bandit
problem is given by:
A set of actions ;
A set of reward values ; and
A probabilistic reward function , i.e.,  is a function that
takes an action and a reward and returns the probability of getting that reward
conditioned on that action being taken,
. Each time the agent takes an action, a
new value is drawn from this distribution.
The most typical bandit problem has  and . This is called a -
armed bandit problem, where the decision is which “arm” (action ) to select, and the
reward is either getting a payoff () or not ().
The important question is usually one of exploration versus exploitation. Imagine you
have tried each action 10 times, and now you have estimates  for the
probabilities . Which arm should you pick next? You could:
exploit your knowledge, choosing the arm with the highest value of expected
reward; or#(s,a,s′)
s(t)=sa(t)=as(t+1)=s′#(s,a)
s(t)=sa(t)=a
|S|
R(s,a)
r (s,a)
^R
^R(s,a)=r=R(s,a)
^T ^R
(S,A,^T,^R)
T R
A
R
Rp:A×R→R Rp
Rp(a,r)=Pr(reward=r∣action=a)
R={0,1} |A|=k k
a
1 0
^Rp(a,r)
Rp(a,r)Conceptua lly, this is similar to
having “initialized” our  estimate
for the transition fun ction with
un iform random probabilities
before making any observations.
Notice that this probablistic
rewards set up  in bandits differs
from the “rewards are
deterministic” assum ptions we
made so far.
W hy “bandit”? In English slang,
“one-armed bandit” refers to a slot
machine becaus e it has one arm
and takes your  money! Here, we
have a similar machine but with 
arms.k explore further, trying some or all actions more times to get better estimates of
the  values.
The theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1
our discount factor), the more time we should spend exploring, so that we don’t
converge prematurely on a bad choice of action.
Bandit problems are reinforcement learning problems (and very different from
batch supervised learning) in that:
The agent gets to inﬂuence what data it obtains (selecting  gives it another
sample from ), and
The agent is penalized for mistakes it makes while it is learning (trying to
maximize the expected reward it gets while behaving).
In a contextual bandit problem, you have multiple possible states from some set ,
and a separate bandit problem associated with each one.
Bandit problems are an essential subset of reinforcement learning. It’s important to
be aware of the issues, but we will not study solutions to them in this class.Rp(a,r)
h
a
R(a,r)
S Th is page contains all content from the legacy PDF  notes; non-parametric models chapter.
As we phase out the PDF , this page may receive up dates not reﬂe cted in the static PDF .
Neural networks have adaptable complexity, in the sense that we can try different
structural models and use cross validation to ﬁnd one that works well on our data.
Beyond neural networks, we may further broaden the class of models that we can
ﬁt to our data, for example as illustrated by the techniques introduced in this
chapter.
Here, we turn to models that automatically adapt their complexity to the training
data. The name non-parametric methods is misleading: it is really a class of methods
that does not have a ﬁxed parameterization in advance. Rather, the complexity of
the parameterization can grow as we acquire more data.
Some non-parametric models, such as nearest-neighbor, rely directly on the data to
make predictions and do not compute a model that summarizes the data. Other
non-parametric methods, such as decision trees, can be seen as dynamically
constructing something that ends up looking like a more traditional parametric
model, but where the actual training data affects exactly what the form of the model
will be.
The non-parametric methods we consider here tend to have the form of a
composition of simple models:
Nearest neighbor models: Section 12.1 where we don’t process data at training
time, but do all the work when making predictions, by looking for the closest
training example(s) to a given new data point.
Tree models: Section 12.2 where we partition the input space and use different
simple predictions on different regions of the space; the hypothesis space can
become arbitrarily large allowing ﬁner and ﬁner partitions of the input space.
Ensemble models: Section 12.2.3 in which we train several different classiﬁers on
the whole space and average the answers; this decreases the estimation error. In
particular, we will look at bootstrap aggregation, or bagging of trees.
Boosting is a way to construct a model composed of a sequence of component
models (e.g., a model consisting of a sequence of trees, each subsequent tree
seeking to correct errors in the previous trees) that decreases both estimation
and structural error. We won’t consider this in detail in this class.12  Non-parametric methods
Note12  Non-parametric methods  *-means clustering methods, Section 12.3 where we partition data into groups
based on similarity without predeﬁned labels, adapting complexity by
adjusting the number of clusters.
Why are we studying these methods, in the heyday of complicated models such as
neural networks ?
They are fast to implement and have few or no hyperparameters to tune.
They often work as well as or better than more complicated methods.
Predictions from some of these models can be easier to explain to a human
user: decision trees are fairly directly human-interpretable, and nearest
neighbor methods can justify their decisions to some extent by showing a few
training examples that the predictions were based on.
12.1 Nearest Neighbor
In nearest-neighbor models, we don’t do any processing of the data at training time
– we just remember it! All the work is done at prediction time.
Input values  can be from any domain  (, documents, tree-structured objects,
etc.). We just need a distance metric, , which satisﬁes the following,
for all :
Given a data-set , our predictor for a new  is
that is, the predicted output associated with the training point that is closest to the
query point . Tie breaking is typically done at random.
This same algorithm works for regression and classiﬁcation!
The nearest neighbor prediction function can be described by dividing the space up
into regions whose closest point is each individual training point as shown below :k
x XRd
d:X×X→R+
x,x′,x′′∈X
d(x,x)=0
d(x,x′)=d(x′,x)
d(x,x′′)≤d(x,x′)+d(x′,x′′)
D={(x(i),y(i))}n
i=1x∈X
h(x)=y(i)wherei=argmin
id(x,x(i)),
x In each region, we predict the associated  value.
There are several useful variations on this method. In -nearest-neighbors, we ﬁnd
the  training points nearest to the query point  and output the majority  value
for classiﬁcation or the average for regression. We can also do locally weighted
regression in which we ﬁt locally linear regression models to the  nearest points,
possibly giving less weight to those that are farther away. In large data-sets, it is
important to use good data structures (e.g., ball trees) to perform the nearest-
neighbor look-ups efﬁciently (without looking at all the data points each time).
12.2 Tree Models
The idea here is that we would like to ﬁnd a partition of the input space and then ﬁt
very simple models to predict the output in each piece. The partition is described
using a (typically binary) “tree” that recursively splits the space.
Tree methods differ by:
The class of possible ways to split the space at each node; these are typically
linear splits, either aligned with the axes of the space, or sometimes using more
general classiﬁers.
The class of predictors within the partitions; these are often simply constants,
but may be more general classiﬁcation or regression models.
The way in which we control the complexity of the hypothesis: it would be
within the capacity of these methods to have a separate partition element for
each individual training example.y
k
k x y
k The algorithm for making the partitions and ﬁtting the models.
One advantage of tree models is that they are easily interpretable by humans. This
is important in application domains, such as medicine, where there are human
experts who often ultimately make critical decisions and who need to feel conﬁdent
in their understanding of recommendations made by an algorithm. Below is an
example decision tree, illustrating how one might be able to understand the
decisions made by the tree.
#Example Here is a sample tree (reproduc ed from Breiman, Friedman, Ol shen, Stone
(1984)):
These methods are most appropriate for domains where the input space is not very
high-dimensional and where the individual input features have some substantially
useful information individually or in small groups. Trees would not be good for
image input, but might be good in cases with, for example, a set of meaningful
measurements of the condition of a patient in the hospital, as in the example above.
We’ll concentrate on the CART/ID3 (“classiﬁcation and regression trees” and
“iterative dichotomizer 3”, respectively) family of algorithms, which were invented
independently in the statistics and the artiﬁcial intelligence communities. They
work by greedily constructing a partition, where the splits are axis aligned and by
ﬁtting a constant model in the leaves. The interesting questions are how to select the
splits and how to control complexity. The regression and classiﬁcation versions are
very similar.
As a concrete example, consider the following images:
Note  
The left image depicts a set of labeled data points in a two-dimensional feature
space. The right shows a partition into regions by a decision tree, in this case having
no classiﬁcation errors in the ﬁnal partitions.
The predictor is made up of
a partition function, , mapping elements of the input space into exactly one of
 regions, , and
a collection of  output values, , one for each region.
If we already knew a division of the space into regions, we would set , the
constant output for region , to be the average of the training output values in
that region. For a training data set , we let  be an
indicator set of all of the elements within , so that  for our whole
data set. We can deﬁne  as the subset of data set samples that are in region , so
that . Then
We can deﬁne the error in a region as . For example,  as the sum of squared
error would be expressed as
Ideally, we should select the partition to minimize
for some regularization constant . It is enough to search over all partitions of the
training data (not all partitions of the input space!) to optimize this, but the problem
is NP-complete.
12.2.1 Regressionπ
M R1,…,RM
M Om
Om
Rm
D={(x(i),y(i))},i=1,…nI
D I={1,…,n}
Im Rm
Im={i∣x(i)∈Rm}
Om=averagei∈Im y(i).
Em Em
Em=∑
i∈Im(y(i)−Om)2.
λM+M
∑
m=1Em,
λ
12.2.1.1 Building a tree So, we’ll be greedy. We establish a criterion, given a set of data, for ﬁnding the best
single split of that data, and then apply it recursively to partition the space. For the
discussion below, we will select the partition of the data that minimizes the sum of the
sum of squared errors of each partition element. Then later, we will consider other
splitting criteria.
Given a data set , we now consider  to be an
indicator of the subset of elements within  that we wish to build a tree (or subtree)
for. That is,  may already indicate a subset of data set , based on prior splits in
constructing our overall tree. We deﬁne terms as follows:
 indicates the set of examples (subset of ) whose feature value in dimension
 is greater than or equal to split point ;
 indicates the set of examples (subset of ) whose feature value in dimension
 is less than ;
 is the average  value of the data points indicated by set ; and
 is the average  value of the data points indicated by set .
Here is the pseudocode. In what follows,  is the largest leaf size that we will allow
in the tree, and is a hyperparameter of the algorithm.
procedure BUILD T REE()
if  then
return 
else
for all split dimension , split value  do
end for
return 
end if
end procedure
In practice, we typically start by calling BuildTree with the ﬁrst input equal to our
whole data set (that is, with ). But then that call of BuildTree can
recursively lead to many other calls of BuildTree.
Let’s think about how long each call of BuildTree takes to run. We have to
consider all possible splits. So we consider a split in each of the  dimensions. In
each dimension, we only need to consider splits between two data points (any otherD={(x(i),y(i))},i=1,…n I
D
I D
I+
j,s I
j s
I−
j,sI
j s
^y+
j,s y I+
j,s
^y−
j,s y I−
j,s
k
1: I,k
2: |I|≤k
3: ^y←1
|I|∑i∈Iy(i)
4: Leaf(value=^y)
5:
6: j s
7: I+
j,s←{i∈I∣x(i)
j≥s}
8: I−
j,s←{i∈I∣x(i)
j<s}
9: ^y+
j,s←1
|I+
j,s|∑i∈I+
j,sy(i)
10: ^y−
j,s←1
|I−
j,s|∑i∈I−
j,sy(i)
11: Ej,s←∑i∈I+
j,s(y(i)−^y+
j,s)2+∑i∈I−
j,s(y(i)−^y−
j,s)2
12:
13: (j∗,s∗)←argminj,sEj,s
14:
15: Node(j∗,s∗,BuildTree(I−
j∗,s∗,k),BuildTree(I+
j∗,s∗,k))
16:
17:
I={1,…,n}
d split will give the same error on the training data). So, in total, we consider 
splits in each call to BuildTree.
It might be tempting to regularize by using a somewhat large value of , or by
stopping when splitting a node does not signiﬁcantly decrease the error. One
problem with short-sighted stopping criteria is that they might not see the value of
a split that will require one more split before it seems useful. So, we will tend to
build a tree that is too large, and then prune it back.
We deﬁne cost complexity of a tree , where  ranges over its leaves, as
and  is the number of leaves. For a ﬁxed , we can ﬁnd a  that (approximately)
minimizes  by “weakest-link” pruning:
Create a sequence of trees by successively removing the bottom-level split that
minimizes the increase in overall error, until the root is reached.
Return the  in the sequence that minimizes the cost complexity.
We can choose an appropriate  using cross validation.
The strategy for building and pruning classiﬁcation trees is very similar to the
strategy for regression trees.
Given a region  corresponding to a leaf of the tree, we would pick the output
class  to be the value that exists most frequently (the majority value) in the data
points whose  values are in that region, i.e., data points indicated by :
Let’s now deﬁne the error in a region as the number of data points that do not have
the value :
We deﬁne the empirical probability of an item from class  occurring in region  as:
where  is the number of training points in region ; that is,  For later
use, we’ll also deﬁne the empirical probabilities of split values, , as the
fraction of points with dimension  in split  occurring in region  (one branch ofO(dn)
12.2.1.2 Pruning
k
T m
Cα(T)=|T|
∑
m=1Em(T)+α|T|,
|T| α T
Cα(T)
T
α
12.2.2 Classiﬁcation
Rm
y
x Im
Om=majorityi∈Im y(i).
Om
Em={i∣i∈Imandy(i)≠Om}.
∣∣k m
^Pm,k=^P(Im,k)={i∣i∈Imandy(i)=k}
Nm,
∣∣Nm m Nm=|Im|.
^Pm,j,s
js m the tree), and  as the complement (the fraction of points in the other
branch).
In our greedy algorithm, we need a way to decide which split to make next. There
are many criteria that express some measure of the “impurity” in child nodes. Some
measures include:
Misclassiﬁcation error:
Gini index:
Entropy:
So that the entropy  is well-deﬁned when , we will stipulate that
.
These splitting criteria are very similar, and it’s not entirely obvious which one is
better. We will focus on entropy, just to be concrete.
Analogous to how for regression we choose the dimension  and split  that
minimizes the sum of squared error , for classiﬁcation, we choose the dimension
 and split  that minimizes the weighted average entropy over the “child” data
points in each of the two corresponding splits,  and . We calculate the entropy
in each split based on the empirical probabilities of class memberships in the split,
and then calculate the weighted average entropy  as
Choosing the split that minimizes the entropy of the children is equivalent to
maximizing the information gain of the test , deﬁned by
In the two-class case (with labels 0 and 1), all of the splitting criteria mentioned
above have the values1−^Pm,j,s
Sp litting criteria
Qm(T)=Em
Nm=1−^Pm,Om
Qm(T)=∑
k^Pm,k(1−^Pm,k)
Qm(T)=H(Im)=−∑
k^Pm,klog2^Pm,k
H ^P=0
0log20=0
j s
Ej,s
j s
I+
j,sI−
j,s
^H
^H=(fractionofpointsinleftdataset)⋅H(I−
j,s)
+(fractionofpointsinrightdataset)⋅H(I+
j,s)
=(1−^Pm,j,s)H(I−
j,s)+^Pm,j,sH(I+
j,s)
=|I−
j,s|
Nm⋅H(I−
j,s)+|I+
j,s|
Nm⋅H(I+
j,s).
xj=s
infoGain(xj=s,Im)=H(Im)−(|I−
j,s|
Nm⋅H(I−
j,s)+|I+
j,s|
Nm⋅H(I+
j,s))
{ The respective impurity curves are shown below, where ; the vertical axis
plots  for each of the three criteria.
There used to be endless haggling about which impurity function one should use. It
seems to be traditional to use entropy to select which node to split while growing the
tree, and misclassiﬁcation error in the pruning criterion.
One important limitation or drawback in conventional trees is that they can have
high estimation error: small changes in the data can result in very big changes in the
resulting tree.
Bootstrap aggregation is a technique for reducing the estimation error of a non-linear
predictor, or one that is adaptive to the data. The key idea applied to trees, is to
build multiple trees with different subsets of the data, and then create an ensemble
model that combines the results from multiple trees to make a prediction.
Construct  new data sets of size . Each data set is constructed by sampling 
data points with replacement from . A single data set is called bootstrap sample
of .
Train a predictor  on each bootstrap sample.
Regression case: bagged predictor is
Classiﬁcation case: Let  be the number of classes. We ﬁnd a majority bagged
predictor as follows. We let  be a “one-hot” vector with a single 1 and{ .0.0when ^Pm,0=0.0
0.0when ^Pm,0=1.0
p=^Pm,0
Qm(T)
12.2.3 Bagging
B n n
D
D
^fb(x)
^fbag(x)=1
BB
∑
b=1^fb(x).
K
^fb(x)  zeros, and deﬁne the predicted output  for predictor  as
. Then
which is a vector containing the proportion of classiﬁers that predicted each
class  for input . Then the overall predicted output is
There are theoretical arguments showing that bagging does, in fact, reduce
estimation error. However, when we bag a model, any simple intrepetability is lost.
Random forests are collections of trees that are constructed to be de-correlated, so
that using them to vote gives maximal advantage. In competitions, they often have
excellent classiﬁcation performance among large collections of much fancier
methods.
In what follows, , , and  are hyperparameters of the algorithm.
procedure RANDOM F OREST( )
for  to  do
Draw a bootstrap sample  of size  from 
Grow tree  on :
while there are splittable nodes do
Select  variables at random from the  total variables
Pick the best variable and split point among those 
Split the current node
end while
end for
return 
end procedure
Given the ensemble of trees, vote to make a prediction on a new .
There are many variations on the tree theme. One is to employ different regression
or classiﬁcation methods in each leaf. For example, a linear regression might be
used to model the examples in each leaf, rather than using a constant value.
In the relatively simple trees that we’ve considered, splits have been based on only
a single feature at a time, and with the resulting splits being axis-parallel. Other
methods for splitting are possible, including consideration of multiple features and
linear classiﬁers based on those, potentially resulting in non-axis-parallel splits.
Complexity is a concern in such cases, as many possible combinations of featuresK−1 ^y fb
^yb(x)=argmaxk^fb(x)k
^fbag(x)=1
BB
∑
b=1^fb(x),
k x
^ybag(x)=argmax
k^fbag(x)k.
12.2.4 Random Forests
Bmn
1: B,m,n
2:b=1B
3: Dbn D
4: TbDb
5:
6: m d
7: m
8:
9:
10:
11:
12: {Tb}B
b=1
13:
x
12.2.5 Tree variants and tradeoffs may need to be considered, to select the best variable combination (rather than a
single split variable).
Another generalization is a hierarchical mixture of experts, where we make a “soft”
version of trees, in which the splits are probabilistic (so every point has some degree
of membership in every leaf). Such trees can be trained using a form of gradient
descent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,
gradient boosted trees) and implementations are readily available (e.g., XGBoost).
Trees have a number of strengths, and remain a valuable tool in the machine
learning toolkit. Some beneﬁts include being relatively easy to interpret, fast to
train, and ability to handle multi-class classiﬁcation in a natural way. Trees can
easily handle different loss functions; one just needs to change the predictor and
loss being applied in the leaves. Methods also exist to identify which features are
particularly important or inﬂuential in forming the tree, which can aid in human
understanding of the data set. Finally, in many situations, trees perform
surprisingly well, often comparable to more complicated regression or classiﬁcation
models. Indeed, in some settings it is considered good practice to start with trees
(especially random forest or boosted trees) as a “baseline” machine learning model,
against which one can evaluate performance of more sophisticated models.
While tree-based methods excel at supervised learning tasks, we now turn to
another important class of non-parametric methods that focus on discovering
structure in unlabeled data. These clustering methods share some conceptual
similarities with tree-based approaches - both aim to partition the input space into
meaningful regions - but clustering methods operate without supervision, making
them particularly valuable for exploratory data analysis and pattern discovery.
12.3 -means Clustering
Clustering is an unsupervised learning method where we aim to discover
meaningful groupings or categories in a dataset based on patterns or similarities
within the data itself, without relying on pre-assigned labels. It is widely used for
exploratory data analysis, pattern recognition, and segmentation tasks, allowing us
to interpret and manage complex datasets by uncovering hidden structures and
relationships.
Oftentimes a dataset can be partitioned into different categories. A doctor may
notice that their patients come in cohorts and different cohorts respond to different
treatments. A biologist may gain insight by identifying that bats and whales,
despite outward appearances, have some underlying similarity, and both should be
considered members of the same category, i.e., “mammal”. The problem of
automatically identifying meaningful groupings in datasets is called clustering.
Once these groupings are found, they can be leveraged toward interpreting the data
and making optimal decisions for each group.k Mathematically, clustering looks a bit like classiﬁcation: we wish to ﬁnd a mapping
from datapoints, , to categories, . However, rather than the categories being
predeﬁned labels, the categories in clustering are automatically discovered partitions
of an unlabeled dataset.
Because clustering does not learn from labeled examples, it is an example of an
unsupervised learning algorithm. Instead of mimicking the mapping implicit in
supervised training pairs , clustering assigns datapoints to categories
based on how the unlabeled data  is distributed in data space.
Intuitively, a “cluster” is a group of datapoints that are all nearby to each other and
far away from other clusters. Let’s consider the following scatter plot. How many
clusters do you think there are?
There seem to be about ﬁve clumps of datapoints and those clumps are what we
would like to call clusters. If we assign all datapoints in each clump to a cluster
corresponding to that clump, then we might desire that nearby datapoints are
assigned to the same cluster, while far apart datapoints are assigned to different
clusters.
In designing clustering algorithms, three critical things we need to decide are:
How do we measure distance between datapoints? What counts as “nearby”
and “far apart”?
How many clusters should we look for?
How do we evaluate how good a clustering is?
We will see how to begin making these decisions as we work through a concrete
clustering algorithm in the next section.
One of the simplest and most commonly used clustering algorithms is called k-
means. The goal of the k-means algorithm is to assign datapoints to  clusters in
such a way that the variance within clusters is as small as possible. Notice that this
12.3.1 Clustering formalismsx y
{x(i),y(i)}n
i=1
{x(i)}n
i=1
12.3.2 The k-means formulation
kFigure 12.1: A dataset we would
like to cluster. How many clusters
do you think there are? matches our intuitive idea that a cluster should be a tightly packed set of
datapoints.
Similar to the way we showed that supervised learning could be formalized
mathematically as the minimization of an objective function (loss function +
regularization), we will show how unsupervised learning can also be formalized as
minimizing an objective function. Let us denote the cluster assignment for a
datapoint  as , i.e.,  means we are assigning datapoint
 to cluster number 1. Then the k-means objective can be quantiﬁed with the
following objective function (which we also call the “k-means loss”):
where  and , so that  is the
mean of all datapoints in cluster , and using  to denote the indicator function
(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum
(over data points) of the loss is the variance of datapoints within cluster . We sum
up the variance of all  clusters to get our overall loss.
The k-means algorithm minimizes this loss by alternating between two steps: given
some initial cluster assignments: 1) compute the mean of all data in each cluster and
assign this as the “cluster mean”, and 2) reassign each datapoint to the cluster with
nearest cluster mean. Figure 12.2 shows what happens when we repeat these steps
on the dataset from above.
Each time we reassign the data to the nearest cluster mean, the k-means loss
decreases (the datapoints end up closer to their assigned cluster mean), or stays the
same. And each time we recompute the cluster means the loss also decreases (the
means end up closer to their assigned datapoints) or stays the same. Overall then,
the clustering gets better and better, according to our objective – until it stops
improving.
After four iterations of cluster assignment + update means in our example, the k-
means algorithm stops improving. We say it has converged, and its ﬁnal solution is
shown in Figure 12.3.x(i)y(i)∈{1,2,…,k}y(i)=1
x(i)
k
∑
j=1n
∑
i=1𝟙(y(i)=j)x(i)−μ(j)2,
∥∥(12.1)
μ(j)=1
Nj∑n
i=1𝟙(y(i)=j)x(i)Nj=∑n
i=1𝟙(y(i)=j)μ(j)
j 𝟙(⋅)
j
k
12.3.2.0.0.1 K-means algorithm
Figure 12.2: The ﬁrst three steps of
running the k-means algorithm on
this data. Datapoints are colored
according to the cluster to which
they are assigned. Cluster means
are the larger X’s with black
outlines. It seems to converge to something reasonable! Now let’s write out the algorithm in
complete detail:
procedure KM EANS( )
Initialize centroids  and assignments  randomly
for  to  do
for  to  do
end for
for  to  do
end for
if  then
break//convergence
end if
end for
return 
end procedure
The for-loop over the  datapoints assigns each datapoint to the nearest cluster
center. The for-loop over the k clusters updates the cluster center to be the mean of
all datapoints currently assigned to that cluster. As suggested above, it can be
shown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it
converges to a local minimum of the loss.
It’s like classiﬁcation except it picked what the classes are rather than being given
examples of what the classes are.
We can also use gradient descent to optimize the k-means objective. To show how to
apply gradient descent, we ﬁrst rewrite the objective as a differentiable function
only of :
 is the value of the k-means loss given that we pick the optimal assignments of
the datapoints to cluster means (that’s what the  does). Now we can use the
gradient  to ﬁnd the values for  that achieve minimum loss when cluster
1: k,τ,{x(i)}n
i=1
2: μ(1),…,μ(k)y(1),…,y(n)
3:t=1τ
4:yold←y
5: i=1n
6: y(i)←argminj∈{1,…,k}x(i)−μ(j)2
∥∥7:
8: j=1k
9: Nj←∑n
i=1𝟙(y(i)=j)
10: μ(j)←1
Nj∑n
i=1𝟙(y(i)=j)x(i)
11:
12:y=yold
13:
14:
15:
16:
17: μ,y
18:
n
12.3.2.0.0.2 Using gradient descent to minimize k-means objective
μ
L(μ)=n
∑
i=1min
jx(i)−μ(j)2.
∥∥L(μ)
minj
∂L(μ)
∂μμFigure 12.3: Converged result. assignments are optimal. Finally, we read off the optimal cluster assignments, given
the optimized , just by assigning datapoints to their nearest cluster mean:
This procedure yields a local minimum of Equation 12.1, as does the standard k-
means algorithm we presented (though they might arrive at different solutions). It
might not be the global optimum since the objective is not convex (due to , as
the minimum of multiple convex functions is not necessarily convex).
The standard k-means algorithm, as well as the variant that uses gradient descent,
both are only guaranteed to converge to a local minimum, not necessarily the global
minimum of the loss. Thus the answer we get out depends on how we initialize the
cluster means. Figure 12.4 is an example of a different initialization on our toy data,
which results in a worse converged clustering:
A variety of methods have been developed to pick good initializations (see, for
example, the k-means++ algorithm). One simple option is to run the standard k-
means algorithm multiple times, with different random initial conditions, and then
pick from these the clustering that achieves the lowest k-means loss.
A very important parameter in cluster algorithms is the number of clusters we are
looking for. Some advanced algorithms can automatically infer a suitable number of
clusters, but most of the time, like with k-means, we will have to pick  – it’s a
hyperparameter of the algorithm.
Figure 12.5 shows an example of the effect. Which result looks more correct? It can
be hard to say! Using higher k we get more clusters, and with more clusters we can
achieve lower within-cluster variance – the k-means objective will never increase,μ
y(i)=argmin
jx(i)−μ(j)2.
∥∥minj
12.3.2.0.0.3 Importance of initialization
12.3.2.0.0.4 Importance of k
k
Figure 12.4: With the initialization
of the means to the left, the yellow
and red means end up splitting
what perhaps should be one cluster
in half.
Figure 12.5: Example of k-means
run on our toy data, with two
different values of k. Setting k=4,
on the left, results in one cluster
being merged, compared to setting
k=5, on the right. Which clustering
do you think is better? How could
you decide? and will typically strictly decrease as we increase k. Eventually, we can increase k to
equal the total number of datapoints, so that each datapoint is assigned to its own
cluster. Then the k-means objective is zero, but the clustering reveals nothing.
Clearly, then, we cannot use the k-means objective itself to choose the best value for
k. In Section 1.3, we will discuss some ways of evaluating the success of clustering
beyond its ability to minimize the k-means objective, and it’s with these sorts of
methods that we might decide on a proper value of k.
Alternatively, you may be wondering: why bother picking a single k? Wouldn’t it be
nice to reveal a hierarchy of clusterings of our data, showing both coarse and ﬁne
groupings? Indeed hierarchical clustering is another important class of clustering
algorithms, beyond k-means. These methods can be useful for discovering tree-like
structure in data, and they work a bit like this: initially a coarse split/clustering of
the data is applied at the root of the tree, and then as we descend the tree we split
and cluster the data in ever more ﬁne-grained ways. A prototypical example of
hierarchical clustering is to discover a taxonomy of life, where creatures may be
grouped at multiple granularities, from species to families to kingdoms. You may
ﬁnd a suite of clustering algorithms in SKLEARN’s cluster module.
Clustering algorithms group data based on a notion of similarity, and thus we need
to deﬁne a distance metric between datapoints. This notion will also be useful in
other machine learning approaches, such as nearest-neighbor methods that we see
in Chapter 12. In k-means and other methods, our choice of distance metric can
have a big impact on the results we will ﬁnd.
Our k-means algorithm uses the Euclidean distance, i.e., , with a loss
function that is the square of this distance. We can modify k-means to use different
distance metrics, but a more common trick is to stick with Euclidean distance but
measured in a feature space. Just like we did for regression and classiﬁcation
problems, we can deﬁne a feature map from the data to a nicer feature
representation, , and then apply k-means to cluster the data in the feature
space.
As a simple example, suppose we have two-dimensional data that is very stretched
out in the ﬁrst dimension and has less dynamic range in the second dimension.
Then we may want to scale the dimensions so that each has similar dynamic range,
prior to clustering. We could use standardization, like we did in Chapter 5.
If we want to cluster more complex data, like images, music, chemical compounds,
etc., then we will usually need more sophisticated feature representations. One
common practice these days is to use feature representations learned with a neural
network. For example, we can use an autoencoder to compress images into feature
vectors, then cluster those feature vectors.
12.3.2.0.0.5 k-means in feature spacex(i)−μ(j)
∥∥ϕ(x)
12.3.3 How to evaluate clustering algorithms One of the hardest aspects of clustering is knowing how to evaluate it. This is
actually a big issue for all unsupervised learning methods, since we are just looking
for patterns in the data, rather than explicitly trying to predict target values (which
was the case with supervised learning).
Remember, evaluation metrics are not the same as loss functions, so we can’t just
measure success by looking at the k-means loss. In prediction problems, it is critical
that the evaluation is on a held-out test set, while the loss is computed over training
data. If we evaluate on training data we cannot detect overﬁtting. Something
similar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too
large can precisely “ﬁt” the data (minimize the loss), but yields no general insight.
One way to evaluate our clusters is to look at the consistency with which they are
found when we run on different subsamples of our training data, or with different
hyperparameters of our clustering algorithm (e.g., initializations). For example, if
running on several bootstrapped samples (random subsets of our data) results in
very different clusters, it should call into question the validity of any of the
individual results.
If we have some notion of what ground truth clusters should be, e.g., a few data
points that we know should be in the same cluster, then we can measure whether or
not our discovered clusters group these examples correctly.
Clustering is often used for visualization and interpretability, to make it easier for
humans to understand the data. Here, human judgment may guide the choice of
clustering algorithm. More quantitatively, discovered clusters may be used as input
to downstream tasks. For example, as we saw in the lab, we may ﬁt a different
regression function on the data within each cluster. Figure 12.6 gives an example
where this might be useful. In cases like this, the success of a clustering algorithm
can be indirectly measured based on the success of the downstream application
(e.g., does it make the downstream predictions more accurate).
Figure 12.6: Averaged across the
whole population, risk of heart
disease positively correlates with
hours of exercise. However, if we
cluster the data, we can observe
that there are four subgroups of the
population which correspond to
different age groups, and within
each subgroup the correlation is
negative. We can make better
predictions, and better capture the
presumed true effect, if we cluster
this data and then model the trend
in each cluster separately.  What are some conventions for derivatives of matrices and vectors? It will always
work to explicitly write all indices and treat everything as scalars, but we
introduce here some shortcuts that are often faster to use and helpful for
understanding.
There are at least two consistent but different systems for describing shapes and
rules for doing matrix derivatives. In the end, they all are correct, but it is
important to be consistent.
We will use what is often called the ‘Hessian’ or denominator layout, in which we
say that for
 of size  and  of size ,  is a matrix of size  with the 
entry . This denominator layout convention has been adopted by the ﬁeld
of machine learning to ensure that the shape of the gradient is the same as the
shape of the respective derivative. This is somewhat controversial at large, but alas,
we shall continue with denominator layout.
The discussion below closely follows the Wikipedia on matrix derivatives.
A.1 The shapes of things
Here are important special cases of the rule above:
Scalar-by-scalar: For  of size  and  of size ,  is the (scalar)
partial derivative of  with respect to .
Scalar-by-vector: For  of size  and  of size ,  (also written
, the gradient of  with respect to ) is a column vector of size  with
the  entry :
Vector-by-scalar: For  of size  and  of size ,  is a row
vector of size  with the  entry :
Vector-by-vector: For  of size  and  of size ,  is a matrix of
size  with the  entry :Appendix A — Matrix derivative common
cases
xn×1 ym×1∂y/∂x n×m (i,j)
∂yj/∂xi
x 1×1y 1×1∂y/∂x
y x
xn×1y 1×1∂y/∂x
∇xy y x n×1
ith∂y/∂xi
∂y/∂x= .⎡
⎢⎣∂y/∂x1
∂y/∂x2
⋮
∂y/∂xn⎤
⎥⎦
x 1×1 ym×1∂y/∂x
1×m jth∂yj/∂x
∂y/∂x=[ ]. ∂y1/∂x∂y2/∂x⋯∂ym/∂x
xn×1 ym×1∂y/∂x
n×m (i,j)∂yj/∂xi Appendices>A  M atrix derivative common cases 
1 Scalar-by-matrix: For  of size  and  of size ,  (also written
, the gradient of  with respect to ) is a matrix of size  with the
 entry :
You may notice that in this list, we have not included matrix-by-matrix, matrix-by-
vector, or vector-by-matrix derivatives. This is because, generally, they cannot be
expressed nicely in matrix form and require higher order objects (e.g., tensors) to
represent their derivatives. These cases are beyond the scope of this course.
Additionally, notice that for all cases, you can explicitly compute each element of
the derivative object using (scalar) partial derivatives. You may ﬁnd it useful to
work through some of these by hand as you are reviewing matrix derivatives.
A.2 Some vector-by-vector identities
Here are some examples of . In each case, assume  is ,  is ,  is
a scalar constant,  is a vector that does not depend on  and  is a matrix that
does not depend on ,  and  are scalars that do depend on , and  and  are
vectors that do depend on . We also have vector-valued functions  and .
First, we will cover a couple of fundamental cases: suppose that  is an 
vector which is not a function of , an  vector. Then,
is an  matrix of 0s. This is similar to the scalar case of differentiating a
constant. Next, we can consider the case of differentiating a vector with respect to
itself:
This is the  identity matrix, with 1’s along the diagonal and 0’s elsewhere. It
makes sense, because  is 1 for  and 0 otherwise. This identity is also
similar to the scalar case.∂y/∂x= .⎡
⎢⎣∂y1/∂x1∂y2/∂x1⋯∂ym/∂x1
∂y1/∂x2∂y2/∂x2⋯∂ym/∂x2
⋮ ⋮⋱ ⋮
∂y1/∂xn∂y2/∂xn⋯∂ym/∂xn⎤
⎥⎦
X n×my 1×1∂y/∂X
∇Xy y X n×m
(i,j)∂y/∂Xi,j
∂y/∂X= .⎡
⎢⎣∂y/∂X1,1⋯∂y/∂X1,m
⋮⋱ ⋮
∂y/∂Xn,1⋯∂y/∂Xn,m⎤
⎥⎦
∂y/∂x xn×1ym×1a
a x A
xuv x u v
x f g
A.2.1 Some fundamental cases
am×1
xn×1
∂a
∂x=0, (A.1)
n×m
∂x
∂x=I
n×n
∂xj/xii=j Let the dimensions of  be . Then the object  is an  vector. We can
then compute the derivative of  with respect to  as:
Note that any element of the column vector  can be written as, for :
Thus, computing the  entry of  requires computing the partial derivative
Therefore, the  entry of  is the  entry of :
Similarly, for objects  of the same shape, one can obtain,
Suppose that  are both vectors of size . Then,
Suppose that  is a scalar constant and  is an  vector that is a function of .
Then,
One can extend the previous identity to vector- and matrix-valued constants.
Suppose that  is a vector with shape  and  is a scalar which depends on .
Then,
First, checking dimensions,  is  and  is  so  is  and our
answer is  as it should be. Now, checking a value, element  of the
A.2.2 Derivatives involving a constant matrixAm×n Axm×1
Ax x
∂Ax
∂x=⎡
⎢⎣∂(Ax)1/∂x1∂(Ax)2/∂x1⋯∂(Ax)m/∂x1
∂(Ax)1/∂x2∂(Ax)2/∂x2⋯∂(Ax)m/∂x2
⋮ ⋮ ⋱ ⋮
∂(Ax)1/∂xn∂(Ax)2/∂xn⋯∂(Ax)m/∂xn⎤
⎥⎦
Ax j=1,…,m
(Ax)j=n
∑
k=1Aj,kxk.
(i,j)∂Ax
∂x
∂(Ax)j/∂xi:
∂(Ax)j/∂xi=∂(n
∑
k=1Aj,kxk)/∂xi=Aj,i
(i,j)∂Ax
∂x(j,i) A
∂Ax
∂x=AT(A.2)
x,A
∂xTA
∂x=A (A.3)
A.2.3 Linearity of derivatives
u,v m×1
∂(u+v)
∂x=∂u
∂x+∂v
∂x(A.4)
a um×1 x
∂au
∂x=a∂u
∂x
a m×1v x
∂va
∂x=∂v
∂xaT
∂v/∂xn×1 am×1aT1×m
n×m (i,j) answer is  =  which corresponds to element  of
.
Similarly, suppose that  is a matrix which does not depend on  and  is a
column vector which does depend on . Then,
Suppose that  is a scalar which depends on , while  is a column vector of shape
 and  is a column vector of shape . Then,
One can see this relationship by expanding the derivative as follows:
Then, one can use the product rule for scalar-valued functions,
to obtain the desired result.
Suppose that  is a vector-valued function with output vector of shape , and
the argument to  is a column vector  of shape  which depends on . Then,
one can obtain the chain rule as,
Following “the shapes of things,”  is  and  is , where
element  is . The same chain rule applies for further compositions
of functions:
A.3 Some other identities
You can get many scalar-by-vector and vector-by-scalar cases as special cases of the
rules above, making one of the relevant vectors just be 1 x 1. Here are some other
ones that are handy. For more, see the Wikipedia article on Matrix derivatives (for
consistency, only use the ones in denominator layout).∂vaj/∂xi(∂v/∂xi)aj (i,j)
(∂v/∂x)aT
A x u
x
∂Au
∂x=∂u
∂xAT
A.2.4 Product rule (vector-valued numerator)
v x u
m×1 x n×1
∂vu
∂x=v∂u
∂x+∂v
∂xuT
∂vu
∂x= .⎡
⎢⎣∂(vu1)/∂x1∂(vu2)/∂x1⋯∂(vum)/∂x1
∂(vu1)/∂x2∂(vu2)/∂x2⋯∂(vum)/∂x2
⋮ ⋮ ⋱ ⋮
∂(vu1)/∂xn∂(vu2)/∂xn⋯∂(vum)/∂xn⎤
⎥⎦
∂(vuj)/∂xi=v(∂uj/∂xi)+(∂v/∂xi)uj,
A.2.5 Chain rule
g m×1
g u d×1 x
∂g(u)
∂x=∂u
∂x∂g(u)
∂u
∂u/∂xn×d∂g(u)/∂ud×m
(i,j)∂g(u)j/∂ui
∂f(g(u))
∂x=∂u
∂x∂g(u)
∂u∂f(g)
∂g
T A.4 Derivation of gradient for linear regression
Recall here that  is a matrix of of size  and  is an  vector.
Applying identities Equation A.3, Equation A.5,Equation A.4, Equation A.2,
Equation A.1
A.5 Matrix derivatives using Einstein summation
You do not have to read or learn this! But you might ﬁnd it interesting or helpful.
Consider the objective function for linear regression, written out as products of
matrices:
where  is ,  is , and  is . How does one show, with no
shortcuts, that
One neat way, which is very explicit, is to simply write all the matrices as variables
with row and column indices, e.g.,  is the row , column  entry of the matrix
. Furthermore, let us use the convention that in any product, all indices which
appear more than once get summed over; this is a popular convention in
theoretical physics, and lets us suppress all the summation symbols which would
otherwise clutter the following expresssions. For example,  would be the
implicit summation notation giving the element at the  row of the matrix-vector
product .
Using implicit summation notation with explicit indices, we can rewrite  as∂uTv
∂x=∂u
∂xv+∂v
∂xu (A.5)
∂uT
∂x=(∂u
∂x)T(A.6)
X n×d Yn×1
∂(Xθ−Y)T(Xθ−Y)/n
∂θ=2
n∂(Xθ−Y)
∂θ(Xθ−Y)
=2
n(∂Xθ
∂θ−∂Y
∂θ)(Xθ−Y)
=2
n(XT−0)(Xθ−Y)
=2
nXT(Xθ−Y)
J(θ)=1
n(Xθ−Y)T(Xθ−Y),
Xn×dYn×1θd×1
∇θJ=2
nXT(Xθ−Y)?
Xab a b
X
Xabθb
ath
Xθ
J(θ)
J(θ)=1
n(Xabθb−Ya)(Xacθc−Ya). Note that we no longer need the transpose on the ﬁrst term, because all that
transpose accomplished was to take a dot product between the vector given by the
left term, and the vector given by the right term. With implicit summation, this is
accomplished by the two terms sharing the repeated index .
Taking the derivative of  with respect to the  element of  thus gives, using the
chain rule for (ordinary scalar) multiplication:
where the second line follows from the ﬁrst, with the deﬁnition that  only
when  (and similarly for ). And the third line follows from the second by
recognizing that the two terms in the second line are identical. Now note that in
this implicit summation notation, the  element of the matrix product of  and
 is . That is, ordinary matrix multiplication sums over indices
which are adjacent to each other, because a row of  times a column of  becomes
a scalar number. So the term in the above equation with  is not a matrix
product of  with . However, taking the transpose  switches row and column
indices, so . And  is a matrix product of  with ! Thus, we
have that
which is the desired result.a
J dthθ
dJ
dθd=1
n[Xabδbd(Xacθc−Ya)+(Xabθb−Ya)Xacδcd]
=1
n[Xad(Xacθc−Ya)+(Xabθb−Ya)Xad]
=2
nXad(Xabθb−Ya),
δbd=1
b=d δcd
a,b A
B(AB)ac=AabBbc
A B
XadXab
XX XT
Xad=XT
daXT
daXab XTX
dJ
dθd=2
nXT
da(Xabθb−Ya)
=2
n[XT(Xθ−Y)]d, B.1 Strategies towards adaptive step-size
We’ll start by looking at the notion of a running average. It’s a computational
strategy for estimating a possibly weighted average of a sequence of data. Let our
data sequence be ; then we deﬁne a sequence of running average values,
 using the equations
where . If  is a constant, then this is a moving average, in which
So, you can see that inputs  closer to the end of the sequence have more effect on
 than early inputs.
If, instead, we set , then we get the actual average.
❓  Study Question
Prove to yourself that the previous assertion holds.
Now, we can use methods that are a bit like running averages to describe strategies
for computing . The simplest method is momentum, in which we try to “average”
recent gradient updates, so that if they have been bouncing back and forth in some
direction, we take out that component of the motion. For momentum, we haveAppendix B — Optimizing Neural
Networks
B.1.1 Running averages
c1,c2,…
C0,C1,C2,…
C0=0,
Ct=γtCt−1+(1−γt)ct,
γt∈(0,1)γt
CT=γCT−1+(1−γ)cT
=γ(γCT−2+(1−γ)cT−1)+(1−γ)cT
=T
∑
t=1γT−t(1−γ)ct.
ct
CT
γt=t−1
t
B.1.2 Momentum
η
V0=0,
Vt=γVt−1+η∇WJ(Wt−1),
Wt=Wt−1−Vt. Appendices>B  Optimizing Neural Networks  This doesn’t quite look like an adaptive step size. But what we can see is that, if we
let , then the rule looks exactly like doing an update with step size 
on a moving average of the gradients with parameter :
❓  Study Question
Prove to yourself that these formulations are equivalent.
We will ﬁnd that  will be bigger in dimensions that consistently have the same
sign for  and smaller for those that don’t. Of course we now have two
parameters to set ( and ), but the hope is that the algorithm will perform better
overall, so it will be worth trying to ﬁnd good values for them. Often  is set to be
something like .
The red arrows show the update after each successive step of mini-batch gradient
descent with momentum. The blue points show the direction of the gradient with
respect to the mini-batch at each step. Momentum smooths the path taken towards
the local minimum and leads to faster convergence.
❓  Study Question
If you set , would momentum have more of an effect or less of an effect
than if you set it to ?
Another useful idea is this: we would like to take larger steps in parts of the space
where  is nearly ﬂat (because there’s no risk of taking too big a step due to the
gradient being large) and smaller steps when it is steep. We’ll apply this idea to
each weight independently, and end up with a method called adadelta, which is aη=η′(1−γ) η′
γ
M0=0,
Mt=γMt−1+(1−γ)∇WJ(Wt−1),
Wt=Wt−1−η′Mt.
Vt
∇W
η γ
γ
0.9
γ=0.1
0.9
B.1.3 Adadelta
J(W)M omentum variant on adagrad (for adaptive gradient). Even though our weights are indexed by
layer, input unit, and output unit, for simplicity here, just let  be any weight in
the network (we will do the same thing for all of them).
The sequence  is a moving average of the square of the th component of the
gradient. We square it in order to be insensitive to the sign—we want to know
whether the magnitude is big or small. Then, we perform a gradient update to
weight , but divide the step size by , which is larger when the surface is
steeper in direction  at point  in weight space; this means that the step size
will be smaller when it’s steep and larger when it’s ﬂat.
Adam has become the default method of managing step sizes in neural networks.
It combines the ideas of momentum and adadelta. We start by writing moving
averages of the gradient and squared gradient, which reﬂect estimates of the mean
and variance of the gradient for weight :
A problem with these estimates is that, if we initialize , they will
always be biased (slightly too small). So we will correct for that bias by deﬁning
Note that  is  raised to the power , and likewise for . To justify these
corrections, note that if we were to expand  in terms of  and
, the coefﬁcients would sum to 1. However, the coefﬁcient behind
 is  and since , the sum of coefﬁcients of nonzero terms is ;
hence the correction. The same justiﬁcation holds for .
❓  Study QuestionWj
gt,j=∇WJ(Wt−1)j,
Gt,j=γGt−1,j+(1−γ)g2
t,j,
Wt,j=Wt−1,j−η
√Gt,j+ϵgt,j.
Gt,j j
j √Gt,j+ϵ
j Wt−1
B.1.4 Adam
j
gt,j=∇WJ(Wt−1)j,
mt,j=B1mt−1,j+(1−B1)gt,j,
vt,j=B2vt−1,j+(1−B2)g2
t,j.
m0=v0=0
^mt,j=mt,j
1−Bt
1,
^vt,j=vt,j
1−Bt
2,
Wt,j=Wt−1,j−η
√^vt,j+ϵ^mt,j.
Bt
1B1 t Bt
2
mt,j m0,j
g0,j,g1,j,…,gt,j
m0,jBt
1 m0,j=0 1−Bt
1
vt,jAlthoug h, interestingly, it may
actua lly violate the convergence
conditions of SG D:
arxiv.org/abs/1705.08292 Deﬁne  directly as a moving average of . What is the decay (
parameter)?
Even though we now have a step size for each weight, and we have to update
various quantities on each iteration of gradient descent, it’s relatively easy to
implement by maintaining a matrix for each quantity (, , , ) in each layer
of the network.
B.2 Batch Normalization Details
Let’s think of the batch-normalization layer as taking  as input and producing an
output . But now, instead of thinking of  as an  vector, we have to
explicitly think about handling a mini-batch of data of size  all at once, so  will
be an  matrix, and so will the output .
Our ﬁrst step will be to compute the batchwise mean and standard deviation. Let 
be the  vector where
and let  be the  vector where
The basic normalized version of our data would be a matrix, element  of which
is
where  is a very small constant to guard against division by zero.
However, if we let these be our  values, we really are forcing something too
strong on our data—our goal was to normalize across the data batch, but not
necessarily force the output values to have exactly mean 0 and standard deviation 1.
So, we will give the layer the opportunity to shift and scale the outputs by adding
new weights to the layer. These weights are  and , each of which is an 
vector. Using the weights, we deﬁne the ﬁnal output to be
That’s the forward pass. Whew!
Now, for the backward pass, we have to do two things: given ,^mt,j gt,j γ
mℓ
tvℓ
tgℓ
tg2
tℓ
Zl
ˆZlZlnl×1
K Zl
nl×K ˆZl
μl
nl×1
μl
i=1
KK
∑
j=1Zl
ij,
σlnl×1
σl
i=1
KK
∑
j=1(Zl
ij−μl
i)2
.
⎷
(i,j)
Zl
ij=Zl
ij−μl
i
σl
i+ϵ, –
ϵ
ˆZl
GlBlnl×1
ˆZl
ij=Gl
iZl
ij+Bl
i. –
∂L
∂ˆZl Compute  for back-propagation, and
Compute  and  for gradient updates of the weights in this layer.
Schematically, we have
It’s hard to think about these derivatives in matrix terms, so we’ll see how it works
for the components.  contributes to  for all data points  in the batch. So,
Similarly,  contributes to  for all data points  in the batch. Thus,
Now, let’s ﬁgure out how to do backprop. We can start schematically:
And because dependencies only exist across the batch, but not across the unit
outputs,
The next step is to note that
And now that
where  if  and 0 otherwise. We need two more pieces:
Putting the whole thing together, we get∂L
∂Zl
∂L
∂Gl∂L
∂Bl
∂L
∂B=∂L
∂ˆZ∂ˆZ
∂B.
BiˆZij j
∂L
∂Bi=∑
j∂L
∂ˆZij∂ˆZij
∂Bi=∑
j∂L
∂ˆZij.
GiˆZij j
∂L
∂Gi=∑
j∂L
∂ˆZij∂ˆZij
∂Gi=∑
j∂L
∂ˆZijZij. –
∂L
∂Z=∂L
∂ˆZ∂ˆZ
∂Z.
∂L
∂Zij=K
∑
k=1∂L
∂ˆZik∂ˆZik
∂Zij.
∂ˆZik
∂Zij=∂ˆZik
∂Zik∂Zik
∂Zij=Gi∂Zik
∂Zij. –––
∂Zik
∂Zij=(δjk−∂μi
∂Zij)1
σi−Zik−μi
σ2
i∂σi
∂Zij, –
δjk=1j=k
∂μi
∂Zij=1
K,∂σi
∂Zij=Zij−μi
Kσi.
∂L
∂Zij=K
∑
k=1∂L
∂ˆZikGi1
Kσi(Kδjk−1−(Zik−μi)(Zij−μi)
σ2
i).  In which we try to describe the outlines of the “lifecycle” of supervised learning,
including hyperparameter tuning and evaluation of the ﬁnal product.
C.1 General case
We start with a very generic setting.
Given: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()
such that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning
algorithm () takes as input a data set of the form
where  and  and returns an .
Given a problem speciﬁcation and a set of data , we evaluate hypothesis 
according to average loss, or error,
If the data used for evaluation were not used during learning of the hypothesis then this
is a reasonable estimate of how well the hypothesis will make additional
predictions on new data from the same source.
A validation strategy  takes an algorithm , a loss function , and a data source 
and produces a real number which measures how well  performs on data from
that distribution.
In the simplest case, we can divide  into two sets,  and , train on the
ﬁrst, and then evaluate the resulting hypothesis on the second. In that case,Appendix C — Supervised learning in a
nutshell
C.1.1 Minimal problem speciﬁcation 
D={(x(1),y(1)),…,(x(n),y(n))}
x(i)∈Xy(i)∈y h∈H
C.1.2 Evaluating a hypothesis
D h
E(h,L,D)=1
|D|D
∑
i=1L(h(x(i)),y(i))
C.1.3 Evaluating a supervised learning algorithm
V A L D
A
C.1.3.1 Using a validation set
D Dtrain Dval 
V(A,L,D)=E(A(Dtrain ),L,Dval ) Appendices>C  Su pervised learning in a nutshell  We can’t reliably evaluate an algorithm based on a single application to a single
training and test set, because there are many aspects of the training and testing
data, as well as, sometimes, randomness in the algorithm itself, that cause variance
in the performance of the algorithm. To get a good idea of how well an algorithm
performs, we need to, multiple times, train it and evaluate the resulting hypothesis,
and report the average over  executions of the algorithm of the error of the
hypothesis it produced each time.
We divide the data into 2 K random non-overlapping subsets:
.
Then,
In cross validation, we do a similar computation, but allow data to be re-used in the
 different iterations of training and testing the algorithm (but never share training
and testing data for a single iteration!). See Se ction 2.8.2.2 for details.
Now, if we have two different algorithms  and , we might be interested in
knowing which one will produce hypotheses that generalize the best, using data
from a particular source. We could compute  and , and
prefer the algorithm with lower validation error. More generally, given algorithms
, we would prefer
Now what? We have to deliver a hypothesis to our customer. We now know how to
ﬁnd the algorithm, , that works best for our type of data. We can apply it to all of
our data to get the best hypothesis we know how to create, which would be
and deliver this resulting hypothesis as our best product.
A majority of learning algorithms have the form of optimizing some objective
involving the training data and a loss function.
C.1.3.2 Using multiple training/evaluation runsK
Dtrain 
1,Dval 
1,…,Dtrain 
K,Dval 
K
V(A,L,D)=1
KK
∑
k=1E(A(Dtrain
k),L,Dval
k).
C.1.3.3 Cross validation
K
C.1.4 Comparing supervised learning algorithms
A1A2
V(A1,L,D)V(A∈,L,D)
A1,…,AM
A∗=argmin
mV(AM,L,D)
C.1.5 Fielding a hypothesis
A∗
h∗=A∗(D)
C.1.6 Learning algorithms as optimizers
Interestingly, this loss fun ction is
not always the same as the loss
fun ction that is us ed for So for example, (assuming a perfect optimizer which doesn’t, of course, exist) we
might say our algorithm is to solve an optimization problem:
Our objective often has the form
where  is a loss to be minimized during training and  is a regularization term.
Often, rather than comparing an arbitrary collection of learning algorithms, we
think of our learning algorithm as having some parameters that affect the way it
maps data to a hypothesis. These are not parameters of the hypothesis itself, but
rather parameters of the algorithm. We call these hyperparameters. A classic example
would be to use a hyperparameter  to govern the weight of a regularization term
on an objective to be optimized:
Then we could think of our algorithm as . Picking a good value of  is the
same as comparing different supervised learning algorithms, which is accomplished
by validating them and picking the best one!
C.2 Concrete case: linear regression
In linear regression the problem formulation is this:
 for values of parameters  and .
Our learning algorithm has hyperparameter  and can be written as:
Our learning algorithm has hyperparameter $ $ and can be written as:
For a particular training data set and parameter , it ﬁnds the best hypothesis on
this data, speciﬁed with parameters , written .A(D)=argmin
h∈HJ(h;D).
J(h;D)=E(h,L,D)+R(h),
L R
C.1.7 Hyperparameters
λ
J(h;D)=E(h,L,D)+λR(h).
A(D;λ) λ
x=Rd
y=R
H={θ⊤x+θ0} θ∈Rdθ0∈R
L(g,y)=(g−y)2
λ
A(D;λ)=Θ∗(λ,D)=argmin
θ,θ01
|D|∑
(x,y)∈D(θ⊤x+θ0−y)2+λ∥θ∥2
A(D;λ)=Θ∗(λ,D)=argmin
θ,θ01
|D|∑
(x,y)∈D(θ⊤x+θ0−y)2+λ∥θ∥2.
λ
Θ=(θ,θ0) Θ∗(λ,D)evalua tion! We will see this in
logistic regression. Picking the best value of the hyperparameter is choosing among learning
algorithms. We could, most simply, optimize using a single training / validation
split, so  , and
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Note that we don’t use regularization here because we just want to measure how
good the output of the algorithm is at predicting values of new points, and so that’s
what we measure. We use the regularizer during training when we don’t want to
focus only on optimizing predictions on the training data.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, , to train, using the best hyperparameters we know, and return
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, as
Here are the same ideas, written out in informal pseudocode:D=Dtrain ∪Dval 
λ∗=argmin
λV(Aλ,L,Dval )
=argmin
λE(Θ∗(λ,Dtrain ), mse, Dval )
=argmin
λ1
|Dval |∑
(x,y)∈Dval (θ∗(λ,Dtrain )⊤x+θ∗
0(λ,Dtrain )−y)2
λ
V
D
Θ∗=A(D;λ∗)
=Θ∗(λ∗,D)
=argmin
θ,θ01
|D|∑
(x,y)∈D(θ⊤x+θ0−y)2+λ∗∥θ∥2
Etest =E(Θ∗, mse ,Dtest )
=1
|Dtest |∑
(x,y)∈Dtot (θ∗Tx+θ∗
0−y)2
# returns theta_best(D, lambda)
define train(D, lambda):
    return minimize(mse(theta, D) + lambda * norm(theta)** 2, theta)
# returns lambda_best using very simple validation
define simple_tune(D_train, D_val, possible_lambda_vals):
    scores = [mse(train(D_train, lambda), D_val) for lambda in 
possible_lambda_vals]
    return possible_lambda_vals[least_index[scores]]
# returns theta_best overall
define theta_best(D_train, D_val, possible_lambda_vals):
    return train(D_train + D_val, simple_tune(D_train, D_val, 
possible_lambda_vals))
# customer evaluation of the theta delivered to them C.3 Concrete case: logistic regression
In binary logistic regression the problem formulation is as follows. We are writing
the class labels as 1 and 0.
 for values of parameters  and .
Proxy loss  Our learning algorithm
has hyperparameter  and can be written as:
For a particular training data set and parameter , it ﬁnds the best hypothesis on
this data, speciﬁed with parameters , written  according to the
proxy loss .
Picking the best value of the hyperparameter is choosing among learning
algorithms based on their actual predictions. We could, most simply, optimize using
a single training / validation split, so , and we use the real 01 loss:
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, , to train, using the best hyperparameters we know, and return
❓  Study Question
What loss function is being optimized inside this algorithm?
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, asdefine customer_val(theta):
    return mse(theta, D_test)
X=Rd
y={+1,0}
H={σ(θ⊤x+θ0)} θ∈Rdθ0∈R
L(g,y)=L01( g, h)
Lnll(g,y)=−(ylog(g)+(1−y)log(1−g))
λ
A(D;λ)=Θ∗(λ,D)=argmin
θ,θ01
|D|∑
(x,y)∈DLnll(σ(θ⊤x+θ0),y)+λ∥θ∥2
λ
Θ=(θ,θ0) Θ∗(λ,D)
Lnll 
D=Dtrain ∪Dval
λ∗=argmin
λV(Aλ,L01,Dval )
=argmin
λE(Θ∗(λ,Dtrain ),L01,Dval )
=argmin
λ1
|Dval |∑
(x,y)∈Dval L01(σ(θ∗(λ,Dtrain )⊤x+θ∗
0(λ,Dtrain )),y)
λ
V
D
Θ∗=A(D;λ∗)
Etest=E(Θ∗,L01,Dtest) The customer just wants to buy the right stocks! So we use the real  here for
validation.L01