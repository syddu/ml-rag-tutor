1 Introduction
The main focus of machine learning (ML) is making decisions or predictions based on
data. There are a number of other fields with significant overlap in technique, but
difference in focus: in economics and psychology, the goal is to discover underlying
causal processes and in statistics it is to find a model that fits a data set well. In
those fields, the end product is a model. In machine learning, we often fit models,
but as a means to the end of making good predictions or decisions.
As ML methods have improved in their capability and scope, ML has become
arguably the best way–measured in terms of speed, human engineering time, and
robustness–to approach many applications. Great examples are face detection,
speech recognition, and many kinds of language-processing tasks. Almost any
application that involves understanding data or signals that come from the real
world can be nicely addressed using machine learning.
One crucial aspect of machine learning approaches to solving problems is that
human engineering plays an important role. A human still has to frame the problem:
acquire and organize data, design a space of possible solutions, select a learning
algorithm and its parameters, apply the algorithm to the data, validate the resulting
solution to decide whether it’s good enough to use, try to understand the impact on
the people who will be affected by its deployment, etc. These steps are of great
importance.
The conceptual basis of learning from data is the problem of induction: Why do we
think that previously seen data will help us predict the future? This is a serious long
standing philosophical problem. We will operationalize it by making assumptions,
such as that all training data are so-called i.i.d.(independent and identically
distributed), and that queries will be drawn from the same distribution as the
training data, or that the answer comes from a set of possible answers known in
advance.
6.390 - Intro to Machine Learning
Course Notes
This description is paraphrased
from a post on 9/4/12 at
andrewgelman.com.
This aspect is often undervalued.
This means that the elements in the
set are related in the sense that
they all come from the same
underlying probability
distribution, but not in other ways.
1  Introduction

 In general, we need to solve these two problems:
estimation: When we have data that are noisy reflections of some underlying
quantity of interest, we have to aggregate the data and make estimates or
predictions about the quantity. How do we deal with the fact that, for example,
the same treatment may end up with different results on different trials? How
can we predict how well an estimate may compare to future results?
generalization: How can we predict results of a situation or experiment that
we have never encountered before in our data set?
We can describe problems and their solutions using six characteristics, three of
which characterize the problem and three of which characterize the solution:
1. Problem class: What is the nature of the training data and what kinds of
queries will be made at testing time?
2. Assumptions: What do we know about the source of the data or the form of
the solution?
3. Evaluation criteria: What is the goal of the prediction or estimation system?
How will the answers to individual queries be evaluated? How will the overall
performance of the system be measured?
4. Model type: Will an intermediate model of the world be made? What aspects
of the data will be modeled in different variables/parameters? How will the
model be used to make predictions?
5. Model class: What particular class of models will be used? What criterion will
we use to pick a particular model from the model class?
6. Algorithm: What computational process will be used to fit the model to the
data and/or to make predictions?
Without making some assumptions about the nature of the process generating the
data, we cannot perform generalization. In the following sections, we elaborate on
these ideas.
1.1 Problem class
There are many different problem classes in machine learning. They vary according to
what kind of data is provided and what kind of conclusions are to be drawn from it.
Five standard problem classes are described below, to establish some notation and
terminology.
In this course, we will focus on classification and regression (two examples of
supervised learning), and we will touch on reinforcement learning, sequence
learning, and clustering.
For example, the same treatment
may end up with different results
on different trials. How can we
predict how well an estimate
compares to future results?
Don’t feel you have to memorize
all these kinds of learning, etc. We
just want you to have a very high-
 The idea of supervised learning is that the learning system is given inputs and told
which specific outputs should be associated with them. We divide up supervised
learning based on whether the outputs are drawn from a small finite set
(classification) or a large finite ordered set or continuous set (regression).
For a regression problem, the training data 
 is in the form of a set of  pairs:
where 
 represents an input, most typically a -dimensional vector of real and/or
discrete values, and 
 is the output to be predicted, in this case a real-number. The
 values are sometimes called target values.
The goal in a regression problem is ultimately, given a new input value 
, to
predict the value of 
. Regression problems are a kind of supervised learning,
because the desired output 
 is specified for each of the training examples 
.
A classification problem is like regression, except that the values that 
 can take
do not have an order. The classification problem is binary or two-class if 
 (also
known as the class) is drawn from a set of two possible values; otherwise, it is called
multi-class.
Unsupervised learning doesn’t involve learning a function from inputs to outputs
based on a set of input-output pairs. Instead, one is given a data set and generally
expected to find some patterns or structure inherent in it.
Given samples 
, the goal is to find a partitioning (or “clustering”)
of the samples that groups together similar samples. There are many different
objectives, depending on the definition of the similarity between samples and
exactly what criterion is to be used (e.g., minimize the average distance between
elements inside a cluster and maximize the average distance between elements
across clusters). Other methods perform a “soft” clustering, in which samples may
be assigned 0.9 membership in one cluster and 0.1 in another. Clustering is
sometimes used as a step in the so-called density estimation (described below), and
sometimes to find useful structure or influential features in data.
1.1.1 Supervised learning
1.1.1.1 Regression
Dtrain
n
Dtrain = {(x(1), y(1)), … , (x(n), y(n))},
x(i)
d
y(i)
y
x(n+1)
y(n+1)
y(i)
x(i)
1.1.1.2 Classification
y(i)
y(i)
1.1.2 Unsupervised learning
1.1.2.1 Clustering
x(1), … , x(n) ∈Rd
level view of (part of) the breadth
of the field.
Many textbooks use 
 and 
instead of 
 and 
. We find that
notation somewhat difficult to
manage when 
 is itself a vector
and we need to talk about its
elements. The notation we are
using is standard in some other
parts of the ML literature.
xi
ti
x(i)
y(i)
x(i)
 Given samples 
 drawn i.i.d. from some distribution 
, the
goal is to predict the probability 
 of an element drawn from the same
distribution. Density estimation sometimes plays a role as a “subroutine” in the
overall learning method for supervised learning, as well.
Given samples 
, the problem is to re-represent them as points in
a -dimensional space, where 
. The goal is typically to retain information in
the data set that will, e.g., allow elements of one class to be distinguished from
another.
Dimensionality reduction is a standard technique that is particularly useful for
visualizing or understanding high-dimensional data. If the goal is ultimately to
perform regression or classification on the data after the dimensionality is reduced,
it is usually best to articulate an objective for the overall prediction problem rather
than to first do dimensionality reduction without knowing which dimensions will
be important for the prediction task.
In sequence learning, the goal is to learn a mapping from input sequences 
to output sequences 
. The mapping is typically represented as a state
machine, with one function 
 used to compute the next hidden internal state given
the input, and another function 
 used to compute the output given the current
hidden state.
It is supervised in the sense that we are told what output sequence to generate for
which input sequence, but the internal functions have to be learned by some
method other than direct supervision, because we don’t know what the hidden
state sequence is.
In reinforcement learning, the goal is to learn a mapping from input values
(typically assumed to be states of an agent or system; for now, think e.g. the velocity
of a moving car) to output values (typically we want control actions; for now, think
e.g. if to accelerate or hit the brake). However, we need to learn the mapping
without a direct supervision signal to specify which output values are best for a
particular input; instead, the learning problem is framed as an agent interacting
with an environment, in the following setting:
The agent observes the current state 
.
It selects an action 
1.1.2.2 Density estimation
x(1), … , x(n) ∈Rd
Pr(X)
Pr(x(n+1))
1.1.2.3 Dimensionality reduction
x(1), … , x(n) ∈RD
d
d < D
1.1.3 Sequence learning
x0, … , xn
y1, … , ym
fs
fo
1.1.4 Reinforcement learning
st
at.
 It receives a reward, 
, which typically depends on 
 and possibly 
.
The environment transitions probabilistically to a new state, 
, with a
distribution that depends only on 
 and 
.
The agent observes the current state, 
.
The goal is to find a policy , mapping  to , (that is, states to actions) such that
some long-term sum or average of rewards  is maximized.
This setting is very different from either supervised learning or unsupervised
learning, because the agent’s action choices affect both its reward and its ability to
observe the environment. It requires careful consideration of the long-term effects of
actions, as well as all of the other issues that pertain to supervised learning.
There are many other problem settings. Here are a few.
In semi-supervised learning, we have a supervised-learning training set, but there
may be an additional set of 
 values with no known 
. These values can still be
used to improve learning performance (if they are drawn from 
 that is the
marginal of 
 that governs the rest of the data set).
In active learning, it is assumed to be expensive to acquire a label 
 (imagine
asking a human to read an x-ray image), so the learning algorithm can sequentially
ask for particular inputs 
 to be labeled, and must carefully select queries in order
to learn as effectively as possible while minimizing the cost of labeling.
In transfer learning (also called meta-learning), there are multiple tasks, with data
drawn from different, but related, distributions. The goal is for experience with
previous tasks to apply to learning a current task in a way that requires decreased
experience with the new task.
1.2 Assumptions
The kinds of assumptions that we can make about the data source or the solution
include:
The data are independent and identically distributed (i.i.d.).
The data are generated by a Markov chain (i.e. outputs only depend only on
the current state, with no additional memory).
The process generating the data might be adversarial.
rt
st
at
st+1
st
at
st+1
…
π
s
a
r
1.1.5 Other settings
x(i)
y(i)
Pr(X)
Pr(X, Y )
y(i)
x(i)
 The “true” model that is generating the data can be perfectly described by one
of some particular set of hypotheses.
The effect of an assumption is often to reduce the “size” or “expressiveness” of the
space of possible hypotheses and therefore reduce the amount of data required to
reliably identify an appropriate hypothesis.
1.3 Evaluation criteria
Once we have specified a problem class, we need to say what makes an output or
the answer to a query good, given the training data. We specify evaluation criteria
at two levels: how an individual prediction is scored, and how the overall behavior
of the prediction or estimation system is scored.
The quality of predictions from a learned model is often expressed in terms of a loss
function. A loss function 
 tells you how much you will be penalized for
making a guess  when the answer is actually . There are many possible loss
functions. Here are some frequently used examples:
0-1 Loss applies to predictions drawn from finite domains.
Squared loss
Absolute loss
Asymmetric loss Consider a situation in which you are trying to predict
whether someone is having a heart attack. It might be much worse to predict
“no” when the answer is really “yes”, than the other way around.
Any given prediction rule will usually be evaluated based on multiple predictions
and the loss of each one. At this level, we might be interested in:
Minimizing expected loss over all the predictions (also known as risk)
Minimizing maximum loss: the loss of the worst prediction
Minimizing or bounding regret: how much worse this predictor performs than
the best one drawn from some class
L(g, a)
g
a
L(g, a) = {0
if g = a
1
otherwise
L(g, a) = (g −a)2
L(g, a) = |g −a|
L(g, a) =
⎧
⎪
⎨
⎪
⎩
1
if g = 1 and a = 0
10
if g = 0 and a = 1
0
otherwise
 Characterizing asymptotic behavior: how well the predictor will perform in the
limit of infinite training data
Finding algorithms that are probably approximately correct: they probably
generate a hypothesis that is right most of the time.
There is a theory of rational agency that argues that you should always select the
action that minimizes the expected loss. This strategy will, for example, make you the
most money in the long run, in a gambling setting. As mentioned above, expected
loss is also sometimes called risk in ML literature, but that term means other things
in economics or other parts of decision theory, so be careful...it’s risky to use it. We
will, most of the time, concentrate on this criterion.
1.4 Model type
Recall that the goal of a ML system is typically to estimate or generalize, based on
data provided. Below, we examine the role of model-making in machine learning.
In some simple cases, in response to queries, we can generate predictions directly
from the training data, without the construction of any intermediate model, or more
precisely, without the learning of any parameters.
For example, in regression or classification, we might generate an answer to a new
query by averaging answers to recent queries, as in the nearest neighbor method.
This two-step process is more typical:
1. “Fit” a model (with some a-prior chosen parameterization) to the training data
2. Use the model directly to make predictions
In the parametric models setting of regression or classification, the model will be
some hypothesis or prediction rule 
 for some functional form . The
term hypothesis has its roots in statistical learning and the scientific method, where
models or hypotheses about the world are tested against real data, and refined with
more evidence, observations, or insights. Note that the parameters themselves are
only part of the assumptions that we’re making about the world. The model itself is
a hypothesis that will be refined with more evidence.
The idea is that 
 is a set of one or more parameter values that will be determined
by fitting the model to the training data and then be held fixed during testing.
Given a new 
, we would then make the prediction 
.
1.4.1 Non-parametric models
1.4.2 Parametric models
y = h(x; Θ)
h
Θ
x(n+1)
h(x(n+1); Θ)
 The fitting process is often articulated as an optimization problem: Find a value of
 that minimizes some criterion involving 
 and the data. An optimal strategy, if
we knew the actual underlying distribution on our data, 
 would be to
predict the value of  that minimizes the expected loss, which is also known as the
test error. If we don’t have that actual underlying distribution, or even an estimate of
it, we can take the approach of minimizing the training error: that is, finding the
prediction rule  that minimizes the average loss on our training data set. So, we
would seek 
 that minimizes
where the loss function 
 measures how bad it would be to make a guess of 
when the actual value is .
We will find that minimizing training error alone is often not a good choice: it is
possible to emphasize fitting the current data too strongly and end up with a
hypothesis that does not generalize well when presented with new  values.
1.5 Model class and parameter fitting
A model class 
 is a set of possible models, typically parameterized by a vector of
parameters 
. What assumptions will we make about the form of the model? When
solving a regression problem using a prediction-rule approach, we might try to find
a linear function 
 that fits our data well. In this example, the
parameter vector 
.
For problem types such as classification, there are huge numbers of model classes
that have been considered...we’ll spend much of this course exploring these model
classes, especially neural networks models. We will almost completely restrict our
attention to model classes with a fixed, finite number of parameters. Models that
relax this assumption are called “non-parametric” models.
How do we select a model class? In some cases, the ML practitioner will have a
good idea of what an appropriate model class is, and will specify it directly. In other
cases, we may consider several model classes and choose the best based on some
objective function. In such situations, we are solving a model selection problem:
model-selection is to pick a model class 
 from a (usually finite) set of possible
model classes, whereas model fitting is to pick a particular model in that class,
specified by (usually continuous) parameters 
.
1.6 Algorithm
Once we have described a class of models and a way of scoring a model given data,
we have an algorithmic problem: what sequence of computational instructions
should we run in order to find a good model from our class? For example,
Θ
Θ
Pr(X, Y )
y
h
Θ
Etrain(h; Θ) = 1
n
n
∑
i=1
L(h(x(i); Θ), y(i)) ,
L(g, a)
g
a
x
M
Θ
h(x; θ, θ0) = θTx + θ0
Θ = (θ, θ0)
M
Θ
 determining the parameter vector which minimizes the training error might be
done using a familiar least-squares minimization algorithm, when the model  is a
function being fit to some data .
Sometimes we can use software that was designed, generically, to perform
optimization. In many other cases, we use algorithms that are specialized for ML
problems, or for particular hypotheses classes. Some algorithms are not easily seen
as trying to optimize a particular criterion. In fact, a historically important method
for finding linear classifiers, the perceptron algorithm, has this character.
h
x
 We had legacy PDF notes that used mixed conventions for data matrices: “each row as a
data point” and “each column as a data point”.
We are standardizing to “each row as a data point.” Thus, 
 aligns with 
 in the PDF
notes if you’ve read those. If you spot inconsistencies or experience any confusion, please
raise an issue. Thanks!
Regression is an important machine-learning problem that provides a good starting
point for diving deeply into the field.
2.1 Problem formulation
A hypothesis  is employed as a model for solving the regression problem, in that it
maps inputs  to outputs ,
where 
 (i.e., a length  column vector of real numbers), and 
 (i.e., a real
number). Real life rarely gives us vectors of real numbers; the  we really want to
take as input is usually something like a song, image, or person. In that case, we’ll
have to define a function 
, whose range is 
, where  represents features of ,
like a person’s height or the amount of bass in a song, and then let the 
. In much of the following, we’ll omit explicit mention of  and assume that the 
are in 
, but you should always have in mind that some additional process was
almost surely required to go from the actual input examples to their feature
representation, and we’ll talk a lot more about features later in the course.
Regression is a supervised learning problem, in which we are given a training dataset
of the form
which gives examples of input values 
 and the output values 
 that should be
associated with them. Because  values are real-valued, our hypotheses will have
the form
This is a good framework when we want to predict a numerical quantity, like
height, stock value, etc., rather than to divide the inputs into discrete categories.
2  Regression
Warning
X
~X
h
x
y
x →
→y ,
h
x ∈Rd
d
y ∈R
x
φ(x)
Rd
φ
x
h : φ(x) →R
φ
x(i)
Rd
Dtrain = {(x(1), y(1)), … , (x(n), y(n))} ,
x(i)
y(i)
y
h : Rd →R .
“Regression,” in common parlance,
means moving backwards. But this
is forward progress!
Real life rarely gives us vectors of
real numbers. The  we really want
to take as input is usually
something like a song, image, or
person. In that case, we’ll have to
define a function 
 whose
range is 
, where  represents
features of  (e.g., a person’s height
or the amount of bass in a song).
x
φ(x)
Rd
φ
x
2  Regression

1
 What makes a hypothesis useful? That it works well on new data—that is, it makes
good predictions on examples it hasn’t seen.
However, we don’t know exactly what data this hypothesis might be tested on in
the real world. So, we must assume a connection between the training data and
testing data. Typically, the assumption is that they are drawn independently from
the same probability distribution.
To make this discussion more concrete, we need a loss function to express how
unhappy we are when we guess an output  given an input  for which the desired
output was .
Given a training set 
 and a hypothesis  with parameters 
, the training error
of  can be defined as the average loss on the training data:
The training error of  gives us some idea of how well it characterizes the
relationship between  and  values in our data, but it isn’t the quantity we most
care about. What we most care about is test error:
on 
 new examples that were not used in the process of finding the hypothesis.
It might be worthwhile to stare at the two errors and think about what’s the difference.
For example, notice how 
 is no longer a variable in the testing error? This is because, in
evaluating the testing error, the parameters will have been “picked” or “fixed” already.
For now, we will try to find a hypothesis with small training error (later, with some
added criteria) and try to make some design choices so that it generalizes well to new
data, meaning that it also has a small test error.
2.2 Regression as an optimization problem
Given data, a loss function, and a hypothesis class, we need a method for finding a
good hypothesis in the class. One of the most general ways to approach this
problem is by framing the machine learning problem as an optimization problem.
One reason for taking this approach is that there is a rich area of math and
algorithms studying and developing efficient methods for solving optimization
g
x
a
Dtrain
h
Θ
h
Etrain(h; Θ) = 1
n
n
∑
i=1
L(h(x(i); Θ), y(i)) .
(2.1)
h
x
y
Etest(h) = 1
n′
n+n′
∑
i=n+1
L(h(x(i)), y(i)) ,
n′
Note
Θ
This process of converting our data
into a numerical form is often
referred to as data pre-processing.
Then  maps 
 to .
In much of the following, we’ll
omit explicit mention of  and
assume that the 
 are in 
.
However, you should always
remember that some additional
process was almost surely required
to go from the actual input
examples to their feature
representation. We will discuss
features more later in the course.
h
φ(x)
R
φ
x(i)
Rd
My favorite analogy is to problem
sets. We evaluate a student’s ability
to generalize by putting questions
on the exam that were not on the
homework (training set).
 problems, and lots of very good software implementations of these methods. So, if
we can turn our problem into one of these problems, then there will be a lot of work
already done for us!
We begin by writing down an objective function 
, where 
 stands for all the
parameters in our model (i.e., all possible choices over parameters). We often write
 to make clear the dependence on the data 
.
The objective function describes how we feel about possible hypotheses 
. We
generally look for parameter values 
 that minimize the objective function:
In the most general case, there is not a guarantee that there exists a unique set of
parameters which minimize the objective function. However, we will ignore that for
now. A very common form for a machine-learning objective is:
The loss measures how unhappy we are about the prediction 
 for the pair
. Minimizing this loss improves prediction accuracy. The regularizer 
is an additional term that encourages the prediction to remain general, and the
constant  adjusts the balance between fitting the training examples and
generalizing to unseen examples. We will discuss this balance and the idea of
regularization further in Section 2.7.
2.3 Linear regression
To make this discussion more concrete, we need to provide a hypothesis class and a
loss function.
We begin by picking a class of hypotheses 
 that might provide a good set of
possible models for the relationship between  and  in our data. We start with a
very simple class of linear hypotheses for regression:
where the model parameters are 
. In one dimension (
), this
corresponds to the familiar slope-intercept form 
 of a line. In two
dimesions (
), this corresponds to a plane. In higher dimensions, this model
describes a hyperplane. This hypothesis class is both simple to study and very
powerful, and will serve as the basis for many other important techniques (even
neural networks!).
J(Θ)
Θ
J(Θ; D)
D
Θ
Θ
Θ∗= arg min
Θ J(Θ) .
J(Θ) =
1
n
n
∑
i=1
L(h(x(i); Θ), y(i))
loss
+
λ
non-negative constant
R(Θ).
⎛
⎜
⎝





⎞
⎟
⎠





(2.2)
h(x(i); Θ)
(x(i), y(i))
R(Θ)
λ
H
x
y
y = h(x; θ, θ0) = θTx + θ0 ,
(2.3)
Θ = (θ, θ0)
d = 1
y = mx + b
d = 2
Don’t be too perturbed by the
semicolon where you expected to
see a comma! It’s a mathematical
way of saying that we are mostly
interested in this as a function of
the arguments before the ; , but we
should remember there’s a
dependence on the stuff after it as
well.
 For now, our objective in linear regression is to find a hypothesis that goes as close
as possible, on average, to all of our training data. We define a loss function to
describe how to evaluate the quality of the predictions our hypothesis is making,
when compared to the “target”  values in the data set. The choice of loss function
is part of modeling your domain. In the absence of additional information about a
regression problem, we typically use squared loss:
where 
 is our “guess” from the hypothesis, or the hypothesis’ prediction,
and  is the “actual” observation (in other words, here  is being used equivalently
as ). With this choice of squared loss, the average loss as generally defined in
Equation 2.1 will become the so-called mean squared error (MSE).
Applying the general optimization framework to the linear regression hypothesis
class of Equation 2.3 with squared loss and no regularization, our objective is to find
values for 
 that minimize the MSE:
resulting in the solution:
For one-dimensional data (
), this corresponds to fitting a line to data. For
, this hypothesis represents a -dimensional hyperplane embedded in a
-dimensional space (the input dimension plus the  dimension).
For example, in the left plot below, we can see data points with labels  and input
dimensions 
 and 
. In the right plot below, we see the result of fitting these
points with a two-dimensional plane that resides in three dimensions. We interpret
the plane as representing a function that provides a  value for any input 
.
y
L(g, a) = (g −a)2 .
g = h(x)
a
a
y
Θ = (θ, θ0)
J(θ, θ0) = 1
n
n
∑
i=1
(θTx(i) + θ0 −y(i))
2
,
(2.4)
θ∗, θ∗
0 = arg min
θ,θ0 J(θ, θ0) .
(2.5)
d = 1
d > 1
d
(d + 1)
y
y
x1
x2
y
(x1, x2)
The squared loss penalizes guesses
that are too high the same amount
as it penalizes guesses that are too
low, and has a good mathematical
justification in the case that your
data are generated from an
underlying linear hypothesis with
the so-called Gaussian-
distributed noise added to the 
values. But there are applications
in which other losses would be
better, and much of the framework
we discuss can be applied to
different loss functions, although
this one has a form that also makes
it particularly computationally
convenient.
We won’t get into the details of
Gaussian distribution in our class;
but it’s one of the most important
distributions and well-worth
studying closely at some point.
One obvious fact about Gaussian is
that it’s symmetric; this is in fact
one of the reasons squared loss
y
 A richer class of hypotheses can be obtained by performing a non-linear feature
transformation before doing the regression, as we will later see, but it will still end
up that we have to solve a linear regression problem.
2.4 A gloriously simple linear regression
algorithm
Okay! Given the objective in Equation 2.4, how can we find good values of  and 
? We’ll study several general-purpose, efficient, interesting algorithms. But before
we do that, let’s start with the simplest one we can think of: guess a whole bunch ( )
of different values of  and 
, see which one has the smallest error on the training set,
and return it.
Algorithm 2.1 Random-Regression
Require: Data , integer 
for 
 to  do
Randomly generate hypothesis 
end for
Let 
return 
This seems kind of silly, but it’s a learning algorithm, and it’s not completely
useless.
❓ Study Question
If your data set has  data points, and the dimension of the  values is , what is
the size of an individual 
?
❓ Study Question
How do you think increasing the number of guesses  will change the training
error of the resulting hypothesis?
2.5 Analytical solution: ordinary least squares
One very interesting aspect of the problem of finding a linear hypothesis that
minimizes mean squared error is that we can find a closed-form formula for the
answer! This general problem is often called the ordinary least squares (ols).
Everything is easier to deal with if we first ignore the offset 
. So, suppose for now,
we have, simply,
θ
θ0
k
θ
θ0
D
k
1:
i = 1
k
2:
θi, θ0(i)
3:
4:
i = arg minj J(θ(j), θ0(j); D)
5:
θ(i), θ0(i)
n
x
d
θ(i)
k
θ0
y = θTx .
(2.6)
works well under Gaussian
settings, as the loss is also
symmetric.
this corresponds to a hyperplane
that goes through the origin.
 In this case, the objective becomes
We approach this just like a minimization problem from calculus homework: take
the derivative of  with respect to , set it to zero, and solve for . There are
additional steps required, to check that the resulting  is a minimum (rather than a
maximum or an inflection point) but we won’t work through that here. It is possible
to approach this problem by:
Finding 
 for  in 
,
Constructing a set of  equations of the form 
, and
Solving the system for values of 
.
That works just fine. To get practice for applying techniques like this to more
complex problems, we will work through a more compact (and cool!) matrix view.
Along the way, it will be helpful to collect all of the derivatives in one vector. In
particular, the gradient of  with respect to  is following column vector of length :
❓ Study Question
Work through the next steps and check your answer against ours below.
We can think of our training data in terms of matrices 
 and 
, where each row of
 is an example, and each row (or rather, element) of 
 is the corresponding target
output value:
❓ Study Question
What are the dimensions of 
 and 
?
J(θ) = 1
n
n
∑
i=1
(θTx(i) −y(i))
2
.
(2.7)
J
θ
θ
θ
∂J/∂θk
k
1, … , d
k
∂J/∂θk = 0
θk
J
θ
d
∇θJ =
.
⎡
⎢
⎣
∂J/∂θ1
⋮
∂J/∂θd
⎤
⎥
⎦
X
Y
X
Y
X =
Y =
.
⎡
⎢
⎣
x(1)
1
…
x(1)
d
⋮
⋱
⋮
x(n)
1
…
x(n)
d
⎤
⎥
⎦
⎡
⎢
⎣
y(1)
⋮
y(n)
⎤
⎥
⎦
X
Y
 Now we can write
and using facts about matrix/vector calculus, we get
Setting this equal to zero and solving for  yields the final closed-form solution:
and the dimensions work out! So, given our data, we can directly compute the
linear regression that minimizes mean squared error. That’s pretty awesome!
Now, how do we deal with the offset? We augment the original feature vector with
a “fake” feature of value 1, and add a corresponding parameter 
 to the  vector.
That is, we define columns vectors 
 such that,
where the “aug” denotes that 
 have been augmented.
Then we can now write the linear hypothesis as if there is no offset,
We can do this “appending a fake feature of 1” to all data points to form the
augmented data matrix 
where  as an -by  vector of all one. Then use the formula in Equation 2.8 to find
the 
 that minimizes the mean squared error.
J(θ) = 1
n
n
∑
i=1
(θTx(i) −y(i))2 = 1
n (Xθ −Y )T(Xθ −Y ).
∇θJ(θ) = 1
n ∇θ [(Xθ)TXθ −Y TXθ −(Xθ)TY + Y TY ]
= 2
n (XTXθ −XTY ).
θ
θ∗= (XTX)
−1XTY
(2.8)
θ0
θ
xaug, θaug ∈Rd+1
xaug =
,
θaug =
⎡
⎢
⎣
x1
x2
⋮
xd
1
⎤
⎥
⎦
⎡
⎢
⎣
θ1
θ2
⋮
θd
θ0
⎤
⎥
⎦
θ, x
y = h(xaug; θaug) = θT
augxaug
(2.9)
Xaug
Xaug =
= [
]
⎡
⎢
⎣
x(1)
1
…
x(1)
d
1
⋮
⋱
⋮
⋮
x
(n)
1
…
x
(n)
d
1
⎤
⎥
⎦
X
𝟙
𝟙
n
1
θaug
See Appendix A if you need some
help finding this gradient.
Here are two related alternate
angles to view this formula, for
intuition’s sake:
1. Note that
 is the
pseudo-inverse of 
. Thus, 
“pseudo-solves” 
(multiply both sides of this on
the left by 
).
2. Note that
is the projection matrix onto
the column space of 
. Thus,
 solves 
.
(X TX)−1X T = X +
:
X
θ∗
Xθ = Y
X +
X(X TX)−1X T = projcol(X)
X
θ∗
Xθ = projcol(X)Y
This is a very special case where
we can find the solution in closed
form. In general, we will need to
use iterative optimization
algorithms to find the best
parameters. Also, this process of
setting the graident/derivatives to
zero and solving for the
parameters works out in this
problem. But there can be
exceptions to this rule, and we will
discuss them later in the course.
But of course, the constant offset is
not really gone, it’s just hidden in
the augmentation.
 ❓ Study Question
Stop and prove to yourself that adding that extra feature with value 1 to every
input vector and getting rid of the 
 parameter, as done in Equation 2.9 is
equivalent to our original model Equation 2.3.
2.6 Centering
In fact, augmenting a “fake” feature of 1, as described above, is also useful for an
important idea: namely, why utilizing the so-called centering eliminates the need
for fitting an intercept, and thereby offers an alternative way to avoid dealing with
 directly.
By centering, we mean subtracting the average (mean) of each feature from all data
points, and we apply the same operation to the labels. For an example of a dataset
before and after centering, see here
The idea is that, with centered dataset, even if we were to search for an offset term
, it would naturally fall out to be 0. Intuitively, this makes sense – if a dataset is
centered around the origin, it seems natural that the best fitting plane would go
through the origin.
Let’s see how this works out mathematically. First, for a centered dataset, two claims
immediately follow (recall that  is an -by-1 vector of all ones):
1. Each column of 
 sums up to zero, that is, 
.
2. Similarly, the mean of the labels is 0, so 
.
Recall that our ultimate goal is to find an optimal fitting hyperplane, parameterized
by  and 
. In other words, we aim to find 
 which at this point, involves
simply plugging 
 into Equation 2.8.
θ0
θ0
θ0
𝟙
n
X
XT𝟙= 0
Y T𝟙= 𝟙TY = 0
θ
θ0
θaug,
Xaug = [
]
X
𝟙
1
 Indeed, the optimal 
 naturally falls out to be 0.
2.7 Regularization
The objective function of Equation 2.2 balances (training-data) memorization,
induced by the loss term, with generalization, induced by the regularization term.
Here, we address the need for regularization specifically for linear regression, and
show how this can be realized using one popular regularization technique called
ridge regression.
If all we cared about was finding a hypothesis with small loss on the training data,
we would have no need for regularization, and could simply omit the second term
in the objective. But remember that our ultimate goal is to perform well on input
values that we haven’t trained on! It may seem that this is an impossible task, but
humans and machine-learning methods do this successfully all the time. What
allows generalization to new input values is a belief that there is an underlying
regularity that governs both the training and testing data. One way to describe an
assumption about such a regularity is by choosing a limited class of possible
hypotheses. Another way to do this is to provide smoother guidance, saying that,
within a hypothesis class, we prefer some hypotheses to others. The regularizer
articulates this preference and the constant  says how much we are willing to trade
off loss on the training data versus preference over hypotheses.
For example, consider what happens when 
 and 
 is highly correlated with
, meaning that the data look like a line, as shown in the left panel of the figure
below. Thus, there isn’t a unique best hyperplane. Such correlations happen often in
real-life data, because of underlying common causes; for example, across a
population, the height of people may depend on both age and amount of food
θ∗
aug = ([
] [
])
−1
[
]Y
= [
]
−1
[
]Y
= [
]
−1
[
]Y
= [
]
−1
[
]Y
= [
]
= [
]
= [
]
XT
𝟙T
X
𝟙
XT
𝟙T
XTX
XT𝟙
𝟙TX
𝟙T𝟙
XT
𝟙T
XTX
XT𝟙
𝟙TX
𝟙T𝟙
XT
𝟙T
XTX
0
0
n
XT
𝟙T
(XTX)−1XTY
n𝟙TY
(XTX)−1XTY
0
θ∗
θ∗
0
θ0
2.7.1 Regularization and linear regression
λ
d = 2,
x2
x1
 intake in the same way. This is especially the case when there are many feature
dimensions used in the regression. Mathematically, this leads to 
 close to
singularity, such that 
 is undefined or has huge values, resulting in
unstable models (see the middle panel of figure and note the range of the  values—
the slope is huge!):
A common strategy for specifying a regularizer is to use the form
when we have some idea in advance that 
 ought to be near some value 
.
Here, the notion of distance is quantified by squaring the  norm of the parameter
vector: for any -dimensional vector 
 the  norm of  is defined as,
In the absence of such knowledge a default is to regularize toward zero:
When this is done in the example depicted above, the regression model becomes
stable, producing the result shown in the right-hand panel in the figure. Now the
slope is much more sensible.
There are some kinds of trouble we can get into in regression problems. What if
 is not invertible?
Another kind of problem is overfitting: we have formulated an objective that is just
about fitting the data as well as possible, but we might also want to regularize to
keep the hypothesis from getting too attached to the data.
We address both the problem of not being able to invert 
 and the problem
of overfitting using a mechanism called ridge regression. We add a regularization
term 
 to the OLS objective, with a non-negative scalar value  to control the
XTX
(XTX)−1
y
R(Θ) = ∥Θ −Θprior∥2
Θ
Θprior
l2
d
v ∈Rd,
l2
v
∥v∥=
d
∑
i=1
|vi|2 .


⎷
R(Θ) = ∥Θ∥2 .
2.7.2 Ridge regression
(XTX)
(XTX)−1
∥θ∥2
λ
 tradeoff between the training error and the regularization term. Here is the ridge
regression objective function:
Larger  values (in magnitude) pressure  values to be near zero.
Note that, when data isn’t centered, we don’t penalize 
; intuitively, 
 is what
“floats” the regression surface to the right level for the data you have, and so we
shouldn’t make it harder to fit a data set where the  values tend to be around one
million than one where they tend to be around one. The other parameters control
the orientation of the regression surface, and we prefer it to have a not-too-crazy
orientation.
There is an analytical expression for the 
 values that minimize 
, even when
the data isn’t centered, but it’s a more complicated to derive than the solution for
OLS, even though the process is conceptually similar: taking the gradient, setting it
to zero, and solving for the parameters.
The good news is, when the dataset is centered, we again have very clean set up and
derivation. In particular, the objective can be written as:
and the solution is:
One other great news is that in Equation 2.13, the matrix we are trying to invert can
always be inverted! Why is the term 
 invertible? Explaining this
requires some linear algebra. The matrix 
 is positive semidefinite, which
implies that its eigenvalues 
 are greater than or equal to 0. The matrix
 has eigenvalues 
 which are guaranteed to be strictly
positive since 
. Recalling that the determinant of a matrix is simply the
product of its eigenvalues, we get that 
 and conclude that
 is invertible.
2.8 Evaluating learning algorithms
Jridge(θ, θ0) = 1
n
n
∑
i=1
(θTx(i) + θ0 −y(i))
2
+ λ∥θ∥2
(2.10)
λ
θ
θ0
θ0
y
θ, θ0
Jridge
Jridge(θ) = 1
n
n
∑
i=1
(θTx(i) −y(i))
2
+ λ∥θ∥2
(2.11)
θridge = (XTX + nλI)
−1XTY
(2.12)
Derivation of the Ridge Regression Solution for Centered Data Set
(XTX + nλI)
XTX
{γi}i
XTX + nλI
{γi + nλ}i
λ > 0
det(XTX + nλI) > 0
XTX + nλI
Compare Equation 2.10 and
Equation 2.11. What is the
difference between the two? How
is it possible to drop the offset
here?
 In this section, we will explore how to evaluate supervised machine-learning
algorithms. We will study the special case of applying them to regression problems,
but the basic ideas of validation, hyper-parameter selection, and cross-validation
apply much more broadly.
We have seen how linear regression is a well-formed optimization problem, which
has an analytical solution when ridge regularization is applied. But how can one
choose the best amount of regularization, as parameterized by ? Two key ideas
involve the evaluation of the performance of a hypothesis, and a separate
evaluation of the algorithm used to produce hypotheses, as described below.
The performance of a given hypothesis  may be evaluated by measuring test error
on data that was not used to train it. Given a training set 
 a regression
hypothesis , and if we choose squared loss, we can define the OLS training error of
 to be the mean square error between its predictions and the expected outputs:
Test error captures the performance of  on unseen data, and is the mean square
error on the test set, with a nearly identical expression as that above, differing only
in the range of index :
on 
 new examples that were not used in the process of constructing .
In machine learning in general, not just regression, it is useful to distinguish two
ways in which a hypothesis 
 might contribute to test error. Two are:
Structural error: This is error that arises because there is no hypothesis 
 that
will perform well on the data, for example because the data was really generated by
a sine wave but we are trying to fit it with a line.
Estimation error: This is error that arises because we do not have enough data (or
the data are in some way unhelpful) to allow us to choose a good 
, or because
we didn’t solve the optimization problem well enough to find the best  given the
data that we had.
When we increase , we tend to increase structural error but decrease estimation
error, and vice versa.
Note that this section is relevant to learning algorithms generally—we are just introducing
the topic here since we now have an algorithm that can be evaluated!
λ
2.8.1 Evaluating hypotheses
h
Dn,
h
h
Etrain(h) = 1
n
n
∑
i=1
[h(x(i)) −y(i)]
2
.
h
i
Etest(h) = 1
n′
n+n′
∑
i=n+1
[h(x(i)) −y(i)]
2
n′
h
h ∈H
h ∈H
h ∈H
h
λ
2.8.2 Evaluating learning algorithms
 A learning algorithm is a procedure that takes a data set 
 as input and returns an
hypothesis  from a hypothesis class 
; it looks like
Keep in mind that  has parameters. The learning algorithm itself may have its own
parameters, and such parameters are often called hyperparameters. The analytical
solutions presented above for linear regression, e.g., Equation 2.12, may be thought
of as learning algorithms, where  is a hyperparameter that governs how the
learning algorithm works and can strongly affect its performance.
How should we evaluate the performance of a learning algorithm? This can be
tricky. There are many potential sources of variability in the possible result of
computing test error on a learned hypothesis :
Which particular training examples occurred in 
Which particular testing examples occurred in 
Randomization inside the learning algorithm itself
Generally, to evaluate how well a learning algorithm works, given an unlimited data
source, we would like to execute the following process multiple times:
Train on a new training set (subset of our big data source)
Evaluate resulting  on a validation set that does not overlap the training set
(but is still a subset of our same big data source)
Running the algorithm multiple times controls for possible poor choices of training
set or unfortunate randomization inside the algorithm itself.
One concern is that we might need a lot of data to do this, and in many applications
data is expensive or difficult to acquire. We can re-use data with cross validation (but
it’s harder to do theoretical analysis).
Algorithm 2.1 Cross-Validate
Require: Data , integer 
Divide  into  chunks 
 (of roughly equal size)
for 
 to  do
Train 
 on 
 (withholding chunk 
 as the validation set)
Compute "test" error 
 on withheld data 
end for
return 
It’s very important to understand that (cross-)validation neither delivers nor
evaluates a single particular hypothesis . It evaluates the learning algorithm that
produces hypotheses.
Dn
h
H
Dtrain ⟶
⟶h
learning alg (H)
h
λ
h
Dtrain
Dtest
2.8.2.1 Validation
h
2.8.2.2 Cross validation
D
k
1:
D
k
D1, D2, … , Dk
2:
i = 1
k
3:
hi
D ∖Di
Di
4:
Ei(hi)
Di
5:
6:
1
k ∑k
i=1 Ei(hi)
h
 The hyper-parameters of a learning algorithm affect how the algorithm works but
they are not part of the resulting hypothesis. So, for example,  in ridge regression
affects which hypothesis will be returned, but  itself doesn’t show up in the
hypothesis (the hypothesis is specified using parameters  and 
).
You can think about each different setting of a hyper-parameter as specifying a
different learning algorithm.
In order to pick a good value of the hyper-parameter, we often end up just trying a
lot of values and seeing which one works best via validation or cross-validation.
❓ Study Question
How could you use cross-validation to decide whether to use analytic ridge
regression or our random-regression algorithm and to pick  for random
regression or  for ridge regression?
2.8.2.3 Hyperparameter tuning
λ
λ
θ
θ0
k
λ
 This page contains all content from the legacy PDF notes; gradient descent chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
In the previous chapter, we showed how to describe an interesting objective
function for machine learning, but we need a way to find the optimal
, particularly when the objective function is not amenable to
analytical optimization. For example, this can be the case when 
 involves a
more complex loss function, or more general forms of regularization. It can also be
the case when there are simply too many parameters to learn for it to be
computationally feasible.
There is an enormous and fascinating literature on the mathematical and
algorithmic foundations of optimization, but for this class, we will consider one of
the simplest methods, called gradient descent.
Intuitively, in one or two dimensions, we can easily think of 
 as defining a
surface over 
; that same idea extends to higher dimensions. Now, our objective is
to find the 
 value at the lowest point on that surface. One way to think about
gradient descent is that you start at some arbitrary point on the surface, look to see
in which direction the “hill” goes down most steeply, take a small step in that
direction, determine the direction of steepest descent from where you are, take
another small step, etc.
Below, we explicitly give gradient descent algorithms for one and multidimensional
objective functions (Section 3.1 and Section 3.2). We then illustrate the application of
gradient descent to a loss function which is not merely mean squared loss
(Section 3.3). And we present an important method known as stochastic gradient
descent (Section 3.4), which is especially useful when datasets are too large for
descent in a single batch, and has some important behaviors of its own.
3.1 Gradient descent in one dimension
We start by considering gradient descent in one dimension. Assume 
, and
that we know both 
 and its first derivative with respect to 
, 
. Here is
pseudo-code for gradient descent on an arbitrary function . Along with  and its
gradient 
 (which, in the case of a scalar 
, is the same as its derivative 
), we
have to specify some hyper-parameters. These hyper-parameters include the initial
value for parameter 
, a step-size hyper-parameter , and an accuracy hyper-
parameter  .
3  Gradient Descent
Note
Θ∗= arg minΘ J(Θ)
J(Θ)
J(Θ)
Θ
Θ
Θ ∈R
J(Θ)
Θ J ′(Θ)
f
f
∇Θf
Θ
f ′
Θ
η
ϵ
You might want to consider
studying optimization some day!
It’s one of the fundamental tools
enabling machine learning, and it’s
a beautiful and deep field.
3  Gradient Descent

 The hyper-parameter  is often called learning rate when gradient descent is applied
in machine learning. For simplicity,  may be taken as a constant, as is the case in
the pseudo-code below; and we’ll see adaptive (non-constant) step-sizes soon.
What’s important to notice though, is that even when  is constant, the actual
magnitude of the change to 
 may not be constant, as that change depends on the
magnitude of the gradient itself too.
procedure 1D-Gradient-Descent(
)
repeat
until 
return 
end procedure
Note that this algorithm terminates when the derivative of the function  is
sufficiently small. There are many other reasonable ways to decide to terminate,
including:
Stop after a fixed number of iterations , i.e., when 
. Practically, this is the
most common choice.
Stop when the change in the value of the parameter 
 is sufficiently small,
i.e., when 
.
❓ Study Question
Consider all of the potential stopping criteria for 1D-Gradient-Descent , both
in the algorithm as it appears and listed separately later. Can you think of ways
that any two of the criteria relate to each other?
Theorem 3.1 Choose any small distance 
. If we assume that  has a minimum, is
sufficiently “smooth” and convex, and if the learning rate  is sufficiently small, gradient
descent will reach a point within  of a global optimum point 
.
However, we must be careful when choosing the learning rate to prevent slow
convergence, non-converging oscillation around the minimum, or divergence.
The following plot illustrates a convex function 
, starting gradient
descent at 
 with a step-size of 
. It is very well-behaved!
η
η
η
Θ
1:
Θinit, η, f, f ′, ϵ
2:
Θ(0) ←Θinit
3:
t ←0
4:
5:
t ←t + 1
6:
Θ(t) = Θ(t−1) −η f ′(Θ(t−1))
7:
|f ′(Θ(t))| < ϵ
8:
Θ(t)
9:
f
T
t = T
Θ
Θ(t) −Θ(t−1) < ϵ
∣∣
~ϵ > 0
f
η
~ϵ
Θ
f(x) = (x −2)2
xinit = 4.0
1/2
 −1
1
2
3
4
5
6
2
4
x
f(x)
If  is non-convex, where gradient descent converges to depends on 
. First, let’s
establish some definitions. Let  be a real-valued function defined over some
domain 
. A point 
 is called a global minimum point of  if 
 for
all other 
. A point 
 is instead called a local minimum point of a function
 if there exists some constant 
 such that for all  within the interval defined
by 
 
, where  is some distance metric, e.g.,
 A global minimum point is also a local minimum point, but a
local minimum point does not have to be a global minimum point.
❓ Study Question
What happens in this example with very small ? With very big ?
If  is non-convex (and sufficiently smooth), one expects that gradient descent (run
long enough with small enough learning rate) will get very close to a point at which
the gradient is zero, though we cannot guarantee that it will converge to a global
minimum point.
There are two notable exceptions to this common sense expectation: First, gradient
descent can get stagnated while approaching a point  which is not a local
minimum or maximum, but satisfies 
. For example, for 
, starting
gradient descent from the initial guess 
, while using learning rate 
will lead to 
 converging to zero as 
. Second, there are functions (even
convex ones) with no minimum points, like 
, for which gradient
descent with a positive learning rate converges to 
.
The plot below shows two different 
, and how gradient descent started from
each point heads toward two different local optimum points.
f
xinit
f
D
x0 ∈D
f
f(x0) ≤f(x)
x ∈D
x0 ∈D
f
ϵ > 0
x
d(x, x0) < ϵ, f(x0) ≤f(x)
d
d(x, x0) = ||x −x0||.
η
η
f
x
f ′(x) = 0
f(x) = x3
xinit = 1
η < 1/3
x(k)
k →∞
f(x) = exp(−x)
+∞
xinit
 −2
−1
1
2
3
4
4
6
8
10
x
f(x)
3.2 Multiple dimensions
The extension to the case of multi-dimensional 
 is straightforward. Let’s assume
, so 
.
The gradient of  with respect to 
 is
The algorithm remains the same, except that the update step in line 5 becomes
and any termination criteria that depended on the dimensionality of 
 would have
to change. The easiest thing is to keep the test in line 6 as 
,
which is sensible no matter the dimensionality of 
.
❓ Study Question
Which termination criteria from the 1D case were defined in a way that assumes
 is one dimensional?
3.3 Application to regression
Θ
Θ ∈Rm
f : Rm →R
f
Θ
∇Θf =
⎡
⎢
⎣
∂f/∂Θ1
⋮
∂f/∂Θm
⎤
⎥
⎦
Θ(t) = Θ(t−1) −η∇Θf(Θ(t−1))
Θ
f(Θ(t)) −f(Θ(t−1)) < ϵ
∣∣
Θ
Θ
 Recall from the previous chapter that choosing a loss function is the first step in
formulating a machine-learning problem as an optimization problem, and for
regression we studied the mean square loss, which captures losws as
. This leads to the ordinary least squares objective
We use the gradient of the objective with respect to the parameters,
to obtain an analytical solution to the linear regression problem. Gradient descent
could also be applied to numerically compute a solution, using the update rule
Now, let’s add in the regularization term, to get the ridge-regression objective:
 
Recall that in ordinary least squares, we finessed handling 
 by adding an extra
dimension of all 1’s. In ridge regression, we really do need to separate the
parameter vector  from the offset 
, and so, from the perspective of our general-
purpose gradient descent method, our whole parameter set 
 is defined to be
. We will go ahead and find the gradients separately for each one:
Note that 
 will be of shape 
 and 
 will be a scalar since we
have separated 
 from  here.
❓ Study Question
(guess −actual)2
J(θ) = 1
n
n
∑
i=1
(θTx(i) −y(i))
2
.
∇θJ = 2
n XT
d×n
(Xθ −Y )
n×1
,









(3.1)
θ(t) = θ(t−1) −η 2
n
n
∑
i=1
([θ(t−1)]
T
x(i) −y(i))x(i) .
3.3.1 Ridge regression
Jridge(θ, θ0) = 1
n
n
∑
i=1
(θTx(i) + θ0 −y(i))
2
+ λ∥θ∥2 .
θ0
θ
θ0
Θ
Θ = (θ, θ0)
∇θJridge(θ, θ0) = 2
n
n
∑
i=1
(θTx(i) + θ0 −y(i))x(i) + 2λθ
∂Jridge(θ, θ0)
∂θ0
= 2
n
n
∑
i=1
(θTx(i) + θ0 −y(i)) .
∇θJridge
d × 1
∂Jridge/∂θ0
θ0
θ
 Convince yourself that the dimensions of all these quantities are correct, under
the assumption that  is 
. How does  relate to 
 as discussed for 
 in the
previous section?
❓ Study Question
Compute 
 by finding the vector of partial derivatives
. What is the shape of 
?
❓ Study Question
Compute 
 by finding the vector of partial derivatives
.
❓ Study Question
Use these last two results to verify our derivation above.
Putting everything together, our gradient descent algorithm for ridge regression
becomes
procedure RR-Gradient-Descent(
)
repeat
until 
return 
end procedure
❓ Study Question
Is it okay that  doesn’t appear in line 8?
❓ Study Question
Is it okay that the 2’s from the gradient definitions don’t appear in the
algorithm?
θ
d × 1
d
m
Θ
∇θ||θ||2
(∂||θ||2/∂θ1, … , ∂||θ||2/∂θd)
∇θ||θ||2
∇θJridge(θTx + θ0, y)
(∂Jridge(θTx + θ0, y)/∂θ1, … , ∂Jridge(θTx + θ0, y)/∂θd)
1:
θinit, θ0init, η, ϵ
2:
θ(0) ←θinit
3:
θ(0)
0
←θ0init
4:
t ←0
5:
6:
t ←t + 1
7:
θ(t) = θ(t−1) −η ( 1
n ∑n
i=1 (θ(t−1)Tx(i) + θ0
(t−1) −y(i))x(i) + λθ(t−1))
8:
θ(t)
0 = θ(t−1)
0
−η ( 1
n ∑n
i=1 (θ(t−1)Tx(i) + θ0(t−1) −y(i)))
9:
Jridge(θ(t), θ(t)
0 ) −Jridge(θ(t−1), θ(t−1)
0
) < ϵ
∣∣
10:
θ(t), θ(t)
0
11:
λ
Beware double superscripts! 
 is
the transpose of the vector .
[θ]T
θ
 3.4 Stochastic gradient descent
When the form of the gradient is a sum, rather than take one big(ish) step in the
direction of the gradient, we can, instead, randomly select one term of the sum, and
take a very small step in that direction. This seems sort of crazy, but remember that
all the little steps would average out to the same direction as the big step if you
were to stay in one place. Of course, you’re not staying in that place, so you move,
in expectation, in the direction of the gradient.
Most objective functions in machine learning can end up being written as an
average over data points, in which case, stochastic gradient descent (sgd) is
implemented by picking a data point randomly out of the data set, computing the
gradient as if there were only that one point in the data set, and taking a small step
in the negative direction.
Let’s assume our objective has the form
where  is the number of data points used in the objective (and this may be
different from the number of points available in the whole data set).
Here is pseudocode for applying sgd to such an objective ; it assumes we know the
form of 
 for all  in 
:
procedure Stochastic-Gradient-Descent(
)
for 
 do
randomly select 
end for
end procedure
Note that now instead of a fixed value of ,  is indexed by the iteration of the
algorithm, . Choosing a good stopping criterion can be a little trickier for sgd than
traditional gradient descent. Here we’ve just chosen to stop after a fixed number of
iterations .
For sgd to converge to a local optimum point as  increases, the learning rate has to
decrease as a function of time. The next result shows one learning rate sequence that
works.
Theorem 3.2 If  is convex, and 
 is a sequence satisfying
f(Θ) = 1
n
n
∑
i=1
fi(Θ) ,
n
f
∇Θfi
i
1 … n
1:
Θinit, η, f, ∇Θf1, . . . , ∇Θfn, T
2:
Θ(0) ←Θinit
3:
t ←1
4:
i ∈{1, 2, … , n}
5:
Θ(t) = Θ(t−1) −η(t) ∇Θfi(Θ(t−1))
6:
7:
η η
t
T
t
f
η(t)
∞
∑
t=1
η(t) = ∞and
∞
∑
t=1
η(t)2 < ∞,
Sometimes you will see that the
objective being written as a sum,
instead of an average. In the “sum”
convention, the 
 normalizing
constant is getting “absorbed” into
individual 
.
1
n
fi
f(Θ) =
n
∑
i=1
fi(Θ) .
 then SGD converges with probability one* to the optimal 
.*
Why these two conditions? The intuition is that the first condition, on 
, is
needed to allow for the possibility of an unbounded potential range of exploration,
while the second condition, on 
, ensures that the learning rates get smaller
and smaller as  increases.
One “legal” way of setting the learning rate is to make 
 but people often
use rules that decrease more slowly, and so don’t strictly satisfy the criteria for
convergence.
❓ Study Question
If you start a long way from the optimum, would making 
 decrease more
slowly tend to make you move more quickly or more slowly to the optimum?
There are multiple intuitions for why sgd might be a better choice algorithmically
than regular gd (which is sometimes called batch gd (bgd)):
bgd typically requires computing some quantity over every data point in a data
set. sgd may perform well after visiting only some of the data. This behavior
can be useful for very large data sets – in runtime and memory savings.
If your  is actually non-convex, but has many shallow local optimum points
that might trap bgd, then taking samples from the gradient at some point 
might “bounce” you around the landscape and away from the local optimum
points.
Sometimes, optimizing  really well is not what we want to do, because it
might overfit the training set; so, in fact, although sgd might not get lower
training error than bgd, it might result in lower test error.
Θ
∑η(t)
∑η(t)2
t
η(t) = 1/t
η(t)
f
Θ
f
 This page contains all content from the legacy PDF notes; classification chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
4.1 Classification
Classification is a machine learning problem seeking to map from inputs 
 to
outputs in an unordered set.
Examples of classification output sets could be 
 if we’re
trying to figure out what type of fruit we have, or 
 if
we’re working in an emergency room and trying to give the best medical care to a
new patient. We focus on an essential simple case, binary classification, where we aim
to find a mapping from 
 to two outputs. While we should think of the outputs as
not having an order, it’s often convenient to encode them as 
. As before, let
the letter  (for hypothesis) represent a classifier, so the classification process looks
like:
Like regression, classification is a supervised learning problem, in which we are given
a training data set of the form
We will assume that each 
 is a 
 column vector. The intended use of this data
is that, when given an input 
, the learned hypothesis should generate output 
.
What makes a classifier useful? As in regression, we want it to work well on new
data, making good predictions on examples it hasn’t seen. But we don’t know
exactly what data this classifier might be tested on when we use it in the real world.
So, we have to assume a connection between the training data and testing data;
typically, they are drawn independently from the same probability distribution.
In classification, we will often use 0-1 loss for evaluation (as discussed in
Section 1.3). For that choice, we can write the training error and the testing error. In
particular, given a training set 
 and a classifier , we define the training error of 
to be
4  Classification
Note
Rd
{apples, oranges, pears}
{heart attack, no heart attack}
Rd
{+1, 0}
h
x →
→y .
h
Dtrain = {(x(1), y(1)), … , (x(n), y(n))} .
x(i)
d × 1
x(i)
y(i)
Dn
h
h
This is in contrast to a continuous
real-valued output, as we saw for
linear regression.
4  Classification

3
 For now, we will try to find a classifier with small training error (later, with some
added criteria) and hope it generalizes well to new data, and has a small test error
on 
 new examples that were not used in the process of finding the classifier.
We begin by introducing the hypothesis class of linear classifiers (Section 4.2) and
then define an optimization framework to learn linear logistic classifiers (Section 4.3).
4.2 Linear classifiers
We start with the hypothesis class of linear classifiers. They are (relatively) easy to
understand, simple in a mathematical sense, powerful on their own, and the basis
for many other more sophisticated methods. Following their definition, we present
a simple learning algorithm for classifiers.
A linear classifier in  dimensions is defined by a vector of parameters 
 and
scalar 
. So, the hypothesis class 
 of linear classifiers in  dimensions is
parameterized by the set of all vectors in 
. We’ll assume that  is a 
column vector.
Given particular values for  and 
, the classifier is defined by
Remember that we can think of 
 as specifying a -dimensional hyperplane
(compare the above with Equation 2.3). But this time, rather than being interested in
that hyperplane’s values at particular points , we will focus on the separator that it
induces. The separator is the set of  values such that 
. This is also a
hyperplane, but in 
 dimensions! We can interpret  as a vector that is
perpendicular to the separator. (We will also say that  is normal to the separator.)
Below is an embedded demo illustrating the separator and normal vector. Open
demo in full screen.
Etrain(h) = 1
n
n
∑
i=1
{
.
1
h(x(i)) ≠y(i)
0
otherwise
(4.1)
Etest(h) = 1
n′
n+n′
∑
i=n+1
{1
h(x(i)) ≠y(i)
0
otherwise
n′
4.2.1 Linear classifiers: definition
d
θ ∈Rd
θ0 ∈R
H
d
Rd+1
θ
d × 1
θ
θ0
h(x; θ, θ0) = step(θTx + θ0) = {
.
+1
if θTx + θ0 > 0
0
otherwise
θ, θ0
d
x
x
θTx + θ0 = 0
d −1
θ
θ
Demo: Linear classifier separator
 θ₁:
0.5
θ₂:
0.5
θ₀:
0.0
Toggle z=0
Surface
Built with ❤️ by Shen² | Report a Bug
Features (x₁, x₂) & z = θ₁x₁ + θ₂x₂ + θ₀
−2
−1
0
1
2
−5
0
5
Separator
Normal vecto
Prediction: P
Prediction: N
Feature space (x₁, x₂
x₁
x₂
 
For example, in two dimensions (
) the separator has dimension 1, which
means it is a line, and the two components of 
 give the orientation of
the separator, as illustrated in the following example.
Let  be the linear classifier defined by 
. The diagram below shows the 
vector (in green) and the separator it defines:
d = 2
θ = [θ1, θ2]T
4.2.2 Linear classifiers: examples
Example:
h
θ = [
], θ0 = 1
1
−1
θ
 θTx + θ0 = 0
x1
x2
θ
θ2
θ1
What is 
? We can solve for it by plugging a point on the line into the equation for the
line. It is often convenient to choose a point on one of the axes, e.g., in this case,
, for which 
, giving 
.
In this example, the separator divides 
, the space our 
 points live in, into two
half-spaces. The one that is on the same side as the normal vector is the positive half-
space, and we classify all points in that space as positive. The half-space on the
other side is negative and all points in it are classified as negative.
Note that we will call a separator a linear separator of a data set if all of the data with
one label falls on one side of the separator and all of the data with the other label
falls on the other side of the separator. For instance, the separator in the next
example is a linear separator for the illustrated data. If there exists a linear separator
on a dataset, we call this dataset linearly separable.
Let  be the linear classifier defined by 
.
The diagram below shows several points classified by . In particular, let 
 and
.
θ0
x = [0, 1]T
θT [ ] + θ0 = 0
0
1
θ0 = 1
Rd
x(i)
Example:
h
θ = [
], θ0 = 3
−1
1.5
h
x(1) = [ ]
3
2
x(2) = [
]
4
−1
(
[ ]
)
 Thus, 
 and 
 are given positive (label +1) and negative (label 0) classifications,
respectively.
❓ Study Question
What is the green vector normal to the separator? Specify it as a column vector.
❓ Study Question
What change would you have to make to 
 if you wanted to have the
separating hyperplane in the same place, but to classify all the points labeled ‘+’
in the diagram as negative and all the points labeled ‘-’ in the diagram as
positive?
4.3 Linear logistic classifiers
Given a data set and the hypothesis class of linear classifiers, our goal will be to find
the linear classifier that optimizes an objective function relating its predictions to
the training data. To make this problem computationally reasonable, we will need
to take care in how we formulate the optimization problem to achieve this goal.
For classification, it is natural to make predictions in 
 and use the 0-1 loss
function, 
, as introduced in Chapter 1:
h(x(1); θ, θ0) = step ([
] [ ] + 3) = step(3) = +1
h(x(2); θ, θ0) = step ([
] [
] + 3) = step(−2.5) = 0
−1
1.5
3
2
−1
1.5
4
−1
x(1)
x(2)
θ, θ0
{+1, 0}
L01
L01(g, a) = {
.
0
if g = a
1
otherwise
 However, even for simple linear classifiers, it is very difficult to find values for 
that minimize simple 0-1 training error
This problem is NP-hard, which probably implies that solving the most difficult
instances of this problem would require computation time exponential in the number
of training examples, .
What makes this a difficult optimization problem is its lack of “smoothness”:
There can be two hypotheses, 
 and 
, where one is closer in
parameter space to the optimal parameter values 
, but they make the
same number of misclassifications so they have the same  value.
All predictions are categorical: the classifier can’t express a degree of certainty
about whether a particular input  should have an associated value .
For these reasons, if we are considering a hypothesis 
 that makes five incorrect
predictions, it is difficult to see how we might change 
 so that it will perform
better, which makes it difficult to design an algorithm that searches in a sensible
way through the space of hypotheses for a good one. For these reasons, we
investigate another hypothesis class: linear logistic classifiers, providing their
definition, then an approach for learning such classifiers using optimization.
The hypotheses in a linear logistic classifier (LLC) are parameterized by a -
dimensional vector  and a scalar 
, just as is the case for linear classifiers.
However, instead of making predictions in 
, LLC hypotheses generate real-
valued outputs in the interval 
. An LLC has the form
This looks familiar! What’s new?
The logistic function, also known as the sigmoid function, is defined as
and is plotted below, as a function of its input . Its output can be interpreted as a
probability, because for any value of  the output is in 
.
θ, θ0
J(θ, θ0) = 1
n
n
∑
i=1
L01(step(θTx(i) + θ0), y(i)) .
n
(θ, θ0)
(θ′, θ′
0)
(θ∗, θ∗
0)
J
x
y
θ, θ0
θ, θ0
4.3.1 Linear logistic classifiers: definition
d
θ
θ0
{+1, 0}
(0, 1)
h(x; θ, θ0) = σ(θTx + θ0) .
σ(z) =
1
1 + e−z
,
z
z
(0, 1)
The “probably” here is not because
we’re too lazy to look it up, but
actually because of a fundamental
unsolved problem in computer-
science theory, known as “P
vs. NP.”
 −4
−2
2
4
0.5
1
z
σ(z)
❓ Study Question
Convince yourself the output of  is always in the interval 
. Why can’t it
equal 0 or equal 1? For what value of  does 
?
What does an LLC look like? Let’s consider the simple case where 
, so our
input points simply lie along the  axis. Classifiers in this case have dimension ,
meaning that they are points. The plot below shows LLCs for three different
parameter settings: 
, 
, and 
−4
−2
2
4
0.5
1
x
σ(θT x + θ0)
❓ Study Question
Which plot is which? What governs the steepness of the curve? What governs
the  value where the output is equal to 0.5?
But wait! Remember that the definition of a classifier is that it’s a mapping from
 or to some other discrete set. So, then, it seems like an LLC is actually
not a classifier!
Given an LLC, with an output value in 
, what should we do if we are forced to
make a prediction in 
? A default answer is to predict 
 if
σ
(0, 1)
z
σ(z) = 0.5
4.3.2 Linear logistic classifier: examples
d = 1
x
0
σ(10x + 1) σ(−2x + 1)
σ(2x −3).
x
Rd →{+1, 0}
(0, 1)
{+1, 0}
+1
  and  otherwise. The value 
 is sometimes called a prediction
threshold.
In fact, for different problem settings, we might prefer to pick a different prediction
threshold. The field of decision theory considers how to make this choice. For
example, if the consequences of predicting 
 when the answer should be 
 are
much worse than the consequences of predicting 
 when the answer should be 
, then we might set the prediction threshold to be greater than 
.
❓ Study Question
Using a prediction threshold of 0.5, for what values of  do each of the LLCs
shown in the figure above predict 
?
When 
, then our inputs  lie in a two-dimensional space with axes 
 and 
,
and the output of the LLC is a surface, as shown below, for 
.
❓ Study Question
Convince yourself that the set of points for which 
, that is, the
``boundary’’ between positive and negative predictions with prediction
threshold 
, is a line in 
 space. What particular line is it for the case in
the figure above? How would the plot change for 
, but now with
? For 
?
Optimization is a key approach to solving machine learning problems; this also
applies to learning linear logistic classifiers (LLCs) by defining an appropriate loss
function for optimization. A first attempt might be to use the simple 0-1 loss
σ(θTx + θ0) > 0.5
0
0.5
+1
−1
−1
+1
0.5
x
+1
d = 2
x
x1
x2
θ = (1, 1), θ0 = 2
σ(θTx + θ0) = 0.5
0.5
(x1, x2)
θ = (1, 1)
θ0 = −2
θ = (−1, −1), θ0 = 2
4.3.3 Learning linear logistic classifiers
 function 
 that gives a value of 0 for a correct prediction, and a 1 for an incorrect
prediction. As noted earlier, however, this gives rise to an objective function that is
very difficult to optimize, and so we pursue another strategy for defining our
objective.
For learning LLCs, we’d have a class of hypotheses whose outputs are in 
, but
for which we have training data with  values in 
. How can we define an
appropriate loss function? We start by changing our interpretation of the output to
be the probability that the input should map to output value 1 (we might also say that
this is the probability that the input is in class 1 or that the input is ‘positive.’)
❓ Study Question
If 
 is the probability that  belongs to class 
, what is the probability that
 belongs to the class 
, assuming there are only these two classes?
Intuitively, we would like to have low loss if we assign a high probability to the correct
class. We’ll define a loss function, called negative log-likelihood (NLL), that does just
this. In addition, it has the cool property that it extends nicely to the case where we
would like to classify our inputs into more than two classes.
In order to simplify the description, we assume that (or transform our data so that)
the labels in the training data are 
.
We would like to pick the parameters of our classifier to maximize the probability
assigned by the LLC to the correct  values, as specified in the training set. Letting
guess 
, that probability is
under the assumption that our predictions are independent. This can be cleverly
rewritten, when 
, as
❓ Study Question
Be sure you can see why these two expressions are the same.
The big product above is kind of hard to deal with in practice, though. So what can
we do? Because the log function is monotonic, the 
 that maximize the quantity
L01
(0, 1)
y
{+1, 0}
h(x)
x
+1
x
−1
y ∈{0, 1}
y
g(i) = σ(θTx(i) + θ0)
n
∏
i=1
{
,
g(i)
if y(i) = 1
1 −g(i)
otherwise
y(i) ∈{0, 1}
n
∏
i=1
g(i)y(i)
(1 −g(i))1−y(i) .
θ, θ0
Remember to be sure your  values
have this form if you try to learn an
LLC using NLL!
y
That crazy huge  represents
taking the product over a bunch of
factors just as huge  represents
taking the sum over a bunch of
terms.
Π
Σ
 above will be the same as the 
 that maximize its log, which is the following:
Finally, we can turn the maximization problem above into a minimization problem
by taking the negative of the above expression, and writing in terms of minimizing
a loss
where 
 is the negative log-likelihood loss function:
This loss function is also sometimes referred to as the log loss or cross entropy. and it
won’t make any real difference. If we ask you for numbers, use log base .
What is the objective function for linear logistic classification? We can finally put
all these pieces together and develop an objective function for optimizing
regularized negative log-likelihood for a linear logistic classifier. In fact, this process
is usually called “logistic regression,” so we’ll call our objective 
, and define it as
❓ Study Question
Consider the case of linearly separable data. What will the  values that
optimize this objective be like if 
? What will they be like if  is very big?
Try to work out an example in one dimension with two data points.
What role does regularization play for classifiers? This objective function has the
same structure as the one we used for regression, Equation 2.2, where the first term
(in parentheses) is the average loss, and the second term is for regularization.
Regularization is needed for building classifiers that can generalize well (just as was
the case for regression). The parameter  governs the trade-off between the two
terms as illustrated in the following example.
Suppose we wish to obtain a linear logistic classifier for this one-dimensional
dataset:
θ, θ0
n
∑
i=1
(y(i) log g(i) + (1 −y(i)) log(1 −g(i))) .
n
∑
i=1
Lnll(g(i), y(i))
Lnll
Lnll(guess, actual) = −(actual ⋅log(guess) + (1 −actual) ⋅log(1 −guess)) .
e
Jlr
Jlr(θ, θ0; D) = ( 1
n
n
∑
i=1
Lnll(σ(θTx(i) + θ0), y(i))) + λ∥θ∥2 .
(4.2)
θ
λ = 0
λ
λ
 Clearly, this can be fit very nicely by a hypothesis 
, but what is the best
value for ? Evidently, when there is no regularization (
), the objective
function 
 will approach zero for large values of , as shown in the plot on the
left, below. However, would the best hypothesis really have an infinite (or very
large) value for ? Such a hypothesis would suggest that the data indicate strong
certainty that a sharp transition between 
 and 
 occurs exactly at 
,
despite the actual data having a wide gap around 
.
In absence of other beliefs about the solution, we might prefer that our linear
logistic classifier not be overly certain about its predictions, and so we might prefer
a smaller  over a large  By not being overconfident, we might expect a somewhat
smaller  to perform better on future examples drawn from this same distribution.
This preference can be realized using a nonzero value of the regularization trade-off
parameter, as illustrated in the plot on the right, above, with 
.
Another nice way of thinking about regularization is that we would like to prevent
our hypothesis from being too dependent on the particular training data that we
were given: we would like for it to be the case that if the training data were changed
slightly, the hypothesis would not change by much.
4.4 Gradient descent for logistic regression
Now that we have a hypothesis class (LLC) and a loss function (NLL), we need to
take some data and find parameters! Sadly, there is no lovely analytical solution like
the one we obtained for regression, in Section 2.7.2. Good thing we studied gradient
h(x) = σ(θx)
θ
λ = 0
Jlr(θ)
θ
θ
y = 0
y = 1
x = 0
x = 0
θ
θ.
θ
λ = 0.2
 descent! We can perform gradient descent on the 
 objective, as we’ll see next. We
can also apply stochastic gradient descent to this problem.
Luckily, 
 has enough nice properties that gradient descent and stochastic
gradient descent should generally “work”. We’ll soon see some more challenging
optimization problems though – in the context of neural networks, in Section 6.7.
First we need derivatives with respect to both 
 (the scalar component) and  (the
vector component) of 
. Explicitly, they are:
Note that 
 will be of shape 
 and 
 will be a scalar since we have
separated 
 from  here.
Putting everything together, our gradient descent algorithm for logistic regression
becomes:
❓ Study Question
Convince yourself that the dimensions of all these quantities are correct, under
the assumption that  is 
.
❓ Study Question
Compute 
 by finding the vector of partial derivatives 
.
What is the shape of 
?
❓ Study Question
Compute 
 by finding the vector of partial derivatives
.
❓ Study Question
Use these last two results to verify our derivation above.
Algorithm 4.1 LR-Gradient-Descent(
)
repeat
Jlr
Jlr
θ0
θ
Θ
∇θJlr(θ, θ0) = 1
n
n
∑
i=1
(g(i) −y(i))x(i) + 2λθ
∂Jlr(θ, θ0)
∂θ0
= 1
n
n
∑
i=1
(g(i) −y(i)) .
∇θJlr
d × 1
∂Jlr
∂θ0
θ0
θ
θ
d × 1
∇θ∥θ∥2
(
∂∥θ∥2
∂θ1 , … ,
∂∥θ∥2
∂θd )
∇θ∥θ∥2
∇θLnll(σ(θTx + θ0), y)
(
∂Lnll(σ(θTx+θ0),y)
∂θ1
, … ,
∂Lnll(σ(θTx+θ0),y)
∂θd
)
θinit, θ0 init, η, ϵ
1: θ(0) ←θinit
2: θ(0)
0
←θ0 init
3: t ←0
4:
 until 
return 
Logistic regression, implemented using batch or stochastic gradient descent, is a
useful and fundamental machine learning technique. We will also see later that it
corresponds to a one-layer neural network with a sigmoidal activation function,
and so is an important step toward understanding neural networks.
Much like the squared-error loss function that we saw for linear regression, the NLL
loss function for linear logistic regression is a convex function of the parameters 
and 
 (below is a proof if you’re interested). This means that running gradient
descent with a reasonable set of hyperparameters will behave nicely.
4.5 Handling multiple classes
So far, we have focused on the binary classification case, with only two possible
classes. But what can we do if we have multiple possible classes (e.g., we want to
predict the genre of a movie)? There are two basic strategies:
Train multiple binary classifiers using different subsets of our data and
combine their outputs to make a class prediction.
Directly train a multi-class classifier using a hypothesis class that is a
generalization of logistic regression, using a one-hot output encoding and NLL
loss.
The method based on NLL is in wider use, especially in the context of neural
networks, and is explored here. In the following, we will assume that we have a
data set 
 in which the inputs 
 but the outputs 
 are drawn from a set of
 classes 
. Next, we extend the idea of NLL directly to multi-class
classification with 
 classes, where the training label is represented with what is
called a one-hot vector 
, where 
 if the example is of class 
and 
 otherwise. Now, we have a problem of mapping an input 
 that is in
 into a 
-dimensional output. Furthermore, we would like this output to be
interpretable as a discrete probability distribution over the possible classes, which
5:
t ←t + 1
6:
θ(t) ←θ(t−1) −η( 1
n ∑n
i=1(σ(θ(t−1)Tx(i) + θ(t−1)
0
) −y(i))x(i) + 2λ θ(t−1))
7:
θ(t)
0 ←θ(t−1)
0
−η( 1
n ∑n
i=1(σ(θ(t−1)Tx(i) + θ(t−1)
0
) −y(i)))
8:
Jlr(θ(t), θ(t)
0 ) −Jlr(θ(t−1), θ(t−1)
0
) < ϵ
∣∣
9:
θ(t), θ(t)
0
4.4.1 Convexity of the NLL Loss Function
θ
θ0
Proof of convexity of the NLL loss function
D
x(i) ∈Rd
y(i)
K
{c1, … , cK}
K
y = [
]T
y1, … , yK
yk = 1
k
yk = 0
x(i)
Rd
K
 means the elements of the output vector have to be non-negative (greater than or
equal to 0) and sum to 1.
We will do this in two steps. First, we will map our input 
 into a vector value
 by letting  be a whole 
 matrix of parameters, and 
 be a 
vector, so that
Next, we have to extend our use of the sigmoid function to the multi-dimensional
softmax function, that takes a whole vector 
 and generates
which can be interpreted as a probability distribution over 
 items. To make the
final prediction of the class label, we can then look at 
 find the most likely
probability over these 
 entries in 
 (i.e. find the largest entry in 
) and return the
corresponding index as the “one-hot” element of  in our prediction.
❓ Study Question
Convince yourself that the vector of  values will be non-negative and sum to 1.
Putting these steps together, our hypotheses will be
Now, we retain the goal of maximizing the probability that our hypothesis assigns
to the correct output 
 for each input . We can write this probability, letting 
stand for our “guess”, 
, for a single example 
 as 
.
❓ Study Question
How many elements that are not equal to 1 will there be in this product?
The negative log of the probability that we are making a correct guess is, then, for
one-hot vector  and probability distribution vector ,
We’ll call this nllm for negative log likelihood multiclass. It is also worth noting that the
NLLM loss function is also convex; however, we will omit the proof.
x(i)
z(i) ∈RK
θ
d × K
θ0
K × 1
z = θTx + θ0 .
z ∈RK
g = softmax(z) =
.
⎡
⎢
⎣
exp(z1)/ ∑i exp(zi)
⋮
exp(zK)/ ∑i exp(zi)
⎤
⎥
⎦
K
g,
K
g,
g,
1
g
h(x; θ, θ0) = softmax(θTx + θ0) .
yk
x
g
h(x)
(x, y)
∏K
k=1 gyk
k
y
g
Lnllm(g, y) = −
K
∑
k=1
yk ⋅log(gk) .
Let’s check dimensions! 
 is
 and  is 
, and 
 is
, so  is 
 and we’re
good!
θT
K × d
x
d × 1
θ0
K × 1
z
K × 1
 ❓ Study Question
Be sure you see that is 
 is minimized when the guess assigns high
probability to the true class.
❓ Study Question
Show that 
 for 
 is the same as 
.
4.6 Prediction accuracy and validation
In order to formulate classification with a smooth objective function that we can
optimize robustly using gradient descent, we changed the output from discrete
classes to probability values and the loss function from 0-1 loss to NLL. However,
when time comes to actually make a prediction we usually have to make a hard
choice: buy stock in Acme or not? And, we get rewarded if we guessed right,
independent of how sure or not we were when we made the guess.
The performance of a classifier is often characterized by its accuracy, which is the
percentage of a data set that it predicts correctly in the case of 0-1 loss. We can see
that accuracy of hypothesis  on data 
 is the fraction of the data set that does not
incur any loss:
where 
 is the final guess for one class or the other that we make from 
,
e.g., after thresholding. It’s noteworthy here that we use a different loss function for
optimization than for evaluation. This is a compromise we make for computational
ease and efficiency.
Lnllm
Lnllm
K = 2
Lnll
h
D
A(h; D) = 1 −1
n
n
∑
i=1
L01(g(i), y(i)) ,
g(i)
h(x(i))
 This page contains all content from the legacy PDF notes; features chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Linear regression and classification are powerful tools, but in the real world, data
often exhibit non-linear behavior that cannot immediately be captured by the linear
models which we have built so far. For example, suppose the true behavior of a
system (with 
) looks like this wavelet:
Such behavior is actually ubiquitous in physical systems, e.g., in the vibrations of
the surface of a drum, or scattering of light through an aperture. However, no single
hyperplane would be a very good fit to such peaked responses!
A richer class of hypotheses can be obtained by performing a non-linear feature
transformation 
 before doing the regression. That is, 
 is a linear
function of , but 
 is a non-linear function of 
 if  is a non-linear
function of .
There are many different ways to construct . Some are relatively systematic and
domain independent. Others are directly related to the semantics (meaning) of the
original features, and we construct them deliberately with our application (goal) in
mind.
5.1 Gaining intuition about feature
transformations
In this section, we explore the effects of non-linear feature transformations on
simple classification problems, to gain intuition.
Let’s look at an example data set that starts in 1-D:
5  Feature Representation
Note
d = 2
ϕ(x)
θTx + θ0
x
θTϕ(x) + θ0
x,
ϕ
x
ϕ
5  Feature Representation

 x
0
These points are not linearly separable, but consider the transformation
. Plotting this transformed data (in two-dimensional space, since
there are now two features), we see that it is now separable. There are lots of
possible separators; we have just shown one of them here.
x
x2
separator
A linear separator in  space is a nonlinear separator in the original space! Let’s see
how this plays out in our simple example. Consider the separator 
(which corresponds to 
 and 
 in our transformed space), which
labels the half-plane 
 as positive. What separator does it correspond to in
the original 1-D space? We have to ask the question: which  values have the
property that 
. The answer is 
 and 
, so those two points constitute
our separator, back in the original space. Similarly, by evaluating where 
and where 
, we can find the regions of 1D space that are labeled positive
and negative (respectively) by this separator.
Example
ϕ(x) = [x, x2]T
Example
ϕ
x2 −1 = 0
θ = [0, 1]T
θ0 = −1
x2 −1 > 0
x
x2 −1 = 0
+1
−1
x2 −1 > 0
x2 −1 < 0
Example
 x
0
1
-1
5.2 Systematic feature construction
Here are two different ways to systematically construct features in a problem
independent way.
If the features in your problem are already naturally numerical, one systematic
strategy for constructing a new feature space is to use a polynomial basis. The idea is
that, if you are using the th-order basis (where  is a positive integer), you include
a feature for every possible product of  different dimensions in your original input.
Here is a table illustrating the th order polynomial basis for different values of ,
calling out the cases when 
 and 
:
Order
in general (
)
0
1
2
3
⋮
⋮
⋮
This transformation can be used in combination with linear regression or logistic
regression (or any other regression or classification model). When we’re using a
linear regression or classification model, the key insight is that a linear regressor or
separator in the transformed space is a non-linear regressor or separator in the
original space.
To give a regression example, the wavelet pictured at the start of this chapter can be
fit much better using a polynomial feature representation up to order 
,
compared to just using a simple hyperplane in the original (single-dimensional)
feature space:
5.2.1 Polynomial basis
k
k
k
k
k
d = 1
d > 1
d = 1
d > 1
[1]
[1]
[1, x]T
[1, x1, … , xd]T
[1, x, x2]T
[1, x1, … , xd, x2
1, x1x2, …]T
[1, x, x2, x3]T
[1, x1, … , xd, x2
1, x1x2, … , x3
1, x1x2
2, x1x2x3, …]T
k = 8
 The raw data (with 
 random samples) is plotted on the left, and the
regression result (curved surface) is on the right.
Now let’s look at a classification example and see how polynomial feature
transformation may help us.
One well-known example is the “exclusive or” (xor) data set, the drosophila of
machine-learning data sets:
Clearly, this data set is not linearly separable. So, what if we try to solve the xor
classification problem using a polynomial basis as the feature transformation? We
can just take our two-dimensional data and transform it into a higher-dimensional
data set, by applying some feature transformation . Now, we have a classification
problem as usual.
Let’s try it for 
 on our xor problem. The feature transformation is
❓ Study Question
If we train a classifier after performing this feature transformation, would we
lose any expressive power if we let 
 (i.e., trained without offset instead of
with offset)?
We might run a classification learning algorithm and find a separator with
coefficients 
 and 
. This corresponds to
n = 1000
Example
ϕ
k = 2
ϕ([x1, x2]T) = [1, x1, x2, x2
1, x1x2, x2
2]T .
θ0 = 0
θ = [0, 0, 0, 0, 4, 0]T
θ0 = 0
2
2
D. Melanogaster is a species of
fruit fly, used as a simple system in
which to study genetics, since 1910.
 and is plotted below, with the gray shaded region classified as negative and the
white region classified as positive:
❓ Study Question
Be sure you understand why this high-dimensional hyperplane is a separator,
and how it corresponds to the figure.
For fun, we show some more plots below. Here is another result for a linear
classifier on xor generated with logistic regression and gradient descent, using a
random initial starting point and second-order polynomial basis:
Here is a harder data set. Logistic regression with gradient descent failed to
separate it with a second, third, or fourth-order basis feature representation, but
0 + 0x1 + 0x2 + 0x2
1 + 4x1x2 + 0x2
2 + 0 = 0
Example
Example
 succeeded with a fifth-order basis. Shown below are some results after 
gradient descent iterations (from random starting points) for bases of order 2
(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).
❓ Study Question
Percy Eptron has a domain with four numeric input features, 
. He
decides to use a representation of the form
where 
 means the vector  concatenated with the vector .
What is the dimension of Percy’s representation? Under what assumptions
about the original features is this a reasonable choice?
Another cool idea is to use the training data itself to construct a feature space. The
idea works as follows. For any particular point  in the input space 
, we can
∼1000
Example
(x1, … , x4)
ϕ(x) = PolyBasis((x1, x2), 3)⌢PolyBasis((x3, x4), 3)
a⌢b
a
b
5.2.2 (Optional) Radial basis functions
p
X
 construct a feature 
 which takes any element 
 and returns a scalar value
that is related to how far  is from the  we started with.
Let’s start with the basic case, in which 
. Then we can define
This function is maximized when 
 and decreases exponentially as  becomes
more distant from .
The parameter  governs how quickly the feature value decays as we move away
from the center point . For large values of , the 
 values are nearly 0 almost
everywhere except right near ; for small values of , the features have a high value
over a larger part of the space.
Now, given a dataset 
 containing  points, we can make a feature transformation
 that maps points in our original space, 
, into points in a new space, 
. It is
defined as follows:
So, we represent a new datapoint  in terms of how far it is from each of the
datapoints in our training set.
This idea can be generalized in several ways and is the fundamental concept
underlying kernel methods, that are not directly covered in this class but we
recommend you read about some time. This idea of describing objects in terms of
their similarity to a set of reference objects is very powerful and can be applied to
cases where 
 is not a simple vector space, but where the inputs are graphs or
strings or other types of objects, as long as there is a distance metric defined on the
input space.
5.3 (Optional) Hand-constructing features for real
domains
In many machine-learning applications, we are given descriptions of the inputs
with many different types of attributes, including numbers, words, and discrete
features. An important factor in the success of an ML application is the way that the
features are chosen to be encoded by the human who is framing the learning
problem.
Getting a good encoding of discrete features is particularly important. You want to
create “opportunities” for the ML system to find the underlying patterns. Although
there are machine-learning methods that have special mechanisms for handling
discrete inputs, most of the methods we consider in this class will assume the input
fp
x ∈X
x
p
X = Rd
fp(x) = e−β∥p−x∥2 .
p = x
x
p
β
p
β
fp
p
β
D
n
ϕ
Rd
Rn
ϕ(x) = [fx(1)(x), fx(2)(x), … , fx(n)(x)]T .
x
X
5.3.1 Discrete features
 vectors  are in 
. So, we have to figure out some reasonable strategies for turning
discrete values into (vectors of) real numbers.
We’ll start by listing some encoding strategies, and then work through some
examples. Let’s assume we have some feature in our raw data that can take on one
of  discrete values.
Numeric: Assign each of these values a number, say 
. We
might want to then do some further processing, as described in Section 1.3.3.
This is a sensible strategy only when the discrete values really do signify some
sort of numeric quantity, so that these numerical values are meaningful.
Thermometer code: If your discrete values have a natural ordering, from
, but not a natural mapping into real numbers, a good strategy is to use
a vector of length  binary variables, where we convert discrete input value
 into a vector in which the first  values are 
 and the rest are 
.
This does not necessarily imply anything about the spacing or numerical
quantities of the inputs, but does convey something about ordering.
Factored code: If your discrete values can sensibly be decomposed into two
parts (say the “maker” and “model” of a car), then it’s best to treat those as two
separate features, and choose an appropriate encoding of each one from this
list.
One-hot code: If there is no obvious numeric, ordering, or factorial structure,
then the best strategy is to use a vector of length , where we convert discrete
input value 
 into a vector in which all values are 
, except for the 
th, which is 
.
Binary code: It might be tempting for the computer scientists among us to use
some binary code, which would let us represent  values using a vector of
length 
. This is a bad idea! Decoding a binary code takes a lot of work, and
by encoding your inputs this way, you’d be forcing your system to learn the
decoding algorithm.
As an example, imagine that we want to encode blood types, that are drawn from
the set 
. There is no obvious linear
numeric scaling or even ordering to this set. But there is a reasonable factoring, into
two features: 
 and 
. And, in fact, we can further reasonably
factor the first group into 
, 
. So, here are two plausible
encodings of the whole set:
Use a 6-D vector, with two components of the vector each encoding the
corresponding factor using a one-hot encoding.
Use a 3-D vector, with one dimension for each factor, encoding its presence as
 and absence as 
 (this is sometimes better than 
). In this case, 
would be 
 and 
 would be 
.
x
Rd
k
1.0/k, 2.0/k, … , 1.0
1, … , k
k
0 < j ≤k
j
1.0
0.0
k
0 < j ≤k
0.0
j
1.0
k
log k
{A+, A−, B+, B−, AB+, AB−, O+, O−}
{A, B, AB, O}
{+, −}
{A, notA} {B, notB}
1.0
−1.0
0.0
AB+
[1.0, 1.0, 1.0]T
O−
[−1.0, −1.0, −1.0]T
 ❓ Study Question
How would you encode 
 in both of these approaches?
The problem of taking a text (such as a tweet or a product review, or even this
document!) and encoding it as an input for a machine-learning algorithm is
interesting and complicated. Much later in the class, we’ll study sequential input
models, where, rather than having to encode a text as a fixed-length feature vector,
we feed it into a hypothesis word by word (or even character by character!).
There are some simple encodings that work well for basic applications. One of them
is the bag of words (bow) model, which can be used to encode documents. The idea is
to let  be the number of words in our vocabulary (either computed from the
training set or some other body of text or dictionary). We will then make a binary
vector (with values 
 and 
) of length , where element  has value 
 if word 
occurs in the document, and 
 otherwise.
If some feature is already encoded as a numeric value (heart rate, stock price,
distance, etc.) then we should generally keep it as a numeric value. An exception
might be a situation in which we know there are natural “breakpoints” in the
semantics: for example, encoding someone’s age in the US, we might make an
explicit distinction between under and over 18 (or 21), depending on what kind of
thing we are trying to predict. It might make sense to divide into discrete bins
(possibly spacing them closer together for the very young) and to use a one-hot
encoding for some sorts of medical situations in which we don’t expect a linear (or
even monotonic) relationship between age and some physiological features.
❓ Study Question
Consider using a polynomial basis of order  as a feature transformation  on
our data. Would increasing  tend to increase or decrease structural error? What
about estimation error?
A+
5.3.2 Text
d
1.0
0.0
d
j
1.0
j
0.0
5.3.3 Numeric values
k
ϕ
k
 This page contains all content from the legacy PDF notes; neural networks chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
You’ve probably been hearing a lot about “neural networks.” Now that we have
several useful machine-learning concepts (hypothesis classes, classification,
regression, gradient descent, regularization, etc.), we are well equipped to
understand neural networks in detail.
This is, in some sense, the “third wave” of neural nets. The basic idea is founded on
the 1943 model of neurons of McCulloch and Pitts and the learning ideas of Hebb.
There was a great deal of excitement, but not a lot of practical success: there were
good training methods (e.g., perceptron) for linear functions, and interesting
examples of non-linear functions, but no good way to train non-linear functions
from data. Interest died out for a while, but was re-kindled in the 1980s when
several people came up with a way to train neural networks with “back-
propagation,” which is a particular style of implementing gradient descent, that we
will study here.
As with many good ideas in science, the basic idea for how to train non-linear
neural networks with gradient descent was independently developed by more than
one researcher.
By the mid-90s, the enthusiasm waned again, because although we could train non-
linear networks, the training tended to be slow and was plagued by a problem of
getting stuck in local optima. Support vector machines (SVMs) that use
regularization of high-dimensional hypotheses by seeking to maximize the margin,
alongside kernel methods that provide an efficient and beautiful way of using
feature transformations to non-linearly transform data into a higher-dimensional
space, provided reliable learning methods with guaranteed convergence and no
local optima.
However, during the SVM enthusiasm, several groups kept working on neural
networks, and their work, in combination with an increase in available data and
computation, has made neural networks rise again. They have become much more
reliable and capable, and are now the method of choice in many applications. There
are many, many variations of neural networks, which we can’t even begin to survey.
We will study the core “feed-forward” networks with “back-propagation” training,
and then, in later chapters, address some of the major advances beyond this core.
We can view neural networks from several different perspectives:
6  Neural Networks
Note
The number of neural network
variants increases daily, as may be
seen on arxiv.org .

6  Neural Networks
6  Neural Networks

 View 1: An application of stochastic gradient descent for classification and
regression with a potentially very rich hypothesis class.
View 2: A brain-inspired network of neuron-like computing elements that learn
distributed representations.
View 3: A method for building applications that make predictions based on huge
amounts of data in very complex domains.
We will mostly take view 1, with the understanding that the techniques we develop
will enable the applications in view 3. View 2 was a major motivation for the early
development of neural networks, but the techniques we will study do not seem to
actually account for the biological learning processes in brains.
6.1 Basic element
The basic element of a neural network is a “neuron,” pictured schematically below.
We will also sometimes refer to a neuron as a “unit” or “node.”

x1
.. .
xm
f(·)
a
w1
wm
w0
z
input
pre-activation
output
activation function
It is a (generally non-linear) function of an input vector 
 to a single output
value 
.
It is parameterized by a vector of weights 
 and an offset or
threshold 
.
We also specify an activation function 
. In general, this is chosen to be a
non-linear function, which means the neuron is non-linear. In the case that the
activation function is the identity (
) or another linear function, then the
neuron is a linear function of ). The activation can theoretically be any function,
though we will only be able to work with it if it is differentiable.
The function represented by the neuron is expressed as:
x ∈Rm
a ∈R
(w1, … , wm) ∈Rm
w0 ∈R
f : R →R
f(x) = x
x
a = f(z) = f ((
m
∑
j=1
xjwj) + w0) = f(wTx + w0) .
Some prominent researchers are, in
fact, working hard to find
analogues of these methods in the
brain.
Sorry for changing our notation
here. We were using  as the
dimension of the input, but we are
trying to be consistent here with
many other accounts of neural
networks. It is impossible to be
consistent with all of them though
—there are many different ways of
telling this story.
d
This should remind you of our 
and 
 for linear models.
θ
θ0

6  Neural Networks
 Before thinking about a whole network, we can consider how to train a single unit.
Given a loss function 
 and a dataset 
,
we can do (stochastic) gradient descent, adjusting the weights 
 to minimize
where 
 is the output of our single-unit neural net for a given input.
We have already studied two special cases of the neuron: linear logistic classifiers
(LLCs) with NLL loss and regressors with quadratic loss! The activation function for
the LLC is 
 and for linear regression it is simply 
.
❓ Study Question
Just for a single neuron, imagine for some reason, that we decide to use
activation function 
 and loss function
. Derive a gradient descent update for 
and 
.
6.2 Networks
Now, we’ll put multiple neurons together into a network. A neural network in
general takes in an input 
 and generates an output 
. It is constructed
out of multiple neurons; the inputs of each neuron might be elements of  and/or
outputs of other neurons. The outputs of the neural network are generated by 
output units.
In this chapter, we will only consider feed-forward networks. In a feed-forward
network, you can think of the network as defining a function-call graph that is
acyclic: that is, the input to a neuron can never depend on that neuron’s output.
Data flows one way, from the inputs to the outputs, and the function computed by
the network is just a composition of the functions computed by the individual
neurons.
Although the graph structure of a feed-forward neural network can really be
anything (as long as it satisfies the feed-forward constraint), for simplicity in
software and analysis, we usually organize them into layers. A layer is a group of
neurons that are essentially “in parallel”: their inputs are the outputs of neurons in
the previous layer, and their outputs are the inputs to the neurons in the next layer.
We’ll start by describing a single layer, and then go on to the case of multiple layers.
L(guess, actual)
{(x(1), y(1)), … , (x(n), y(n))}
w, w0
J(w, w0) = ∑
i
L (NN(x(i); w, w0), y(i)) ,
NN
f(x) = σ(x)
f(x) = x
f(z) = ez
L(guess, actual) = (guess −actual)2
w
w0
x ∈Rm
a ∈Rn
x
n
6.2.1 Single layer

6  Neural Networks
 A layer is a set of units that, as we have just described, are not connected to each
other. The layer is called fully connected if, as in the diagram below, all of the inputs
(i.e., 
 in this case) are connected to every unit in the layer. A layer has
input 
 and output (also known as activation) 
.



.. .

x1
x2
.. .
xm
f
f
f
.. .
f
a1
a2
a3
.. .
an
W, W0
Since each unit has a vector of weights and a single offset, we can think of the
weights of the whole layer as a matrix, 
, and the collection of all the offsets as a
vector 
. If we have 
 inputs,  units, and  outputs, then
 is an 
 matrix,
 is an 
 column vector,
, the input, is an 
 column vector,
, the pre-activation, is an 
 column vector,
, the activation, is an 
 column vector,
and the output vector is
The activation function  is applied element-wise to the pre-activation values .
A single neural network generally combines multiple layers, most typically by
feeding the outputs of one layer into the inputs of another layer.
x1, x2, … xm
x ∈Rm
a ∈Rn
W
W0
m
n
n
W
m × n
W0
n × 1
X
m × 1
Z = W TX + W0
n × 1
A
n × 1
A = f(Z) = f(W TX + W0) .
f
Z
6.2.2 Many layers

6  Neural Networks
 We have to start by establishing some nomenclature. We will use  to name a layer,
and let 
 be the number of inputs to the layer and 
 be the number of outputs
from the layer. Then, 
 and 
 are of shape 
 and 
, respectively.
Note that the input to layer  is the output from layer 
, so we have 
,
and as a result 
 is of shape 
, or equivalently 
. Let 
 be the
activation function of layer . Then, the pre-activation outputs are the 
 vector
and the activation outputs are simply the 
 vector
Here’s a diagram of a many-layered network, with two blocks for each layer, one
representing the linear part of the operation and one representing the non-linear
activation function. We will use this structural decomposition to organize our
algorithmic thinking and implementation.
W 1
W 1
0
f 1
W 2
W 2
0
f 2
· · ·
W L
W L
0
f L
X = A0
Z1
A1
Z2
A2
AL−1
ZL
AL
lay er 1
lay er 2
lay er L
6.3 Choices of activation function
There are many possible choices for the activation function. We will start by
thinking about whether it’s really necessary to have an  at all.
What happens if we let  be the identity? Then, in a network with  layers (we’ll
leave out 
 for simplicity, but keeping it wouldn’t change the form of this
argument),
So, multiplying out the weight matrices, we find that
which is a linear function of 
! Having all those layers did not change the
representational capacity of the network: the non-linearity of the activation function
is crucial.
❓ Study Question
Convince yourself that any function representable by any number of linear
layers (where  is the identity function) can be represented by a single layer.
l
ml
nl
W l
W l
0
ml × nl
nl × 1
l
l −1
ml = nl−1
Al−1
ml × 1
nl−1 × 1
f l
l
nl × 1
Z l = W lTAl−1 + W l
0
nl × 1
Al = f l(Z l) .
f
f
L
W0
AL = W LTAL−1 = W LTW L−1T ⋯W 1TX .
AL = W totalX ,
X
f
It is technically possible to have
different activation functions
within the same layer, but, again,
for convenience in specification
and implementation, we generally
have the same activation function
within a layer.

6  Neural Networks
 Now that we are convinced we need a non-linear activation, let’s examine a few
common choices. These are shown mathematically below, followed by plots of these
functions.
Step function:
Rectified linear unit (ReLU):
Sigmoid function: Also known as a logistic function. This can sometimes be
interpreted as probability, because for any value of  the output is in 
:
Hyperbolic tangent: Always in the range 
:
Softmax function: Takes a whole vector 
 and generates as output a vector
 with the property that 
, which means we can interpret it as
a probability distribution over  items:
−2
−1
1
2
−0.5
0.5
1
1.5
z
step(z)
−2
−1
1
2
−0.5
0.5
1
1.5
z
ReLU(z)
−4
−2
2
4
−1
−0.5
0.5
1
z
σ(z)
−4
−2
2
4
−1
−0.5
0.5
1
z
tanh(z)
step(z) = {0
if z < 0
1
otherwise
ReLU(z) = {
= max(0, z)
0
if z < 0
z
otherwise
z
(0, 1)
σ(z) =
1
1 + e−z
(−1, 1)
tanh(z) = ez −e−z
ez + e−z
Z ∈Rn
A ∈(0, 1)n
∑n
i=1 Ai = 1
n
softmax(z) =
⎡
⎢
⎣
exp(z1)/ ∑i exp(zi)
⋮
exp(zn)/ ∑i exp(zi)
⎤
⎥
⎦

6  Neural Networks
 The original idea for neural networks involved using the step function as an
activation, but because the derivative of the step function is zero everywhere except
at the discontinuity (and there it is undefined), gradient-descent methods won’t be
useful in finding a good setting of the weights, and so we won’t consider the step
function further. Step functions have been replaced, in a sense, by the sigmoid,
ReLU, and tanh activation functions.
❓ Study Question
Consider sigmoid, ReLU, and tanh activations. Which one is most like a step
function? Is there an additional parameter you could add to a sigmoid that
would make it be more like a step function?
❓ Study Question
What is the derivative of the ReLU function? Are there some values of the input
for which the derivative vanishes?
ReLUs are especially common in internal (“hidden”) layers, sigmoid activations are
common for the output for binary classification, and softmax activations are
common for the output for multi-class classification (see Section 4.3.3 for an
explanation).
6.4 Loss functions and activation functions
At layer 
 which is the output layer, we need to specify a loss function, and
possibly an activation function as well. Different loss functions make different
assumptions about the range of values they will get as input and, as we have seen,
different activation functions will produce output values in different ranges. When
you are designing a neural network, it’s important to make these things fit together
well. In particular, we will think about matching loss functions with the activation
function in the last layer, 
. Here is a table of loss functions and activations that
make sense for them:
Loss
task
squared
linear
regression
nll
sigmoid
binary classification
nllm
softmax
multi-class classification
We explored squared loss in Chapter 2 and (nll and nllm) in Chapter 4.
L,
f L
f L

6  Neural Networks
 6.5 Error back-propagation
We will train neural networks using gradient descent methods. It’s possible to use
batch gradient descent, in which we sum up the gradient over all the points (as in
Section 3.2 of Chapter 3) or stochastic gradient descent (SGD), in which we take a
small step with respect to the gradient considering a single point at a time (as in
Section 3.4 of Chapter 3).
Our notation is going to get pretty hairy pretty quickly. To keep it as simple as we
can, we’ll focus on computing the contribution of one data point 
 to the gradient
of the loss with respect to the weights, for SGD; you can simply sum up these
gradients over all the data points if you wish to do batch descent.
So, to do SGD for a training example 
, we need to compute
, where 
 represents all weights 
 in all the layers
. This seems terrifying, but is actually quite easy to do using the chain
rule.
Remember that we are always computing the gradient of the loss function with
respect to the weights for a particular value of 
. That tells us how much we want
to change the weights, in order to reduce the loss incurred on this particular
training example.
To get some intuition for how these derivations work, we’ll first suppose everything
in our neural network is one-dimensional. In particular, we’ll assume there are
 inputs and 
 outputs at every layer. So layer  looks like:
In the equation above, we’re using the lowercase letters 
 to
emphasize that all of these quantities are scalars just for the moment. We’ll look at
the more general matrix case below.
To use SGD, then, we want to compute 
 and
 for each layer  and each data point 
. Below we’ll write
“loss” as an abbreviation for 
. Then our first quantity of interest is
. The chain rule gives us the following.
First, let’s look at the case 
:
x(i)
(x, y)
∇WL(NN(x; W), y)
W
W l, W l
0
l = (1, … , L)
(x, y)
6.5.1 First, suppose everything is one-dimensional
ml = 1
nl = 1
l
al = f l(zl),
zl = wlal−1 + wl
0.
al, zl, wl, al−1, wl
0
∂L(NN(x; W), y)/∂wl
∂L(NN(x; W), y)/∂wl
0
l
(x, y)
L(NN(x; W), y)
∂loss/∂wl
l = L
∂loss
∂wL = ∂loss
∂aL ⋅∂aL
∂zL ⋅∂zL
∂wL
= ∂loss
∂aL ⋅(f L)′(zL) ⋅aL−1.
Remember the chain rule! If
 and 
, so that
, then
a = f(b)
b = g(c)
a = f(g(c))
da
dc = da
db ⋅db
dc
= f ′(b)g′(c)
= f ′(g(c))g′(c)
Check your understanding: why
do we need exactly these quantities
for SGD?

6  Neural Networks
 Now we can look at the case of general :
Note that every multiplication above is scalar multiplication because every term in
every product above is a scalar. And though we solved for all the other terms in the
product, we haven’t solved for 
 because the derivative will depend on
which loss function you choose. Once you choose a loss function though, you
should be able to compute this derivative.
❓ Study Question
Suppose you choose squared loss. What is 
?
❓ Study Question
Check the derivations above yourself. You should use the chain rule and also
solve for the individual derivatives that arise in the chain rule.
❓ Study Question
Check that the final layer (
) case is a special case of the general layer  case
above.
❓ Study Question
Derive 
 for yourself, for both the final layer (
) and
general .
❓ Study Question
Does the 
 case remind you of anything from earlier in this course?
❓ Study Question
Write out the full SGD algorithm for this neural network.
l
∂loss
∂wl
= ∂loss
∂aL ⋅∂aL
∂zL ⋅
∂zL
∂aL−1 ⋅∂aL−1
∂zL−1 ⋯∂zl+1
∂al
⋅∂al
∂zl ⋅∂zl
∂wl
= ∂loss
∂aL ⋅(f L)′(zL) ⋅wL ⋅(f L−1)′(zL−1) ⋯⋅wl+1 ⋅(f l)′(zl) ⋅al−1
= ∂loss
∂zl
⋅al−1.
∂loss/∂aL
∂loss/∂aL
l = L
l
∂L(NN(x; W), y)/∂wl
0
l = L
l
L = 1

6  Neural Networks
 It’s pretty typical to run the chain rule from left to right like we did above. But, for
where we’re going next, it will be useful to notice that it’s completely equivalent to
write it in the other direction. So we can rewrite our result from above as follows:
Next we’re going to do everything that we did above, but this time we’ll allow any
number of inputs 
 and outputs 
 at every layer. First, we’ll tell you the results
that correspond to our derivations above. Then we’ll talk about why they make
sense. And finally we’ll derive them carefully.
OK, let’s start with the results! Again, below we’ll be using “loss” as an
abbreviation for 
. Then,
where
or equivalently,
First, compare each equation to its one-dimensional counterpart, and make sure
you see the similarities. That is, compare the general weight derivatives in
Equation 6.4 to the one-dimensional case in Equation 6.1. Compare the intermediate
derivative of loss with respect to the pre-activations 
 in Equation 6.5 to the one-
dimensional case in Equation 6.2. And finally compare the version where we’ve
substituted in some of the derivatives in Equation 6.6 to Equation 6.3. Hopefully
∂loss
∂wl
= al−1 ⋅∂loss
∂zl
(6.1)
∂loss
∂zl
= ∂al
∂zl ⋅∂zl+1
∂al
⋯∂aL−1
∂zL−1 ⋅
∂zL
∂aL−1 ⋅∂aL
∂zL ⋅∂loss
∂aL
(6.2)
= ∂al
∂zl ⋅wl+1 ⋯∂aL−1
∂zL−1 ⋅wL ⋅∂aL
∂zL ⋅∂loss
∂aL .
(6.3)
6.5.2 The general case
ml
nl
L(NN(x; W), y)
∂loss
∂W l
ml×nl
= Al−1
ml×1
( ∂loss
∂Z l )
T
1×nl















(6.4)
∂loss
∂Z l = ∂Al
∂Z l ⋅∂Z l+1
∂Al
⋯⋅∂AL−1
∂Z L−1 ⋅
∂Z L
∂AL−1 ⋅∂AL
∂Z L ⋅∂loss
∂AL
(6.5)
∂loss
∂Z l = ∂Al
∂Z l ⋅W l+1 ⋯⋅∂AL−1
∂Z L−1 ⋅W L ⋅∂AL
∂Z L ⋅∂loss
∂AL .
(6.6)
Z l
Even though we have reordered
the gradients for notational
convenience, when actually
computing the product in
Equation 6.3, it is computationally
much cheaper to run the
multiplications from right-to-left
than from left-to-right. Convince
yourself of this, by reasoning
through the cost of the matrix
multiplications in each case.
There are lots of weights in a
neural network, which means we
need to compute a lot of gradients.
Luckily, as we can see, the
gradients associated with weights
in earlier layers depend on the
same terms as the gradients
associated with weights in later
layers. This means we can reuse
terms and save ourselves some
computation!

6  Neural Networks
 you see how the forms are very analogous. But in the matrix case, we now have to
be careful about the matrix dimensions. We’ll check these matrix dimensions below.
Let’s start by talking through each of the terms in the matrix version of these
equations. Recall that loss is a scalar, and 
 is a matrix of size 
. You can
read about the conventions in the course for derivatives starting in this chapter in
Appendix A. By these conventions (not the only possible conventions!), we have
that 
 will be a matrix of size 
 whose 
 entry is the scalar
. In some sense, we’re just doing a bunch of traditional scalar
derivatives, and the matrix notation lets us write them all simultaneously and
succinctly. In particular, for SGD, we need to find the derivative of the loss with
respect to every scalar component of the weights because these are our model’s
parameters and therefore are the things we want to update in SGD.
The next quantity we see in Equation 6.4 is 
, which we recall has size 
 (or
equivalently 
 since it represents the outputs of the 
 layer). Finally, we
see 
. Again, loss is a scalar, and 
 is a 
 vector. So by the
conventions in Appendix A, we have that 
 has size 
. The transpose
then has size 
. Now you should be able to check that the dimensions all make
sense in Equation 6.4; in particular, you can check that inner dimensions agree in
the matrix multiplication and that, after the multiplication, we should be left with
something that has the dimensions on the lefthand side.
Now let’s look at Equation 6.6. We’re computing 
 so that we can use it in
Equation 6.4. The weights are familiar. The one part that remains is terms of the
form 
. Checking out Appendix A, we see that this term should be a matrix
of size 
 since 
 and 
 both have size 
. The 
 entry of this matrix
is 
. This scalar derivative is something that you can compute when you
know your activation function. If you’re not using a softmax activation function, 
typically is a function only of 
, which means that 
 should equal 0
whenever 
, and that 
.
❓ Study Question
Compute the dimensions of every term in Equation 6.5 and Equation 6.6 using
Appendix A. After you’ve done that, check that all the matrix multiplications
work; that is, check that the inner dimensions agree and that the lefthand side
and righthand side of these equations have the same dimensions.
❓ Study Question
If I use the identity activation function, what is 
 for any ? What is the
full matrix 
?
W l
ml × nl
∂loss/∂W l
ml × nl
(i, j)
∂loss/∂W l
i,j
Al−1
ml × 1
nl−1 × 1
l −1
∂loss/∂Z l
Z l
nl × 1
∂loss/∂Z l
nl × 1
1 × nl
∂loss/∂Z l
∂Al/∂Z l
nl × nl
Al
Z l
nl × 1
(i, j)
∂Al
j/∂Z l
i
Al
j
Z l
j
∂Al
j/∂Z l
i
i ≠j
∂Al
j/∂Z l
j = (f l)′(Z l
j)
∂Al
j/∂Z l
j
j
∂Al/∂Z l

6  Neural Networks
 You can use everything above without deriving it yourself. But if you want to find
the gradients of loss with respect to 
 (which we need for SGD!), then you’ll want
to know how to actually do these derivations. So next we’ll work out the
derivations.
The key trick is to just break every equation down into its scalar meaning. For
instance, the 
 element of 
 is 
. If you think about it for a
moment (and it might help to go back to the one-dimensional case), the loss is a
function of the elements of 
, and the elements of 
 are a function of the 
.
There are 
 elements of 
, so we can use the chain rule to write
To figure this out, let’s remember that 
. We can write one
element of the 
 vector, then, as 
. It follows that
 will be zero except when 
 (check you agree!). So we can rewrite
Equation 6.7 as
Finally, then, we match entries of the matrices on both sides of the equation above
to recover Equation 6.4.
❓ Study Question
Check that Equation 6.8 and Equation 6.4 say the same thing.
❓ Study Question
Convince yourself that 
 by comparing the entries of the
matrices on both sides on the equality sign.
❓ Study Question
Convince yourself that Equation 6.5 is true.
❓ Study Question
Apply the same reasoning to find the gradients of 
 with respect to 
.
6.5.3 Derivations for the general case
W l
0
(i, j)
∂loss/∂W l
∂loss/∂W l
i,j
Z l
Z l
W l
i,j
nl
Z l
∂loss
∂W l
i,j
=
nl
∑
k=1
∂loss
∂Z l
k
∂Z l
k
∂W l
i,j
.
(6.7)
Z l = (W l)⊤Al−1 + W l
0
Z l
Z l
b = ∑ml
a=1 W l
a,bAl−1
a
+ (W l
0)b
∂Z l
k/∂W l
i,j
k = j
∂loss
∂W l
i,j
= ∂loss
∂Z l
j
∂Z l
j
∂W l
i,j
= ∂loss
∂Z l
j
Al−1
i
.
(6.8)
∂Z l/∂Al−1 = W l
loss
W l
0

6  Neural Networks
 This general process of computing the gradients of the loss with respect to the
weights is called error back-propagation.
The idea is that we first do a forward pass to compute all the  and  values at all the
layers, and finally the actual loss. Then, we can work backward and compute the
gradient of the loss with respect to the weights in each layer, starting at layer  and
going back to layer 1.
W 1
W 1
0
f 1
W 2
W 2
0
f 2
· · ·
W L
W L
0
f L
Loss
X = A0
Z1
A1
Z2
A2
AL−1
ZL
AL
y
∂loss
∂AL
∂loss
∂ZL
∂loss
∂AL−1
∂loss
∂A2
∂loss
∂Z2
∂loss
∂A1
∂loss
∂Z1
If we view our neural network as a sequential composition of modules (in our work
so far, it has been an alternation between a linear transformation with a weight
matrix, and a component-wise application of a non-linear activation function), then
we can define a simple API for a module that will let us compute the forward and
backward passes, as well as do the necessary weight updates for gradient descent.
Each module has to provide the following “methods.” We are already using letters
 with particular meanings, so here we will use  as the vector input to the
module and  as the vector output:
forward: 
backward: 
weight grad: 
 only needed for modules that have weights
In homework we will ask you to implement these modules for neural network
components, and then use them to construct a network and train it as described in
the next section.
6.6 Training
Here we go! Here’s how to do stochastic gradient descent training on a feed-
forward neural network. After this pseudo-code, we motivate the choice of
initialization in lines 2 and 3. The actual computation of the gradient values (e.g.,
) is not directly defined in this code, because we want to make the
structure of the computation clear.
❓ Study Question
6.5.4 Reflecting on backpropagation
a
z
L
a, x, y, z
u
v
u →v
u, v, ∂L/∂v →∂L/∂u
u, ∂L/∂v →∂L/∂W
W
∂loss/∂AL
Notice that the backward pass does
not output 
, even though the
forward pass maps from  to . In
the backward pass, we are always
directly computing and ``passing
around’’ gradients of the loss.
∂v/∂u
u
v

6  Neural Networks
 What is 
?
❓ Study Question
Which terms in the code below depend on 
?
procedure SGD-NEURAL-NET(
)
for 
 to  do
end for
for 
 to  do
//forward pass to compute 
for 
 to  do
end for
for 
 down to  do//error back-propagation
//SGD update
end for
end for
end procedure
Initializing 
 is important; if you do it badly there is a good chance the neural
network training won’t work well. First, it is important to initialize the weights to
random values. We want different parts of the network to tend to “address”
different aspects of the problem; if they all start at the same weights, the symmetry
will often keep the values from moving in useful directions. Second, many of our
activation functions have (near) zero slope when the pre-activation  values have
large magnitude, so we generally want to keep the initial weights small so we will
be in a situation where the gradients are non-zero, so that gradient descent will
have some useful signal about which way to go.
One good general-purpose strategy is to choose each weight at random from a
Gaussian (normal) distribution with mean 0 and standard deviation 
 where
 is the number of inputs to the unit.
❓ Study Question
∂Z l/∂W l
f L
1:
Dn, T, L, (m1, … , mL), (f 1, … , f L), Loss
2:
l ←1
L
3:
W l
ij ∼Gaussian(0, 1/ml)
4:
W l
0j ∼Gaussian(0, 1)
5:
6:
t ←1
T
7:
i ←random sample from {1, … , n}
8:
A0 ←x(i)
AL
9:
l ←1
L
10:
Z l ←W lTAl−1 + W l
0
11:
Al ←f l(Z l)
12:
13:
loss ←Loss(AL,  y(i))
14:
l ←L
1
15:
∂loss
∂Al
←{
∂Z l+1
∂Al
⋅
∂loss
∂Z l+1
if l < L,
∂loss
∂AL
otherwise
16:
∂loss
∂Z l ←∂Al
∂Z l ⋅∂loss
∂Al
17:
∂loss
∂W l ←Al−1 ( ∂loss
∂Z l )
⊤
18:
∂loss
∂W l
0
←∂loss
∂Z l
19:
W l ←W l −η(t) ∂loss
∂W l
20:
W l
0 ←W l
0 −η(t) ∂loss
∂W l
0
21:
22:
23:
W
z
(1/m)
m

6  Neural Networks
 If the input  to this unit is a vector of 1’s, what would the expected pre-
activation  value be with these initial weights?
We write this choice (where 
 means “is drawn randomly from the distribution”) as
It will often turn out (especially for fancier activations and loss functions) that
computing 
 is easier than computing 
 and 
 So, we may instead ask for
an implementation of a loss function to provide a backward method that computes
 directly.
6.7 Optimizing neural network parameters
Because neural networks are just parametric functions, we can optimize loss with
respect to the parameters using standard gradient-descent software, but we can take
advantage of the structure of the loss function and the hypothesis class to improve
optimization. As we have seen, the modular function-composition structure of a
neural network hypothesis makes it easy to organize the computation of the
gradient. As we have also seen earlier, the structure of the loss function as a sum
over terms, one per training data point, allows us to consider stochastic gradient
methods. In this section we’ll consider some alternative strategies for organizing
training, and also for making it easier to handle the step-size parameter.
Assume that we have an objective of the form
where  is the function computed by a neural network, and 
 stands for all the
weight matrices and vectors in the network.
Recall that, when we perform batch (or the vanilla) gradient descent, we use the
update rule
which is equivalent to
So, we sum up the gradient of loss at each training point, with respect to 
, and
then take a step in the negative direction of the gradient.
x
z
∼
W l
ij ∼Gaussian (0,
1
ml ).
∂loss
∂Z L
∂loss
∂AL
∂AL
∂Z L .
∂loss/∂Z L
6.7.1 Batches
J(W) = 1
n
n
∑
i=1
L(h(x(i); W), y(i)) ,
h
W
Wt = Wt−1 −η∇WJ(Wt−1) ,
Wt = Wt−1 −η
n
∑
i=1
∇WL(h(x(i); Wt−1), y(i)) .
W

6  Neural Networks
 In stochastic gradient descent, we repeatedly pick a point 
 at random from
the data set, and execute a weight update on that point alone:
As long as we pick points uniformly at random from the data set, and decrease  at
an appropriate rate, we are guaranteed, with high probability, to converge to at least
a local optimum.
These two methods have offsetting virtues. The batch method takes steps in the
exact gradient direction but requires a lot of computation before even a single step
can be taken, especially if the data set is large. The stochastic method begins moving
right away, and can sometimes make very good progress before looking at even a
substantial fraction of the whole data set, but if there is a lot of variability in the
data, it might require a very small  to effectively average over the individual steps
moving in “competing” directions.
An effective strategy is to “average” between batch and stochastic gradient descent
by using mini-batches. For a mini-batch of size 
, we select 
 distinct data points
uniformly at random from the data set and do the update based just on their
contributions to the gradient
Most neural network software packages are set up to do mini-batches.
❓ Study Question
For what value of 
 is mini-batch gradient descent equivalent to stochastic
gradient descent? To batch gradient descent?
Picking 
 unique data points at random from a large data-set is potentially
computationally difficult. An alternative strategy, if you have an efficient procedure
for randomly shuffling the data set (or randomly shuffling a list of indices into the
data set) is to operate in a loop, roughly as follows:
procedure Mini-Batch-SGD(NN, data, K)
while not done do
Random-Shuffle(data)
for 
 to 
 do
Batch-Gradient-Update(NN, data[(i-1)K : iK])
end for
end while
end procedure
(x(i), y(i))
Wt = Wt−1 −η∇WL(h(x(i); Wt−1), y(i)) .
η
η
K
K
Wt = Wt−1 −η
K
K
∑
i=1
∇WL(h(x(i); Wt−1), y(i)) .
K
K
1:
2:
n ←length(data)
3:
4:
5:
i ←1
⌈n
K ⌉
6:
7:
8:
9:
In line 4 of the algorithm above, 
is known as the ceiling function; it
returns the smallest integer greater
than or equal to its input. E.g.,
 and 
.
⌈⋅⌉
⌈2.5⌉= 3
⌈3⌉= 3

6  Neural Networks
 Picking a value for  is difficult and time-consuming. If it’s too small, then
convergence is slow and if it’s too large, then we risk divergence or slow
convergence due to oscillation. This problem is even more pronounced in stochastic
or mini-batch mode, because we know we need to decrease the step size for the
formal guarantees to hold.
It’s also true that, within a single neural network, we may well want to have
different step sizes. As our networks become deep (with increasing numbers of
layers) we can find that magnitude of the gradient of the loss with respect the
weights in the last layer, 
, may be substantially different from the
gradient of the loss with respect to the weights in the first layer 
. If you
look carefully at Equation 6.6, you can see that the output gradient is multiplied by
all the weight matrices of the network and is “fed back” through all the derivatives
of all the activation functions. This can lead to a problem of exploding or vanishing
gradients, in which the back-propagated gradient is much too big or small to be
used in an update rule with the same step size.
So, we can consider having an independent step-size parameter for each weight, and
updating it based on a local view of how the gradient updates have been going.
Some common strategies for this include momentum (“averaging” recent gradient
updates), Adadelta (take larger steps in parts of the space where 
 is nearly flat),
and Adam (which combines these two previous ideas). Details of these approaches
are described in Section B.1.
6.8 Regularization
So far, we have only considered optimizing loss on the training data as our objective
for neural network training. But, as we have discussed before, there is a risk of
overfitting if we do this. The pragmatic fact is that, in current deep neural networks,
which tend to be very large and to be trained with a large amount of data,
overfitting is not a huge problem. This runs counter to our current theoretical
understanding and the study of this question is a hot area of research. Nonetheless,
there are several strategies for regularizing a neural network, and they can
sometimes be important.
One group of strategies can, interestingly, be shown to have similar effects to each
other: early stopping, weight decay, and adding noise to the training data.
Early stopping is the easiest to implement and is in fairly common use. The idea is
to train on your training set, but at every epoch (a pass through the whole training
6.7.2 Adaptive step-size
η
∂loss/∂WL
∂loss/∂W1
J(W)
6.8.1 Methods related to ridge regression
This section is very strongly
influenced by Sebastian Ruder’s
excellent blog posts on the topic:
{ruder.io/optimizing-gradient-
descent}
Result is due to Bishop, described
in his textbook and here.
Warning: If you use your
validation set in this way – i.e., to

6  Neural Networks
 set, or possibly more frequently), evaluate the loss of the current 
 on a validation
set. It will generally be the case that the loss on the training set goes down fairly
consistently with each iteration, the loss on the validation set will initially decrease,
but then begin to increase again. Once you see that the validation loss is
systematically increasing, you can stop training and return the weights that had the
lowest validation error.
Another common strategy is to simply penalize the norm of all the weights, as we
did in ridge regression. This method is known as weight decay, because when we
take the gradient of the objective
we end up with an update of the form
This rule has the form of first “decaying” 
 by a factor of 
 and then
taking a gradient step.
Finally, the same effect can be achieved by perturbing the 
 values of the training
data by adding a small amount of zero-mean normally distributed noise before each
gradient computation. It makes intuitive sense that it would be more difficult for
the network to overfit to particular training data if they are changed slightly on
each training step.
Dropout is a regularization method that was designed to work with deep neural
networks. The idea behind it is, rather than perturbing the data every time we train,
we’ll perturb the network! We’ll do this by randomly, on each training step,
selecting a set of units in each layer and prohibiting them from participating. Thus,
all of the units will have to take a kind of “collective” responsibility for getting the
answer right, and will not be able to rely on any small subset of the weights to do
all the necessary computation. This tends also to make the network more robust to
data perturbations.
During the training phase, for each training example, for each unit, randomly with
probability  temporarily set 
. There will be no contribution to the output and
no gradient update for the associated unit.
When we are done training and want to use the network to make predictions, we
multiply all weights by  to achieve the same average activation levels.
W
J(W) =
n
∑
i=1
L(NN(x(i)), y(i); W) + λ∥W∥2
Wt = Wt−1 −η ((∇WL(NN(x(i)), y(i); Wt−1)) + 2λWt−1)
= Wt−1(1 −2λη) −η (∇WL(NN(x(i)), y(i); Wt−1)) .
Wt−1
(1 −2λη)
x(i)
6.8.2 Dropout
p
aℓ
j = 0
p
set the number of epochs (or any
other hyperparameter associated
with your learning algorithm) –
then error on the validation set no
longer provides a “pure” estimate
of error on the test set (i.e.,
generalization error). This is
because information about the
validation set has “leaked” into the
design of your algorithm. See also
the discussion on Validation and
Cross-Validation in Chapter 2.

6  Neural Networks
 Implementing dropout is easy! In the forward pass during training, we let
where  denotes component-wise product and 
 is a vector of ’s and ’s drawn
randomly with probability . The backwards pass depends on 
, so we do not need
to make any further changes to the algorithm.
It is common to set  to 
, but this is something one might experiment with to get
good results on your problem and data.
Another strategy that seems to help with regularization and robustness in training
is batch normalization.
It was originally developed to address a problem of covariate shift: that is, if you
consider the second layer of a two-layer neural network, the distribution of its input
values is changing over time as the first layer’s weights change. Learning when the
input distribution is changing is extra difficult: you have to change your weights to
improve your predictions, but also just to compensate for a change in your inputs
(imagine, for instance, that the magnitude of the inputs to your layer is increasing
over time—then your weights will have to decrease, just to keep your predictions
the same).
So, when training with mini-batches, the idea is to standardize the input values for
each mini-batch, just in the way that we did it in Section 5.3.3 of Chapter 5,
subtracting off the mean and dividing by the standard deviation of each input
dimension. This means that the scale of the inputs to each layer remains the same,
no matter how the weights in previous layers change. However, this somewhat
complicates matters, because the computation of the weight updates will need to
take into account that we are performing this transformation. In the modular view,
batch normalization can be seen as a module that is applied to 
, interposed after
the product with 
 and before input to 
.
Although batch-norm was originally justified based on the problem of covariate
shift, it’s not clear that that is actually why it seems to improve performance. Batch
normalization can also end up having a regularizing effect for similar reasons that
adding noise and dropout do: each mini-batch of data ends up being mildly
perturbed, which prevents the network from exploiting very particular values of
the data points. For those interested, the equations for batch normalization,
including a derivation of the forward pass and backward pass, are described in
Section B.2.
aℓ= f(zℓ) ∗dℓ
∗
dℓ
0
1
p
aℓ
p
0.5
6.8.3 Batch normalization
zl
W l
f l
For more details see
arxiv.org/abs/1502.03167.
We follow here the suggestion from
the original paper of applying
batch normalization before the
activation function. Since then it
has been shown that, in some
cases, applying it after works a bit
better. But there aren’t any definite
findings on which works better
and when.

6  Neural Networks
 
6  Neural Networks
 This page contains all content from the legacy PDF notes; convolutional neural networks
chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
So far, we have studied what are called fully connected neural networks, in which all
of the units at one layer are connected to all of the units in the next layer. This is a
good arrangement when we don’t know anything about what kind of mapping
from inputs to outputs we will be asking the network to learn to approximate. But if
we do know something about our problem, it is better to build it into the structure
of our neural network. Doing so can save computation time and significantly
diminish the amount of training data required to arrive at a solution that
generalizes robustly.
One very important application domain of neural networks, where the methods
have achieved an enormous amount of success in recent years, is signal processing.
Signals might be spatial (in two-dimensional camera images or three-dimensional
depth or CAT scans) or temporal (speech or music). If we know that we are
addressing a signal-processing problem, we can take advantage of invariant
properties of that problem. In this chapter, we will focus on two-dimensional spatial
problems (images) but use one-dimensional ones as a simple example. In a later
chapter, we will address temporal problems.
Imagine that you are given the problem of designing and training a neural network
that takes an image as input, and outputs a classification, which is positive if the
image contains a cat and negative if it does not. An image is described as a two-
dimensional array of pixels, each of which may be represented by three integer
values, encoding intensity levels in red, green, and blue color channels.
There are two important pieces of prior structural knowledge we can bring to bear
on this problem:
Spatial locality: The set of pixels we will have to take into consideration to find
a cat will be near one another in the image.
Translation invariance: The pattern of pixels that characterizes a cat is the
same no matter where in the image the cat occurs.
We will design neural network structures that take advantage of these properties.
7.1 Filters
7  Convolutional Neural Networks
Note
A pixel is a “picture element.”
So, for example, we won’t have to
consider some combination of
pixels in the four corners of the
image, in order to see if they
encode cat-ness.
Cats don’t look different if they’re
on the left or the right side of the
image.
7  Convolutional Neural Networks

 We begin by discussing image filters.
An image filter is a function that takes in a local spatial neighborhood of pixel
values and detects the presence of some pattern in that data.
Let’s consider a very simple case to start, in which we have a 1-dimensional binary
“image” and a filter  of size two. The filter is a vector of two numbers, which we
will move along the image, taking the dot product between the filter values and the
image values at each step, and aggregating the outputs to produce a new image.
Let 
 be the original image, of size ; then pixel  of the the output image is
specified by
To ensure that the output image is also of dimension , we will generally “pad” the
input image with 0 values if we need to access pixels that are beyond the bounds of
the input image. This process of applying the filter to the image to create a new
image is called “convolution.”
If you are already familiar with what a convolution is, you might notice that this
definition corresponds to what is often called a correlation and not to a convolution.
Indeed, correlation and convolution refer to different operations in signal
processing. However, in the neural networks literature, most libraries implement
the correlation (as described in this chapter) but call it convolution. The distinction
is not significant; in principle, if convolution is required to solve the problem, the
network could learn the necessary weights. For a discussion of the difference
between convolution and correlation and the conventions used in the literature you
can read Section 9.1 in this excellent book: Deep Learning.
Here is a concrete example. Let the filter 
. Then given the image in
the first line below, we can convolve it with filter 
 to obtain the second image.
You can think of this filter as a detector for “left edges” in the original image—to see
this, look at the places where there is a  in the output image, and see what pattern
exists at that position in the input image. Another interesting filter is
. The third image (the last line below) shows the result of
convolving the first image with 
, where we see that the output pixel 
corresponds to when the center of 
 is aligned at input pixel .
❓ Study Question
Convince yourself that filter 
 can be understood as a detector for isolated
positive pixels in the binary image.
F
X
d
i
Yi = F ⋅(Xi−1, Xi) .
d
F1 = (−1, +1)
F1
1
F2 = (−1, +1, −1)
F2
i
F2
i
F2
Unfortunately in
AI/ML/CS/Math, the word
``filter’’ gets used in many ways: in
addition to the one we describe
here, it can describe a temporal
process (in fact, our moving
averages are a kind of filter) and
even a somewhat esoteric algebraic
structure.
And filters are also sometimes
called convolutional kernels.
 0
0
1
1
1
0
1
0
0
0
Image:
F1:
-1
+1
0
0
1
0
0
-1
1
-1
0
0
After con v olution (with F1):
0
-1
0
-1
0
-2
1
-1
0
0
After con v olution (with F2):
F2
-1
+1
-1
Two-dimensional versions of filters like these are thought to be found in the visual
cortex of all mammalian brains. Similar patterns arise from statistical analysis of
natural images. Computer vision people used to spend a lot of time hand-designing
filter banks. A filter bank is a set of sets of filters, arranged as shown in the diagram
below.
Image
All of the filters in the first group are applied to the original image; if there are 
such filters, then the result is  new images, which are called channels. Now imagine
stacking all these new images up so that we have a cube of data, indexed by the
original row and column indices of the image, as well as by the channel. The next
set of filters in the filter bank will generally be three-dimensional: each one will be
applied to a sub-range of the row and column indices of the image and to all of the
channels.
These 3D chunks of data are called tensors. The algebra of tensors is fun, and a lot
like matrix algebra, but we won’t go into it in any detail.
Here is a more complex example of two-dimensional filtering. We have two 
filters in the first layer, 
 and 
. You can think of each one as “looking” for three
pixels in a row, 
 vertically and 
 horizontally. Assuming our input image is
, then the result of filtering with these two filters is an 
 tensor. Now
k
k
3 × 3
f1
f2
f1
f2
n × n
n × n × 2
There are now many useful neural-
network software packages, such
as TensorFlow and PyTorch that
make operations on tensors easy.
 we apply a tensor filter (hard to draw!) that “looks for” a combination of two
horizontal and two vertical bars (now represented by individual pixels in the two
channels), resulting in a single final 
 image.
When we have a color image as input, we treat it as having three channels, and
hence as an 
 tensor.
f2
f1
tensor
ﬁlter
We are going to design neural networks that have this structure. Each “bank” of the
filter bank will correspond to a neural-network layer. The numbers in the individual
filters will be the “weights” (plus a single additive bias or offset value for each
filter) of the network, that we will train using gradient descent. What makes this
interesting and powerful (and somewhat confusing at first) is that the same weights
are used many many times in the computation of each layer. This weight sharing
means that we can express a transformation on a large image with relatively few
parameters; it also means we’ll have to take care in figuring out exactly how to train
it!
We will define a filter layer  formally with:
number of filters 
;
size of one filter is 
 plus  bias value (for this one filter);
stride 
 is the spacing at which we apply the filter to the image; in all of our
examples so far, we have used a stride of 1, but if we were to “skip” and apply
the filter only at odd-numbered indices of the image, then it would have a
stride of two (and produce a resulting image of half the size);
input tensor size 
padding: 
 is how many extra pixels – typically with value 0 – we add around
the edges of the input. For an input of size 
, our new
effective input size with padding becomes
.
n × n
n × n × 3
l
ml
kl × kl × ml−1
1
sl
nl−1 × nl−1 × ml−1
pl
nl−1 × nl−1 × ml−1
(nl−1 + 2 ⋅pl) × (nl−1 + 2 ⋅pl) × ml−1
For simplicity, we are assuming
that all images and filters are
square (having the same number of
rows and columns). That is in no
way necessary, but is usually fine
and definitely simplifies our
notation.
 This layer will produce an output tensor of size 
, where
. The weights are the values defining the filter:
there will be 
 different 
 tensors of weight values; plus each filter
may have a bias term, which means there is one more weight value per filter. A filter
with a bias operates just like the filter examples above, except we add the bias to the
output. For instance, if we incorporated a bias term of 0.5 into the filter 
 above,
the output would be 
 instead of
.
This may seem complicated, but we get a rich class of mappings that exploit image
structure and have many fewer weights than a fully connected layer would.
❓ Study Question
How many weights are in a convolutional layer specified as above?
❓ Study Question
If we used a fully-connected layer with the same size inputs and outputs, how
many weights would it have?
7.2 Max pooling
It is typical (both in engineering and in natrure) to structure filter banks into a
pyramid, in which the image sizes get smaller in successive layers of processing. The
idea is that we find local patterns, like bits of edges in the early layers, and then
look for patterns in those patterns, etc. This means that, effectively, we are looking
for patterns in larger pieces of the image as we apply successive filters. Having a
stride greater than one makes the images smaller, but does not necessarily
aggregate information over that spatial range.
Another common layer type, which accomplishes this aggregation, is max pooling. A
max pooling layer operates like a filter, but has no weights. You can think of it as
purely functional, like a ReLU in a fully connected network. It has a filter size, as in a
filter layer, but simply returns the maximum value in its field.
Usually, we apply max pooling with the following traits:
, so that the resulting image is smaller than the input image; and
, so that the whole image is covered.
As a result of applying a max pooling layer, we don’t keep track of the precise
location of a pattern. This helps our filters to learn to recognize patterns
nl × nl × ml
nl = ⌈(nl−1 + 2 ⋅pl −(kl −1))/sl⌉
ml
kl × kl × ml−1
F2
(−0.5, 0.5, −0.5, 0.5, −1.5, 1.5, −0.5, 0.5)
(−1, 0, −1, 0, −2, 1, −1, 0)
stride > 1
k ≥stride
Recall that 
 is the function; it
returns the smallest integer greater
than or equal to its input. E.g.,
 and 
.
⌈⋅⌉
⌈2.5⌉= 3
⌈3⌉= 3
We sometimes use the term
receptive field or just field to mean
the area of an input image that a
filter is being applied to.
 independent of their location.
Consider a max pooling layer where both the strides and  are set to be 2. This
would map a 
 image to a 
 image. Note that max pooling
layers do not have additional bias or offset values.
❓ Study Question
Maximilian Poole thinks it would be a good idea to add two max pooling layers
of size , one right after the other, to their network. What single layer would be
equivalent?
One potential concern about max-pooling layers is that they actually don’t
completely preserve translation invariance. If you do max-pooling with a stride
other than 1 (or just pool over the whole image size), then shifting the pattern you
are hoping to detect within the image by a small amount can change the output of
the max-pooling layer substantially, just because there are discontinuities induced
by the way the max-pooling window matches up with its input image. Here is an
interesting paper that illustrates this phenomenon clearly and suggests that one
should first do max-pooling with a stride of 1, then do “downsampling” by
averaging over a window of outputs.
7.3 Typical architecture
Here is the form of a typical convolutional network:
At the end of each filter layer, we typically apply a ReLU activation function. There
may be multiple filter plus ReLU layers. Then we have a max pooling layer. Then
we have some more filter + ReLU layers. Then we have max pooling again. Once
the output is down to a relatively small size, there is typically a last fully-connected
layer, leading into an activation function such as softmax that produces the final
output. The exact design of these structures is an art—there is not currently any
clear theoretical (or even systematic empirical) understanding of how these various
design choices affect overall performance of the network.
The critical point for us is that this is all just a big neural network, which takes an
input and computes an output. The mapping is a differentiable function of the
weights, which means we can adjust the weights to decrease the loss by performing
k
64 × 64 × 3
32 × 32 × 3
k
The “depth” dimension in the
layers shown as cuboids
corresponds to the number of
channels in the output tensor.
(Figure source: Mathworks)
Well, technically the derivative
does not exist at every point, both
because of the ReLU and the max
 gradient descent, and we can compute the relevant gradients using back-
propagation!
7.4 Backpropagation in a simple CNN
Let’s work through a very simple example of how back-propagation can work on a
convolutional network. The architecture is shown below. Assume we have a one-
dimensional single-channel image 
 of size 
, and a single filter 
 of size
 (where we omit the filter bias) for the first convolutional operation
denoted “conv” in the figure below. Then we pass the intermediate result 
through a ReLU layer to obtain the activation 
, and finally through a fully-
connected layer with weights 
, denoted “fc” below, with no additional
activation function, resulting in the output 
.
X = A0
0
0
pad with 0’s 
(to get output 
of same shap e)
W 1
Z1
A1
Z2 = A2
W 2
conv
ReLU
fc
For simplicity assume  is odd, let the input image 
, and assume we are
using squared loss. Then we can describe the forward pass as follows:
❓ Study Question
Assuming a stride of 
 for a filter of size , how much padding do we need to
add to the top and bottom of the image? We see one zero at the top and bottom
in the figure just above; what filter size is implicitly being shown in the figure?
(Recall the padding is for the sake of getting an output the same size as the
input.)
X
n × 1 × 1
W 1
k × 1 × 1
Z 1
A1
W 2
A2
k
X = A0
Z 1
i = W 1TA0
[i−⌊k/2⌋:i+⌊k/2⌋]
A1 = ReLU(Z 1)
A2 = Z 2 = W 2TA1
Lsquare(A2, y) = (A2 −y)2
1,
k
7.4.1 Weight update
pooling operations, but we ignore
that fact.
 How do we update the weights in filter 
?
 is the 
 matrix such that 
. So, for
example, if 
, which corresponds to column 10 in this matrix, which
illustrates the dependence of pixel 10 of the output image on the weights, and
if 
, then the elements in column 10 will be 
.
 is the 
 diagonal matrix such that
, an 
 vector
Multiplying these components yields the desired gradient, of shape 
.
One last point is how to handle back-propagation through a max-pooling operation.
Let’s study this via a simple example. Imagine
where 
 and 
 are each computed by some network. Consider doing back-
propagation through the maximum. First consider the case where 
. Then the
error value at  is propagated back entirely to the network computing the value 
.
The weights in the network computing 
 will ultimately be adjusted, and the
network computing 
 will be untouched.
❓ Study Question
What is 
 ?
W 1
∂loss
∂W 1 = ∂Z 1
∂W 1
∂A1
∂Z 1
∂loss
∂A1
∂Z 1/∂W 1
k × n
∂Z 1
i /∂W 1
j = Xi−⌊k/2⌋+j−1
i = 10
k = 5
X8, X9, X10, X11, X12
∂A1/∂Z 1
n × n
∂A1
i /∂Z 1
i = {1
if Z 1
i > 0
0
otherwise
∂loss/∂A1 = (∂loss/∂A2)(∂A2/∂A1) = 2(A2 −y)W 2
n × 1
k × 1
7.4.2 Max pooling
y = max(a1, a2) ,
a1
a2
a1 > a2
y
a1
a1
a2
∇(x,y) max(x, y)
 This page contains all content from the legacy PDF notes; autoencoders chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
In previous chapters, we have largely focused on classification and regression
problems, where we use supervised learning with training samples that have both
features/inputs and corresponding outputs or labels, to learn hypotheses or models
that can then be used to predict labels for new data.
In contrast to supervised learning paradigm, we can also have an unsupervised
learning setting, where we only have features but no corresponding outputs or
labels for our dataset. On natural question aries then: if there are no labels, what are
we learning?
One canonical example of unsupervised learning is clustering, which is discussed in
Section 12.3. In clustering, the goal is to develop algorithms that can reason about
“similarity” among data points’s features, and group the data points into clusters.
Autoencoders are another family of unsupervised learning algorithms, in this case
seeking to obtain insights about our data by learning compressed versions of the
original data, or, in other words, by finding a good lower-dimensional feature
representations of the same data set. Such insights might help us to discover and
characterize underlying factors of variation in data, which can aid in scientific
discovery; to compress data for efficient storage or communication; or to pre-
process our data prior to supervised learning, perhaps to reduce the amount of data
that is needed to learn a good classifier or regressor.
8.1 Autoencoder structure
Assume that we have input data 
, where 
. We seek to
learn an autoencoder that will output a new dataset 
, where
 with 
. We can think about 
 as the new representation of data point
. For example, in Figure 8.1 we show the learned representations of a dataset of
MNIST digits with 
. We see, after inspecting the individual data points, that
unsupervised learning has found a compressed (or latent) representation where
images of the same digit are close to each other, potentially greatly aiding
subsequent clustering or classification tasks.
8  Representation Learning (Autoencoders)
Note
D = {x(1), … , x(n)}
x(i) ∈Rd
Dout = {a(1), … , a(n)}
a(i) ∈Rk
k < d
a(i)
x(i)
k = 2
8  Representation Learning (Autoencoders)

 Formally, an autoencoder consists of two functions, a vector-valued encoder
 that deterministically maps the data to the representation space 
, and a decoder 
 that maps the representation space back into the
original data space.
In general, the encoder and decoder functions might be any functions appropriate
to the domain. Here, we are particularly interested in neural network embodiments
of encoders and decoders. The basic architecture of one such autoencoder,
consisting of only a single layer neural network in each of the encoder and decoder,
is shown in Figure 8.2; note that bias terms 
 and 
 into the summation nodes
exist, but are omitted for clarity in the figure. In this example, the original -
dimensional input is compressed into 
 dimensions via the encoder
 with 
 and 
, and where the
non-linearity 
 is applied to each dimension of the vector. To recover (an
approximation to) the original instance, we then apply the decoder
, where 
 denotes a different non-linearity
(activation function). In general, both the decoder and the encoder could involve
multiple layers, as opposed to the single layer shown here. Learning seeks
parameters 
 and 
 such that the reconstructed instances,
, are close to the original input 
.
8.2 Autoencoder Learning
g : Rd →Rk
a ∈Rk
h : Rk →Rd
W 1
0
W 2
0
d
k = 3
g(x; W 1, W 1
0 ) = f1(W 1Tx + W 1
0 )
W 1 ∈Rd×k
W 1
0 ∈Rk
f1
h(a; W 2, W 2
0 ) = f2(W 2Ta + W 2
0 )
f2
W 1, W 1
0
W 2, W 2
0
h(g(x(i); W 1, W 1
0 ); W 2, W 2
0 )
x(i)
Figure 8.1: Compression of digits
dataset into two dimensions. The
input 
, an image of a
handwritten digit, is shown at the
new low-dimensional
representation 
.
x(i)
(a1, a2)
Figure 8.2: Autoencoder structure,
showing the encoder (left half,
light green), and the decoder (right
half, light blue), encoding inputs 
to the representation , and
decoding the representation to
produce , the reconstruction. In
this specific example, the
representation (
, 
, 
) only has
three dimensions.
x
a
~x
a1 a2 a3
 We learn the weights in an autoencoder using the same tools that we previously
used for supervised learning, namely (stochastic) gradient descent of a multi-layer
neural network to minimize a loss function. All that remains is to specify the loss
function 
, which tells us how to measure the discrepancy between the
reconstruction 
 and the original input . For
example, for continuous-valued  it might make sense to use squared loss, i.e.,
.
Learning then seeks to optimize the parameters of  and  so as to minimize the
reconstruction error, measured according to this loss function:
8.3 Evaluating an autoencoder
What makes a good learned representation in an autoencoder? Notice that, without
further constraints, it is always possible to perfectly reconstruct the input. For
example, we could let 
 and  and  be the identity functions. In this case, we
would not obtain any compression of the data.
To learn something useful, we must create a bottleneck by making  to be smaller
(often much smaller) than . This forces the learning algorithm to seek
transformations that describe the original data using as simple a description as
possible. Thinking back to the digits dataset, for example, an example of a
compressed representation might be the digit label (i.e., 0–9), rotation, and stroke
thickness. Of course, there is no guarantee that the learning algorithm will discover
precisely this representation. After learning, we can inspect the learned
representations, such as by artificially increasing or decreasing one of the
dimensions (e.g., 
) and seeing how it affects the output 
, to try to better
understand what it has learned.
As with clustering, autoencoders can be a preliminary step toward building other
models, such as a regressor or classifier. For example, once a good encoder has been
learned, the decoder might be replaced with another neural network that is then
trained with supervised learning (perhaps using a smaller dataset that does include
labels).
8.4 Linear encoders and decoders
We close by mentioning that even linear encoders and decoders can be very
powerful. In this case, rather than minimizing the above objective with gradient
descent, a technique called principal components analysis (PCA) can be used to obtain
L(~x, x)
~x = h(g(x; W 1, W 1
0 ); W 2, W 2
0 )
x
x
LSE(~x, x) = ∑d
j=1(xj −~xj)2
h
g
min
W 1,W 1
0 ,W 2,W 2
0
n
∑
i=1
LSE (h(g(x(i); W 1, W 1
0 ); W 2, W 2
0 ), x(i))
k = d
h
g
k
d
a1
h(a)
Alternatively, you could think of
this as multi-task learning, where
the goal is to predict each
dimension of . One can mix-and-
match loss functions as appropriate
for each dimension’s data type.
x
 a closed-form solution to the optimization problem using a singular value
decomposition (SVD). Just as a multilayer neural network with nonlinear
activations for regression (learned by gradient descent) can be thought of as a
nonlinear generalization of a linear regressor (fit by matrix algebraic operations),
the neural network based autoencoders discussed above (and learned with gradient
descent) can be thought of as a generalization of linear PCA (as solved with matrix
algebra by SVD).
8.5 Advanced encoders and decoders
Advanced neural networks built on encoder-decoder architectures have become
increasingly powerful. One prominent example is generative networks, designed to
create new outputs that resemble—but differ from—existing training examples. A
notable type, variational autoencoders, learns a compressed representation
capturing statistical properties (such as mean and variance) of training data. These
latent representations can then be sampled to generate novel outputs using the
decoder.
Another influential encoder-decoder architecture is the Transformer, covered in
Chapter 9. Transformers consist of multiple encoder and decoder layers combined
with self-attention mechanisms, which excel at predicting sequential data, such as
words and sentences in natural language processing (NLP).
Central to autoencoders and Transformers is the idea of learning representations.
Autoencoders compress data into efficient, informative representations, while NLP
models encode language—words, phrases, sentences—into numerical forms. This
numerical encoding leads us to the concept of vector embeddings.
8.6 Embeddings
In NLP, words are represented as vectors, commonly known as word embeddings. A
key property of good embeddings is that their numerical closeness mirrors semantic
similarity. For instance, semantically related words such as “dog” and “cat” should
have vectors close together, while unrelated words like “cat” and “table” should be
farther apart.
Similarity between embeddings is frequently measured using the inner product:
The inner product indicates how aligned two vectors are: highly positive values
imply strong similarity, negative values indicate opposition, and values near zero
suggest no similarity (up to a scaling factor related to the magnitude).
aTb = a ⋅b
 A groundbreaking embedding method, word2vec (2012), significantly advanced NLP
by producing embeddings where vector arithmetic corresponded to real-world
semantic relationships. For instance:
Such embeddings revealed meaningful semantic relationships like analogies across
diverse vocabulary (e.g., uncle – man + woman ≈ aunt).
Importantly, embeddings don’t need exact coordinates—it’s their relative
positioning within the vector space that matters. Embeddings are considered
effective if they facilitate downstream NLP tasks, such as predicting missing words,
classifying texts, or language translation.
For example, effective embeddings allow models to accurately predict a missing
word in a sentence:
After the rain, the grass was ____.
Or a model could be built that tries to correctly predict words in the middle of
sentences:
The child fell __ __ during the long car ride
This task exemplifies self-supervision, a training approach where models generate
labels directly from the data itself, eliminating the need for manual labeling.
Training neural networks through self-supervision involves optimizing their ability
to predict words accurately from large text corpora (e.g., Wikipedia). Through such
optimization, embeddings capture subtle semantic and syntactic nuances, greatly
enhancing NLP capabilities.
The idea of good embeddings will play a central role when we discuss attention
mechanisms in Chapter 9, where embeddings dynamically adjust based on context
(via the so-called attention mechanism), enabling a more nuanced understanding of
language.
embeddingparis −embeddingfrance + embeddingitaly ≈embeddingrome
 We are actively overhauling the Transformers chapter from the legacy PDF notes to
enhance clarity and presentation. Please feel free to raise issues or request more
explanation on specific topics.
Transformers are a very recent family of architectures that were originally
introduced in the field of natural language processing (NLP) in 2017, as an
approach to process and understand human language. Since then, they have
revolutionized not only NLP but also other domains such as image processing and
multi-modal generative AI. Their scalability and parallelizability have made them
the backbone of large-scale foundation models, such as GPT, BERT, and Vision
Transformers (ViT), powering many state-of-the-art applications.
Like CNNs, transformers factorize signal processing into stages, each involving
independently and identically processed chunks. Transformers have many intricate
components; however, we’ll focus on their most crucial innovation: a new type of
layer called the attention layer. Attention layers enable transformers to effectively
mix information across chunks, allowing the entire transformer pipeline to model
long-range dependencies among these chunks. To help make Transformers more
digestible, in this chapter, we will first succinctly motivate and describe them in an
overview Section 9.1. Then, we will dive into the details following the flow of data –
first describing how to represent inputs Section 9.2, and then describe the attention
mechanism Section 9.3, and finally we then assemble all these ideas together to
arrive at the full transformer architecture in Section 9.5.
9.1 Transformers Overview
Transformers are powerful neural architectures designed primarily for sequential
data, such as text. At their core, transformers are typically auto-regressive, meaning
they generate sequences by predicting each token sequentially, conditioned on
previously generated tokens. This auto-regressive property ensures that the
transformer model inherently captures temporal dependencies, making them
especially suited for language modeling tasks like text generation and completion.
Suppose our training data contains this sentence: “To date, the cleverest thinker was
Issac.” The transformer model will learn to predict the next token in the sequence,
given the previous tokens. For example, when predicting the token “cleverest,” the
model will condition its prediction on the tokens “To,” “date,” and “the.” This
9  Transformers
Caution
Human language is inherently
sequential in nature (e.g.,
characters form words, words form
sentences, and sentences form
paragraphs and documents). Prior
to the advent of the transformers
architecture, recurrent neural
networks (RNNs) briefly
dominated the field for their ability
to process sequential information.
However, RNNs, like many other
architectures, processed sequential
information in an
iterative/sequential fashion,
whereby each item of a sequence
was individually processed one
after another. Transformers offer
many advantages over RNNs,
including their ability to process all
items in a sequence in a parallel
fashion (as do CNNs).
9  Transformers

 process continues until the entire sequence is generated.
The animation above illustrates the auto-regressive nature of transformers.
Below is another example. Suppose the sentence is the 2nd law of robotics: “A robot
must obey the orders given it by human beings…” The training objective of a
transformer would be to make each token’s prediction, conditioning on previously
generated tokens, forming a step-by-step probability distribution over the
vocabulary.
The transformer architecture processes inputs by applying multiple identical
building blocks stacked in layers. Each block performs a transformation that
progressively refines the internal representation of the data.
Specifically, each block consists of two primary sub-layers: an attention layer
Section 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6.
Attention layers mix information across different positions (or "chunks") in the
sequence, allowing the model to effectively capture dependencies regardless of
distance. Meanwhile, the feed-forward network significantly enhances the
expressiveness of these representations by applying non-linear transformations
independently to each position.
A notable strength of transformers is their capacity for parallel processing.
Transformers process entire sequences simultaneously rather than sequentially
token-by-token. This parallelization significantly boosts computational efficiency
and makes it feasible to train larger and deeper models.
In this overview, we emphasize the auto-regressive nature of transformers, their
layered approach to transforming representations, the parallel processing
 advantage, and the critical role of the feed-forward layers in enhancing their
expressive power.
There are additional essential components and enhancements—such as causal
attention mechanisms and positional encoding—that further empower
transformers. We’ll explore these "bells and whistles" in greater depth in subsequent
discussions.
9.2 Embedding and Representations
We start by describing how language is commonly represented, then we provide a
brief explanation of why it can be useful to predict subsequent items (e.g.,
words/tokens) in a sequence.
As a reminder, two key components of any ML system are: (1) the representation of
the data; and (2) the actual modelling to perform some desired task. Computers, by
default, have no natural way of representing human language. Modern computers
are based on the Von Neumann architecture and are essentially very powerful
calculators, with no natural understanding of what any particular string of
characters means to us humans. Considering the rich complexities of language (e.g.,
humor, sarcasm, social and cultural references and implications, slang, homonyms,
etc), you can imagine the innate difficulties of appropriately representing
languages, along with the challenges for computers to then model and
“understand” language.
The field of NLP aims to represent words with vectors of floating-point numbers
(aka word embeddings) such that they capture semantic meaning. More precisely,
the degree to which any two words are related in the ‘real-world’ to us humans
should be reflected by their corresponding vectors (in terms of their numeric
values). So, words such as ‘dog’ and ‘cat’ should be represented by vectors that are
more similar to one another than, say, ‘cat’ and ‘table’ are.
To measure how similar any two word embeddings are (in terms of their numeric
values) it is common to use some similarity as the metric, e.g. the dot-product
similarity we saw in Chapter 8.
Thus, one can imagine plotting every word embedding in -dimensional space and
observing natural clusters to form, whereby similar words (e.g., synonyms) are
located near each other. The problem of determining how to parse (aka tokenize)
individual words is known as tokenization. This is an entire topic of its own, so we
will not dive into the full details here. However, the high-level idea of tokenization
is straightforward: the individual inputs of data that are represented and processed
by a model are referred to as tokens. And, instead of processing each word as a
whole, words are typically split into smaller, meaningful pieces (akin to syllables).
For example, the word “evaluation” may be input into a model as 3 individual
d
How can we define an optimal
vocabulary of such tokens? How
many distinct tokens should we
have in our vocabulary? How
should we handle digits or other
punctuation? How does this work
for non-English languages, in
particular, script-based languages
where word boundaries are less
obvious (e.g., Chinese or
Japanese)? All of these are open
 tokens (eval + ua + tion). Thus, when we refer to tokens, know that we’re referring
to these sub-word units. For any given application/model, all of the language data
must be predefined by a finite vocabulary of valid tokens (typically on the order of
40,000 distinct tokens).
9.3 Query, Key, Value, and Attention Output
Attention mechanisms efficiently process global information by selectively focusing
on the most relevant parts of the input. Given an input sentence, each token is
processed sequentially to predict subsequent tokens. As more context (previous
tokens) accumulates, this context ideally becomes increasingly beneficial—provided
the model can appropriately utilize it. Transformers employ a mechanism known as
attention, which enables models to identify and prioritize contextually relevant
tokens.
For example, consider the partial sentence: “Anqi forgot ___“. At this point, the
model has processed tokens”Anqi” and “forgot,” and aims to predict the next
token. Numerous valid completions exist, such as articles (“the,” “an”),
prepositions (“to,” “about”), or possessive pronouns (“her,” “his,” “their”). A well-
trained model should assign higher probabilities to contextually relevant tokens,
such as “her,” based on the feminine-associated name “Anqi.” Attention
mechanisms guide the model to selectively focus on these relevant contextual cues
using query, key, and value vectors.
Our goal is for each input token to learn how much attention it should give to every
other token in the sequence. To achieve this, each token is assigned a unique query
vector used to “probe” or assess other tokens—including itself—to determine
relevance.
A token’s query vector 
 is computed by multiplying the input token 
 (a -
dimensional vector) by a learnable query weight matrix 
 (of dimension 
,
 is a hyperparameter typically chosen such that 
):
Thus, for a sequence of  tokens, we generate  distinct query vectors.
To complement query vectors, we introduce key vectors, which tokens use to
“answer” queries about their relevance. Specifically, when evaluating token 
, its
query vector 
 is compared to each token’s key vector 
 to determine the attention
weight. Each key vector 
 is computed similarly using a learnable key weight
matrix 
:
9.3.1 Query Vectors
qi
xi
d
Wq
d × dk
dk
dk < d
qi = W T
q xi
n
n
9.3.2 Key Vectors
x3
q3
kj
ki
Wk
T
NLP research problems receiving
increased attention lately.
 The attention mechanism calculates similarity using the dot product, which
efficiently measures vector similarity:
The vector 
 (softmax’d attention scores) quantifies how much attention token 
should pay to each token in the sequence, normalized so that elements sum to 1.
Normalizing by 
 prevents large dot-product magnitudes, stabilizing training.
To incorporate meaningful contributions from attended tokens, we use value
vectors (
), providing distinct representations for contribution to attention outputs.
Each token’s value vector is computed with another learnable matrix 
:
Finally, attention outputs are computed as weighted sums of value vectors, using
the softmax’d attention scores:
This vector 
 represents token 
’s enriched embedding, incorporating context
from across the sequence, weighted by learned attention.
9.4 Self-attention Layer
Self-attention is an attention mechanism where the keys, values, and queries are all
generated from the same input.
At a very high level, typical transformer with self-attention layers maps
. In particular, the transformer takes in  tokens, each having feature
dimension 
 and through many layers of transformation (most important of which
are self-attention layers); the transformer finally outputs a sequence of  tokens,
each of which -dimensional still.
With a self-attention layer, there can be multiple attention head. We start with
understanding a single head.
A single self-attention head is largely the same as our discussion in Section 9.3. The
main additional info introduced in this part is a compact matrix form. The layer
ki = W T
k xi
ai = softmax( [qT
i k1, qT
i k2, … , qT
i kn]
√dk
)
T
∈R1×n
ai
qi
√dk
9.3.3 Value Vectors
vi
Wv
vi = W T
v xi
9.3.4 Attention Output
zi =
n
∑
j=1
aijvj ∈Rdk
zi
xi
Rn×d ⟶Rn×d
n
d,
n
d
9.4.1 A Single Self-attention Head
 takes in  tokens, each having feature dimension . Thus, all tokens can be
collectively written as 
, where the -th row of 
 stores the -th token,
denoted as 
. For each token 
, self-attention computes (via learned
projection matrices, discussed in Section 9.3), a query 
, key 
, and
value 
, and overall, we will have  queries,  keys, and  values; all of
these vectors live in the same dimension in practice, and we often denote all three
embedding dimension via a unified 
.
The self-attention output is calculated as a weighted sum:
where 
 is the th element in 
.
So far, we’ve discussed self-attention focusing on a single token input-output.
Actually, we can calculate all outputs 
 (
) at the same time using a
matrix form. For clearness, we first introduce the 
 query matrix,
 key matrix, and 
 value matrix:
It should be straightforward to understand that the 
, 
, 
 matrices simply stack
, 
, and 
 in a row-wise manner, respectively. Now, the the full attention matrix
 is:
which often time is shorten as:
Note that the Softmax operation is applied in a row-wise manner. The th row  of
this matrix corresponds to the softmax’d attention scores computed for query 
over all keys (i.e., 
). The full output of the self-attention layer can then be written
compactly as:
n
d
X ∈Rn×d
i
X
i
xi ∈R1×d
xi
qi ∈Rdq
ki ∈Rdk
vi ∈Rdv
n
n
n
dk
zi =
n
∑
j=1
aijvj ∈Rdk
aij
j
ai
zi i = 1, 2, … , n
Q ∈Rn×dk
K ∈Rn×dk
V ∈Rn×dk
Q =
∈Rn×d,
K =
∈Rn×d,
V =
∈Rn×dv
⎡
⎢
⎣
q⊤
1
q⊤
2
⋮
q⊤
n
⎤
⎥
⎦
⎡
⎢
⎣
k⊤
1
k⊤
2
⋮
k⊤
n
⎤
⎥
⎦
⎡
⎢
⎣
v⊤
1
v⊤
2
⋮
v⊤
n
⎤
⎥
⎦
Q K V
qi ki
vi
A ∈Rn×n
A =
⎡
⎢
⎣
softmax ([
]/√dk)
softmax ([
]/√dk)
⋮
softmax ([
]/√dk)
q⊤
1 k1
q⊤
1 k2
⋯
q⊤
1 kn
q⊤
2 k1
q⊤
2 k2
⋯
q⊤
2 kn
q⊤
n k1
q⊤
n k2
⋯
q⊤
n kn
⎤
⎥
⎦
(9.1)
= softmax ( QK ⊤
√dk
)
A = softmax
1
√dk
⎛
⎜
⎝
⎡
⎢
⎣
q⊤
1 k1
q⊤
1 k2
⋯
q⊤
1 kn
q⊤
2 k1
q⊤
2 k2
⋯
q⊤
2 kn
⋮
⋮
⋱
⋮
q⊤
n k1
q⊤
n k2
⋯
q⊤
n kn
⎤
⎥
⎦
⎞
⎟
⎠
i
A
qi
αi
⊤
 where 
 is the matrix of value vectors stacked row-wise, and 
 is
the output, whose th row corresponds to the attention output for the th query (i.e.,
).
You will also see this compact notation Attention  in the literature, which is an
operation of three arguments 
, 
, and 
 (and we add an emphasis that the
softmax is performed on each row):
Human language can be very nuanced. There are many properties of language that
collectively contribute to a human’s understanding of any given sentence. For
example, words have different tenses (past, present, future, etc), genders,
abbreviations, slang references, implied words or meanings, cultural references,
situational relevance, etc. While the attention mechanism allows us to appropriately
focus on tokens in the input sentence, it’s unreasonable to expect a single set of
 matrices to fully represent – and for a model to capture – the meaning of
a sentence with all of its complexities.
To address this limitation, the idea of multi-head attention is introduced. Instead of
relying on just one attention head (i.e., a single set of 
 matrices), the
model uses multiple attention heads, each with its own independently learned set
of 
 matrices. This allows each head to attend to different parts of the
input tokens and to model different types of semantic relationships. For instance,
one head might focus on syntactic structure and another on verb tense or sentiment.
These different “perspectives” are then concatenated and projected to produce a
richer, more expressive representation of the input.
Now, we introduce the formal math notations. Let us denote the number of head as
. For the th head, the input 
 is linearly projected into query, key, and
value matrices using the projection matrices 
, 
, and
 (recall that usually 
):
The output of the -th head is 
: 
. After
computing all  heads, we concatenate their outputs and apply a final linear
Z =
= AV ∈Rn×dk
⎡
⎢
⎣
z⊤
1
z⊤
2
⋮
z⊤
n
⎤
⎥
⎦
V ∈Rn×dk
Z ∈Rn×dk
i
i
zi
Q K
V
Attention(Q, K, V ) = softmaxrow ( QK ⊤
√dk
)V
9.4.2 Multi-head Self-attention
{Q, K, V }
{Q, K, V }
{Q, K, V }
H
h
X ∈Rn×d
W h
q ∈Rd×dq W h
k ∈Rd×dk
W h
v ∈Rd×dv
dq = dk = dv
Qh = XW h
q
K h = XW h
k
V h = XW h
v
i
Z h Z h = Attention(Qh, K h, V h) ∈Rn×dk
h
 projection:
where the concatenation operation concatenates 
 horizontally, yielding a matrix
of size 
, and 
 is a final linear projection matrix.
9.5 Transformers Architecture Details
An extremely observant reader might have been suspicious of a small but very
important detail that we have not yet discussed: the attention mechanism, as
introduced so far, does not encode the order of the input tokens. For instance, when
computing softmax’d attention scores and building token representations, the
model is fundamentally permutation-equivariant — the same set of tokens, even if
scrambled into a different order, would result in identical outputs permuted in the
same order — Formally, when we fix 
 and switch the input 
 with 
, then the output 
 and 
 will be switched. However, natural language is not a bag
of words: meaning is tied closely to word order.
To address this, transformers incorporate positional embeddings — additional
information that encodes the position of each token in the sequence. These
embeddings are added to the input token embeddings before any attention layers
are applied, effectively injecting ordering information into the model.
There are two main strategies for positional embeddings: (i) learned positional
embeddings, where a trainable vector 
 is assigned to each position (i.e.,
token index) 
. These vectors are learned alongside all other model
parameters and allow the model to discover how best to encode position for a given
task, (ii) fixed positional embeddings, such as sinusoidal positional embedding
proposed in the original Transformer paper:
where 
 is the token index, while 
 is the dimension index.
Namely, this sinusoidal positional embedding uses sine for the even dimension and
cosine for the odd dimension. Regardless of learnable or fixed positional
embedding, it will enter the computation of attention at the input place:
​ where 
 is the th original input token, and 
 is its positional
embedding. The 
 will now be what we really feed into the attention layer, so that
the input to the attention mechanism now carries information about both what the
token is and where it appears in the sequence.
MultiHead(X) = Concat(Z 1, … , Z H)(W O)T
Z h
n × Hdk
W O ∈Rd×Hdk
9.5.1 Positional Embeddings
{Wq, Wk, Wv}
xi
xj
zi
zj
pi ∈Rd
i = 0, 1, 2, . . . , n
p(i,2k) = sin (
i
100002k/d )
p(i,2k+1) = cos (
i
100002k/d )
i = 1, 2, . . , n
k = 1, 2, . . . , d
x∗
i = xi + pi ,
xi
i
pi
x∗
i
 This simple additive design enables attention layers to leverage both semantic
content and ordering structure when deciding where to focus. In practice, this
addition occurs at the very first layer of the transformer stack, and all subsequent
layers operate on position-aware representations. This is a key design choice that
allows transformers to work effectively with sequences of text, audio, or even image
patches (as in Vision Transformers).
More generally, a mask may be applied to limit which tokens are used in the
attention computation. For example, one common mask limits the attention
computation to tokens that occur previously in time to the one being used for the
query. This prevents the attention mechanism from “looking ahead” in scenarios
where the transformer is being used to generate one token at a time. This causal
masking is done by introducing a mask matrix 
 that restricts attention to
only current and previous positions. A typical causal mask is a lower-triangular
matrix:
and we now have the masked attention matrix:
The softmax is performed to each row independently. The attention output is still
. Essentially, the lower-triangular property of 
 ensures that the self-
attention operation for the -th query only considers tokens 
. Note that we
should apply the masking before performing softmax, so that the attention matrix
can be properly normalized (i.e., each row sum to 1).
Each self-attention stage is trained to have key, value, and query embeddings that
lead it to pay specific attention to some particular feature of the input. We generally
want to pay attention to many different kinds of features in the input; for example,
in translation one feature might be be the verbs, and another might be objects or
subjects. A transformer utilizes multiple instances of self-attention, each known as
an “attention head,” to allow combinations of attention paid to many different
features.
9.5.2 Causal Self-attention
M ∈Rn×n
M =
⎡
⎢
⎣
0
−∞
−∞
⋯
−∞
0
0
−∞
⋯
−∞
0
0
0
⋯
−∞
⋮
⋮
⋮
⋱
⋮
0
0
0
⋯
0
⎤
⎥
⎦
A = softmax
1
√dk
+ M
⎛
⎜
⎝
⎡
⎢
⎣
q⊤
1 k1
q⊤
1 k2
⋯
q⊤
1 kn
q⊤
2 k1
q⊤
2 k2
⋯
q⊤
2 kn
⋮
⋮
⋱
⋮
q⊤
n k1
q⊤
n k2
⋯
q⊤
n kn
⎤
⎥
⎦
⎞
⎟
⎠
Y = AV
M
j
0, 1, . . . , j
  This page contains all content from the legacy PDF notes; markov decision processes
chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Consider a robot learning to navigate through a maze, a game-playing AI
developing strategies through self-play, or a self-driving car making driving
decisions in real-time. These problems share a common challenge: the agent must
make a sequence of decisions where each choice affects future possibilities and
rewards. Unlike static prediction tasks where we learn a one-time mapping from
inputs to outputs, these problems require reasoning about the consequences of
actions over time.
This sequential and dynamical nature demands mathematical tools beyond the
more static supervised or unsupervised learning approaches. The most general
framework for such problems is reinforcement learning (RL), where an agent learns to
take actions in an unknown environment to maximize cumulative rewards over
time.
In this chapter, we’ll first study Markov decision processes (MDPs), which provide the
mathematical foundation for understanding and solving sequential decision
making problems like RL. MDPs formalize the interaction between an agent and its
environment, capturing the key elements of states, actions, rewards, and transitions.
10.1 Definition and value functions
Formally, a Markov decision process is 
 where  is the state space, 
is the action space, and:
 is a transition model, where
specifying a conditional probability distribution;
 is a reward function, where 
 specifies an immediate
reward for taking action  when in state ; and
 is a discount factor, which we’ll discuss in Section 10.1.2.
In this class, we assume the rewards are deterministic functions. Further, in this
MDP chapter, we assume the state space and action space are discrete and finite.
10  Markov Decision Processes
Note
⟨S, A, T, R, γ⟩
S
A
T : S × A × S →R
T(s, a, s′) = Pr(St = s′|St−1 = s, At−1 = a) ,
R : S × A →R
R(s, a)
a
s
γ ∈[0, 1]
The notation 
 uses a capital
letter  to stand for a random
variable, and small letter  to stand
for a concrete value. So 
 here is a
random variable that can take on
elements of  as values.
St = s′
S
s
St
S
10  Markov Decision Processes

 The following description of a simple machine as Markov decision process provides a
concrete example of an MDP.
The machine has three possible operations (actions): wash , paint , and eject  (each with
a corresponding button). Objects are put into the machine, and each time you push a
button, something is done to the object. However, it’s an old machine, so it’s not very
reliable. The machine has a camera inside that can clearly detect what is going on with the
object and will output the state of the object: dirty , clean , painted , or ejected .
For each action, this is what is done to the object:
Wash
If you perform the wash  operation on any object—whether it’s dirty, clean, or
painted—it will end up clean  with probability 0.9 and dirty  otherwise.
Paint
If you perform the paint  operation on a clean object, it will become nicely painted
with probability 0.8. With probability 0.1, the paint misses but the object stays clean,
and with probability 0.1, the machine dumps rusty dust all over the object, making it
dirty .
If you perform the paint  operation on a painted  object, it stays painted  with
probability 1.0.
If you perform the paint  operation on a dirty  object, it stays dirty  with
probability 1.0.
Eject
If you perform an eject  operation on any object, the object comes out of the
machine and the process is terminated. The object remains ejected  regardless of
any further actions.
These descriptions specify the transition model , and the transition function for each
action can be depicted as a state machine diagram. For example, here is the diagram for
wash :
Example
T
 dirty
clean
painted
ejected
0.1
0.9
0.9
0.1
0.1
0.9
1.0
You get reward +10 for ejecting a painted object, reward 0 for ejecting a non-painted
object, reward 0 for any action on an “ejected” object, and reward -3 otherwise. The MDP
description would be completed by also specifying a discount factor.
A policy is a function  that specifies what action to take in each state. The policy is
what we will want to learn; it is akin to the strategy that a player employs to win a
given game. Below, we take just the initial steps towards this eventual goal. We
describe how to evaluate how good a policy is, first in the finite horizon case
Section 10.1.1 when the total number of transition steps is finite. In the finite
horizon case, we typically denote the policy as 
, where  is a non-negative
integer denoting the number of steps remaining and 
 is the current state. Then
we consider the infinite horizon case Section 10.1.2, when you don’t know when the
game will be over.
The goal of a policy is to maximize the expected total reward, averaged over the
stochastic transitions that the domain makes. Let’s first consider the case where
there is a finite horizon , indicating the total number of steps of interaction that the
agent will have with the MDP.
We seek to measure the goodness of a policy. We do so by defining for a given
horizon  and MDP policy 
, the “horizon  value” of a state, 
. We do this by
induction on the horizon, which is the number of steps left to go.
The base case is when there are no steps remaining, in which case, no matter what
state we’re in, the value is 0, so
Then, the value of a policy in state  at horizon 
 is equal to the reward it will
get in state  plus the next state’s expected horizon  value, discounted by a factor 
π
πh(s)
h
s ∈S
10.1.1 Finite-horizon value functions
h
h
πh
h
Vπ
h(s)
Vπ
0(s) = 0 .
(10.1)
s
h + 1
s
h
γ
 . So, starting with horizons 1 and 2, and then moving to the general case, we have:
The sum over  is an expectation: it considers all possible next states , and
computes an average of their 
-horizon values, weighted by the probability
that the transition function from state  with the action chosen by the policy 
assigns to arriving in state , and discounted by .
❓ Study Question
What is the value of
for any given state–action pair 
?
❓ Study Question
Convince yourself that the definitions in Equation 10.1 and Equation 10.3 are
special cases of the more general formulation in Equation 10.4.
Then we can say that a policy  is better than policy  for horizon  if and only if
for all 
, 
 and there exists at least one 
 such that
.
More typically, the actual finite horizon is not known, i.e., when you don’t know
when the game will be over! This is called the infinite horizon version of the problem.
How does one evaluate the goodness of a policy in the infinite horizon case?
If we tried to simply take our definitions above and use them for an infinite
horizon, we could get in trouble. Imagine we get a reward of 1 at each step under
one policy and a reward of 2 at each step under a different policy. Then the reward
as the number of steps grows in each case keeps growing to become infinite in the
limit of more and more steps. Even though it seems intuitive that the second policy
should be better, we can’t justify that by saying 
.
Vπ
1(s) = R(s, π1(s)) + 0
(10.2)
Vπ
2(s) = R(s, π2(s)) + γ ∑
s′
T(s, π2(s), s′)Vπ
1(s′)
(10.3)
⋮
Vπ
h(s) = R(s, πh(s)) + γ ∑
s′
T(s, πh(s), s′)Vπ
h−1(s′)
(10.4)
s′
s′
(h −1)
s
πh(s)
s′
γ
∑
s′
T(s, a, s′)
(s, a)
π
¯π
h
s ∈S Vπ
h(s) ≥V¯π
h(s)
s ∈S
Vπ
h(s) > V¯π
h(s)
10.1.2 Infinite-horizon value functions
∞< ∞
 One standard approach to deal with this problem is to consider the discounted
infinite horizon. We will generalize from the finite-horizon case by adding a
discount factor.
In the finite-horizon case, we valued a policy based on an expected finite-horizon
value:
where 
 is the reward received at time .
What is 
? This mathematical notation indicates an expectation, i.e., an average taken
over all the random possibilities which may occur for the argument. Here, the expectation
is taken over the conditional probability 
, where 
 is the random variable
for the reward, subject to the policy being  and the state being 
. Since  is a function,
this notation is shorthand for conditioning on all of the random variables implied by
policy  and the stochastic transitions of the MDP.
A very important point is that 
 is always deterministic (in this class) for any given
 and . Here 
 represents the set of all possible 
 at time step ; this 
 is a
random variable because the state we’re in at step  is itself a random variable, due to
prior stochastic state transitions up to but not including at step  and prior (deterministic)
actions dictated by policy 
Now, for the infinite-horizon case, we select a discount factor 
, and
evaluate a policy based on its expected infinite horizon value:
Note that the  indices here are not the number of steps to go, but actually the
number of steps forward from the starting state (there is no sensible notion of “steps
to go” in the infinite horizon case).
Equation 10.5 and Equation 10.6 are a conceptual stepping stone. Our main objective is to
get to Equation 10.8, which can also be viewed as including  in Equation 10.4, with the
appropriate definition of the infinite-horizon value.
There are two good intuitive motivations for discounting. One is related to
economic theory and the present value of money: you’d generally rather have some
money today than that same amount of money next week (because you could use it
now or invest it). The other is to think of the whole process terminating, with
probability 
 on each step of the interaction. (At every step, your expected
future lifetime, given that you have survived until now, is 
.) This value is
the expected amount of reward the agent would gain under this terminating model.
E [
h−1
∑
t=0
γ tRt ∣π, s0] ,
(10.5)
Rt
t
Note
E [⋅]
Pr(Rt = r ∣π, s0)
Rt
π
s0
π
π
R(s, a)
s
a
Rt
R(st, a)
t
Rt
t
t
π.
0 ≤γ ≤1
E [
∞
∑
t=0
γ tRt ∣π, s0] = E [R0 + γR1 + γ 2R2 + … ∣π, s0] .
(10.6)
t
Note
γ
1 −γ
1/(1 −γ)
 ❓ Study Question
Verify this fact: if, on every day you wake up, there is a probability of 
 that
today will be your last day, then your expected lifetime is 
 days.
Let us now evaluate a policy in terms of the expected discounted infinite-horizon
value that the agent will get in the MDP if it executes that policy. We define the
infinite-horizon value of a state  under policy  as
Because the expectation of a linear combination of random variables is the linear
combination of the expectations, we have
The equation defined in Equation 10.8 is known as the Bellman Equation, which
breaks down the value function into the immediate reward and the (discounted)
future value function. You could write down one of these equations for each of the
 states. There are  unknowns 
. These are linear equations, and
standard software (e.g., using Gaussian elimination or other linear algebraic
methods) will, in most cases, enable us to find the value of each state under this
policy.
10.2 Finding policies for MDPs
Given an MDP, our goal is typically to find a policy that is optimal in the sense that
it gets as much total reward as possible, in expectation over the stochastic
transitions that the domain makes. We build on what we have learned about
evaluating the goodness of a policy (Section 10.1.2), and find optimal policies for the
finite horizon case (Section 10.2.1), then the infinite horizon case (Section 10.2.2).
How can we go about finding an optimal policy for an MDP? We could imagine
enumerating all possible policies and calculating their value functions as in the
previous section and picking the best one – but that’s too much work!
The first observation to make is that, in a finite-horizon problem, the best action to
take depends on the current state, but also on the horizon: imagine that you are in a
situation where you could reach a state with reward 5 in one step or a state with
reward 100 in two steps. If you have at least two steps to go, then you’d move
1 −γ
1/(1 −γ)
s
π
Vπ
∞(s) = E[R0 + γR1 + γ 2R2 + ⋯∣π, S0 = s]
= E[R0 + γ(R1 + γ(R2 + γ …))) ∣π, S0 = s] .
(10.7)
Vπ
∞(s) = E[R0 ∣π, S0 = s] + γE[R1 + γ(R2 + γ …))) ∣π, S0 = s]
= R(s, π(s)) + γ ∑
s′
T(s, π(s), s′)Vπ
∞(s′) .
(10.8)
n = |S|
n
Vπ(s)
10.2.1 Finding optimal finite-horizon policies
 toward the reward 100 state, but if you only have one step left to go, you should go
in the direction that will allow you to gain 5!
For the finite-horizon case, we define 
 to be the expected value of
starting in state ,
executing action , and
continuing for 
 more steps executing an optimal policy for the
appropriate horizon on each step.
Similar to our definition of 
 for evaluating a policy, we define the 
 function
recursively according to the horizon. The only difference is that, on each step with
horizon , rather than selecting an action specified by a given policy, we select the
value of  that will maximize the expected 
 value of the next state.
where 
 denotes the next time-step state/action pair. We can solve for the
values of 
 with a simple recursive algorithm called finite-horizon value iteration
that just computes 
 starting from horizon 0 and working backward to the desired
horizon . Given 
, an optimal 
 can be found as follows:
which gives the immediate best action(s) to take when there are  steps left; then
 gives the best action(s) when there are 
 steps left, and so on. In the
case where there are multiple best actions, we typically can break ties randomly.
Additionally, it is worth noting that in order for such an optimal policy to be
computed, we assume that the reward function 
 is bounded on the set of all
possible (state, action) pairs. Furthermore, we will assume that the set of all possible
actions is finite.
❓ Study Question
The optimal value function is unique, but the optimal policy is not. Think of a
situation in which there is more than one optimal policy.
Q∗
h(s, a)
s
a
h −1
V∗
h
Q∗
h
h
a
Q∗
h
Q∗
0(s, a) = 0
Q∗
1(s, a) = R(s, a) + 0
Q∗
2(s, a) = R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Q∗
1(s′, a′)
⋮
Q∗
h(s, a) = R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Q∗
h−1(s′, a′)
(s′, a′)
Q∗
h
Q∗
h
h
Q∗
h
π∗
h
π∗
h(s) = arg max
a
Q∗
h(s, a) .
h
π∗
h−1(s)
(h −1)
R(s, a)
10.2.2 Finding optimal infinite-horizon policies
We can also define the action-value
function for a fixed policy ,
denoted by 
. This quantity
represents the expected sum of
discounted rewards obtained by
taking action  in state  and
thereafter following the policy 
over the remaining horizon of
 steps.
Similar to 
, 
 satisfies
the Bellman recursion/equations
introduced earlier. In fact, for a
deterministic policy :
However, since our primary goal in
dealing with action values is
typically to identify an optimal
policy, we will not dwell
extensively on (
). Instead,
we will place more emphasis on
the optimal action-value functions
.
π
Qπ
h(s, a)
a
s
π
h −1
Vπ
h(s) Qπ
h(s, a)
π
Qπ
h(s, π(s)) = Vπ
h(s).
Qπ
h(s, a)
Q∗
h(s, a)
 In contrast to the finite-horizon case, the best way of behaving in an infinite-horizon
discounted MDP is not time-dependent. That is, the decisions you make at time
 looking forward to infinity, will be the same decisions that you make at time
 for any positive , also looking forward to infinity.
An important theorem about MDPs is: in the infinite-horizon case, there exists a
stationary optimal policy 
 (there may be more than one) such that for all 
and all other policies , we have
There are many methods for finding an optimal policy for an MDP. We have already
seen the finite-horizon value iteration case. Here we will study a very popular and
useful method for the infinite-horizon case, infinite-horizon value iteration. It is also
important to us, because it is the basis of many reinforcement-learning methods.
We will again assume that the reward function 
 is bounded on the set of all
possible (state, action) pairs and additionally that the number of actions in the
action space is finite. Define 
 to be the expected infinite-horizon value of
being in state , executing action , and executing an optimal policy 
 thereafter.
Using similar reasoning to the recursive definition of 
 we can express this value
recursively as
This is also a set of equations, one for each 
 pair. This time, though, they are
not linear (due to the 
 operation), and so they are not easy to solve. But there is
a theorem that says they have a unique solution!
Once we know the optimal action-value function 
, then we can extract an
optimal policy 
 as
We can iteratively solve for the 
 values with the infinite-horizon value iteration
algorithm, shown below:
Algorithm 10.1 Infinite-Horizon-Value-Iteration
Require: , 
, , , , 
Initialization:
for each 
 and 
 do
end for
while not converged do
for each 
 and 
 do
end for
if 
 then
return 
end if
t = 0
t = T
T
π∗
s ∈S
π
Vπ(s) ≤Vπ∗(s) .
R(s, a)
Q∗
∞(s, a)
s
a
π∗
Vπ,
Q∗
∞(s, a) = R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Q∗
∞(s′, a′) .
(s, a)
max
Q∗
∞(s, a)
π∗
π∗(s) = arg max
a
Q∗
∞(s, a)
Q∗
S A T R γ ϵ
1:
2:
s ∈S
a ∈A
3:
Qold(s, a) ←0
4:
5:
6:
s ∈S
a ∈A
7:
Qnew(s, a) ←R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Qold(s′, a′)
8:
9:
max
s,a |Qold(s, a) −Qnew(s, a)| < ϵ
10:
Qnew
11:
 end while
There are a lot of nice theoretical results about infinite-horizon value iteration. For
some given (not necessarily optimal) 
 function, define 
.
After executing infinite-horizon value iteration with convergence hyper-
parameter ,
There is a value of  such that
As the algorithm executes, 
 decreases monotonically on each
iteration.
The algorithm can be executed asynchronously, in parallel: as long as all 
pairs are updated infinitely often in an infinite run, it still converges to the
optimal value.
12:
Qold ←Qnew
13:
Theory
Q
πQ(s) = arg maxa Q(s, a)
ϵ
∥VπQnew −Vπ∗∥max < ϵ .
ϵ
∥Qold −Qnew∥max < ϵ ⟹πQnew = π∗
∥VπQnew −Vπ∗∥max
(s, a)
 This page contains all content from the legacy PDF notes; reinforcement learning chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Reinforcement learning (RL) is a type of machine learning where an agent learns to
make decisions by interacting with an environment. Unlike other learning
paradigms, RL has several distinctive characteristics:
The agent interacts directly with an environment, receiving feedback in the
form of rewards or penalties
The agent can choose actions that influences what information it gains from the
environment
The agent updates its decision-making strategy incrementally as it gains more
experience
In a reinforcement learning problem, the interaction between the agent and
environment follows a specific pattern:
Learner
Environmen t
reward
state
action
The interaction cycle proceeds as follows:
1. Agent observes the current state 
2. Agent selects and executes an action 
3. Agent receives a reward 
 from the environment
4. Agent observes the new state 
5. Agent selects and executes a new action 
6. Agent receives a new reward 
7. This cycle continues…
Similar to MDP Chapter 10, in an RL problem, the agent’s goal is to learn a policy - a
mapping from states to actions - that maximizes its expected cumulative reward
over time. This policy guides the agent’s decision-making process, helping it choose
actions that lead to the most favorable outcomes.
11  Reinforcement Learning
Note
s(i)
a(i)
r(i)
s(i+1)
a(i+1)
r(i+1)
11  Reinforcement Learning

 11.1 Reinforcement learning algorithms overview
Approaches to reinforcement learning differ significantly according to what kind of
hypothesis or model is being learned. Roughly speaking, RL methods can be
categorized into model-free methods and model-based methods. The main
distinction is that model-based methods explicitly learn the transition and reward
models to assist the end-goal of learning a policy; model-free methods do not. We
will start our discussion with the model-free methods, and introduce two of the
arguably most popular types of algorithms, Q-learning Section 11.1.2 and policy
gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,
we briefly consider “bandit” problems Section 11.5, which differ from our MDP
learning context by having probabilistic rewards.
Model-free methods are methods that do not explicitly learn transition and reward
models. Depending on what is explicitly being learned, model-free methods are
sometimes further categorized into value-based methods (where the goal is to
learn/estimate a value function) and policy-based methods (where the goal is to
directly learn an optimal policy). It’s important to note that such categorization is
approximate and the boundaries are blurry. In fact, current RL research tends to
combine the learning of value functions, policies, and transition and reward models
all into a complex learning algorithm, in an attempt to combine the strengths of
each approach.
Q-learning is a frequently used class of RL algorithms that concentrates on learning
(estimating) the state-action value function, i.e., the 
 function. Specifically, recall
the MDP value-iteration update:
The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,
where we do not know the transition function  or reward function 
, and instead
rely on samples to perform the updates.
procedure Q-Learning(
)
for all 
 do
end for
while 
 do
11.1.1 Model-free methods
11.1.2 Q-learning
Q
Q(s, a) = R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Q(s′, a′)
T
R
1:
S, A, γ, α, s0, max_iter
2:
i ←0
3:
s ∈S, a ∈A
4:
Qold(s, a) ←0
5:
6:
s ←s0
7:
i < max_iter
The thing that most students seem
to get confused about is when we
do value iteration and when we do
Q-learning. Value iteration
assumes you know  and 
 and
just need to compute 
. In Q-
learning, we don’t know or even
directly estimate  and 
: we
estimate 
 directly from
experience!
T
R
Q
T
R
Q
 end while
return 
end procedure
With the pseudo‑code provided for Q‑Learning, there are a few key things to note.
First, we must determine which state to initialize the learning from. In the context of
a game, this initial state may be well defined. In the context of a robot navigating an
environment, one may consider sampling the initial state at random. In any case,
the initial state is necessary to determine the trajectory the agent will experience as
it navigates the environment.
Second, different contexts will influence how we want to choose when to stop
iterating through the while loop. Again, in some games there may be a clear
terminating state based on the rules of how it is played. On the other hand, a robot
may be allowed to explore an environment ad infinitum. In such a case, one may
consider either setting a fixed number of transitions (as done explicitly in the
pseudo‑code) to take; or we may want to stop iterating in the example once the
values in the Q‑table are not changing, after the algorithm has been running for a
while.
Finally, a single trajectory through the environment may not be sufficient to
adequately explore all state‑action pairs. In these instances, it becomes necessary to
run through a number of iterations of the Q‑Learning algorithm, potentially with
different choices of initial state 
.
Of course, we would then want to modify Q‑Learning such that the Q table is not
reset with each call.
Now, let’s dig into what is happening in Q‑Learning. Here, 
 represents the
learning rate, which needs to decay for convergence purposes, but in practice is often
set to a constant. It’s also worth mentioning that Q-learning assumes a discrete state
and action space where states and actions take on discrete values like 
 etc.
In contrast, a continuous state space would allow the state to take values from, say,
a continuous range of numbers; for example, the state could be any real number in
the interval 
. Similarly, a continuous action space would allow the action to be
drawn from, e.g., a continuous range of numbers. There are now many extensions
developed based on Q-learning that can handle continuous state and action spaces
(we’ll look at one soon), and therefore the algorithm above is also sometimes
referred to more specifically as tabular Q-learning.
In the Q-learning update rule
8:
a ←select_action(s, Qold(s, a))
9:
(r, s′) ←execute(a)
10:
Qnew(s, a) ←(1 −α) Qold(s, a) + α(r + γ maxa′ Qold(s′, a′))
11:
s ←s′
12:
i ←i + 1
13:
Qold ←Qnew
14:
15:
Qnew
16:
s0
α ∈(0, 1]
1, 2, 3, …
[1, 3]
Q[s, a] ←(1 −α)Q[s, a] + α(r + γ max
a′
Q[s′, a′])
(11.1)
This notion of running a number of
instances of Q‑Learning is often
referred to as experiencing
multiple episodes.
 the term 
 is often referred to as the one-step look-ahead target.
The update can be viewed as a combination of two different iterative processes that
we have already seen: the combination of an old estimate with the target using a
running average with a learning rate 
Equation 11.1 can also be equivalently rewritten as
which allows us to interpret Q-learning in yet another way: we make an update (or
correction) based on the temporal difference between the target and the current
estimated value 
The Q-learning algorithm above includes a procedure called select_action , that,
given the current state  and current 
 function, has to decide which action to take.
If the 
 value is estimated very accurately and the agent is deployed to “behave” in
the world (as opposed to “learn” in the world), then generally we would want to
choose the apparently optimal action 
.
But, during learning, the 
 value estimates won’t be very good and exploration is
important. However, exploring completely at random is also usually not the best
strategy while learning, because it is good to focus your attention on the parts of the
state space that are likely to be visited when executing a good policy (not a bad or
random one).
A typical action-selection strategy that attempts to address this exploration versus
exploitation dilemma is the so-called -greedy strategy:
with probability 
, choose 
;
with probability , choose the action 
 uniformly at random.
where the  probability of choosing a random action helps the agent to explore and
try out actions that might not seem so desirable at the moment.
Q-learning has the surprising property that it is guaranteed to converge to the actual
optimal 
 function! The conditions specified in the theorem are: visit every state-
action pair infinitely often, and the learning rate  satisfies a scheduling condition.
This implies that for exploration strategy specifically, any strategy is okay as long as
it tries every state-action infinitely often on an infinite run (so that it doesn’t
converge prematurely to a bad action choice).
Q-learning can be very inefficient. Imagine a robot that has a choice between
moving to the left and getting a reward of 1, then returning to its initial state, or
moving to the right and walking down a 10-step hallway in order to get a reward of
1000, then returning to its initial state.
(r + γ maxa′ Q[s′, a′])
α.
Q[s, a] ←Q[s, a] + α((r + γ max
a′
Q[s′, a′]) −Q[s, a]),
(11.2)
Q[s, a].
s
Q
Q
arg maxa∈A Q(s, a)
Q
ϵ
1 −ϵ
arg maxa∈A Q(s, a)
ϵ
a ∈A
ϵ
Q
α
 robot
1
2
3
4
5
6
7
8
9
10
+1000
+1
-1
The first time the robot moves to the right and goes down the hallway, it will
update the 
 value just for state 9 on the hallway and action ``right’’ to have a high
value, but it won’t yet understand that moving to the right in the earlier steps was a
good choice. The next time it moves down the hallway it updates the value of the
state before the last one, and so on. After 10 trips down the hallway, it now can see
that it is better to move to the right than to the left.
More concretely, consider the vector of Q values 
, representing
the Q values for moving right at each of the positions 
. Position index 0
is the starting position of the robot as pictured above.
Then, for 
 and 
, Equation 11.2 becomes
Starting with Q values of 0,
Since the only nonzero reward from moving right is 
, after our
robot makes it down the hallway once, our new Q vector is
After making its way down the hallway again,
updates:
Similarly,
Q
Q(i = 0, … , 9; right)
i = 0, … , 9
α = 1
γ = 0.9
Q(i, right) = R(i, right) + 0.9 max
a
Q(i + 1, a) .
Q(0)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
0
0
R(9, right) = 1000
Q(1)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
0
1000
Q(8, right) = 0 + 0.9 Q(9, right) = 900
Q(2)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
900
1000
Q(3)(i = 0, … , 9; right) = [
] ,
0
0
0
0
0
0
0
810
900
1000
Q(4)(i = 0, … , 9; right) = [
] ,
0
0
0
0
0
0
729
810
900
1000
We are violating our usual
notational conventions here, and
writing 
 to mean the Q value
function that results after the robot
runs all the way to the end of the
hallway, when executing the policy
that always moves to the right.
Qi
 and the robot finally sees the value of moving right from position 0.
❓ Study Question
Determine the Q value functions that result from always executing the “move
left” policy.
11.2 Function approximation: Deep Q learning
In our Q-learning algorithm above, we essentially keep track of each 
 value in a
table, indexed by  and . What do we do if  and/or 
 are large (or continuous)?
We can use a function approximator like a neural network to store Q values. For
example, we could design a neural network that takes inputs  and , and outputs
. We can treat this as a regression problem, optimizing this loss:
where 
 is now the output of the neural network.
There are several different architectural choices for using a neural network to
approximate 
 values:
One network for each action , that takes  as input and produces 
 as
output;
One single network that takes  as input and produces a vector 
,
consisting of the 
 values for each action; or
One single network that takes 
 concatenated into a vector (if  is discrete,
we would probably use a one-hot encoding, unless it had some useful internal
structure) and produces 
 as output.
The first two choices are only suitable for discrete (and not too big) action sets. The
last choice can be applied for continuous actions, but then it is difficult to find
.
There are not many theoretical guarantees about Q-learning with function
approximation and, indeed, it can sometimes be fairly unstable (learning to perform
well for a while, and then suddenly getting worse, for example). But neural
network Q-learning has also had some significant successes.
…
Q(10)(i = 0, … , 9; right) = [
] ,
387.4
420.5
478.3
531.4
590.5
656.1
729
810
900
1000
Q
s
a
S
A
s
a
Q(s, a)
(Q(s, a) −(r + γ max
a′
Q(s′, a′)))
2
Q(s, a)
Q
a
s
Q(s, a)
s
Q(s, ⋅)
Q
s, a
a
Q(s, a)
arg maxa∈A Q(s, a)
Here, we can see the
exploration/exploitation dilemma
in action: from the perspective of
, it will seem that getting the
immediate reward of  is a better
strategy without exploring the long
hallway.
s0 = 0
1
This is the so-called squared
Bellman error; as the name
suggests, it’s closely related to the
Bellman equation we saw in MDPs
in Chapter Chapter 10. Roughly
speaking, this error measures how
much the Bellman equality is
violated.
For continuous action spaces, it is
popular to use a class of methods
called actor-critic methods, which
combine policy and value-function
learning. We won’t get into them in
detail here, though.
 One form of instability that we do know how to guard against is catastrophic
forgetting. In standard supervised learning, we expect that the training  values
were drawn independently from some distribution.
But when a learning agent, such as a robot, is moving through an environment, the
sequence of states it encounters will be temporally correlated. For example, the
robot might spend 12 hours in a dark environment and then 12 in a light one. This
can mean that while it is in the dark, the neural-network weight-updates will make
the 
 function "forget" the value function for when it’s light.
One way to handle this is to use experience replay, where we save our 
experiences in a replay buffer. Whenever we take a step in the world, we add the
 to the replay buffer and use it to do a Q-learning update. Then we also
randomly select some number of tuples from the replay buffer, and do Q-learning
updates based on them as well. In general, it may help to keep a sliding window of
just the 1000 most recent experiences in the replay buffer. (A larger buffer will be
necessary for situations when the optimal policy might visit a large part of the state
space, but we like to keep the buffer size small for memory reasons and also so that
we don’t focus on parts of the state space that are irrelevant for the optimal policy.)
The idea is that it will help us propagate reward values through our state space
more efficiently if we do these updates. We can see it as doing something like value
iteration, but using samples of experience rather than a known model.
An alternative strategy for learning the 
 function that is somewhat more robust
than the standard 
-learning algorithm is a method called fitted Q.
procedure Fitted-Q-Learning(
)
//e.g., 
 can be drawn randomly from 
initialize neural-network representation of 
while True do
 experience from executing -greedy policy based on  for 
 steps
 represented as tuples 
for each tuple 
 do
end for
re-initialize neural-network representation of 
end while
end procedure
Here, we alternate between using the policy induced by the current 
 function to
gather a batch of data 
, adding it to our overall data set 
, and then using
supervised neural-network training to learn a representation of the 
 value
function on the whole data set. This method does not mix the dynamic-
x
Q
(s, a, s′, r)
(s, a, s′, r)
11.2.1 Fitted Q-learning
Q
Q
1:
A, s0, γ, α, ϵ, m
2:
s ←s0
s0
S
3:
D ←∅
4:
Q
5:
6:
Dnew ←
ϵ
Q
m
7:
D ←D ∪Dnew
(s, a, s′, r)
8:
Dsupervised ←∅
9:
(s, a, s′, r) ∈D
10:
x ←(s, a)
11:
y ←r + γ maxa′∈A Q(s′, a′)
12:
Dsupervised ←Dsupervised ∪{(x, y)}
13:
14:
Q
15:
Q ←supervised-NN-regression(Dsupervised)
16:
17:
Q
Dnew
D
Q
And, in fact, we routinely shuffle
their order in the data file, anyway.
 programming phase (computing new 
 values based on old ones) with the function
approximation phase (supervised training of the neural network) and avoids
catastrophic forgetting. The regression training in line 10 typically uses squared
error as a loss function and would be trained until the fit is good (possibly
measured on held-out data).
11.3 Policy gradient
A different model-free strategy is to search directly for a good policy. The strategy
here is to define a functional form 
 for the policy, where  represents the
parameters we learn from experience. We choose  to be differentiable, and often
define
, a conditional probability distribution over our possible actions.
Now, we can train the policy parameters using gradient descent:
When  has relatively low dimension, we can compute a numeric estimate of
the gradient by running the policy multiple times for different values of , and
computing the resulting rewards.
When  has higher dimensions (e.g., it represents the set of parameters in a
complicated neural network), there are more clever algorithms, e.g., one called
REINFORCE, but they can often be difficult to get to work reliably.
Policy search is a good choice when the policy has a simple known form, but the
MDP would be much more complicated to estimate.
11.4 Model-based RL
The conceptually simplest approach to RL is to model 
 and  from the data we
have gotten so far, and then use those models, together with an algorithm for
solving MDPs (such as value iteration) to find a policy that is near-optimal given
the current models.
Assume that we have had some set of interactions with the environment, which can
be characterized as a set of tuples of the form 
.
Because the transition function 
 specifies probabilities, multiple
observations of 
 may be needed to model the transition function. One
approach to building a model 
 for the true 
 is to estimate it using
a simple counting strategy:
Q
f(s; θ) = a
θ
f
f(s, a; θ) = Pr(a|s)
θ
θ
θ
R
T
(s(t), a(t), s(t+1), r(t))
T(s, a, s′)
(s, a, s′)
^T(s, a, s′)
T(s, a, s′)
^T(s, a, s′) = #(s, a, s′) + 1
#(s, a) + |S| .
This means the chance of choosing
an action depends on which state
the agent is in. Suppose, e.g., a
robot is trying to get to a goal and
can go left or right. An
unconditional policy can say: I go
left 99% of the time; a conditional
policy can consider the robot’s
state, and say: if I’m to the right of
the goal, I go left 99% of the time.
 Here, 
 represents the number of times in our data set we have the
situation where 
, 
, 
, and 
 represents the number of
times in our data set we have the situation where 
, 
.
Adding 1 and 
 to the numerator and denominator, respectively, is a form of
smoothing called the Laplace correction. It ensures that we never estimate that a
probability is 0, and keeps us from dividing by 0. As the amount of data we gather
increases, the influence of this correction fades away.
In contrast, the reward function 
 is a deterministic function, such that
knowing the reward  for a given 
 is sufficient to fully determine the function
at that point. Our model 
 can simply be a record of observed rewards, such that
.
Given empirical models  and 
 for the transition and reward functions, we can
now solve the MDP 
 to find an optimal policy using value iteration, or
use a search algorithm to find an action to take for a particular state.
This approach is effective for problems with small state and action spaces, where it
is not too hard to get enough experience to model  and 
 well; but it is difficult to
generalize this method to handle continuous (or very large discrete) state spaces,
and is a topic of current research.
11.5 Bandit problems
Bandit problems are a subset of reinforcement learning problems. A basic bandit
problem is given by:
A set of actions 
;
A set of reward values 
; and
A probabilistic reward function 
, i.e., 
 is a function that
takes an action and a reward and returns the probability of getting that reward
conditioned on that action being taken,
. Each time the agent takes an action, a
new value is drawn from this distribution.
The most typical bandit problem has 
 and 
. This is called a -
armed bandit problem, where the decision is which “arm” (action ) to select, and the
reward is either getting a payoff ( ) or not ( ).
The important question is usually one of exploration versus exploitation. Imagine you
have tried each action 10 times, and now you have estimates 
 for the
probabilities 
. Which arm should you pick next? You could:
exploit your knowledge, choosing the arm with the highest value of expected
reward; or
#(s, a, s′)
s(t) = s a(t) = a s(t+1) = s′
#(s, a)
s(t) = s a(t) = a
|S|
R(s, a)
r
(s, a)
^R
^R(s, a) = r = R(s, a)
^T
^R
(S, A, ^T, ^R)
T
R
A
R
Rp : A × R →R
Rp
Rp(a, r) = Pr(reward = r ∣action = a)
R = {0, 1}
|A| = k
k
a
1
0
^Rp(a, r)
Rp(a, r)
Conceptually, this is similar to
having “initialized” our estimate
for the transition function with
uniform random probabilities
before making any observations.
Notice that this probablistic
rewards set up in bandits differs
from the “rewards are
deterministic” assumptions we
made so far.
Why “bandit”? In English slang,
“one-armed bandit” refers to a slot
machine because it has one arm
and takes your money! Here, we
have a similar machine but with 
arms.
k
 explore further, trying some or all actions more times to get better estimates of
the 
 values.
The theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1
our discount factor), the more time we should spend exploring, so that we don’t
converge prematurely on a bad choice of action.
Bandit problems are reinforcement learning problems (and very different from
batch supervised learning) in that:
The agent gets to influence what data it obtains (selecting  gives it another
sample from 
), and
The agent is penalized for mistakes it makes while it is learning (trying to
maximize the expected reward it gets while behaving).
In a contextual bandit problem, you have multiple possible states from some set ,
and a separate bandit problem associated with each one.
Bandit problems are an essential subset of reinforcement learning. It’s important to
be aware of the issues, but we will not study solutions to them in this class.
Rp(a, r)
h
a
R(a, r)
S
 This page contains all content from the legacy PDF notes; non-parametric models chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Neural networks have adaptable complexity, in the sense that we can try different
structural models and use cross validation to find one that works well on our data.
Beyond neural networks, we may further broaden the class of models that we can
fit to our data, for example as illustrated by the techniques introduced in this
chapter.
Here, we turn to models that automatically adapt their complexity to the training
data. The name non-parametric methods is misleading: it is really a class of methods
that does not have a fixed parameterization in advance. Rather, the complexity of
the parameterization can grow as we acquire more data.
Some non-parametric models, such as nearest-neighbor, rely directly on the data to
make predictions and do not compute a model that summarizes the data. Other
non-parametric methods, such as decision trees, can be seen as dynamically
constructing something that ends up looking like a more traditional parametric
model, but where the actual training data affects exactly what the form of the model
will be.
The non-parametric methods we consider here tend to have the form of a
composition of simple models:
Nearest neighbor models: Section 12.1 where we don’t process data at training
time, but do all the work when making predictions, by looking for the closest
training example(s) to a given new data point.
Tree models: Section 12.2 where we partition the input space and use different
simple predictions on different regions of the space; the hypothesis space can
become arbitrarily large allowing finer and finer partitions of the input space.
Ensemble models: Section 12.2.3 in which we train several different classifiers on
the whole space and average the answers; this decreases the estimation error. In
particular, we will look at bootstrap aggregation, or bagging of trees.
Boosting is a way to construct a model composed of a sequence of component
models (e.g., a model consisting of a sequence of trees, each subsequent tree
seeking to correct errors in the previous trees) that decreases both estimation
and structural error. We won’t consider this in detail in this class.
12  Non-parametric methods
Note
12  Non-parametric methods

 * -means clustering methods, Section 12.3 where we partition data into groups
based on similarity without predefined labels, adapting complexity by
adjusting the number of clusters.
Why are we studying these methods, in the heyday of complicated models such as
neural networks ?
They are fast to implement and have few or no hyperparameters to tune.
They often work as well as or better than more complicated methods.
Predictions from some of these models can be easier to explain to a human
user: decision trees are fairly directly human-interpretable, and nearest
neighbor methods can justify their decisions to some extent by showing a few
training examples that the predictions were based on.
12.1 Nearest Neighbor
In nearest-neighbor models, we don’t do any processing of the data at training time
– we just remember it! All the work is done at prediction time.
Input values  can be from any domain 
 (
, documents, tree-structured objects,
etc.). We just need a distance metric, 
, which satisfies the following,
for all 
:
Given a data-set 
, our predictor for a new 
 is
that is, the predicted output associated with the training point that is closest to the
query point . Tie breaking is typically done at random.
This same algorithm works for regression and classification!
The nearest neighbor prediction function can be described by dividing the space up
into regions whose closest point is each individual training point as shown below :
k
x
X Rd
d : X × X →R+
x, x′, x′′ ∈X
d(x, x) = 0
d(x, x′) = d(x′, x)
d(x, x′′) ≤d(x, x′) + d(x′, x′′)
D = {(x(i), y(i))}n
i=1
x ∈X
h(x) = y(i)
where
i = arg min
i
d(x, x(i)) ,
x
 In each region, we predict the associated  value.
There are several useful variations on this method. In -nearest-neighbors, we find
the  training points nearest to the query point  and output the majority  value
for classification or the average for regression. We can also do locally weighted
regression in which we fit locally linear regression models to the  nearest points,
possibly giving less weight to those that are farther away. In large data-sets, it is
important to use good data structures (e.g., ball trees) to perform the nearest-
neighbor look-ups efficiently (without looking at all the data points each time).
12.2 Tree Models
The idea here is that we would like to find a partition of the input space and then fit
very simple models to predict the output in each piece. The partition is described
using a (typically binary) “tree” that recursively splits the space.
Tree methods differ by:
The class of possible ways to split the space at each node; these are typically
linear splits, either aligned with the axes of the space, or sometimes using more
general classifiers.
The class of predictors within the partitions; these are often simply constants,
but may be more general classification or regression models.
The way in which we control the complexity of the hypothesis: it would be
within the capacity of these methods to have a separate partition element for
each individual training example.
y
k
k
x
y
k
 The algorithm for making the partitions and fitting the models.
One advantage of tree models is that they are easily interpretable by humans. This
is important in application domains, such as medicine, where there are human
experts who often ultimately make critical decisions and who need to feel confident
in their understanding of recommendations made by an algorithm. Below is an
example decision tree, illustrating how one might be able to understand the
decisions made by the tree.
#Example Here is a sample tree (reproduced from Breiman, Friedman, Olshen, Stone
(1984)):
These methods are most appropriate for domains where the input space is not very
high-dimensional and where the individual input features have some substantially
useful information individually or in small groups. Trees would not be good for
image input, but might be good in cases with, for example, a set of meaningful
measurements of the condition of a patient in the hospital, as in the example above.
We’ll concentrate on the CART/ID3 (“classification and regression trees” and
“iterative dichotomizer 3”, respectively) family of algorithms, which were invented
independently in the statistics and the artificial intelligence communities. They
work by greedily constructing a partition, where the splits are axis aligned and by
fitting a constant model in the leaves. The interesting questions are how to select the
splits and how to control complexity. The regression and classification versions are
very similar.
As a concrete example, consider the following images:
Note
  
The left image depicts a set of labeled data points in a two-dimensional feature
space. The right shows a partition into regions by a decision tree, in this case having
no classification errors in the final partitions.
The predictor is made up of
a partition function, , mapping elements of the input space into exactly one of
 regions, 
, and
a collection of 
 output values, 
, one for each region.
If we already knew a division of the space into regions, we would set 
, the
constant output for region 
, to be the average of the training output values in
that region. For a training data set 
, we let  be an
indicator set of all of the elements within 
, so that 
 for our whole
data set. We can define 
 as the subset of data set samples that are in region 
, so
that 
. Then
We can define the error in a region as 
. For example, 
 as the sum of squared
error would be expressed as
Ideally, we should select the partition to minimize
for some regularization constant . It is enough to search over all partitions of the
training data (not all partitions of the input space!) to optimize this, but the problem
is NP-complete.
12.2.1 Regression
π
M
R1, … , RM
M
Om
Om
Rm
D = {(x(i), y(i))}, i = 1, … n
I
D
I = {1, … , n}
Im
Rm
Im = {i ∣x(i) ∈Rm}
Om = averagei∈Im y(i) .
Em
Em
Em = ∑
i∈Im
(y(i) −Om)2 .
λM +
M
∑
m=1
Em ,
λ
12.2.1.1 Building a tree
 So, we’ll be greedy. We establish a criterion, given a set of data, for finding the best
single split of that data, and then apply it recursively to partition the space. For the
discussion below, we will select the partition of the data that minimizes the sum of the
sum of squared errors of each partition element. Then later, we will consider other
splitting criteria.
Given a data set 
, we now consider  to be an
indicator of the subset of elements within 
 that we wish to build a tree (or subtree)
for. That is,  may already indicate a subset of data set 
, based on prior splits in
constructing our overall tree. We define terms as follows:
 indicates the set of examples (subset of ) whose feature value in dimension
 is greater than or equal to split point ;
 indicates the set of examples (subset of ) whose feature value in dimension
 is less than ;
 is the average  value of the data points indicated by set 
; and
 is the average  value of the data points indicated by set 
.
Here is the pseudocode. In what follows,  is the largest leaf size that we will allow
in the tree, and is a hyperparameter of the algorithm.
procedure BuildTree(
)
if 
 then
return 
else
for all split dimension , split value  do
end for
return 
end if
end procedure
In practice, we typically start by calling BuildTree  with the first input equal to our
whole data set (that is, with 
). But then that call of BuildTree  can
recursively lead to many other calls of BuildTree .
Let’s think about how long each call of BuildTree  takes to run. We have to
consider all possible splits. So we consider a split in each of the  dimensions. In
each dimension, we only need to consider splits between two data points (any other
D = {(x(i), y(i))}, i = 1, … n
I
D
I
D
I +
j,s
I
j
s
I −
j,s
I
j
s
^y+
j,s
y
I +
j,s
^y−
j,s
y
I −
j,s
k
1:
I, k
2:
|I| ≤k
3:
^y ←
1
|I| ∑i∈I y(i)
4:
Leaf(value = ^y)
5:
6:
j
s
7:
I +
j,s ←{i ∈I ∣x(i)
j
≥s}
8:
I −
j,s ←{i ∈I ∣x(i)
j
< s}
9:
^y+
j,s ←
1
|I +
j,s| ∑i∈I +
j,s y(i)
10:
^y−
j,s ←
1
|I −
j,s| ∑i∈I −
j,s y(i)
11:
Ej,s ←∑i∈I +
j,s(y(i) −^y+
j,s)2 + ∑i∈I −
j,s(y(i) −^y−
j,s)2
12:
13:
(j∗, s∗) ←arg minj,s Ej,s
14:
15:
Node(j∗, s∗, BuildTree(I −
j∗,s∗, k), BuildTree(I +
j∗,s∗, k))
16:
17:
I = {1, … , n}
d
 split will give the same error on the training data). So, in total, we consider 
splits in each call to BuildTree .
It might be tempting to regularize by using a somewhat large value of , or by
stopping when splitting a node does not significantly decrease the error. One
problem with short-sighted stopping criteria is that they might not see the value of
a split that will require one more split before it seems useful. So, we will tend to
build a tree that is too large, and then prune it back.
We define cost complexity of a tree , where 
 ranges over its leaves, as
and 
 is the number of leaves. For a fixed , we can find a  that (approximately)
minimizes 
 by “weakest-link” pruning:
Create a sequence of trees by successively removing the bottom-level split that
minimizes the increase in overall error, until the root is reached.
Return the  in the sequence that minimizes the cost complexity.
We can choose an appropriate  using cross validation.
The strategy for building and pruning classification trees is very similar to the
strategy for regression trees.
Given a region 
 corresponding to a leaf of the tree, we would pick the output
class  to be the value that exists most frequently (the majority value) in the data
points whose  values are in that region, i.e., data points indicated by 
:
Let’s now define the error in a region as the number of data points that do not have
the value 
:
We define the empirical probability of an item from class  occurring in region 
 as:
where 
 is the number of training points in region 
; that is, 
 For later
use, we’ll also define the empirical probabilities of split values, 
, as the
fraction of points with dimension  in split  occurring in region 
 (one branch of
O(dn)
12.2.1.2 Pruning
k
T
m
Cα(T) =
|T|
∑
m=1
Em(T) + α|T| ,
|T|
α
T
Cα(T)
T
α
12.2.2 Classification
Rm
y
x
Im
Om = majorityi∈Im y(i) .
Om
Em = {i ∣i ∈Im and y(i) ≠Om}
.
∣∣
k
m
^Pm,k = ^P(Im, k) =
{i ∣i ∈Im and y(i) = k}
Nm
,
∣∣
Nm
m
Nm = |Im|.
^Pm,j,s
j
s
m
 the tree), and 
 as the complement (the fraction of points in the other
branch).
In our greedy algorithm, we need a way to decide which split to make next. There
are many criteria that express some measure of the “impurity” in child nodes. Some
measures include:
Misclassification error:
Gini index:
Entropy:
So that the entropy 
 is well-defined when 
, we will stipulate that
.
These splitting criteria are very similar, and it’s not entirely obvious which one is
better. We will focus on entropy, just to be concrete.
Analogous to how for regression we choose the dimension  and split  that
minimizes the sum of squared error 
, for classification, we choose the dimension
 and split  that minimizes the weighted average entropy over the “child” data
points in each of the two corresponding splits, 
 and 
. We calculate the entropy
in each split based on the empirical probabilities of class memberships in the split,
and then calculate the weighted average entropy 
 as
Choosing the split that minimizes the entropy of the children is equivalent to
maximizing the information gain of the test 
, defined by
In the two-class case (with labels 0 and 1), all of the splitting criteria mentioned
above have the values
1 −^Pm,j,s
Splitting criteria
Qm(T) = Em
Nm
= 1 −^Pm,Om
Qm(T) = ∑
k
^Pm,k(1 −^Pm,k)
Qm(T) = H(Im) = −∑
k
^Pm,k log2 ^Pm,k
H
^P = 0
0 log2 0 = 0
j
s
Ej,s
j
s
I +
j,s
I −
j,s
^H
^H = (fraction of points in left data set) ⋅H(I −
j,s)
+(fraction of points in right data set) ⋅H(I +
j,s)
= (1 −^Pm,j,s)H(I −
j,s) + ^Pm,j,sH(I +
j,s)
=
|I −
j,s|
Nm
⋅H(I −
j,s) +
|I +
j,s|
Nm
⋅H(I +
j,s) .
xj = s
infoGain(xj = s, Im) =
H(Im) −(
|I −
j,s|
Nm
⋅H(I −
j,s) +
|I +
j,s|
Nm
⋅H(I +
j,s))
{
 The respective impurity curves are shown below, where 
; the vertical axis
plots 
 for each of the three criteria.
There used to be endless haggling about which impurity function one should use. It
seems to be traditional to use entropy to select which node to split while growing the
tree, and misclassification error in the pruning criterion.
One important limitation or drawback in conventional trees is that they can have
high estimation error: small changes in the data can result in very big changes in the
resulting tree.
Bootstrap aggregation is a technique for reducing the estimation error of a non-linear
predictor, or one that is adaptive to the data. The key idea applied to trees, is to
build multiple trees with different subsets of the data, and then create an ensemble
model that combines the results from multiple trees to make a prediction.
Construct 
 new data sets of size . Each data set is constructed by sampling 
data points with replacement from 
. A single data set is called bootstrap sample
of 
.
Train a predictor 
 on each bootstrap sample.
Regression case: bagged predictor is
Classification case: Let 
 be the number of classes. We find a majority bagged
predictor as follows. We let 
 be a “one-hot” vector with a single 1 and
{
.
0.0
when  ^Pm,0 = 0.0
0.0
when  ^Pm,0 = 1.0
p = ^Pm,0
Qm(T)
12.2.3 Bagging
B
n
n
D
D
^f b(x)
^fbag(x) = 1
B
B
∑
b=1
^f b(x) .
K
^f b(x)
  zeros, and define the predicted output  for predictor 
 as
. Then
which is a vector containing the proportion of classifiers that predicted each
class  for input . Then the overall predicted output is
There are theoretical arguments showing that bagging does, in fact, reduce
estimation error. However, when we bag a model, any simple intrepetability is lost.
Random forests are collections of trees that are constructed to be de-correlated, so
that using them to vote gives maximal advantage. In competitions, they often have
excellent classification performance among large collections of much fancier
methods.
In what follows, 
, 
, and  are hyperparameters of the algorithm.
procedure RandomForest(
)
for 
 to  do
Draw a bootstrap sample 
 of size  from 
Grow tree 
 on 
:
while there are splittable nodes do
Select 
 variables at random from the  total variables
Pick the best variable and split point among those 
Split the current node
end while
end for
return 
end procedure
Given the ensemble of trees, vote to make a prediction on a new .
There are many variations on the tree theme. One is to employ different regression
or classification methods in each leaf. For example, a linear regression might be
used to model the examples in each leaf, rather than using a constant value.
In the relatively simple trees that we’ve considered, splits have been based on only
a single feature at a time, and with the resulting splits being axis-parallel. Other
methods for splitting are possible, including consideration of multiple features and
linear classifiers based on those, potentially resulting in non-axis-parallel splits.
Complexity is a concern in such cases, as many possible combinations of features
K −1
^y
f b
^yb(x) = arg maxk ^f b(x)k
^fbag(x) = 1
B
B
∑
b=1
^f b(x),
k
x
^ybag(x) = arg max
k
^fbag(x)k .
12.2.4 Random Forests
B m
n
1:
B, m, n
2:
b = 1
B
3:
Db
n
D
4:
Tb
Db
5:
6:
m
d
7:
m
8:
9:
10:
11:
12:
{Tb}B
b=1
13:
x
12.2.5 Tree variants and tradeoffs
 may need to be considered, to select the best variable combination (rather than a
single split variable).
Another generalization is a hierarchical mixture of experts, where we make a “soft”
version of trees, in which the splits are probabilistic (so every point has some degree
of membership in every leaf). Such trees can be trained using a form of gradient
descent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,
gradient boosted trees) and implementations are readily available (e.g., XGBoost).
Trees have a number of strengths, and remain a valuable tool in the machine
learning toolkit. Some benefits include being relatively easy to interpret, fast to
train, and ability to handle multi-class classification in a natural way. Trees can
easily handle different loss functions; one just needs to change the predictor and
loss being applied in the leaves. Methods also exist to identify which features are
particularly important or influential in forming the tree, which can aid in human
understanding of the data set. Finally, in many situations, trees perform
surprisingly well, often comparable to more complicated regression or classification
models. Indeed, in some settings it is considered good practice to start with trees
(especially random forest or boosted trees) as a “baseline” machine learning model,
against which one can evaluate performance of more sophisticated models.
While tree-based methods excel at supervised learning tasks, we now turn to
another important class of non-parametric methods that focus on discovering
structure in unlabeled data. These clustering methods share some conceptual
similarities with tree-based approaches - both aim to partition the input space into
meaningful regions - but clustering methods operate without supervision, making
them particularly valuable for exploratory data analysis and pattern discovery.
12.3 -means Clustering
Clustering is an unsupervised learning method where we aim to discover
meaningful groupings or categories in a dataset based on patterns or similarities
within the data itself, without relying on pre-assigned labels. It is widely used for
exploratory data analysis, pattern recognition, and segmentation tasks, allowing us
to interpret and manage complex datasets by uncovering hidden structures and
relationships.
Oftentimes a dataset can be partitioned into different categories. A doctor may
notice that their patients come in cohorts and different cohorts respond to different
treatments. A biologist may gain insight by identifying that bats and whales,
despite outward appearances, have some underlying similarity, and both should be
considered members of the same category, i.e., “mammal”. The problem of
automatically identifying meaningful groupings in datasets is called clustering.
Once these groupings are found, they can be leveraged toward interpreting the data
and making optimal decisions for each group.
k
 Mathematically, clustering looks a bit like classification: we wish to find a mapping
from datapoints, , to categories, . However, rather than the categories being
predefined labels, the categories in clustering are automatically discovered partitions
of an unlabeled dataset.
Because clustering does not learn from labeled examples, it is an example of an
unsupervised learning algorithm. Instead of mimicking the mapping implicit in
supervised training pairs 
, clustering assigns datapoints to categories
based on how the unlabeled data 
 is distributed in data space.
Intuitively, a “cluster” is a group of datapoints that are all nearby to each other and
far away from other clusters. Let’s consider the following scatter plot. How many
clusters do you think there are?
There seem to be about five clumps of datapoints and those clumps are what we
would like to call clusters. If we assign all datapoints in each clump to a cluster
corresponding to that clump, then we might desire that nearby datapoints are
assigned to the same cluster, while far apart datapoints are assigned to different
clusters.
In designing clustering algorithms, three critical things we need to decide are:
How do we measure distance between datapoints? What counts as “nearby”
and “far apart”?
How many clusters should we look for?
How do we evaluate how good a clustering is?
We will see how to begin making these decisions as we work through a concrete
clustering algorithm in the next section.
One of the simplest and most commonly used clustering algorithms is called k-
means. The goal of the k-means algorithm is to assign datapoints to  clusters in
such a way that the variance within clusters is as small as possible. Notice that this
12.3.1 Clustering formalisms
x
y
{x(i), y(i)}n
i=1
{x(i)}n
i=1
12.3.2 The k-means formulation
k
Figure 12.1: A dataset we would
like to cluster. How many clusters
do you think there are?
 matches our intuitive idea that a cluster should be a tightly packed set of
datapoints.
Similar to the way we showed that supervised learning could be formalized
mathematically as the minimization of an objective function (loss function +
regularization), we will show how unsupervised learning can also be formalized as
minimizing an objective function. Let us denote the cluster assignment for a
datapoint 
 as 
, i.e., 
 means we are assigning datapoint
 to cluster number 1. Then the k-means objective can be quantified with the
following objective function (which we also call the “k-means loss”):
where 
 and 
, so that 
 is the
mean of all datapoints in cluster , and using 
 to denote the indicator function
(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum
(over data points) of the loss is the variance of datapoints within cluster . We sum
up the variance of all  clusters to get our overall loss.
The k-means algorithm minimizes this loss by alternating between two steps: given
some initial cluster assignments: 1) compute the mean of all data in each cluster and
assign this as the “cluster mean”, and 2) reassign each datapoint to the cluster with
nearest cluster mean. Figure 12.2 shows what happens when we repeat these steps
on the dataset from above.
Each time we reassign the data to the nearest cluster mean, the k-means loss
decreases (the datapoints end up closer to their assigned cluster mean), or stays the
same. And each time we recompute the cluster means the loss also decreases (the
means end up closer to their assigned datapoints) or stays the same. Overall then,
the clustering gets better and better, according to our objective – until it stops
improving.
After four iterations of cluster assignment + update means in our example, the k-
means algorithm stops improving. We say it has converged, and its final solution is
shown in Figure 12.3.
x(i)
y(i) ∈{1, 2, … , k}
y(i) = 1
x(i)
k
∑
j=1
n
∑
i=1
𝟙(y(i) = j) x(i) −μ(j)
2 ,
∥∥
(12.1)
μ(j) =
1
Nj ∑n
i=1 𝟙(y(i) = j)x(i)
Nj = ∑n
i=1 𝟙(y(i) = j)
μ(j)
j
𝟙(⋅)
j
k
12.3.2.0.0.1 K-means algorithm
Figure 12.2: The first three steps of
running the k-means algorithm on
this data. Datapoints are colored
according to the cluster to which
they are assigned. Cluster means
are the larger X’s with black
outlines.
 It seems to converge to something reasonable! Now let’s write out the algorithm in
complete detail:
procedure KMeans(
)
Initialize centroids 
 and assignments 
 randomly
for 
 to  do
for 
 to  do
end for
for 
 to  do
end for
if 
 then
break//convergence
end if
end for
return 
end procedure
The for-loop over the  datapoints assigns each datapoint to the nearest cluster
center. The for-loop over the k clusters updates the cluster center to be the mean of
all datapoints currently assigned to that cluster. As suggested above, it can be
shown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it
converges to a local minimum of the loss.
It’s like classification except it picked what the classes are rather than being given
examples of what the classes are.
We can also use gradient descent to optimize the k-means objective. To show how to
apply gradient descent, we first rewrite the objective as a differentiable function
only of :
 is the value of the k-means loss given that we pick the optimal assignments of
the datapoints to cluster means (that’s what the 
 does). Now we can use the
gradient 
 to find the values for  that achieve minimum loss when cluster
1:
k, τ, {x(i)}n
i=1
2:
μ(1), … , μ(k)
y(1), … , y(n)
3:
t = 1
τ
4:
yold ←y
5:
i = 1
n
6:
y(i) ←arg minj∈{1,…,k} x(i) −μ(j)
2
∥∥
7:
8:
j = 1
k
9:
Nj ←∑n
i=1 𝟙(y(i) = j)
10:
μ(j) ←
1
Nj ∑n
i=1 𝟙(y(i) = j) x(i)
11:
12:
y = yold
13:
14:
15:
16:
17:
μ, y
18:
n
12.3.2.0.0.2 Using gradient descent to minimize k-means objective
μ
L(μ) =
n
∑
i=1
min
j
x(i) −μ(j)
2 .
∥∥
L(μ)
minj
∂L(μ)
∂μ
μ
Figure 12.3: Converged result.
 assignments are optimal. Finally, we read off the optimal cluster assignments, given
the optimized , just by assigning datapoints to their nearest cluster mean:
This procedure yields a local minimum of Equation 12.1, as does the standard k-
means algorithm we presented (though they might arrive at different solutions). It
might not be the global optimum since the objective is not convex (due to 
, as
the minimum of multiple convex functions is not necessarily convex).
The standard k-means algorithm, as well as the variant that uses gradient descent,
both are only guaranteed to converge to a local minimum, not necessarily the global
minimum of the loss. Thus the answer we get out depends on how we initialize the
cluster means. Figure 12.4 is an example of a different initialization on our toy data,
which results in a worse converged clustering:
A variety of methods have been developed to pick good initializations (see, for
example, the k-means++ algorithm). One simple option is to run the standard k-
means algorithm multiple times, with different random initial conditions, and then
pick from these the clustering that achieves the lowest k-means loss.
A very important parameter in cluster algorithms is the number of clusters we are
looking for. Some advanced algorithms can automatically infer a suitable number of
clusters, but most of the time, like with k-means, we will have to pick  – it’s a
hyperparameter of the algorithm.
Figure 12.5 shows an example of the effect. Which result looks more correct? It can
be hard to say! Using higher k we get more clusters, and with more clusters we can
achieve lower within-cluster variance – the k-means objective will never increase,
μ
y(i) = arg min
j
x(i) −μ(j)
2 .
∥∥
minj
12.3.2.0.0.3 Importance of initialization
12.3.2.0.0.4 Importance of k
k
Figure 12.4: With the initialization
of the means to the left, the yellow
and red means end up splitting
what perhaps should be one cluster
in half.
Figure 12.5: Example of k-means
run on our toy data, with two
different values of k. Setting k=4,
on the left, results in one cluster
being merged, compared to setting
k=5, on the right. Which clustering
do you think is better? How could
you decide?
 and will typically strictly decrease as we increase k. Eventually, we can increase k to
equal the total number of datapoints, so that each datapoint is assigned to its own
cluster. Then the k-means objective is zero, but the clustering reveals nothing.
Clearly, then, we cannot use the k-means objective itself to choose the best value for
k. In Section 1.3, we will discuss some ways of evaluating the success of clustering
beyond its ability to minimize the k-means objective, and it’s with these sorts of
methods that we might decide on a proper value of k.
Alternatively, you may be wondering: why bother picking a single k? Wouldn’t it be
nice to reveal a hierarchy of clusterings of our data, showing both coarse and fine
groupings? Indeed hierarchical clustering is another important class of clustering
algorithms, beyond k-means. These methods can be useful for discovering tree-like
structure in data, and they work a bit like this: initially a coarse split/clustering of
the data is applied at the root of the tree, and then as we descend the tree we split
and cluster the data in ever more fine-grained ways. A prototypical example of
hierarchical clustering is to discover a taxonomy of life, where creatures may be
grouped at multiple granularities, from species to families to kingdoms. You may
find a suite of clustering algorithms in SKLEARN’s cluster module.
Clustering algorithms group data based on a notion of similarity, and thus we need
to define a distance metric between datapoints. This notion will also be useful in
other machine learning approaches, such as nearest-neighbor methods that we see
in Chapter 12. In k-means and other methods, our choice of distance metric can
have a big impact on the results we will find.
Our k-means algorithm uses the Euclidean distance, i.e., 
, with a loss
function that is the square of this distance. We can modify k-means to use different
distance metrics, but a more common trick is to stick with Euclidean distance but
measured in a feature space. Just like we did for regression and classification
problems, we can define a feature map from the data to a nicer feature
representation, 
, and then apply k-means to cluster the data in the feature
space.
As a simple example, suppose we have two-dimensional data that is very stretched
out in the first dimension and has less dynamic range in the second dimension.
Then we may want to scale the dimensions so that each has similar dynamic range,
prior to clustering. We could use standardization, like we did in Chapter 5.
If we want to cluster more complex data, like images, music, chemical compounds,
etc., then we will usually need more sophisticated feature representations. One
common practice these days is to use feature representations learned with a neural
network. For example, we can use an autoencoder to compress images into feature
vectors, then cluster those feature vectors.
12.3.2.0.0.5 k-means in feature space
x(i) −μ(j)
∥∥
ϕ(x)
12.3.3 How to evaluate clustering algorithms
 One of the hardest aspects of clustering is knowing how to evaluate it. This is
actually a big issue for all unsupervised learning methods, since we are just looking
for patterns in the data, rather than explicitly trying to predict target values (which
was the case with supervised learning).
Remember, evaluation metrics are not the same as loss functions, so we can’t just
measure success by looking at the k-means loss. In prediction problems, it is critical
that the evaluation is on a held-out test set, while the loss is computed over training
data. If we evaluate on training data we cannot detect overfitting. Something
similar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too
large can precisely “fit” the data (minimize the loss), but yields no general insight.
One way to evaluate our clusters is to look at the consistency with which they are
found when we run on different subsamples of our training data, or with different
hyperparameters of our clustering algorithm (e.g., initializations). For example, if
running on several bootstrapped samples (random subsets of our data) results in
very different clusters, it should call into question the validity of any of the
individual results.
If we have some notion of what ground truth clusters should be, e.g., a few data
points that we know should be in the same cluster, then we can measure whether or
not our discovered clusters group these examples correctly.
Clustering is often used for visualization and interpretability, to make it easier for
humans to understand the data. Here, human judgment may guide the choice of
clustering algorithm. More quantitatively, discovered clusters may be used as input
to downstream tasks. For example, as we saw in the lab, we may fit a different
regression function on the data within each cluster. Figure 12.6 gives an example
where this might be useful. In cases like this, the success of a clustering algorithm
can be indirectly measured based on the success of the downstream application
(e.g., does it make the downstream predictions more accurate).
Figure 12.6: Averaged across the
whole population, risk of heart
disease positively correlates with
hours of exercise. However, if we
cluster the data, we can observe
that there are four subgroups of the
population which correspond to
different age groups, and within
each subgroup the correlation is
negative. We can make better
predictions, and better capture the
presumed true effect, if we cluster
this data and then model the trend
in each cluster separately.
  What are some conventions for derivatives of matrices and vectors? It will always
work to explicitly write all indices and treat everything as scalars, but we
introduce here some shortcuts that are often faster to use and helpful for
understanding.
There are at least two consistent but different systems for describing shapes and
rules for doing matrix derivatives. In the end, they all are correct, but it is
important to be consistent.
We will use what is often called the ‘Hessian’ or denominator layout, in which we
say that for
 of size 
 and  of size 
, 
 is a matrix of size 
 with the 
entry 
. This denominator layout convention has been adopted by the field
of machine learning to ensure that the shape of the gradient is the same as the
shape of the respective derivative. This is somewhat controversial at large, but alas,
we shall continue with denominator layout.
The discussion below closely follows the Wikipedia on matrix derivatives.
A.1 The shapes of things
Here are important special cases of the rule above:
Scalar-by-scalar: For  of size 
 and  of size 
, 
 is the (scalar)
partial derivative of  with respect to .
Scalar-by-vector: For  of size 
 and  of size 
, 
 (also written
, the gradient of  with respect to ) is a column vector of size 
 with
the 
 entry 
:
Vector-by-scalar: For  of size 
 and  of size 
, 
 is a row
vector of size 
 with the 
 entry 
:
Vector-by-vector: For  of size 
 and  of size 
, 
 is a matrix of
size 
 with the 
 entry 
:
Appendix A — Matrix derivative common
cases
x
n × 1
y
m × 1 ∂y/∂x
n × m
(i, j)
∂yj/∂xi
x
1 × 1
y
1 × 1 ∂y/∂x
y
x
x
n × 1
y
1 × 1 ∂y/∂x
∇xy
y
x
n × 1
ith
∂y/∂xi
∂y/∂x =
.
⎡
⎢
⎣
∂y/∂x1
∂y/∂x2
⋮
∂y/∂xn
⎤
⎥
⎦
x
1 × 1
y
m × 1 ∂y/∂x
1 × m
jth
∂yj/∂x
∂y/∂x = [
].
∂y1/∂x
∂y2/∂x
⋯
∂ym/∂x
x
n × 1
y
m × 1 ∂y/∂x
n × m
(i, j)
∂yj/∂xi
Appendices > A  Matrix derivative common cases

1
 Scalar-by-matrix: For 
 of size 
 and  of size 
, 
 (also written
, the gradient of  with respect to 
) is a matrix of size 
 with the
 entry 
:
You may notice that in this list, we have not included matrix-by-matrix, matrix-by-
vector, or vector-by-matrix derivatives. This is because, generally, they cannot be
expressed nicely in matrix form and require higher order objects (e.g., tensors) to
represent their derivatives. These cases are beyond the scope of this course.
Additionally, notice that for all cases, you can explicitly compute each element of
the derivative object using (scalar) partial derivatives. You may find it useful to
work through some of these by hand as you are reviewing matrix derivatives.
A.2 Some vector-by-vector identities
Here are some examples of 
. In each case, assume  is 
,  is 
,  is
a scalar constant,  is a vector that does not depend on  and 
 is a matrix that
does not depend on ,  and  are scalars that do depend on , and  and  are
vectors that do depend on . We also have vector-valued functions  and .
First, we will cover a couple of fundamental cases: suppose that  is an 
vector which is not a function of , an 
 vector. Then,
is an 
 matrix of 0s. This is similar to the scalar case of differentiating a
constant. Next, we can consider the case of differentiating a vector with respect to
itself:
This is the 
 identity matrix, with 1’s along the diagonal and 0’s elsewhere. It
makes sense, because 
 is 1 for 
 and 0 otherwise. This identity is also
similar to the scalar case.
∂y/∂x =
.
⎡
⎢
⎣
∂y1/∂x1
∂y2/∂x1
⋯
∂ym/∂x1
∂y1/∂x2
∂y2/∂x2
⋯
∂ym/∂x2
⋮
⋮
⋱
⋮
∂y1/∂xn
∂y2/∂xn
⋯
∂ym/∂xn
⎤
⎥
⎦
X
n × m
y
1 × 1 ∂y/∂X
∇Xy
y
X
n × m
(i, j)
∂y/∂Xi,j
∂y/∂X =
.
⎡
⎢
⎣
∂y/∂X1,1
⋯
∂y/∂X1,m
⋮
⋱
⋮
∂y/∂Xn,1
⋯
∂y/∂Xn,m
⎤
⎥
⎦
∂y/∂x
x
n × 1 y
m × 1 a
a
x
A
x u
v
x
u
v
x
f
g
A.2.1 Some fundamental cases
a
m × 1
x
n × 1
∂a
∂x = 0,
(A.1)
n × m
∂x
∂x = I
n × n
∂xj/xi
i = j
 Let the dimensions of 
 be 
. Then the object 
 is an 
 vector. We can
then compute the derivative of 
 with respect to  as:
Note that any element of the column vector 
 can be written as, for 
:
Thus, computing the 
 entry of 
 requires computing the partial derivative
Therefore, the 
 entry of 
 is the 
 entry of 
:
Similarly, for objects 
 of the same shape, one can obtain,
Suppose that 
 are both vectors of size 
. Then,
Suppose that  is a scalar constant and  is an 
 vector that is a function of .
Then,
One can extend the previous identity to vector- and matrix-valued constants.
Suppose that  is a vector with shape 
 and  is a scalar which depends on .
Then,
First, checking dimensions, 
 is 
 and  is 
 so 
 is 
 and our
answer is 
 as it should be. Now, checking a value, element 
 of the
A.2.2 Derivatives involving a constant matrix
A
m × n
Ax
m × 1
Ax
x
∂Ax
∂x
=
⎡
⎢
⎣
∂(Ax)1/∂x1
∂(Ax)2/∂x1
⋯
∂(Ax)m/∂x1
∂(Ax)1/∂x2
∂(Ax)2/∂x2
⋯
∂(Ax)m/∂x2
⋮
⋮
⋱
⋮
∂(Ax)1/∂xn
∂(Ax)2/∂xn
⋯
∂(Ax)m/∂xn
⎤
⎥
⎦
Ax
j = 1, … , m
(Ax)j =
n
∑
k=1
Aj,kxk.
(i, j)
∂Ax
∂x
∂(Ax)j/∂xi :
∂(Ax)j/∂xi = ∂(
n
∑
k=1
Aj,kxk)/∂xi = Aj,i
(i, j)
∂Ax
∂x
(j, i)
A
∂Ax
∂x
= AT
(A.2)
x, A
∂xTA
∂x
= A
(A.3)
A.2.3 Linearity of derivatives
u, v
m × 1
∂(u + v)
∂x
= ∂u
∂x + ∂v
∂x
(A.4)
a
u
m × 1
x
∂au
∂x = a ∂u
∂x
a
m × 1
v
x
∂va
∂x = ∂v
∂x aT
∂v/∂x
n × 1
a
m × 1
aT
1 × m
n × m
(i, j)
 answer is 
 = 
 which corresponds to element 
 of
.
Similarly, suppose that 
 is a matrix which does not depend on  and  is a
column vector which does depend on . Then,
Suppose that  is a scalar which depends on , while  is a column vector of shape
 and  is a column vector of shape 
. Then,
One can see this relationship by expanding the derivative as follows:
Then, one can use the product rule for scalar-valued functions,
to obtain the desired result.
Suppose that  is a vector-valued function with output vector of shape 
, and
the argument to  is a column vector  of shape 
 which depends on . Then,
one can obtain the chain rule as,
Following “the shapes of things,” 
 is 
 and 
 is 
, where
element 
 is 
. The same chain rule applies for further compositions
of functions:
A.3 Some other identities
You can get many scalar-by-vector and vector-by-scalar cases as special cases of the
rules above, making one of the relevant vectors just be 1 x 1. Here are some other
ones that are handy. For more, see the Wikipedia article on Matrix derivatives (for
consistency, only use the ones in denominator layout).
∂vaj/∂xi
(∂v/∂xi)aj
(i, j)
(∂v/∂x)aT
A
x
u
x
∂Au
∂x
= ∂u
∂x AT
A.2.4 Product rule (vector-valued numerator)
v
x
u
m × 1
x
n × 1
∂vu
∂x = v ∂u
∂x + ∂v
∂x uT
∂vu
∂x =
.
⎡
⎢
⎣
∂(vu1)/∂x1
∂(vu2)/∂x1
⋯
∂(vum)/∂x1
∂(vu1)/∂x2
∂(vu2)/∂x2
⋯
∂(vum)/∂x2
⋮
⋮
⋱
⋮
∂(vu1)/∂xn
∂(vu2)/∂xn
⋯
∂(vum)/∂xn
⎤
⎥
⎦
∂(vuj)/∂xi = v(∂uj/∂xi) + (∂v/∂xi)uj,
A.2.5 Chain rule
g
m × 1
g
u
d × 1
x
∂g(u)
∂x
= ∂u
∂x
∂g(u)
∂u
∂u/∂x
n × d
∂g(u)/∂u
d × m
(i, j)
∂g(u)j/∂ui
∂f(g(u))
∂x
= ∂u
∂x
∂g(u)
∂u
∂f(g)
∂g
T
 A.4 Derivation of gradient for linear regression
Recall here that 
 is a matrix of of size 
 and 
 is an 
 vector.
Applying identities Equation A.3, Equation A.5,Equation A.4, Equation A.2,
Equation A.1
A.5 Matrix derivatives using Einstein summation
You do not have to read or learn this! But you might find it interesting or helpful.
Consider the objective function for linear regression, written out as products of
matrices:
where 
 is 
, 
 is 
, and  is 
. How does one show, with no
shortcuts, that
One neat way, which is very explicit, is to simply write all the matrices as variables
with row and column indices, e.g., 
 is the row , column  entry of the matrix
. Furthermore, let us use the convention that in any product, all indices which
appear more than once get summed over; this is a popular convention in
theoretical physics, and lets us suppress all the summation symbols which would
otherwise clutter the following expresssions. For example, 
 would be the
implicit summation notation giving the element at the 
 row of the matrix-vector
product 
.
Using implicit summation notation with explicit indices, we can rewrite 
 as
∂uTv
∂x
= ∂u
∂x v + ∂v
∂x u
(A.5)
∂uT
∂x
= ( ∂u
∂x )
T
(A.6)
X
n × d
Y
n × 1
∂(Xθ −Y)T(Xθ −Y)/n
∂θ
= 2
n
∂(Xθ −Y)
∂θ
(Xθ −Y)
= 2
n ( ∂Xθ
∂θ
−∂Y
∂θ )(Xθ −Y)
= 2
n (XT −0)(Xθ −Y)
= 2
n XT(Xθ −Y)
J(θ) = 1
n (Xθ −Y )T(Xθ −Y ) ,
X
n × d Y
n × 1
θ
d × 1
∇θJ = 2
n XT(Xθ −Y ) ?
Xab
a
b
X
Xabθb
ath
Xθ
J(θ)
J(θ) = 1
n (Xabθb −Ya) (Xacθc −Ya) .
 Note that we no longer need the transpose on the first term, because all that
transpose accomplished was to take a dot product between the vector given by the
left term, and the vector given by the right term. With implicit summation, this is
accomplished by the two terms sharing the repeated index .
Taking the derivative of  with respect to the 
 element of  thus gives, using the
chain rule for (ordinary scalar) multiplication:
where the second line follows from the first, with the definition that 
 only
when 
 (and similarly for 
). And the third line follows from the second by
recognizing that the two terms in the second line are identical. Now note that in
this implicit summation notation, the 
 element of the matrix product of  and
 is 
. That is, ordinary matrix multiplication sums over indices
which are adjacent to each other, because a row of  times a column of 
 becomes
a scalar number. So the term in the above equation with 
 is not a matrix
product of 
 with 
. However, taking the transpose 
 switches row and column
indices, so 
. And 
 is a matrix product of 
 with 
! Thus, we
have that
which is the desired result.
a
J
dth
θ
dJ
dθd
=
1
n [Xabδbd (Xacθc −Ya) + (Xabθb −Ya)Xacδcd]
=
1
n [Xad (Xacθc −Ya) + (Xabθb −Ya)Xad]
=
2
n Xad (Xabθb −Ya) ,
δbd = 1
b = d
δcd
a, b
A
B
(AB)ac = AabBbc
A
B
XadXab
X
X
XT
Xad = X T
da
X T
daXab
XT
X
dJ
dθd
= 2
n X T
da (Xabθb −Ya)
= 2
n [XT (Xθ −Y )]d ,
 B.1 Strategies towards adaptive step-size
We’ll start by looking at the notion of a running average. It’s a computational
strategy for estimating a possibly weighted average of a sequence of data. Let our
data sequence be 
; then we define a sequence of running average values,
 using the equations
where 
. If 
 is a constant, then this is a moving average, in which
So, you can see that inputs 
 closer to the end of the sequence have more effect on
 than early inputs.
If, instead, we set 
, then we get the actual average.
❓ Study Question
Prove to yourself that the previous assertion holds.
Now, we can use methods that are a bit like running averages to describe strategies
for computing . The simplest method is momentum, in which we try to “average”
recent gradient updates, so that if they have been bouncing back and forth in some
direction, we take out that component of the motion. For momentum, we have
Appendix B — Optimizing Neural
Networks
B.1.1 Running averages
c1, c2, …
C0, C1, C2, …
C0 = 0,
Ct = γt Ct−1 + (1 −γt) ct,
γt ∈(0, 1)
γt
CT = γ CT−1 + (1 −γ) cT
= γ(γ CT−2 + (1 −γ) cT−1) + (1 −γ) cT
=
T
∑
t=1
γ T−t(1 −γ) ct.
ct
CT
γt = t−1
t
B.1.2 Momentum
η
V0 = 0,
Vt = γ Vt−1 + η ∇WJ(Wt−1),
Wt = Wt−1 −Vt.
Appendices > B  Optimizing Neural Networks

 This doesn’t quite look like an adaptive step size. But what we can see is that, if we
let 
, then the rule looks exactly like doing an update with step size 
on a moving average of the gradients with parameter :
❓ Study Question
Prove to yourself that these formulations are equivalent.
We will find that 
 will be bigger in dimensions that consistently have the same
sign for 
 and smaller for those that don’t. Of course we now have two
parameters to set (  and ), but the hope is that the algorithm will perform better
overall, so it will be worth trying to find good values for them. Often  is set to be
something like 
.
The red arrows show the update after each successive step of mini-batch gradient
descent with momentum. The blue points show the direction of the gradient with
respect to the mini-batch at each step. Momentum smooths the path taken towards
the local minimum and leads to faster convergence.
❓ Study Question
If you set 
, would momentum have more of an effect or less of an effect
than if you set it to 
?
Another useful idea is this: we would like to take larger steps in parts of the space
where 
 is nearly flat (because there’s no risk of taking too big a step due to the
gradient being large) and smaller steps when it is steep. We’ll apply this idea to
each weight independently, and end up with a method called adadelta, which is a
η = η′(1 −γ)
η′
γ
M0 = 0,
Mt = γ Mt−1 + (1 −γ) ∇WJ(Wt−1),
Wt = Wt−1 −η′ Mt.
Vt
∇W
η
γ
γ
0.9
γ = 0.1
0.9
B.1.3 Adadelta
J(W)
Momentum
 variant on adagrad (for adaptive gradient). Even though our weights are indexed by
layer, input unit, and output unit, for simplicity here, just let 
 be any weight in
the network (we will do the same thing for all of them).
The sequence 
 is a moving average of the square of the th component of the
gradient. We square it in order to be insensitive to the sign—we want to know
whether the magnitude is big or small. Then, we perform a gradient update to
weight , but divide the step size by 
, which is larger when the surface is
steeper in direction  at point 
 in weight space; this means that the step size
will be smaller when it’s steep and larger when it’s flat.
Adam has become the default method of managing step sizes in neural networks.
It combines the ideas of momentum and adadelta. We start by writing moving
averages of the gradient and squared gradient, which reflect estimates of the mean
and variance of the gradient for weight :
A problem with these estimates is that, if we initialize 
, they will
always be biased (slightly too small). So we will correct for that bias by defining
Note that 
 is 
 raised to the power , and likewise for 
. To justify these
corrections, note that if we were to expand 
 in terms of 
 and
, the coefficients would sum to 1. However, the coefficient behind
 is 
 and since 
, the sum of coefficients of nonzero terms is 
;
hence the correction. The same justification holds for 
.
❓ Study Question
Wj
gt,j = ∇WJ(Wt−1)j,
Gt,j = γ Gt−1,j + (1 −γ) g2
t,j,
Wt,j = Wt−1,j −
η
√Gt,j + ϵ
gt,j.
Gt,j
j
j
√Gt,j + ϵ
j
Wt−1
B.1.4 Adam
j
gt,j = ∇WJ(Wt−1)j,
mt,j = B1 mt−1,j + (1 −B1) gt,j,
vt,j = B2 vt−1,j + (1 −B2) g2
t,j.
m0 = v0 = 0
^mt,j =
mt,j
1 −Bt
1
,
^vt,j =
vt,j
1 −Bt
2
,
Wt,j = Wt−1,j −
η
√^vt,j + ϵ
^mt,j.
Bt
1
B1
t
Bt
2
mt,j
m0,j
g0,j, g1,j, … , gt,j
m0,j
Bt
1
m0,j = 0
1 −Bt
1
vt,j
Although, interestingly, it may
actually violate the convergence
conditions of SGD:
arxiv.org/abs/1705.08292
 Define 
 directly as a moving average of 
. What is the decay (
parameter)?
Even though we now have a step size for each weight, and we have to update
various quantities on each iteration of gradient descent, it’s relatively easy to
implement by maintaining a matrix for each quantity (
, 
, 
, 
) in each layer
of the network.
B.2 Batch Normalization Details
Let’s think of the batch-normalization layer as taking 
 as input and producing an
output 
. But now, instead of thinking of 
 as an 
 vector, we have to
explicitly think about handling a mini-batch of data of size 
 all at once, so 
 will
be an 
 matrix, and so will the output 
.
Our first step will be to compute the batchwise mean and standard deviation. Let 
be the 
 vector where
and let 
 be the 
 vector where
The basic normalized version of our data would be a matrix, element 
 of which
is
where  is a very small constant to guard against division by zero.
However, if we let these be our 
 values, we really are forcing something too
strong on our data—our goal was to normalize across the data batch, but not
necessarily force the output values to have exactly mean 0 and standard deviation 1.
So, we will give the layer the opportunity to shift and scale the outputs by adding
new weights to the layer. These weights are 
 and 
, each of which is an 
vector. Using the weights, we define the final output to be
That’s the forward pass. Whew!
Now, for the backward pass, we have to do two things: given 
,
^mt,j
gt,j
γ
mℓ
t vℓ
t gℓ
t g2
t
ℓ
Z l
ˆZ l
Z l
nl × 1
K
Z l
nl × K
ˆZ l
μl
nl × 1
μl
i = 1
K
K
∑
j=1
Z l
ij,
σl
nl × 1
σl
i =
1
K
K
∑
j=1
(Z l
ij −μl
i)
2
.


⎷
(i, j)
Z
l
ij =
Z l
ij −μl
i
σl
i + ϵ
,
–
ϵ
ˆZ l
Gl
Bl
nl × 1
ˆZ l
ij = Gl
i Z
l
ij + Bl
i.
–
∂L
∂ˆZ l
 Compute 
 for back-propagation, and
Compute 
 and 
 for gradient updates of the weights in this layer.
Schematically, we have
It’s hard to think about these derivatives in matrix terms, so we’ll see how it works
for the components. 
 contributes to 
 for all data points  in the batch. So,
Similarly, 
 contributes to 
 for all data points  in the batch. Thus,
Now, let’s figure out how to do backprop. We can start schematically:
And because dependencies only exist across the batch, but not across the unit
outputs,
The next step is to note that
And now that
where 
 if 
 and 0 otherwise. We need two more pieces:
Putting the whole thing together, we get
∂L
∂Z l
∂L
∂Gl
∂L
∂Bl
∂L
∂B = ∂L
∂ˆZ
∂ˆZ
∂B .
Bi
ˆZij
j
∂L
∂Bi
= ∑
j
∂L
∂ˆZij
∂ˆZij
∂Bi
= ∑
j
∂L
∂ˆZij
.
Gi
ˆZij
j
∂L
∂Gi
= ∑
j
∂L
∂ˆZij
∂ˆZij
∂Gi
= ∑
j
∂L
∂ˆZij
Zij.
–
∂L
∂Z = ∂L
∂ˆZ
∂ˆZ
∂Z .
∂L
∂Zij
=
K
∑
k=1
∂L
∂ˆZik
∂ˆZik
∂Zij
.
∂ˆZik
∂Zij
= ∂ˆZik
∂Zik
∂Zik
∂Zij
= Gi
∂Zik
∂Zij
.
–
–
–
∂Zik
∂Zij
= (δjk −∂μi
∂Zij
) 1
σi
−Zik −μi
σ2
i
∂σi
∂Zij
,
–
δjk = 1
j = k
∂μi
∂Zij
= 1
K ,
∂σi
∂Zij
= Zij −μi
K σi
.
∂L
∂Zij
=
K
∑
k=1
∂L
∂ˆZik
Gi
1
K σi
(K δjk −1 −(Zik −μi)(Zij −μi)
σ2
i
).
  In which we try to describe the outlines of the “lifecycle” of supervised learning,
including hyperparameter tuning and evaluation of the final product.
C.1 General case
We start with a very generic setting.
Given: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()
such that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning
algorithm () takes as input a data set of the form
where 
 and 
 and returns an 
.
Given a problem specification and a set of data 
, we evaluate hypothesis 
according to average loss, or error,
If the data used for evaluation were not used during learning of the hypothesis then this
is a reasonable estimate of how well the hypothesis will make additional
predictions on new data from the same source.
A validation strategy  takes an algorithm 
, a loss function , and a data source 
and produces a real number which measures how well 
 performs on data from
that distribution.
In the simplest case, we can divide 
 into two sets, 
 and 
, train on the
first, and then evaluate the resulting hypothesis on the second. In that case,
Appendix C — Supervised learning in a
nutshell
C.1.1 Minimal problem specification 
D = {(x(1), y(1)), … , (x(n), y(n))}
x(i) ∈X
y(i) ∈y
h ∈H
C.1.2 Evaluating a hypothesis
D
h
E(h, L, D) =
1
|D|
D
∑
i=1
L (h (x(i)), y(i))
C.1.3 Evaluating a supervised learning algorithm
V
A
L
D
A
C.1.3.1 Using a validation set
D
Dtrain 
Dval 
V(A, L, D) = E (A (Dtrain ), L, Dval )
Appendices > C  Supervised learning in a nutshell

 We can’t reliably evaluate an algorithm based on a single application to a single
training and test set, because there are many aspects of the training and testing
data, as well as, sometimes, randomness in the algorithm itself, that cause variance
in the performance of the algorithm. To get a good idea of how well an algorithm
performs, we need to, multiple times, train it and evaluate the resulting hypothesis,
and report the average over 
 executions of the algorithm of the error of the
hypothesis it produced each time.
We divide the data into 2 K random non-overlapping subsets:
.
Then,
In cross validation, we do a similar computation, but allow data to be re-used in the
 different iterations of training and testing the algorithm (but never share training
and testing data for a single iteration!). See Section 2.8.2.2 for details.
Now, if we have two different algorithms 
 and 
, we might be interested in
knowing which one will produce hypotheses that generalize the best, using data
from a particular source. We could compute 
 and 
, and
prefer the algorithm with lower validation error. More generally, given algorithms
, we would prefer
Now what? We have to deliver a hypothesis to our customer. We now know how to
find the algorithm, 
, that works best for our type of data. We can apply it to all of
our data to get the best hypothesis we know how to create, which would be
and deliver this resulting hypothesis as our best product.
A majority of learning algorithms have the form of optimizing some objective
involving the training data and a loss function.
C.1.3.2 Using multiple training/evaluation runs
K
Dtrain 
1
, Dval 
1 , … , Dtrain 
K
, Dval 
K
V(A, L, D) = 1
K
K
∑
k=1
E(A(Dtrain
k
), L, Dval
k ) .
C.1.3.3 Cross validation
K
C.1.4 Comparing supervised learning algorithms
A1
A2
V (A1, L, D)
V (A∈, L, D)
A1, … , AM
A∗= arg min
m V (AM, L, D)
C.1.5 Fielding a hypothesis
A∗
h∗= A∗(D)
C.1.6 Learning algorithms as optimizers
Interestingly, this loss function is
not always the same as the loss
function that is used for
 So for example, (assuming a perfect optimizer which doesn’t, of course, exist) we
might say our algorithm is to solve an optimization problem:
Our objective often has the form
where  is a loss to be minimized during training and 
 is a regularization term.
Often, rather than comparing an arbitrary collection of learning algorithms, we
think of our learning algorithm as having some parameters that affect the way it
maps data to a hypothesis. These are not parameters of the hypothesis itself, but
rather parameters of the algorithm. We call these hyperparameters. A classic example
would be to use a hyperparameter  to govern the weight of a regularization term
on an objective to be optimized:
Then we could think of our algorithm as 
. Picking a good value of  is the
same as comparing different supervised learning algorithms, which is accomplished
by validating them and picking the best one!
C.2 Concrete case: linear regression
In linear regression the problem formulation is this:
 for values of parameters 
 and 
.
Our learning algorithm has hyperparameter  and can be written as:
Our learning algorithm has hyperparameter $ $ and can be written as:
For a particular training data set and parameter , it finds the best hypothesis on
this data, specified with parameters 
, written 
.
A(D) = arg min
h∈H J (h; D).
J (h; D) = E(h, L, D) + R(h),
L
R
C.1.7 Hyperparameters
λ
J (h; D) = E(h, L, D) + λR(h).
A(D; λ)
λ
x = Rd
y = R
H = {θ⊤x + θ0}
θ ∈Rd
θ0 ∈R
L(g, y) = (g −y)2
λ
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∥θ∥2
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∥θ∥2.
λ
Θ = (θ, θ0)
Θ∗(λ, D)
evaluation! We will see this in
logistic regression.
 Picking the best value of the hyperparameter is choosing among learning
algorithms. We could, most simply, optimize using a single training / validation
split, so 
 
, and
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Note that we don’t use regularization here because we just want to measure how
good the output of the algorithm is at predicting values of new points, and so that’s
what we measure. We use the regularizer during training when we don’t want to
focus only on optimizing predictions on the training data.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, 
, to train, using the best hyperparameters we know, and return
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, as
Here are the same ideas, written out in informal pseudocode:
D = Dtrain ∪Dval 
λ∗= arg min
λ V (Aλ, L, Dval )
= arg min
λ E (Θ∗(λ, Dtrain ),  mse, Dval )
= arg min
λ
1
|Dval |
∑
(x,y)∈Dval 
(θ∗(λ, Dtrain )
⊤x + θ∗
0 (λ, Dtrain ) −y)
2
λ
V
D
Θ∗= A (D; λ∗)
= Θ∗(λ∗, D)
= arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∗∥θ∥2
E test  = E (Θ∗,  mse , Dtest )
=
1
|Dtest |
∑
(x,y)∈Dtot 
(θ∗Tx + θ∗
0 −y)
2
# returns theta_best(D, lambda)
define train(D, lambda):
    return minimize(mse(theta, D) + lambda * norm(theta)**2, theta)
# returns lambda_best using very simple validation
define simple_tune(D_train, D_val, possible_lambda_vals):
    scores = [mse(train(D_train, lambda), D_val) for lambda in 
possible_lambda_vals]
    return possible_lambda_vals[least_index[scores]]
# returns theta_best overall
define theta_best(D_train, D_val, possible_lambda_vals):
    return train(D_train + D_val, simple_tune(D_train, D_val, 
possible_lambda_vals))
# customer evaluation of the theta delivered to them
 C.3 Concrete case: logistic regression
In binary logistic regression the problem formulation is as follows. We are writing
the class labels as 1 and 0.
 for values of parameters 
 and 
.
Proxy loss 
 Our learning algorithm
has hyperparameter  and can be written as:
For a particular training data set and parameter , it finds the best hypothesis on
this data, specified with parameters 
, written 
 according to the
proxy loss 
.
Picking the best value of the hyperparameter is choosing among learning
algorithms based on their actual predictions. We could, most simply, optimize using
a single training / validation split, so 
, and we use the real 01 loss:
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, 
, to train, using the best hyperparameters we know, and return
❓ Study Question
What loss function is being optimized inside this algorithm?
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, as
define customer_val(theta):
    return mse(theta, D_test)
X = Rd
y = {+1, 0}
H = {σ (θ⊤x + θ0)}
θ ∈Rd
θ0 ∈R
L(g, y) = L01( g,  h)
Lnll(g, y) = −(y log(g) + (1 −y) log(1 −g))
λ
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
Lnll (σ (θ⊤x + θ0), y) + λ∥θ∥2
λ
Θ = (θ, θ0)
Θ∗(λ, D)
Lnll 
D = Dtrain  ∪Dval
λ∗= arg min
λ V (Aλ, L01, Dval )
= arg min
λ E (Θ∗(λ, Dtrain ), L01, Dval )
= arg min
λ
1
|Dval |
∑
(x,y)∈Dval 
L01 (σ (θ∗(λ, Dtrain )
⊤x + θ∗
0 (λ, Dtrain )), y)
λ
V
D
Θ∗= A(D; λ∗)
E test = E(Θ∗, L01, Dtest)
 The customer just wants to buy the right stocks! So we use the real 
 here for
validation.
L01