In which we try to describe the outlines of the “lifecycle” of supervised learning,
including hyperparameter tuning and evaluation of the final product.
C.1 General case
We start with a very generic setting.
Given: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()
such that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning
algorithm () takes as input a data set of the form
where 
 and 
 and returns an 
.
Given a problem specification and a set of data 
, we evaluate hypothesis 
according to average loss, or error,
If the data used for evaluation were not used during learning of the hypothesis then this
is a reasonable estimate of how well the hypothesis will make additional
predictions on new data from the same source.
A validation strategy  takes an algorithm 
, a loss function , and a data source 
and produces a real number which measures how well 
 performs on data from
that distribution.
In the simplest case, we can divide 
 into two sets, 
 and 
, train on the
first, and then evaluate the resulting hypothesis on the second. In that case,
Appendix C — Supervised learning in a
nutshell
C.1.1 Minimal problem specification 
D = {(x(1), y(1)), … , (x(n), y(n))}
x(i) ∈X
y(i) ∈y
h ∈H
C.1.2 Evaluating a hypothesis
D
h
E(h, L, D) =
1
|D|
D
∑
i=1
L (h (x(i)), y(i))
C.1.3 Evaluating a supervised learning algorithm
V
A
L
D
A
C.1.3.1 Using a validation set
D
Dtrain 
Dval 
V(A, L, D) = E (A (Dtrain ), L, Dval )
Appendices > C  Supervised learning in a nutshell

 We can’t reliably evaluate an algorithm based on a single application to a single
training and test set, because there are many aspects of the training and testing
data, as well as, sometimes, randomness in the algorithm itself, that cause variance
in the performance of the algorithm. To get a good idea of how well an algorithm
performs, we need to, multiple times, train it and evaluate the resulting hypothesis,
and report the average over 
 executions of the algorithm of the error of the
hypothesis it produced each time.
We divide the data into 2 K random non-overlapping subsets:
.
Then,
In cross validation, we do a similar computation, but allow data to be re-used in the
 different iterations of training and testing the algorithm (but never share training
and testing data for a single iteration!). See Section 2.8.2.2 for details.
Now, if we have two different algorithms 
 and 
, we might be interested in
knowing which one will produce hypotheses that generalize the best, using data
from a particular source. We could compute 
 and 
, and
prefer the algorithm with lower validation error. More generally, given algorithms
, we would prefer
Now what? We have to deliver a hypothesis to our customer. We now know how to
find the algorithm, 
, that works best for our type of data. We can apply it to all of
our data to get the best hypothesis we know how to create, which would be
and deliver this resulting hypothesis as our best product.
A majority of learning algorithms have the form of optimizing some objective
involving the training data and a loss function.
C.1.3.2 Using multiple training/evaluation runs
K
Dtrain 
1
, Dval 
1 , … , Dtrain 
K
, Dval 
K
V(A, L, D) = 1
K
K
∑
k=1
E(A(Dtrain
k
), L, Dval
k ) .
C.1.3.3 Cross validation
K
C.1.4 Comparing supervised learning algorithms
A1
A2
V (A1, L, D)
V (A∈, L, D)
A1, … , AM
A∗= arg min
m V (AM, L, D)
C.1.5 Fielding a hypothesis
A∗
h∗= A∗(D)
C.1.6 Learning algorithms as optimizers
Interestingly, this loss function is
not always the same as the loss
function that is used for
 So for example, (assuming a perfect optimizer which doesn’t, of course, exist) we
might say our algorithm is to solve an optimization problem:
Our objective often has the form
where  is a loss to be minimized during training and 
 is a regularization term.
Often, rather than comparing an arbitrary collection of learning algorithms, we
think of our learning algorithm as having some parameters that affect the way it
maps data to a hypothesis. These are not parameters of the hypothesis itself, but
rather parameters of the algorithm. We call these hyperparameters. A classic example
would be to use a hyperparameter  to govern the weight of a regularization term
on an objective to be optimized:
Then we could think of our algorithm as 
. Picking a good value of  is the
same as comparing different supervised learning algorithms, which is accomplished
by validating them and picking the best one!
C.2 Concrete case: linear regression
In linear regression the problem formulation is this:
 for values of parameters 
 and 
.
Our learning algorithm has hyperparameter  and can be written as:
Our learning algorithm has hyperparameter $ $ and can be written as:
For a particular training data set and parameter , it finds the best hypothesis on
this data, specified with parameters 
, written 
.
A(D) = arg min
h∈H J (h; D).
J (h; D) = E(h, L, D) + R(h),
L
R
C.1.7 Hyperparameters
λ
J (h; D) = E(h, L, D) + λR(h).
A(D; λ)
λ
x = Rd
y = R
H = {θ⊤x + θ0}
θ ∈Rd
θ0 ∈R
L(g, y) = (g −y)2
λ
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∥θ∥2
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∥θ∥2.
λ
Θ = (θ, θ0)
Θ∗(λ, D)
evaluation! We will see this in
logistic regression.
 Picking the best value of the hyperparameter is choosing among learning
algorithms. We could, most simply, optimize using a single training / validation
split, so 
 
, and
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Note that we don’t use regularization here because we just want to measure how
good the output of the algorithm is at predicting values of new points, and so that’s
what we measure. We use the regularizer during training when we don’t want to
focus only on optimizing predictions on the training data.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, 
, to train, using the best hyperparameters we know, and return
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, as
Here are the same ideas, written out in informal pseudocode:
D = Dtrain ∪Dval 
λ∗= arg min
λ V (Aλ, L, Dval )
= arg min
λ E (Θ∗(λ, Dtrain ),  mse, Dval )
= arg min
λ
1
|Dval |
∑
(x,y)∈Dval 
(θ∗(λ, Dtrain )
⊤x + θ∗
0 (λ, Dtrain ) −y)
2
λ
V
D
Θ∗= A (D; λ∗)
= Θ∗(λ∗, D)
= arg min
θ,θ0
1
|D|
∑
(x,y)∈D
(θ⊤x + θ0 −y)
2 + λ∗∥θ∥2
E test  = E (Θ∗,  mse , Dtest )
=
1
|Dtest |
∑
(x,y)∈Dtot 
(θ∗Tx + θ∗
0 −y)
2
# returns theta_best(D, lambda)
define train(D, lambda):
    return minimize(mse(theta, D) + lambda * norm(theta)**2, theta)
# returns lambda_best using very simple validation
define simple_tune(D_train, D_val, possible_lambda_vals):
    scores = [mse(train(D_train, lambda), D_val) for lambda in 
possible_lambda_vals]
    return possible_lambda_vals[least_index[scores]]
# returns theta_best overall
define theta_best(D_train, D_val, possible_lambda_vals):
    return train(D_train + D_val, simple_tune(D_train, D_val, 
possible_lambda_vals))
# customer evaluation of the theta delivered to them
 C.3 Concrete case: logistic regression
In binary logistic regression the problem formulation is as follows. We are writing
the class labels as 1 and 0.
 for values of parameters 
 and 
.
Proxy loss 
 Our learning algorithm
has hyperparameter  and can be written as:
For a particular training data set and parameter , it finds the best hypothesis on
this data, specified with parameters 
, written 
 according to the
proxy loss 
.
Picking the best value of the hyperparameter is choosing among learning
algorithms based on their actual predictions. We could, most simply, optimize using
a single training / validation split, so 
, and we use the real 01 loss:
It would be much better to select the best  using multiple runs or cross-validation;
that would just be a different choices of the validation procedure  in the top line.
Finally! To make a predictor to ship out into the world, we would use all the data
we have, 
, to train, using the best hyperparameters we know, and return
❓ Study Question
What loss function is being optimized inside this algorithm?
Finally, a customer might evaluate this hypothesis on their data, which we have
never seen during training or validation, as
define customer_val(theta):
    return mse(theta, D_test)
X = Rd
y = {+1, 0}
H = {σ (θ⊤x + θ0)}
θ ∈Rd
θ0 ∈R
L(g, y) = L01( g,  h)
Lnll(g, y) = −(y log(g) + (1 −y) log(1 −g))
λ
A(D; λ) = Θ∗(λ, D) = arg min
θ,θ0
1
|D|
∑
(x,y)∈D
Lnll (σ (θ⊤x + θ0), y) + λ∥θ∥2
λ
Θ = (θ, θ0)
Θ∗(λ, D)
Lnll 
D = Dtrain  ∪Dval
λ∗= arg min
λ V (Aλ, L01, Dval )
= arg min
λ E (Θ∗(λ, Dtrain ), L01, Dval )
= arg min
λ
1
|Dval |
∑
(x,y)∈Dval 
L01 (σ (θ∗(λ, Dtrain )
⊤x + θ∗
0 (λ, Dtrain )), y)
λ
V
D
Θ∗= A(D; λ∗)
E test = E(Θ∗, L01, Dtest)
 The customer just wants to buy the right stocks! So we use the real 
 here for
validation.
L01