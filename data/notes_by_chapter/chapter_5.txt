This page contains all content from the legacy PDF notes; features chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Linear regression and classification are powerful tools, but in the real world, data
often exhibit non-linear behavior that cannot immediately be captured by the linear
models which we have built so far. For example, suppose the true behavior of a
system (with 
) looks like this wavelet:
Such behavior is actually ubiquitous in physical systems, e.g., in the vibrations of
the surface of a drum, or scattering of light through an aperture. However, no single
hyperplane would be a very good fit to such peaked responses!
A richer class of hypotheses can be obtained by performing a non-linear feature
transformation 
 before doing the regression. That is, 
 is a linear
function of , but 
 is a non-linear function of 
 if  is a non-linear
function of .
There are many different ways to construct . Some are relatively systematic and
domain independent. Others are directly related to the semantics (meaning) of the
original features, and we construct them deliberately with our application (goal) in
mind.
5.1 Gaining intuition about feature
transformations
In this section, we explore the effects of non-linear feature transformations on
simple classification problems, to gain intuition.
Let’s look at an example data set that starts in 1-D:
5  Feature Representation
Note
d = 2
ϕ(x)
θTx + θ0
x
θTϕ(x) + θ0
x,
ϕ
x
ϕ
5  Feature Representation

 x
0
These points are not linearly separable, but consider the transformation
. Plotting this transformed data (in two-dimensional space, since
there are now two features), we see that it is now separable. There are lots of
possible separators; we have just shown one of them here.
x
x2
separator
A linear separator in  space is a nonlinear separator in the original space! Let’s see
how this plays out in our simple example. Consider the separator 
(which corresponds to 
 and 
 in our transformed space), which
labels the half-plane 
 as positive. What separator does it correspond to in
the original 1-D space? We have to ask the question: which  values have the
property that 
. The answer is 
 and 
, so those two points constitute
our separator, back in the original space. Similarly, by evaluating where 
and where 
, we can find the regions of 1D space that are labeled positive
and negative (respectively) by this separator.
Example
ϕ(x) = [x, x2]T
Example
ϕ
x2 −1 = 0
θ = [0, 1]T
θ0 = −1
x2 −1 > 0
x
x2 −1 = 0
+1
−1
x2 −1 > 0
x2 −1 < 0
Example
 x
0
1
-1
5.2 Systematic feature construction
Here are two different ways to systematically construct features in a problem
independent way.
If the features in your problem are already naturally numerical, one systematic
strategy for constructing a new feature space is to use a polynomial basis. The idea is
that, if you are using the th-order basis (where  is a positive integer), you include
a feature for every possible product of  different dimensions in your original input.
Here is a table illustrating the th order polynomial basis for different values of ,
calling out the cases when 
 and 
:
Order
in general (
)
0
1
2
3
⋮
⋮
⋮
This transformation can be used in combination with linear regression or logistic
regression (or any other regression or classification model). When we’re using a
linear regression or classification model, the key insight is that a linear regressor or
separator in the transformed space is a non-linear regressor or separator in the
original space.
To give a regression example, the wavelet pictured at the start of this chapter can be
fit much better using a polynomial feature representation up to order 
,
compared to just using a simple hyperplane in the original (single-dimensional)
feature space:
5.2.1 Polynomial basis
k
k
k
k
k
d = 1
d > 1
d = 1
d > 1
[1]
[1]
[1, x]T
[1, x1, … , xd]T
[1, x, x2]T
[1, x1, … , xd, x2
1, x1x2, …]T
[1, x, x2, x3]T
[1, x1, … , xd, x2
1, x1x2, … , x3
1, x1x2
2, x1x2x3, …]T
k = 8
 The raw data (with 
 random samples) is plotted on the left, and the
regression result (curved surface) is on the right.
Now let’s look at a classification example and see how polynomial feature
transformation may help us.
One well-known example is the “exclusive or” (xor) data set, the drosophila of
machine-learning data sets:
Clearly, this data set is not linearly separable. So, what if we try to solve the xor
classification problem using a polynomial basis as the feature transformation? We
can just take our two-dimensional data and transform it into a higher-dimensional
data set, by applying some feature transformation . Now, we have a classification
problem as usual.
Let’s try it for 
 on our xor problem. The feature transformation is
❓ Study Question
If we train a classifier after performing this feature transformation, would we
lose any expressive power if we let 
 (i.e., trained without offset instead of
with offset)?
We might run a classification learning algorithm and find a separator with
coefficients 
 and 
. This corresponds to
n = 1000
Example
ϕ
k = 2
ϕ([x1, x2]T) = [1, x1, x2, x2
1, x1x2, x2
2]T .
θ0 = 0
θ = [0, 0, 0, 0, 4, 0]T
θ0 = 0
2
2
D. Melanogaster is a species of
fruit fly, used as a simple system in
which to study genetics, since 1910.
 and is plotted below, with the gray shaded region classified as negative and the
white region classified as positive:
❓ Study Question
Be sure you understand why this high-dimensional hyperplane is a separator,
and how it corresponds to the figure.
For fun, we show some more plots below. Here is another result for a linear
classifier on xor generated with logistic regression and gradient descent, using a
random initial starting point and second-order polynomial basis:
Here is a harder data set. Logistic regression with gradient descent failed to
separate it with a second, third, or fourth-order basis feature representation, but
0 + 0x1 + 0x2 + 0x2
1 + 4x1x2 + 0x2
2 + 0 = 0
Example
Example
 succeeded with a fifth-order basis. Shown below are some results after 
gradient descent iterations (from random starting points) for bases of order 2
(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).
❓ Study Question
Percy Eptron has a domain with four numeric input features, 
. He
decides to use a representation of the form
where 
 means the vector  concatenated with the vector .
What is the dimension of Percy’s representation? Under what assumptions
about the original features is this a reasonable choice?
Another cool idea is to use the training data itself to construct a feature space. The
idea works as follows. For any particular point  in the input space 
, we can
∼1000
Example
(x1, … , x4)
ϕ(x) = PolyBasis((x1, x2), 3)⌢PolyBasis((x3, x4), 3)
a⌢b
a
b
5.2.2 (Optional) Radial basis functions
p
X
 construct a feature 
 which takes any element 
 and returns a scalar value
that is related to how far  is from the  we started with.
Let’s start with the basic case, in which 
. Then we can define
This function is maximized when 
 and decreases exponentially as  becomes
more distant from .
The parameter  governs how quickly the feature value decays as we move away
from the center point . For large values of , the 
 values are nearly 0 almost
everywhere except right near ; for small values of , the features have a high value
over a larger part of the space.
Now, given a dataset 
 containing  points, we can make a feature transformation
 that maps points in our original space, 
, into points in a new space, 
. It is
defined as follows:
So, we represent a new datapoint  in terms of how far it is from each of the
datapoints in our training set.
This idea can be generalized in several ways and is the fundamental concept
underlying kernel methods, that are not directly covered in this class but we
recommend you read about some time. This idea of describing objects in terms of
their similarity to a set of reference objects is very powerful and can be applied to
cases where 
 is not a simple vector space, but where the inputs are graphs or
strings or other types of objects, as long as there is a distance metric defined on the
input space.
5.3 (Optional) Hand-constructing features for real
domains
In many machine-learning applications, we are given descriptions of the inputs
with many different types of attributes, including numbers, words, and discrete
features. An important factor in the success of an ML application is the way that the
features are chosen to be encoded by the human who is framing the learning
problem.
Getting a good encoding of discrete features is particularly important. You want to
create “opportunities” for the ML system to find the underlying patterns. Although
there are machine-learning methods that have special mechanisms for handling
discrete inputs, most of the methods we consider in this class will assume the input
fp
x ∈X
x
p
X = Rd
fp(x) = e−β∥p−x∥2 .
p = x
x
p
β
p
β
fp
p
β
D
n
ϕ
Rd
Rn
ϕ(x) = [fx(1)(x), fx(2)(x), … , fx(n)(x)]T .
x
X
5.3.1 Discrete features
 vectors  are in 
. So, we have to figure out some reasonable strategies for turning
discrete values into (vectors of) real numbers.
We’ll start by listing some encoding strategies, and then work through some
examples. Let’s assume we have some feature in our raw data that can take on one
of  discrete values.
Numeric: Assign each of these values a number, say 
. We
might want to then do some further processing, as described in Section 1.3.3.
This is a sensible strategy only when the discrete values really do signify some
sort of numeric quantity, so that these numerical values are meaningful.
Thermometer code: If your discrete values have a natural ordering, from
, but not a natural mapping into real numbers, a good strategy is to use
a vector of length  binary variables, where we convert discrete input value
 into a vector in which the first  values are 
 and the rest are 
.
This does not necessarily imply anything about the spacing or numerical
quantities of the inputs, but does convey something about ordering.
Factored code: If your discrete values can sensibly be decomposed into two
parts (say the “maker” and “model” of a car), then it’s best to treat those as two
separate features, and choose an appropriate encoding of each one from this
list.
One-hot code: If there is no obvious numeric, ordering, or factorial structure,
then the best strategy is to use a vector of length , where we convert discrete
input value 
 into a vector in which all values are 
, except for the 
th, which is 
.
Binary code: It might be tempting for the computer scientists among us to use
some binary code, which would let us represent  values using a vector of
length 
. This is a bad idea! Decoding a binary code takes a lot of work, and
by encoding your inputs this way, you’d be forcing your system to learn the
decoding algorithm.
As an example, imagine that we want to encode blood types, that are drawn from
the set 
. There is no obvious linear
numeric scaling or even ordering to this set. But there is a reasonable factoring, into
two features: 
 and 
. And, in fact, we can further reasonably
factor the first group into 
, 
. So, here are two plausible
encodings of the whole set:
Use a 6-D vector, with two components of the vector each encoding the
corresponding factor using a one-hot encoding.
Use a 3-D vector, with one dimension for each factor, encoding its presence as
 and absence as 
 (this is sometimes better than 
). In this case, 
would be 
 and 
 would be 
.
x
Rd
k
1.0/k, 2.0/k, … , 1.0
1, … , k
k
0 < j ≤k
j
1.0
0.0
k
0 < j ≤k
0.0
j
1.0
k
log k
{A+, A−, B+, B−, AB+, AB−, O+, O−}
{A, B, AB, O}
{+, −}
{A, notA} {B, notB}
1.0
−1.0
0.0
AB+
[1.0, 1.0, 1.0]T
O−
[−1.0, −1.0, −1.0]T
 ❓ Study Question
How would you encode 
 in both of these approaches?
The problem of taking a text (such as a tweet or a product review, or even this
document!) and encoding it as an input for a machine-learning algorithm is
interesting and complicated. Much later in the class, we’ll study sequential input
models, where, rather than having to encode a text as a fixed-length feature vector,
we feed it into a hypothesis word by word (or even character by character!).
There are some simple encodings that work well for basic applications. One of them
is the bag of words (bow) model, which can be used to encode documents. The idea is
to let  be the number of words in our vocabulary (either computed from the
training set or some other body of text or dictionary). We will then make a binary
vector (with values 
 and 
) of length , where element  has value 
 if word 
occurs in the document, and 
 otherwise.
If some feature is already encoded as a numeric value (heart rate, stock price,
distance, etc.) then we should generally keep it as a numeric value. An exception
might be a situation in which we know there are natural “breakpoints” in the
semantics: for example, encoding someone’s age in the US, we might make an
explicit distinction between under and over 18 (or 21), depending on what kind of
thing we are trying to predict. It might make sense to divide into discrete bins
(possibly spacing them closer together for the very young) and to use a one-hot
encoding for some sorts of medical situations in which we don’t expect a linear (or
even monotonic) relationship between age and some physiological features.
❓ Study Question
Consider using a polynomial basis of order  as a feature transformation  on
our data. Would increasing  tend to increase or decrease structural error? What
about estimation error?
A+
5.3.2 Text
d
1.0
0.0
d
j
1.0
j
0.0
5.3.3 Numeric values
k
ϕ
k