This page contains all content from the legacy PDF notes; gradient descent chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
In the previous chapter, we showed how to describe an interesting objective
function for machine learning, but we need a way to find the optimal
, particularly when the objective function is not amenable to
analytical optimization. For example, this can be the case when 
 involves a
more complex loss function, or more general forms of regularization. It can also be
the case when there are simply too many parameters to learn for it to be
computationally feasible.
There is an enormous and fascinating literature on the mathematical and
algorithmic foundations of optimization, but for this class, we will consider one of
the simplest methods, called gradient descent.
Intuitively, in one or two dimensions, we can easily think of 
 as defining a
surface over 
; that same idea extends to higher dimensions. Now, our objective is
to find the 
 value at the lowest point on that surface. One way to think about
gradient descent is that you start at some arbitrary point on the surface, look to see
in which direction the “hill” goes down most steeply, take a small step in that
direction, determine the direction of steepest descent from where you are, take
another small step, etc.
Below, we explicitly give gradient descent algorithms for one and multidimensional
objective functions (Section 3.1 and Section 3.2). We then illustrate the application of
gradient descent to a loss function which is not merely mean squared loss
(Section 3.3). And we present an important method known as stochastic gradient
descent (Section 3.4), which is especially useful when datasets are too large for
descent in a single batch, and has some important behaviors of its own.
3.1 Gradient descent in one dimension
We start by considering gradient descent in one dimension. Assume 
, and
that we know both 
 and its first derivative with respect to 
, 
. Here is
pseudo-code for gradient descent on an arbitrary function . Along with  and its
gradient 
 (which, in the case of a scalar 
, is the same as its derivative 
), we
have to specify some hyper-parameters. These hyper-parameters include the initial
value for parameter 
, a step-size hyper-parameter , and an accuracy hyper-
parameter  .
3  Gradient Descent
Note
Θ∗= arg minΘ J(Θ)
J(Θ)
J(Θ)
Θ
Θ
Θ ∈R
J(Θ)
Θ J ′(Θ)
f
f
∇Θf
Θ
f ′
Θ
η
ϵ
You might want to consider
studying optimization some day!
It’s one of the fundamental tools
enabling machine learning, and it’s
a beautiful and deep field.
3  Gradient Descent

 The hyper-parameter  is often called learning rate when gradient descent is applied
in machine learning. For simplicity,  may be taken as a constant, as is the case in
the pseudo-code below; and we’ll see adaptive (non-constant) step-sizes soon.
What’s important to notice though, is that even when  is constant, the actual
magnitude of the change to 
 may not be constant, as that change depends on the
magnitude of the gradient itself too.
procedure 1D-Gradient-Descent(
)
repeat
until 
return 
end procedure
Note that this algorithm terminates when the derivative of the function  is
sufficiently small. There are many other reasonable ways to decide to terminate,
including:
Stop after a fixed number of iterations , i.e., when 
. Practically, this is the
most common choice.
Stop when the change in the value of the parameter 
 is sufficiently small,
i.e., when 
.
❓ Study Question
Consider all of the potential stopping criteria for 1D-Gradient-Descent , both
in the algorithm as it appears and listed separately later. Can you think of ways
that any two of the criteria relate to each other?
Theorem 3.1 Choose any small distance 
. If we assume that  has a minimum, is
sufficiently “smooth” and convex, and if the learning rate  is sufficiently small, gradient
descent will reach a point within  of a global optimum point 
.
However, we must be careful when choosing the learning rate to prevent slow
convergence, non-converging oscillation around the minimum, or divergence.
The following plot illustrates a convex function 
, starting gradient
descent at 
 with a step-size of 
. It is very well-behaved!
η
η
η
Θ
1:
Θinit, η, f, f ′, ϵ
2:
Θ(0) ←Θinit
3:
t ←0
4:
5:
t ←t + 1
6:
Θ(t) = Θ(t−1) −η f ′(Θ(t−1))
7:
|f ′(Θ(t))| < ϵ
8:
Θ(t)
9:
f
T
t = T
Θ
Θ(t) −Θ(t−1) < ϵ
∣∣
~ϵ > 0
f
η
~ϵ
Θ
f(x) = (x −2)2
xinit = 4.0
1/2
 −1
1
2
3
4
5
6
2
4
x
f(x)
If  is non-convex, where gradient descent converges to depends on 
. First, let’s
establish some definitions. Let  be a real-valued function defined over some
domain 
. A point 
 is called a global minimum point of  if 
 for
all other 
. A point 
 is instead called a local minimum point of a function
 if there exists some constant 
 such that for all  within the interval defined
by 
 
, where  is some distance metric, e.g.,
 A global minimum point is also a local minimum point, but a
local minimum point does not have to be a global minimum point.
❓ Study Question
What happens in this example with very small ? With very big ?
If  is non-convex (and sufficiently smooth), one expects that gradient descent (run
long enough with small enough learning rate) will get very close to a point at which
the gradient is zero, though we cannot guarantee that it will converge to a global
minimum point.
There are two notable exceptions to this common sense expectation: First, gradient
descent can get stagnated while approaching a point  which is not a local
minimum or maximum, but satisfies 
. For example, for 
, starting
gradient descent from the initial guess 
, while using learning rate 
will lead to 
 converging to zero as 
. Second, there are functions (even
convex ones) with no minimum points, like 
, for which gradient
descent with a positive learning rate converges to 
.
The plot below shows two different 
, and how gradient descent started from
each point heads toward two different local optimum points.
f
xinit
f
D
x0 ∈D
f
f(x0) ≤f(x)
x ∈D
x0 ∈D
f
ϵ > 0
x
d(x, x0) < ϵ, f(x0) ≤f(x)
d
d(x, x0) = ||x −x0||.
η
η
f
x
f ′(x) = 0
f(x) = x3
xinit = 1
η < 1/3
x(k)
k →∞
f(x) = exp(−x)
+∞
xinit
 −2
−1
1
2
3
4
4
6
8
10
x
f(x)
3.2 Multiple dimensions
The extension to the case of multi-dimensional 
 is straightforward. Let’s assume
, so 
.
The gradient of  with respect to 
 is
The algorithm remains the same, except that the update step in line 5 becomes
and any termination criteria that depended on the dimensionality of 
 would have
to change. The easiest thing is to keep the test in line 6 as 
,
which is sensible no matter the dimensionality of 
.
❓ Study Question
Which termination criteria from the 1D case were defined in a way that assumes
 is one dimensional?
3.3 Application to regression
Θ
Θ ∈Rm
f : Rm →R
f
Θ
∇Θf =
⎡
⎢
⎣
∂f/∂Θ1
⋮
∂f/∂Θm
⎤
⎥
⎦
Θ(t) = Θ(t−1) −η∇Θf(Θ(t−1))
Θ
f(Θ(t)) −f(Θ(t−1)) < ϵ
∣∣
Θ
Θ
 Recall from the previous chapter that choosing a loss function is the first step in
formulating a machine-learning problem as an optimization problem, and for
regression we studied the mean square loss, which captures losws as
. This leads to the ordinary least squares objective
We use the gradient of the objective with respect to the parameters,
to obtain an analytical solution to the linear regression problem. Gradient descent
could also be applied to numerically compute a solution, using the update rule
Now, let’s add in the regularization term, to get the ridge-regression objective:
 
Recall that in ordinary least squares, we finessed handling 
 by adding an extra
dimension of all 1’s. In ridge regression, we really do need to separate the
parameter vector  from the offset 
, and so, from the perspective of our general-
purpose gradient descent method, our whole parameter set 
 is defined to be
. We will go ahead and find the gradients separately for each one:
Note that 
 will be of shape 
 and 
 will be a scalar since we
have separated 
 from  here.
❓ Study Question
(guess −actual)2
J(θ) = 1
n
n
∑
i=1
(θTx(i) −y(i))
2
.
∇θJ = 2
n XT
d×n
(Xθ −Y )
n×1
,









(3.1)
θ(t) = θ(t−1) −η 2
n
n
∑
i=1
([θ(t−1)]
T
x(i) −y(i))x(i) .
3.3.1 Ridge regression
Jridge(θ, θ0) = 1
n
n
∑
i=1
(θTx(i) + θ0 −y(i))
2
+ λ∥θ∥2 .
θ0
θ
θ0
Θ
Θ = (θ, θ0)
∇θJridge(θ, θ0) = 2
n
n
∑
i=1
(θTx(i) + θ0 −y(i))x(i) + 2λθ
∂Jridge(θ, θ0)
∂θ0
= 2
n
n
∑
i=1
(θTx(i) + θ0 −y(i)) .
∇θJridge
d × 1
∂Jridge/∂θ0
θ0
θ
 Convince yourself that the dimensions of all these quantities are correct, under
the assumption that  is 
. How does  relate to 
 as discussed for 
 in the
previous section?
❓ Study Question
Compute 
 by finding the vector of partial derivatives
. What is the shape of 
?
❓ Study Question
Compute 
 by finding the vector of partial derivatives
.
❓ Study Question
Use these last two results to verify our derivation above.
Putting everything together, our gradient descent algorithm for ridge regression
becomes
procedure RR-Gradient-Descent(
)
repeat
until 
return 
end procedure
❓ Study Question
Is it okay that  doesn’t appear in line 8?
❓ Study Question
Is it okay that the 2’s from the gradient definitions don’t appear in the
algorithm?
θ
d × 1
d
m
Θ
∇θ||θ||2
(∂||θ||2/∂θ1, … , ∂||θ||2/∂θd)
∇θ||θ||2
∇θJridge(θTx + θ0, y)
(∂Jridge(θTx + θ0, y)/∂θ1, … , ∂Jridge(θTx + θ0, y)/∂θd)
1:
θinit, θ0init, η, ϵ
2:
θ(0) ←θinit
3:
θ(0)
0
←θ0init
4:
t ←0
5:
6:
t ←t + 1
7:
θ(t) = θ(t−1) −η ( 1
n ∑n
i=1 (θ(t−1)Tx(i) + θ0
(t−1) −y(i))x(i) + λθ(t−1))
8:
θ(t)
0 = θ(t−1)
0
−η ( 1
n ∑n
i=1 (θ(t−1)Tx(i) + θ0(t−1) −y(i)))
9:
Jridge(θ(t), θ(t)
0 ) −Jridge(θ(t−1), θ(t−1)
0
) < ϵ
∣∣
10:
θ(t), θ(t)
0
11:
λ
Beware double superscripts! 
 is
the transpose of the vector .
[θ]T
θ
 3.4 Stochastic gradient descent
When the form of the gradient is a sum, rather than take one big(ish) step in the
direction of the gradient, we can, instead, randomly select one term of the sum, and
take a very small step in that direction. This seems sort of crazy, but remember that
all the little steps would average out to the same direction as the big step if you
were to stay in one place. Of course, you’re not staying in that place, so you move,
in expectation, in the direction of the gradient.
Most objective functions in machine learning can end up being written as an
average over data points, in which case, stochastic gradient descent (sgd) is
implemented by picking a data point randomly out of the data set, computing the
gradient as if there were only that one point in the data set, and taking a small step
in the negative direction.
Let’s assume our objective has the form
where  is the number of data points used in the objective (and this may be
different from the number of points available in the whole data set).
Here is pseudocode for applying sgd to such an objective ; it assumes we know the
form of 
 for all  in 
:
procedure Stochastic-Gradient-Descent(
)
for 
 do
randomly select 
end for
end procedure
Note that now instead of a fixed value of ,  is indexed by the iteration of the
algorithm, . Choosing a good stopping criterion can be a little trickier for sgd than
traditional gradient descent. Here we’ve just chosen to stop after a fixed number of
iterations .
For sgd to converge to a local optimum point as  increases, the learning rate has to
decrease as a function of time. The next result shows one learning rate sequence that
works.
Theorem 3.2 If  is convex, and 
 is a sequence satisfying
f(Θ) = 1
n
n
∑
i=1
fi(Θ) ,
n
f
∇Θfi
i
1 … n
1:
Θinit, η, f, ∇Θf1, . . . , ∇Θfn, T
2:
Θ(0) ←Θinit
3:
t ←1
4:
i ∈{1, 2, … , n}
5:
Θ(t) = Θ(t−1) −η(t) ∇Θfi(Θ(t−1))
6:
7:
η η
t
T
t
f
η(t)
∞
∑
t=1
η(t) = ∞and
∞
∑
t=1
η(t)2 < ∞,
Sometimes you will see that the
objective being written as a sum,
instead of an average. In the “sum”
convention, the 
 normalizing
constant is getting “absorbed” into
individual 
.
1
n
fi
f(Θ) =
n
∑
i=1
fi(Θ) .
 then SGD converges with probability one* to the optimal 
.*
Why these two conditions? The intuition is that the first condition, on 
, is
needed to allow for the possibility of an unbounded potential range of exploration,
while the second condition, on 
, ensures that the learning rates get smaller
and smaller as  increases.
One “legal” way of setting the learning rate is to make 
 but people often
use rules that decrease more slowly, and so don’t strictly satisfy the criteria for
convergence.
❓ Study Question
If you start a long way from the optimum, would making 
 decrease more
slowly tend to make you move more quickly or more slowly to the optimum?
There are multiple intuitions for why sgd might be a better choice algorithmically
than regular gd (which is sometimes called batch gd (bgd)):
bgd typically requires computing some quantity over every data point in a data
set. sgd may perform well after visiting only some of the data. This behavior
can be useful for very large data sets – in runtime and memory savings.
If your  is actually non-convex, but has many shallow local optimum points
that might trap bgd, then taking samples from the gradient at some point 
might “bounce” you around the landscape and away from the local optimum
points.
Sometimes, optimizing  really well is not what we want to do, because it
might overfit the training set; so, in fact, although sgd might not get lower
training error than bgd, it might result in lower test error.
Θ
∑η(t)
∑η(t)2
t
η(t) = 1/t
η(t)
f
Θ
f