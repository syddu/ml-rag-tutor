This page contains all content from the legacy PDF notes; non-parametric models chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Neural networks have adaptable complexity, in the sense that we can try different
structural models and use cross validation to find one that works well on our data.
Beyond neural networks, we may further broaden the class of models that we can
fit to our data, for example as illustrated by the techniques introduced in this
chapter.
Here, we turn to models that automatically adapt their complexity to the training
data. The name non-parametric methods is misleading: it is really a class of methods
that does not have a fixed parameterization in advance. Rather, the complexity of
the parameterization can grow as we acquire more data.
Some non-parametric models, such as nearest-neighbor, rely directly on the data to
make predictions and do not compute a model that summarizes the data. Other
non-parametric methods, such as decision trees, can be seen as dynamically
constructing something that ends up looking like a more traditional parametric
model, but where the actual training data affects exactly what the form of the model
will be.
The non-parametric methods we consider here tend to have the form of a
composition of simple models:
Nearest neighbor models: Section 12.1 where we don‚Äôt process data at training
time, but do all the work when making predictions, by looking for the closest
training example(s) to a given new data point.
Tree models: Section 12.2 where we partition the input space and use different
simple predictions on different regions of the space; the hypothesis space can
become arbitrarily large allowing finer and finer partitions of the input space.
Ensemble models: Section 12.2.3 in which we train several different classifiers on
the whole space and average the answers; this decreases the estimation error. In
particular, we will look at bootstrap aggregation, or bagging of trees.
Boosting is a way to construct a model composed of a sequence of component
models (e.g., a model consisting of a sequence of trees, each subsequent tree
seeking to correct errors in the previous trees) that decreases both estimation
and structural error. We won‚Äôt consider this in detail in this class.
12  Non-parametric methods
Note
Ôë°12  Non-parametric methods
Ôî™
 * -means clustering methods, Section 12.3 where we partition data into groups
based on similarity without predefined labels, adapting complexity by
adjusting the number of clusters.
Why are we studying these methods, in the heyday of complicated models such as
neural networks ?
They are fast to implement and have few or no hyperparameters to tune.
They often work as well as or better than more complicated methods.
Predictions from some of these models can be easier to explain to a human
user: decision trees are fairly directly human-interpretable, and nearest
neighbor methods can justify their decisions to some extent by showing a few
training examples that the predictions were based on.
12.1 Nearest Neighbor
In nearest-neighbor models, we don‚Äôt do any processing of the data at training time
‚Äì we just remember it! All the work is done at prediction time.
Input values  can be from any domain 
 (
, documents, tree-structured objects,
etc.). We just need a distance metric, 
, which satisfies the following,
for all 
:
Given a data-set 
, our predictor for a new 
 is
that is, the predicted output associated with the training point that is closest to the
query point . Tie breaking is typically done at random.
This same algorithm works for regression and classification!
The nearest neighbor prediction function can be described by dividing the space up
into regions whose closest point is each individual training point as shown below :
k
x
X Rd
d : X √ó X ‚ÜíR+
x, x‚Ä≤, x‚Ä≤‚Ä≤ ‚ààX
d(x, x) = 0
d(x, x‚Ä≤) = d(x‚Ä≤, x)
d(x, x‚Ä≤‚Ä≤) ‚â§d(x, x‚Ä≤) + d(x‚Ä≤, x‚Ä≤‚Ä≤)
D = {(x(i), y(i))}n
i=1
x ‚ààX
h(x) = y(i)
where
i = arg min
i
d(x, x(i)) ,
x
 In each region, we predict the associated  value.
There are several useful variations on this method. In -nearest-neighbors, we find
the  training points nearest to the query point  and output the majority  value
for classification or the average for regression. We can also do locally weighted
regression in which we fit locally linear regression models to the  nearest points,
possibly giving less weight to those that are farther away. In large data-sets, it is
important to use good data structures (e.g., ball trees) to perform the nearest-
neighbor look-ups efficiently (without looking at all the data points each time).
12.2 Tree Models
The idea here is that we would like to find a partition of the input space and then fit
very simple models to predict the output in each piece. The partition is described
using a (typically binary) ‚Äútree‚Äù that recursively splits the space.
Tree methods differ by:
The class of possible ways to split the space at each node; these are typically
linear splits, either aligned with the axes of the space, or sometimes using more
general classifiers.
The class of predictors within the partitions; these are often simply constants,
but may be more general classification or regression models.
The way in which we control the complexity of the hypothesis: it would be
within the capacity of these methods to have a separate partition element for
each individual training example.
y
k
k
x
y
k
 The algorithm for making the partitions and fitting the models.
One advantage of tree models is that they are easily interpretable by humans. This
is important in application domains, such as medicine, where there are human
experts who often ultimately make critical decisions and who need to feel confident
in their understanding of recommendations made by an algorithm. Below is an
example decision tree, illustrating how one might be able to understand the
decisions made by the tree.
#Example Here is a sample tree (reproduced from Breiman, Friedman, Olshen, Stone
(1984)):
These methods are most appropriate for domains where the input space is not very
high-dimensional and where the individual input features have some substantially
useful information individually or in small groups. Trees would not be good for
image input, but might be good in cases with, for example, a set of meaningful
measurements of the condition of a patient in the hospital, as in the example above.
We‚Äôll concentrate on the CART/ID3 (‚Äúclassification and regression trees‚Äù and
‚Äúiterative dichotomizer 3‚Äù, respectively) family of algorithms, which were invented
independently in the statistics and the artificial intelligence communities. They
work by greedily constructing a partition, where the splits are axis aligned and by
fitting a constant model in the leaves. The interesting questions are how to select the
splits and how to control complexity. The regression and classification versions are
very similar.
As a concrete example, consider the following images:
Note
  
The left image depicts a set of labeled data points in a two-dimensional feature
space. The right shows a partition into regions by a decision tree, in this case having
no classification errors in the final partitions.
The predictor is made up of
a partition function, , mapping elements of the input space into exactly one of
 regions, 
, and
a collection of 
 output values, 
, one for each region.
If we already knew a division of the space into regions, we would set 
, the
constant output for region 
, to be the average of the training output values in
that region. For a training data set 
, we let  be an
indicator set of all of the elements within 
, so that 
 for our whole
data set. We can define 
 as the subset of data set samples that are in region 
, so
that 
. Then
We can define the error in a region as 
. For example, 
 as the sum of squared
error would be expressed as
Ideally, we should select the partition to minimize
for some regularization constant . It is enough to search over all partitions of the
training data (not all partitions of the input space!) to optimize this, but the problem
is NP-complete.
12.2.1 Regression
œÄ
M
R1, ‚Ä¶ , RM
M
Om
Om
Rm
D = {(x(i), y(i))}, i = 1, ‚Ä¶ n
I
D
I = {1, ‚Ä¶ , n}
Im
Rm
Im = {i ‚à£x(i) ‚ààRm}
Om = averagei‚ààIm y(i) .
Em
Em
Em = ‚àë
i‚ààIm
(y(i) ‚àíOm)2 .
ŒªM +
M
‚àë
m=1
Em ,
Œª
12.2.1.1 Building a tree
 So, we‚Äôll be greedy. We establish a criterion, given a set of data, for finding the best
single split of that data, and then apply it recursively to partition the space. For the
discussion below, we will select the partition of the data that minimizes the sum of the
sum of squared errors of each partition element. Then later, we will consider other
splitting criteria.
Given a data set 
, we now consider  to be an
indicator of the subset of elements within 
 that we wish to build a tree (or subtree)
for. That is,  may already indicate a subset of data set 
, based on prior splits in
constructing our overall tree. We define terms as follows:
 indicates the set of examples (subset of ) whose feature value in dimension
 is greater than or equal to split point ;
 indicates the set of examples (subset of ) whose feature value in dimension
 is less than ;
 is the average  value of the data points indicated by set 
; and
 is the average  value of the data points indicated by set 
.
Here is the pseudocode. In what follows,  is the largest leaf size that we will allow
in the tree, and is a hyperparameter of the algorithm.
procedure BuildTree(
)
if 
 then
return 
else
for all split dimension , split value  do
end for
return 
end if
end procedure
In practice, we typically start by calling BuildTree  with the first input equal to our
whole data set (that is, with 
). But then that call of BuildTree  can
recursively lead to many other calls of BuildTree .
Let‚Äôs think about how long each call of BuildTree  takes to run. We have to
consider all possible splits. So we consider a split in each of the  dimensions. In
each dimension, we only need to consider splits between two data points (any other
D = {(x(i), y(i))}, i = 1, ‚Ä¶ n
I
D
I
D
I +
j,s
I
j
s
I ‚àí
j,s
I
j
s
^y+
j,s
y
I +
j,s
^y‚àí
j,s
y
I ‚àí
j,s
k
1:
I, k
2:
|I| ‚â§k
3:
^y ‚Üê
1
|I| ‚àëi‚ààI y(i)
4:
Leaf(value = ^y)
5:
6:
j
s
7:
I +
j,s ‚Üê{i ‚ààI ‚à£x(i)
j
‚â•s}
8:
I ‚àí
j,s ‚Üê{i ‚ààI ‚à£x(i)
j
< s}
9:
^y+
j,s ‚Üê
1
|I +
j,s| ‚àëi‚ààI +
j,s y(i)
10:
^y‚àí
j,s ‚Üê
1
|I ‚àí
j,s| ‚àëi‚ààI ‚àí
j,s y(i)
11:
Ej,s ‚Üê‚àëi‚ààI +
j,s(y(i) ‚àí^y+
j,s)2 + ‚àëi‚ààI ‚àí
j,s(y(i) ‚àí^y‚àí
j,s)2
12:
13:
(j‚àó, s‚àó) ‚Üêarg minj,s Ej,s
14:
15:
Node(j‚àó, s‚àó, BuildTree(I ‚àí
j‚àó,s‚àó, k), BuildTree(I +
j‚àó,s‚àó, k))
16:
17:
I = {1, ‚Ä¶ , n}
d
 split will give the same error on the training data). So, in total, we consider 
splits in each call to BuildTree .
It might be tempting to regularize by using a somewhat large value of , or by
stopping when splitting a node does not significantly decrease the error. One
problem with short-sighted stopping criteria is that they might not see the value of
a split that will require one more split before it seems useful. So, we will tend to
build a tree that is too large, and then prune it back.
We define cost complexity of a tree , where 
 ranges over its leaves, as
and 
 is the number of leaves. For a fixed , we can find a  that (approximately)
minimizes 
 by ‚Äúweakest-link‚Äù pruning:
Create a sequence of trees by successively removing the bottom-level split that
minimizes the increase in overall error, until the root is reached.
Return the  in the sequence that minimizes the cost complexity.
We can choose an appropriate  using cross validation.
The strategy for building and pruning classification trees is very similar to the
strategy for regression trees.
Given a region 
 corresponding to a leaf of the tree, we would pick the output
class  to be the value that exists most frequently (the majority value) in the data
points whose  values are in that region, i.e., data points indicated by 
:
Let‚Äôs now define the error in a region as the number of data points that do not have
the value 
:
We define the empirical probability of an item from class  occurring in region 
 as:
where 
 is the number of training points in region 
; that is, 
 For later
use, we‚Äôll also define the empirical probabilities of split values, 
, as the
fraction of points with dimension  in split  occurring in region 
 (one branch of
O(dn)
12.2.1.2 Pruning
k
T
m
CŒ±(T) =
|T|
‚àë
m=1
Em(T) + Œ±|T| ,
|T|
Œ±
T
CŒ±(T)
T
Œ±
12.2.2 Classification
Rm
y
x
Im
Om = majorityi‚ààIm y(i) .
Om
Em = {i ‚à£i ‚ààIm and y(i) ‚â†Om}
.
‚à£‚à£
k
m
^Pm,k = ^P(Im, k) =
{i ‚à£i ‚ààIm and y(i) = k}
Nm
,
‚à£‚à£
Nm
m
Nm = |Im|.
^Pm,j,s
j
s
m
 the tree), and 
 as the complement (the fraction of points in the other
branch).
In our greedy algorithm, we need a way to decide which split to make next. There
are many criteria that express some measure of the ‚Äúimpurity‚Äù in child nodes. Some
measures include:
Misclassification error:
Gini index:
Entropy:
So that the entropy 
 is well-defined when 
, we will stipulate that
.
These splitting criteria are very similar, and it‚Äôs not entirely obvious which one is
better. We will focus on entropy, just to be concrete.
Analogous to how for regression we choose the dimension  and split  that
minimizes the sum of squared error 
, for classification, we choose the dimension
 and split  that minimizes the weighted average entropy over the ‚Äúchild‚Äù data
points in each of the two corresponding splits, 
 and 
. We calculate the entropy
in each split based on the empirical probabilities of class memberships in the split,
and then calculate the weighted average entropy 
 as
Choosing the split that minimizes the entropy of the children is equivalent to
maximizing the information gain of the test 
, defined by
In the two-class case (with labels 0 and 1), all of the splitting criteria mentioned
above have the values
1 ‚àí^Pm,j,s
Splitting criteria
Qm(T) = Em
Nm
= 1 ‚àí^Pm,Om
Qm(T) = ‚àë
k
^Pm,k(1 ‚àí^Pm,k)
Qm(T) = H(Im) = ‚àí‚àë
k
^Pm,k log2 ^Pm,k
H
^P = 0
0 log2 0 = 0
j
s
Ej,s
j
s
I +
j,s
I ‚àí
j,s
^H
^H = (fraction of points in left data set) ‚ãÖH(I ‚àí
j,s)
+(fraction of points in right data set) ‚ãÖH(I +
j,s)
= (1 ‚àí^Pm,j,s)H(I ‚àí
j,s) + ^Pm,j,sH(I +
j,s)
=
|I ‚àí
j,s|
Nm
‚ãÖH(I ‚àí
j,s) +
|I +
j,s|
Nm
‚ãÖH(I +
j,s) .
xj = s
infoGain(xj = s, Im) =
H(Im) ‚àí(
|I ‚àí
j,s|
Nm
‚ãÖH(I ‚àí
j,s) +
|I +
j,s|
Nm
‚ãÖH(I +
j,s))
{
 The respective impurity curves are shown below, where 
; the vertical axis
plots 
 for each of the three criteria.
There used to be endless haggling about which impurity function one should use. It
seems to be traditional to use entropy to select which node to split while growing the
tree, and misclassification error in the pruning criterion.
One important limitation or drawback in conventional trees is that they can have
high estimation error: small changes in the data can result in very big changes in the
resulting tree.
Bootstrap aggregation is a technique for reducing the estimation error of a non-linear
predictor, or one that is adaptive to the data. The key idea applied to trees, is to
build multiple trees with different subsets of the data, and then create an ensemble
model that combines the results from multiple trees to make a prediction.
Construct 
 new data sets of size . Each data set is constructed by sampling 
data points with replacement from 
. A single data set is called bootstrap sample
of 
.
Train a predictor 
 on each bootstrap sample.
Regression case: bagged predictor is
Classification case: Let 
 be the number of classes. We find a majority bagged
predictor as follows. We let 
 be a ‚Äúone-hot‚Äù vector with a single 1 and
{
.
0.0
when  ^Pm,0 = 0.0
0.0
when  ^Pm,0 = 1.0
p = ^Pm,0
Qm(T)
12.2.3 Bagging
B
n
n
D
D
^f b(x)
^fbag(x) = 1
B
B
‚àë
b=1
^f b(x) .
K
^f b(x)
  zeros, and define the predicted output  for predictor 
 as
. Then
which is a vector containing the proportion of classifiers that predicted each
class  for input . Then the overall predicted output is
There are theoretical arguments showing that bagging does, in fact, reduce
estimation error. However, when we bag a model, any simple intrepetability is lost.
Random forests are collections of trees that are constructed to be de-correlated, so
that using them to vote gives maximal advantage. In competitions, they often have
excellent classification performance among large collections of much fancier
methods.
In what follows, 
, 
, and  are hyperparameters of the algorithm.
procedure RandomForest(
)
for 
 to  do
Draw a bootstrap sample 
 of size  from 
Grow tree 
 on 
:
while there are splittable nodes do
Select 
 variables at random from the  total variables
Pick the best variable and split point among those 
Split the current node
end while
end for
return 
end procedure
Given the ensemble of trees, vote to make a prediction on a new .
There are many variations on the tree theme. One is to employ different regression
or classification methods in each leaf. For example, a linear regression might be
used to model the examples in each leaf, rather than using a constant value.
In the relatively simple trees that we‚Äôve considered, splits have been based on only
a single feature at a time, and with the resulting splits being axis-parallel. Other
methods for splitting are possible, including consideration of multiple features and
linear classifiers based on those, potentially resulting in non-axis-parallel splits.
Complexity is a concern in such cases, as many possible combinations of features
K ‚àí1
^y
f b
^yb(x) = arg maxk ^f b(x)k
^fbag(x) = 1
B
B
‚àë
b=1
^f b(x),
k
x
^ybag(x) = arg max
k
^fbag(x)k .
12.2.4 Random Forests
B m
n
1:
B, m, n
2:
b = 1
B
3:
Db
n
D
4:
Tb
Db
5:
6:
m
d
7:
m
8:
9:
10:
11:
12:
{Tb}B
b=1
13:
x
12.2.5 Tree variants and tradeoffs
 may need to be considered, to select the best variable combination (rather than a
single split variable).
Another generalization is a hierarchical mixture of experts, where we make a ‚Äúsoft‚Äù
version of trees, in which the splits are probabilistic (so every point has some degree
of membership in every leaf). Such trees can be trained using a form of gradient
descent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,
gradient boosted trees) and implementations are readily available (e.g., XGBoost).
Trees have a number of strengths, and remain a valuable tool in the machine
learning toolkit. Some benefits include being relatively easy to interpret, fast to
train, and ability to handle multi-class classification in a natural way. Trees can
easily handle different loss functions; one just needs to change the predictor and
loss being applied in the leaves. Methods also exist to identify which features are
particularly important or influential in forming the tree, which can aid in human
understanding of the data set. Finally, in many situations, trees perform
surprisingly well, often comparable to more complicated regression or classification
models. Indeed, in some settings it is considered good practice to start with trees
(especially random forest or boosted trees) as a ‚Äúbaseline‚Äù machine learning model,
against which one can evaluate performance of more sophisticated models.
While tree-based methods excel at supervised learning tasks, we now turn to
another important class of non-parametric methods that focus on discovering
structure in unlabeled data. These clustering methods share some conceptual
similarities with tree-based approaches - both aim to partition the input space into
meaningful regions - but clustering methods operate without supervision, making
them particularly valuable for exploratory data analysis and pattern discovery.
12.3 -means Clustering
Clustering is an unsupervised learning method where we aim to discover
meaningful groupings or categories in a dataset based on patterns or similarities
within the data itself, without relying on pre-assigned labels. It is widely used for
exploratory data analysis, pattern recognition, and segmentation tasks, allowing us
to interpret and manage complex datasets by uncovering hidden structures and
relationships.
Oftentimes a dataset can be partitioned into different categories. A doctor may
notice that their patients come in cohorts and different cohorts respond to different
treatments. A biologist may gain insight by identifying that bats and whales,
despite outward appearances, have some underlying similarity, and both should be
considered members of the same category, i.e., ‚Äúmammal‚Äù. The problem of
automatically identifying meaningful groupings in datasets is called clustering.
Once these groupings are found, they can be leveraged toward interpreting the data
and making optimal decisions for each group.
k
 Mathematically, clustering looks a bit like classification: we wish to find a mapping
from datapoints, , to categories, . However, rather than the categories being
predefined labels, the categories in clustering are automatically discovered partitions
of an unlabeled dataset.
Because clustering does not learn from labeled examples, it is an example of an
unsupervised learning algorithm. Instead of mimicking the mapping implicit in
supervised training pairs 
, clustering assigns datapoints to categories
based on how the unlabeled data 
 is distributed in data space.
Intuitively, a ‚Äúcluster‚Äù is a group of datapoints that are all nearby to each other and
far away from other clusters. Let‚Äôs consider the following scatter plot. How many
clusters do you think there are?
There seem to be about five clumps of datapoints and those clumps are what we
would like to call clusters. If we assign all datapoints in each clump to a cluster
corresponding to that clump, then we might desire that nearby datapoints are
assigned to the same cluster, while far apart datapoints are assigned to different
clusters.
In designing clustering algorithms, three critical things we need to decide are:
How do we measure distance between datapoints? What counts as ‚Äúnearby‚Äù
and ‚Äúfar apart‚Äù?
How many clusters should we look for?
How do we evaluate how good a clustering is?
We will see how to begin making these decisions as we work through a concrete
clustering algorithm in the next section.
One of the simplest and most commonly used clustering algorithms is called k-
means. The goal of the k-means algorithm is to assign datapoints to  clusters in
such a way that the variance within clusters is as small as possible. Notice that this
12.3.1 Clustering formalisms
x
y
{x(i), y(i)}n
i=1
{x(i)}n
i=1
12.3.2 The k-means formulation
k
Figure 12.1: A dataset we would
like to cluster. How many clusters
do you think there are?
 matches our intuitive idea that a cluster should be a tightly packed set of
datapoints.
Similar to the way we showed that supervised learning could be formalized
mathematically as the minimization of an objective function (loss function +
regularization), we will show how unsupervised learning can also be formalized as
minimizing an objective function. Let us denote the cluster assignment for a
datapoint 
 as 
, i.e., 
 means we are assigning datapoint
 to cluster number 1. Then the k-means objective can be quantified with the
following objective function (which we also call the ‚Äúk-means loss‚Äù):
where 
 and 
, so that 
 is the
mean of all datapoints in cluster , and using 
 to denote the indicator function
(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum
(over data points) of the loss is the variance of datapoints within cluster . We sum
up the variance of all  clusters to get our overall loss.
The k-means algorithm minimizes this loss by alternating between two steps: given
some initial cluster assignments: 1) compute the mean of all data in each cluster and
assign this as the ‚Äúcluster mean‚Äù, and 2) reassign each datapoint to the cluster with
nearest cluster mean. Figure 12.2 shows what happens when we repeat these steps
on the dataset from above.
Each time we reassign the data to the nearest cluster mean, the k-means loss
decreases (the datapoints end up closer to their assigned cluster mean), or stays the
same. And each time we recompute the cluster means the loss also decreases (the
means end up closer to their assigned datapoints) or stays the same. Overall then,
the clustering gets better and better, according to our objective ‚Äì until it stops
improving.
After four iterations of cluster assignment + update means in our example, the k-
means algorithm stops improving. We say it has converged, and its final solution is
shown in Figure 12.3.
x(i)
y(i) ‚àà{1, 2, ‚Ä¶ , k}
y(i) = 1
x(i)
k
‚àë
j=1
n
‚àë
i=1
ùüô(y(i) = j) x(i) ‚àíŒº(j)
2 ,
‚à•‚à•
(12.1)
Œº(j) =
1
Nj ‚àën
i=1 ùüô(y(i) = j)x(i)
Nj = ‚àën
i=1 ùüô(y(i) = j)
Œº(j)
j
ùüô(‚ãÖ)
j
k
12.3.2.0.0.1 K-means algorithm
Figure 12.2: The first three steps of
running the k-means algorithm on
this data. Datapoints are colored
according to the cluster to which
they are assigned. Cluster means
are the larger X‚Äôs with black
outlines.
 It seems to converge to something reasonable! Now let‚Äôs write out the algorithm in
complete detail:
procedure KMeans(
)
Initialize centroids 
 and assignments 
 randomly
for 
 to  do
for 
 to  do
end for
for 
 to  do
end for
if 
 then
break//convergence
end if
end for
return 
end procedure
The for-loop over the  datapoints assigns each datapoint to the nearest cluster
center. The for-loop over the k clusters updates the cluster center to be the mean of
all datapoints currently assigned to that cluster. As suggested above, it can be
shown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it
converges to a local minimum of the loss.
It‚Äôs like classification except it picked what the classes are rather than being given
examples of what the classes are.
We can also use gradient descent to optimize the k-means objective. To show how to
apply gradient descent, we first rewrite the objective as a differentiable function
only of :
 is the value of the k-means loss given that we pick the optimal assignments of
the datapoints to cluster means (that‚Äôs what the 
 does). Now we can use the
gradient 
 to find the values for  that achieve minimum loss when cluster
1:
k, œÑ, {x(i)}n
i=1
2:
Œº(1), ‚Ä¶ , Œº(k)
y(1), ‚Ä¶ , y(n)
3:
t = 1
œÑ
4:
yold ‚Üêy
5:
i = 1
n
6:
y(i) ‚Üêarg minj‚àà{1,‚Ä¶,k} x(i) ‚àíŒº(j)
2
‚à•‚à•
7:
8:
j = 1
k
9:
Nj ‚Üê‚àën
i=1 ùüô(y(i) = j)
10:
Œº(j) ‚Üê
1
Nj ‚àën
i=1 ùüô(y(i) = j) x(i)
11:
12:
y = yold
13:
14:
15:
16:
17:
Œº, y
18:
n
12.3.2.0.0.2 Using gradient descent to minimize k-means objective
Œº
L(Œº) =
n
‚àë
i=1
min
j
x(i) ‚àíŒº(j)
2 .
‚à•‚à•
L(Œº)
minj
‚àÇL(Œº)
‚àÇŒº
Œº
Figure 12.3: Converged result.
 assignments are optimal. Finally, we read off the optimal cluster assignments, given
the optimized , just by assigning datapoints to their nearest cluster mean:
This procedure yields a local minimum of Equation 12.1, as does the standard k-
means algorithm we presented (though they might arrive at different solutions). It
might not be the global optimum since the objective is not convex (due to 
, as
the minimum of multiple convex functions is not necessarily convex).
The standard k-means algorithm, as well as the variant that uses gradient descent,
both are only guaranteed to converge to a local minimum, not necessarily the global
minimum of the loss. Thus the answer we get out depends on how we initialize the
cluster means. Figure 12.4 is an example of a different initialization on our toy data,
which results in a worse converged clustering:
A variety of methods have been developed to pick good initializations (see, for
example, the k-means++ algorithm). One simple option is to run the standard k-
means algorithm multiple times, with different random initial conditions, and then
pick from these the clustering that achieves the lowest k-means loss.
A very important parameter in cluster algorithms is the number of clusters we are
looking for. Some advanced algorithms can automatically infer a suitable number of
clusters, but most of the time, like with k-means, we will have to pick  ‚Äì it‚Äôs a
hyperparameter of the algorithm.
Figure 12.5 shows an example of the effect. Which result looks more correct? It can
be hard to say! Using higher k we get more clusters, and with more clusters we can
achieve lower within-cluster variance ‚Äì the k-means objective will never increase,
Œº
y(i) = arg min
j
x(i) ‚àíŒº(j)
2 .
‚à•‚à•
minj
12.3.2.0.0.3 Importance of initialization
12.3.2.0.0.4 Importance of k
k
Figure 12.4: With the initialization
of the means to the left, the yellow
and red means end up splitting
what perhaps should be one cluster
in half.
Figure 12.5: Example of k-means
run on our toy data, with two
different values of k. Setting k=4,
on the left, results in one cluster
being merged, compared to setting
k=5, on the right. Which clustering
do you think is better? How could
you decide?
 and will typically strictly decrease as we increase k. Eventually, we can increase k to
equal the total number of datapoints, so that each datapoint is assigned to its own
cluster. Then the k-means objective is zero, but the clustering reveals nothing.
Clearly, then, we cannot use the k-means objective itself to choose the best value for
k. In Section 1.3, we will discuss some ways of evaluating the success of clustering
beyond its ability to minimize the k-means objective, and it‚Äôs with these sorts of
methods that we might decide on a proper value of k.
Alternatively, you may be wondering: why bother picking a single k? Wouldn‚Äôt it be
nice to reveal a hierarchy of clusterings of our data, showing both coarse and fine
groupings? Indeed hierarchical clustering is another important class of clustering
algorithms, beyond k-means. These methods can be useful for discovering tree-like
structure in data, and they work a bit like this: initially a coarse split/clustering of
the data is applied at the root of the tree, and then as we descend the tree we split
and cluster the data in ever more fine-grained ways. A prototypical example of
hierarchical clustering is to discover a taxonomy of life, where creatures may be
grouped at multiple granularities, from species to families to kingdoms. You may
find a suite of clustering algorithms in SKLEARN‚Äôs cluster module.
Clustering algorithms group data based on a notion of similarity, and thus we need
to define a distance metric between datapoints. This notion will also be useful in
other machine learning approaches, such as nearest-neighbor methods that we see
in Chapter 12. In k-means and other methods, our choice of distance metric can
have a big impact on the results we will find.
Our k-means algorithm uses the Euclidean distance, i.e., 
, with a loss
function that is the square of this distance. We can modify k-means to use different
distance metrics, but a more common trick is to stick with Euclidean distance but
measured in a feature space. Just like we did for regression and classification
problems, we can define a feature map from the data to a nicer feature
representation, 
, and then apply k-means to cluster the data in the feature
space.
As a simple example, suppose we have two-dimensional data that is very stretched
out in the first dimension and has less dynamic range in the second dimension.
Then we may want to scale the dimensions so that each has similar dynamic range,
prior to clustering. We could use standardization, like we did in Chapter 5.
If we want to cluster more complex data, like images, music, chemical compounds,
etc., then we will usually need more sophisticated feature representations. One
common practice these days is to use feature representations learned with a neural
network. For example, we can use an autoencoder to compress images into feature
vectors, then cluster those feature vectors.
12.3.2.0.0.5 k-means in feature space
x(i) ‚àíŒº(j)
‚à•‚à•
œï(x)
12.3.3 How to evaluate clustering algorithms
 One of the hardest aspects of clustering is knowing how to evaluate it. This is
actually a big issue for all unsupervised learning methods, since we are just looking
for patterns in the data, rather than explicitly trying to predict target values (which
was the case with supervised learning).
Remember, evaluation metrics are not the same as loss functions, so we can‚Äôt just
measure success by looking at the k-means loss. In prediction problems, it is critical
that the evaluation is on a held-out test set, while the loss is computed over training
data. If we evaluate on training data we cannot detect overfitting. Something
similar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too
large can precisely ‚Äúfit‚Äù the data (minimize the loss), but yields no general insight.
One way to evaluate our clusters is to look at the consistency with which they are
found when we run on different subsamples of our training data, or with different
hyperparameters of our clustering algorithm (e.g., initializations). For example, if
running on several bootstrapped samples (random subsets of our data) results in
very different clusters, it should call into question the validity of any of the
individual results.
If we have some notion of what ground truth clusters should be, e.g., a few data
points that we know should be in the same cluster, then we can measure whether or
not our discovered clusters group these examples correctly.
Clustering is often used for visualization and interpretability, to make it easier for
humans to understand the data. Here, human judgment may guide the choice of
clustering algorithm. More quantitatively, discovered clusters may be used as input
to downstream tasks. For example, as we saw in the lab, we may fit a different
regression function on the data within each cluster. Figure 12.6 gives an example
where this might be useful. In cases like this, the success of a clustering algorithm
can be indirectly measured based on the success of the downstream application
(e.g., does it make the downstream predictions more accurate).
Figure 12.6: Averaged across the
whole population, risk of heart
disease positively correlates with
hours of exercise. However, if we
cluster the data, we can observe
that there are four subgroups of the
population which correspond to
different age groups, and within
each subgroup the correlation is
negative. We can make better
predictions, and better capture the
presumed true effect, if we cluster
this data and then model the trend
in each cluster separately.