This page contains all content from the legacy PDF notes; reinforcement learning chapter.
As we phase out the PDF, this page may receive updates not reflected in the static PDF.
Reinforcement learning (RL) is a type of machine learning where an agent learns to
make decisions by interacting with an environment. Unlike other learning
paradigms, RL has several distinctive characteristics:
The agent interacts directly with an environment, receiving feedback in the
form of rewards or penalties
The agent can choose actions that influences what information it gains from the
environment
The agent updates its decision-making strategy incrementally as it gains more
experience
In a reinforcement learning problem, the interaction between the agent and
environment follows a specific pattern:
Learner
Environmen t
reward
state
action
The interaction cycle proceeds as follows:
1. Agent observes the current state 
2. Agent selects and executes an action 
3. Agent receives a reward 
 from the environment
4. Agent observes the new state 
5. Agent selects and executes a new action 
6. Agent receives a new reward 
7. This cycle continues…
Similar to MDP Chapter 10, in an RL problem, the agent’s goal is to learn a policy - a
mapping from states to actions - that maximizes its expected cumulative reward
over time. This policy guides the agent’s decision-making process, helping it choose
actions that lead to the most favorable outcomes.
11  Reinforcement Learning
Note
s(i)
a(i)
r(i)
s(i+1)
a(i+1)
r(i+1)
11  Reinforcement Learning

 11.1 Reinforcement learning algorithms overview
Approaches to reinforcement learning differ significantly according to what kind of
hypothesis or model is being learned. Roughly speaking, RL methods can be
categorized into model-free methods and model-based methods. The main
distinction is that model-based methods explicitly learn the transition and reward
models to assist the end-goal of learning a policy; model-free methods do not. We
will start our discussion with the model-free methods, and introduce two of the
arguably most popular types of algorithms, Q-learning Section 11.1.2 and policy
gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,
we briefly consider “bandit” problems Section 11.5, which differ from our MDP
learning context by having probabilistic rewards.
Model-free methods are methods that do not explicitly learn transition and reward
models. Depending on what is explicitly being learned, model-free methods are
sometimes further categorized into value-based methods (where the goal is to
learn/estimate a value function) and policy-based methods (where the goal is to
directly learn an optimal policy). It’s important to note that such categorization is
approximate and the boundaries are blurry. In fact, current RL research tends to
combine the learning of value functions, policies, and transition and reward models
all into a complex learning algorithm, in an attempt to combine the strengths of
each approach.
Q-learning is a frequently used class of RL algorithms that concentrates on learning
(estimating) the state-action value function, i.e., the 
 function. Specifically, recall
the MDP value-iteration update:
The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,
where we do not know the transition function  or reward function 
, and instead
rely on samples to perform the updates.
procedure Q-Learning(
)
for all 
 do
end for
while 
 do
11.1.1 Model-free methods
11.1.2 Q-learning
Q
Q(s, a) = R(s, a) + γ ∑
s′
T(s, a, s′) max
a′
Q(s′, a′)
T
R
1:
S, A, γ, α, s0, max_iter
2:
i ←0
3:
s ∈S, a ∈A
4:
Qold(s, a) ←0
5:
6:
s ←s0
7:
i < max_iter
The thing that most students seem
to get confused about is when we
do value iteration and when we do
Q-learning. Value iteration
assumes you know  and 
 and
just need to compute 
. In Q-
learning, we don’t know or even
directly estimate  and 
: we
estimate 
 directly from
experience!
T
R
Q
T
R
Q
 end while
return 
end procedure
With the pseudo‑code provided for Q‑Learning, there are a few key things to note.
First, we must determine which state to initialize the learning from. In the context of
a game, this initial state may be well defined. In the context of a robot navigating an
environment, one may consider sampling the initial state at random. In any case,
the initial state is necessary to determine the trajectory the agent will experience as
it navigates the environment.
Second, different contexts will influence how we want to choose when to stop
iterating through the while loop. Again, in some games there may be a clear
terminating state based on the rules of how it is played. On the other hand, a robot
may be allowed to explore an environment ad infinitum. In such a case, one may
consider either setting a fixed number of transitions (as done explicitly in the
pseudo‑code) to take; or we may want to stop iterating in the example once the
values in the Q‑table are not changing, after the algorithm has been running for a
while.
Finally, a single trajectory through the environment may not be sufficient to
adequately explore all state‑action pairs. In these instances, it becomes necessary to
run through a number of iterations of the Q‑Learning algorithm, potentially with
different choices of initial state 
.
Of course, we would then want to modify Q‑Learning such that the Q table is not
reset with each call.
Now, let’s dig into what is happening in Q‑Learning. Here, 
 represents the
learning rate, which needs to decay for convergence purposes, but in practice is often
set to a constant. It’s also worth mentioning that Q-learning assumes a discrete state
and action space where states and actions take on discrete values like 
 etc.
In contrast, a continuous state space would allow the state to take values from, say,
a continuous range of numbers; for example, the state could be any real number in
the interval 
. Similarly, a continuous action space would allow the action to be
drawn from, e.g., a continuous range of numbers. There are now many extensions
developed based on Q-learning that can handle continuous state and action spaces
(we’ll look at one soon), and therefore the algorithm above is also sometimes
referred to more specifically as tabular Q-learning.
In the Q-learning update rule
8:
a ←select_action(s, Qold(s, a))
9:
(r, s′) ←execute(a)
10:
Qnew(s, a) ←(1 −α) Qold(s, a) + α(r + γ maxa′ Qold(s′, a′))
11:
s ←s′
12:
i ←i + 1
13:
Qold ←Qnew
14:
15:
Qnew
16:
s0
α ∈(0, 1]
1, 2, 3, …
[1, 3]
Q[s, a] ←(1 −α)Q[s, a] + α(r + γ max
a′
Q[s′, a′])
(11.1)
This notion of running a number of
instances of Q‑Learning is often
referred to as experiencing
multiple episodes.
 the term 
 is often referred to as the one-step look-ahead target.
The update can be viewed as a combination of two different iterative processes that
we have already seen: the combination of an old estimate with the target using a
running average with a learning rate 
Equation 11.1 can also be equivalently rewritten as
which allows us to interpret Q-learning in yet another way: we make an update (or
correction) based on the temporal difference between the target and the current
estimated value 
The Q-learning algorithm above includes a procedure called select_action , that,
given the current state  and current 
 function, has to decide which action to take.
If the 
 value is estimated very accurately and the agent is deployed to “behave” in
the world (as opposed to “learn” in the world), then generally we would want to
choose the apparently optimal action 
.
But, during learning, the 
 value estimates won’t be very good and exploration is
important. However, exploring completely at random is also usually not the best
strategy while learning, because it is good to focus your attention on the parts of the
state space that are likely to be visited when executing a good policy (not a bad or
random one).
A typical action-selection strategy that attempts to address this exploration versus
exploitation dilemma is the so-called -greedy strategy:
with probability 
, choose 
;
with probability , choose the action 
 uniformly at random.
where the  probability of choosing a random action helps the agent to explore and
try out actions that might not seem so desirable at the moment.
Q-learning has the surprising property that it is guaranteed to converge to the actual
optimal 
 function! The conditions specified in the theorem are: visit every state-
action pair infinitely often, and the learning rate  satisfies a scheduling condition.
This implies that for exploration strategy specifically, any strategy is okay as long as
it tries every state-action infinitely often on an infinite run (so that it doesn’t
converge prematurely to a bad action choice).
Q-learning can be very inefficient. Imagine a robot that has a choice between
moving to the left and getting a reward of 1, then returning to its initial state, or
moving to the right and walking down a 10-step hallway in order to get a reward of
1000, then returning to its initial state.
(r + γ maxa′ Q[s′, a′])
α.
Q[s, a] ←Q[s, a] + α((r + γ max
a′
Q[s′, a′]) −Q[s, a]),
(11.2)
Q[s, a].
s
Q
Q
arg maxa∈A Q(s, a)
Q
ϵ
1 −ϵ
arg maxa∈A Q(s, a)
ϵ
a ∈A
ϵ
Q
α
 robot
1
2
3
4
5
6
7
8
9
10
+1000
+1
-1
The first time the robot moves to the right and goes down the hallway, it will
update the 
 value just for state 9 on the hallway and action ``right’’ to have a high
value, but it won’t yet understand that moving to the right in the earlier steps was a
good choice. The next time it moves down the hallway it updates the value of the
state before the last one, and so on. After 10 trips down the hallway, it now can see
that it is better to move to the right than to the left.
More concretely, consider the vector of Q values 
, representing
the Q values for moving right at each of the positions 
. Position index 0
is the starting position of the robot as pictured above.
Then, for 
 and 
, Equation 11.2 becomes
Starting with Q values of 0,
Since the only nonzero reward from moving right is 
, after our
robot makes it down the hallway once, our new Q vector is
After making its way down the hallway again,
updates:
Similarly,
Q
Q(i = 0, … , 9; right)
i = 0, … , 9
α = 1
γ = 0.9
Q(i, right) = R(i, right) + 0.9 max
a
Q(i + 1, a) .
Q(0)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
0
0
R(9, right) = 1000
Q(1)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
0
1000
Q(8, right) = 0 + 0.9 Q(9, right) = 900
Q(2)(i = 0, … , 9; right) = [
] .
0
0
0
0
0
0
0
0
900
1000
Q(3)(i = 0, … , 9; right) = [
] ,
0
0
0
0
0
0
0
810
900
1000
Q(4)(i = 0, … , 9; right) = [
] ,
0
0
0
0
0
0
729
810
900
1000
We are violating our usual
notational conventions here, and
writing 
 to mean the Q value
function that results after the robot
runs all the way to the end of the
hallway, when executing the policy
that always moves to the right.
Qi
 and the robot finally sees the value of moving right from position 0.
❓ Study Question
Determine the Q value functions that result from always executing the “move
left” policy.
11.2 Function approximation: Deep Q learning
In our Q-learning algorithm above, we essentially keep track of each 
 value in a
table, indexed by  and . What do we do if  and/or 
 are large (or continuous)?
We can use a function approximator like a neural network to store Q values. For
example, we could design a neural network that takes inputs  and , and outputs
. We can treat this as a regression problem, optimizing this loss:
where 
 is now the output of the neural network.
There are several different architectural choices for using a neural network to
approximate 
 values:
One network for each action , that takes  as input and produces 
 as
output;
One single network that takes  as input and produces a vector 
,
consisting of the 
 values for each action; or
One single network that takes 
 concatenated into a vector (if  is discrete,
we would probably use a one-hot encoding, unless it had some useful internal
structure) and produces 
 as output.
The first two choices are only suitable for discrete (and not too big) action sets. The
last choice can be applied for continuous actions, but then it is difficult to find
.
There are not many theoretical guarantees about Q-learning with function
approximation and, indeed, it can sometimes be fairly unstable (learning to perform
well for a while, and then suddenly getting worse, for example). But neural
network Q-learning has also had some significant successes.
…
Q(10)(i = 0, … , 9; right) = [
] ,
387.4
420.5
478.3
531.4
590.5
656.1
729
810
900
1000
Q
s
a
S
A
s
a
Q(s, a)
(Q(s, a) −(r + γ max
a′
Q(s′, a′)))
2
Q(s, a)
Q
a
s
Q(s, a)
s
Q(s, ⋅)
Q
s, a
a
Q(s, a)
arg maxa∈A Q(s, a)
Here, we can see the
exploration/exploitation dilemma
in action: from the perspective of
, it will seem that getting the
immediate reward of  is a better
strategy without exploring the long
hallway.
s0 = 0
1
This is the so-called squared
Bellman error; as the name
suggests, it’s closely related to the
Bellman equation we saw in MDPs
in Chapter Chapter 10. Roughly
speaking, this error measures how
much the Bellman equality is
violated.
For continuous action spaces, it is
popular to use a class of methods
called actor-critic methods, which
combine policy and value-function
learning. We won’t get into them in
detail here, though.
 One form of instability that we do know how to guard against is catastrophic
forgetting. In standard supervised learning, we expect that the training  values
were drawn independently from some distribution.
But when a learning agent, such as a robot, is moving through an environment, the
sequence of states it encounters will be temporally correlated. For example, the
robot might spend 12 hours in a dark environment and then 12 in a light one. This
can mean that while it is in the dark, the neural-network weight-updates will make
the 
 function "forget" the value function for when it’s light.
One way to handle this is to use experience replay, where we save our 
experiences in a replay buffer. Whenever we take a step in the world, we add the
 to the replay buffer and use it to do a Q-learning update. Then we also
randomly select some number of tuples from the replay buffer, and do Q-learning
updates based on them as well. In general, it may help to keep a sliding window of
just the 1000 most recent experiences in the replay buffer. (A larger buffer will be
necessary for situations when the optimal policy might visit a large part of the state
space, but we like to keep the buffer size small for memory reasons and also so that
we don’t focus on parts of the state space that are irrelevant for the optimal policy.)
The idea is that it will help us propagate reward values through our state space
more efficiently if we do these updates. We can see it as doing something like value
iteration, but using samples of experience rather than a known model.
An alternative strategy for learning the 
 function that is somewhat more robust
than the standard 
-learning algorithm is a method called fitted Q.
procedure Fitted-Q-Learning(
)
//e.g., 
 can be drawn randomly from 
initialize neural-network representation of 
while True do
 experience from executing -greedy policy based on  for 
 steps
 represented as tuples 
for each tuple 
 do
end for
re-initialize neural-network representation of 
end while
end procedure
Here, we alternate between using the policy induced by the current 
 function to
gather a batch of data 
, adding it to our overall data set 
, and then using
supervised neural-network training to learn a representation of the 
 value
function on the whole data set. This method does not mix the dynamic-
x
Q
(s, a, s′, r)
(s, a, s′, r)
11.2.1 Fitted Q-learning
Q
Q
1:
A, s0, γ, α, ϵ, m
2:
s ←s0
s0
S
3:
D ←∅
4:
Q
5:
6:
Dnew ←
ϵ
Q
m
7:
D ←D ∪Dnew
(s, a, s′, r)
8:
Dsupervised ←∅
9:
(s, a, s′, r) ∈D
10:
x ←(s, a)
11:
y ←r + γ maxa′∈A Q(s′, a′)
12:
Dsupervised ←Dsupervised ∪{(x, y)}
13:
14:
Q
15:
Q ←supervised-NN-regression(Dsupervised)
16:
17:
Q
Dnew
D
Q
And, in fact, we routinely shuffle
their order in the data file, anyway.
 programming phase (computing new 
 values based on old ones) with the function
approximation phase (supervised training of the neural network) and avoids
catastrophic forgetting. The regression training in line 10 typically uses squared
error as a loss function and would be trained until the fit is good (possibly
measured on held-out data).
11.3 Policy gradient
A different model-free strategy is to search directly for a good policy. The strategy
here is to define a functional form 
 for the policy, where  represents the
parameters we learn from experience. We choose  to be differentiable, and often
define
, a conditional probability distribution over our possible actions.
Now, we can train the policy parameters using gradient descent:
When  has relatively low dimension, we can compute a numeric estimate of
the gradient by running the policy multiple times for different values of , and
computing the resulting rewards.
When  has higher dimensions (e.g., it represents the set of parameters in a
complicated neural network), there are more clever algorithms, e.g., one called
REINFORCE, but they can often be difficult to get to work reliably.
Policy search is a good choice when the policy has a simple known form, but the
MDP would be much more complicated to estimate.
11.4 Model-based RL
The conceptually simplest approach to RL is to model 
 and  from the data we
have gotten so far, and then use those models, together with an algorithm for
solving MDPs (such as value iteration) to find a policy that is near-optimal given
the current models.
Assume that we have had some set of interactions with the environment, which can
be characterized as a set of tuples of the form 
.
Because the transition function 
 specifies probabilities, multiple
observations of 
 may be needed to model the transition function. One
approach to building a model 
 for the true 
 is to estimate it using
a simple counting strategy:
Q
f(s; θ) = a
θ
f
f(s, a; θ) = Pr(a|s)
θ
θ
θ
R
T
(s(t), a(t), s(t+1), r(t))
T(s, a, s′)
(s, a, s′)
^T(s, a, s′)
T(s, a, s′)
^T(s, a, s′) = #(s, a, s′) + 1
#(s, a) + |S| .
This means the chance of choosing
an action depends on which state
the agent is in. Suppose, e.g., a
robot is trying to get to a goal and
can go left or right. An
unconditional policy can say: I go
left 99% of the time; a conditional
policy can consider the robot’s
state, and say: if I’m to the right of
the goal, I go left 99% of the time.
 Here, 
 represents the number of times in our data set we have the
situation where 
, 
, 
, and 
 represents the number of
times in our data set we have the situation where 
, 
.
Adding 1 and 
 to the numerator and denominator, respectively, is a form of
smoothing called the Laplace correction. It ensures that we never estimate that a
probability is 0, and keeps us from dividing by 0. As the amount of data we gather
increases, the influence of this correction fades away.
In contrast, the reward function 
 is a deterministic function, such that
knowing the reward  for a given 
 is sufficient to fully determine the function
at that point. Our model 
 can simply be a record of observed rewards, such that
.
Given empirical models  and 
 for the transition and reward functions, we can
now solve the MDP 
 to find an optimal policy using value iteration, or
use a search algorithm to find an action to take for a particular state.
This approach is effective for problems with small state and action spaces, where it
is not too hard to get enough experience to model  and 
 well; but it is difficult to
generalize this method to handle continuous (or very large discrete) state spaces,
and is a topic of current research.
11.5 Bandit problems
Bandit problems are a subset of reinforcement learning problems. A basic bandit
problem is given by:
A set of actions 
;
A set of reward values 
; and
A probabilistic reward function 
, i.e., 
 is a function that
takes an action and a reward and returns the probability of getting that reward
conditioned on that action being taken,
. Each time the agent takes an action, a
new value is drawn from this distribution.
The most typical bandit problem has 
 and 
. This is called a -
armed bandit problem, where the decision is which “arm” (action ) to select, and the
reward is either getting a payoff ( ) or not ( ).
The important question is usually one of exploration versus exploitation. Imagine you
have tried each action 10 times, and now you have estimates 
 for the
probabilities 
. Which arm should you pick next? You could:
exploit your knowledge, choosing the arm with the highest value of expected
reward; or
#(s, a, s′)
s(t) = s a(t) = a s(t+1) = s′
#(s, a)
s(t) = s a(t) = a
|S|
R(s, a)
r
(s, a)
^R
^R(s, a) = r = R(s, a)
^T
^R
(S, A, ^T, ^R)
T
R
A
R
Rp : A × R →R
Rp
Rp(a, r) = Pr(reward = r ∣action = a)
R = {0, 1}
|A| = k
k
a
1
0
^Rp(a, r)
Rp(a, r)
Conceptually, this is similar to
having “initialized” our estimate
for the transition function with
uniform random probabilities
before making any observations.
Notice that this probablistic
rewards set up in bandits differs
from the “rewards are
deterministic” assumptions we
made so far.
Why “bandit”? In English slang,
“one-armed bandit” refers to a slot
machine because it has one arm
and takes your money! Here, we
have a similar machine but with 
arms.
k
 explore further, trying some or all actions more times to get better estimates of
the 
 values.
The theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1
our discount factor), the more time we should spend exploring, so that we don’t
converge prematurely on a bad choice of action.
Bandit problems are reinforcement learning problems (and very different from
batch supervised learning) in that:
The agent gets to influence what data it obtains (selecting  gives it another
sample from 
), and
The agent is penalized for mistakes it makes while it is learning (trying to
maximize the expected reward it gets while behaving).
In a contextual bandit problem, you have multiple possible states from some set ,
and a separate bandit problem associated with each one.
Bandit problems are an essential subset of reinforcement learning. It’s important to
be aware of the issues, but we will not study solutions to them in this class.
Rp(a, r)
h
a
R(a, r)
S