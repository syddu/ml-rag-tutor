["1 Introduction\nThe main focus of machine learning (ML) is making decisions or predictions based on\ndata. There are a number of other \ufb01elds with signi\ufb01cant overlap in technique, but", "difference in focus: in economics and psychology, the goal is to discover underlying\ncausal processes and in statistics it is to \ufb01nd a model that \ufb01ts a data set well. In", "those \ufb01elds, the end product is a model. In machine learning, we often \ufb01t models,\nbut as a means to the end of making good predictions or decisions.", "As ML methods have improved in their capability and scope, ML has become\narguably the best way\u2013measured in terms of speed, human engineering time, and", "robustness\u2013to approach many applications. Great examples are face detection,\nspeech recognition, and many kinds of language-processing tasks. Almost any", "application that involves understanding data or signals that come from the real\nworld can be nicely addressed using machine learning.", "One crucial aspect of machine learning approaches to solving problems is that\nhuman engineering plays an important role. A human still has to frame the problem:", "acquire and organize data, design a space of possible solutions, select a learning\nalgorithm and its parameters, apply the algorithm to the data, validate the resulting", "solution to decide whether it\u2019s good enough to use, try to understand the impact on\nthe people who will be affected by its deployment, etc. These steps are of great\nimportance.", "importance.\nThe conceptual basis of learning from data is the problem of induction: Why do we\nthink that previously seen data will help us predict the future? This is a serious long", "standing philosophical problem. We will operationalize it by making assumptions,\nsuch as that all training data are so-called i.i.d.(independent and identically", "distributed), and that queries will be drawn from the same distribution as the\ntraining data, or that the answer comes from a set of possible answers known in", "advance.6.390 - Intro to Machine Learning\nCourse Notes\nThis description is paraphrased\nfrom a post on 9/4/12 at\nandrewgelman.com.\nThis aspect is often undervalued.\nThis means that the elements in the", "set are related in the sense that\nthey all come from the same\nunderlying probability\ndistribution, but not in other ways.\uf461 1\u00a0 Introduction \uf52a In general, we need to solve these two problems:", "estimation: When we have data that are noisy re\ufb02ections of some underlying\nquantity of interest, we have to aggregate the data and make estimates or", "predictions about the quantity. How do we deal with the fact that, for example,\nthe same treatment may end up with different results on different trials? How", "can we predict how well an estimate may compare to future results?\ngeneralization: How can we predict results of a situation or experiment that\nwe have never encountered before in our data set?", "We can describe problems and their solutions using six characteristics, three of\nwhich characterize the problem and three of which characterize the solution:", "1. Problem class: What is the nature of the training data and what kinds of\nqueries will be made at testing time?\n2. Assumptions: What do we know about the source of the data or the form of", "the solution?\n3. Evaluation criteria: What is the goal of the prediction or estimation system?\nHow will the answers to individual queries be evaluated? How will the overall", "performance of the system be measured?\n4. Model type: Will an intermediate model of the world be made? What aspects\nof the data will be modeled in different variables/parameters? How will the", "model be used to make predictions?\n5. Model class: What particular class of models will be used? What criterion will\nwe use to pick a particular model from the model class?", "6. Algorithm: What computational process will be used to \ufb01t the model to the\ndata and/or to make predictions?\nWithout making some assumptions about the nature of the process generating the", "data, we cannot perform generalization. In the following sections, we elaborate on\nthese ideas.\n1.1 Problem class\nThere are many different problem classes in machine learning. They vary according to", "what kind of data is provided and what kind of conclusions are to be drawn from it.\nFive standard problem classes are described below, to establish some notation and\nterminology.", "terminology.\nIn this course, we will focus on classi\ufb01cation and regression (two examples of\nsupervised learning), and we will touch on reinforcement learning, sequence", "learning, and clustering.For example, the same treatment\nmay end up with different results\non different trials. How can we\npredict how well an estimate\ncompares to future results?", "Don\u2019t feel you have to memorize\nall these kinds of learning, etc. We\njust want you to have a very high- The idea of supervised learning is that the learning system is given inputs and told", "which speci\ufb01c outputs should be associated with them. We divide up supervised\nlearning based on whether the outputs are drawn from a small \ufb01nite set", "(classi\ufb01cation) or a large \ufb01nite ordered set or continuous set (regression).\nFor a regression problem, the training data  is in the form of a set of  pairs:", "where  represents an input, most typically a -dimensional vector of real and/or\ndiscrete values, and  is the output to be predicted, in this case a real-number. The", "values are sometimes called target values.\nThe goal in a regression problem is ultimately, given a new input value , to\npredict the value of . Regression problems are a kind of supervised learning,", "because the desired output  is speci\ufb01ed for each of the training examples .\nA classi\ufb01cation problem is like regression, except that the values that  can take", "do not have an order. The classi\ufb01cation problem is binary or two-class if  (also\nknown as the class) is drawn from a set of two possible values; otherwise, it is called\nmulti-class.", "multi-class.\nUnsupervised learning doesn\u2019t involve learning a function from inputs to outputs\nbased on a set of input-output pairs. Instead, one is given a data set and generally", "expected to \ufb01nd some patterns or structure inherent in it.\nGiven samples , the goal is to \ufb01nd a partitioning (or \u201cclustering\u201d)", "of the samples that groups together similar samples. There are many different\nobjectives, depending on the de\ufb01nition of the similarity between samples and", "exactly what criterion is to be used (e.g., minimize the average distance between\nelements inside a cluster and maximize the average distance between elements", "across clusters). Other methods perform a \u201csoft\u201d clustering, in which samples may\nbe assigned 0.9 membership in one cluster and 0.1 in another. Clustering is", "sometimes used as a step in the so-called density estimation (described below), and\nsometimes to \ufb01nd useful structure or in\ufb02uential features in data.\n1.1.1 Supervised learning", "1.1.1.1 RegressionDtrain n\nDtrain={(x(1),y(1)),\u2026,(x(n),y(n))},\nx(i)d\ny(i)\ny\nx(n+1)\ny(n+1)\ny(i)x(i)\n1.1.1.2 Classi\ufb01cation\ny(i)\ny(i)\n1.1.2 Unsupervised learning\n1.1.2.1 Clustering", "1.1.2.1 Clustering\nx(1),\u2026,x(n)\u2208Rdlevel view of (part of) the breadth\nof the \ufb01eld.\nMany textbooks use  and \ninstead of  and . We \ufb01nd that\nnotation somewhat dif\ufb01cult to\nmanage when  is itself a vector", "and we need to talk about its\nelements. The notation we are\nusing is standard in some other\nparts of the ML literature.xi ti\nx(i)y(i)\nx(i) Given samples  drawn i.i.d. from some distribution , the", "goal is to predict the probability  of an element drawn from the same\ndistribution. Density estimation sometimes plays a role as a \u201csubroutine\u201d in the", "overall learning method for supervised learning, as well.\nGiven samples , the problem is to re-represent them as points in\na -dimensional space, where . The goal is typically to retain information in", "the data set that will, e.g., allow elements of one class to be distinguished from\nanother.\nDimensionality reduction is a standard technique that is particularly useful for", "visualizing or understanding high-dimensional data. If the goal is ultimately to\nperform regression or classi\ufb01cation on the data after the dimensionality is reduced,", "it is usually best to articulate an objective for the overall prediction problem rather\nthan to \ufb01rst do dimensionality reduction without knowing which dimensions will", "be important for the prediction task.\nIn sequence learning, the goal is to learn a mapping from input sequences \nto output sequences . The mapping is typically represented as a state", "machine, with one function  used to compute the next hidden internal state given\nthe input, and another function  used to compute the output given the current\nhidden state.", "hidden state.\nIt is supervised in the sense that we are told what output sequence to generate for\nwhich input sequence, but the internal functions have to be learned by some", "method other than direct supervision, because we don\u2019t know what the hidden\nstate sequence is.\nIn reinforcement learning, the goal is to learn a mapping from input values", "(typically assumed to be states of an agent or system; for now, think e.g.\u00a0the velocity\nof a moving car) to output values (typically we want control actions; for now, think", "e.g.\u00a0if to accelerate or hit the brake). However, we need to learn the mapping\nwithout a direct supervision signal to specify which output values are best for a", "particular input; instead, the learning problem is framed as an agent interacting\nwith an environment, in the following setting:\nThe agent observes the current state .\nIt selects an action", "1.1.2.2 Density estimationx(1),\u2026,x(n)\u2208RdPr(X)\nPr(x(n+1))\n1.1.2.3 Dimensionality reduction\nx(1),\u2026,x(n)\u2208RD\nd d<D\n1.1.3 Sequence learning\nx0,\u2026,xn\ny1,\u2026,ym\nfs\nfo\n1.1.4 Reinforcement learning\nst", "st\nat. It receives a reward, , which typically depends on  and possibly .\nThe environment transitions probabilistically to a new state, , with a\ndistribution that depends only on  and .", "The agent observes the current state, .\nThe goal is to \ufb01nd a policy , mapping  to , (that is, states to actions) such that\nsome long-term sum or average of rewards  is maximized.", "This setting is very different from either supervised learning or unsupervised\nlearning, because the agent\u2019s action choices affect both its reward and its ability to", "observe the environment. It requires careful consideration of the long-term effects of\nactions, as well as all of the other issues that pertain to supervised learning.", "There are many other problem settings. Here are a few.\nIn semi-supervised learning, we have a supervised-learning training set, but there", "may be an additional set of  values with no known . These values can still be\nused to improve learning performance (if they are drawn from  that is the", "marginal of  that governs the rest of the data set).\nIn active learning, it is assumed to be expensive to acquire a label  (imagine", "asking a human to read an x-ray image), so the learning algorithm can sequentially\nask for particular inputs  to be labeled, and must carefully select queries in order", "to learn as effectively as possible while minimizing the cost of labeling.\nIn transfer learning (also called meta-learning), there are multiple tasks, with data", "drawn from different, but related, distributions. The goal is for experience with\nprevious tasks to apply to learning a current task in a way that requires decreased\nexperience with the new task.", "1.2 Assumptions\nThe kinds of assumptions that we can make about the data source or the solution\ninclude:\nThe data are independent and identically distributed (i.i.d.).", "The data are generated by a Markov chain (i.e.\u00a0outputs only depend only on\nthe current state, with no additional memory).\nThe process generating the data might be adversarial.rt st at\nst+1\nstat\nst+1", "st+1\nstat\nst+1\n\u2026\n\u03c0 sa\nr\n1.1.5 Other settings\nx(i)y(i)\nPr(X)\nPr(X,Y)\ny(i)\nx(i) The \u201ctrue\u201d model that is generating the data can be perfectly described by one\nof some particular set of hypotheses.", "The effect of an assumption is often to reduce the \u201csize\u201d or \u201cexpressiveness\u201d of the\nspace of possible hypotheses and therefore reduce the amount of data required to", "reliably identify an appropriate hypothesis.\n1.3 Evaluation criteria\nOnce we have speci\ufb01ed a problem class, we need to say what makes an output or", "the answer to a query good, given the training data. We specify evaluation criteria\nat two levels: how an individual prediction is scored, and how the overall behavior", "of the prediction or estimation system is scored.\nThe quality of predictions from a learned model is often expressed in terms of a loss", "function. A loss function  tells you how much you will be penalized for\nmaking a guess  when the answer is actually . There are many possible loss\nfunctions. Here are some frequently used examples:", "0-1 Loss applies to predictions drawn from \ufb01nite domains.\nSquared loss\nAbsolute loss\nAsymmetric loss Consider a situation in which you are trying to predict", "whether someone is having a heart attack. It might be much worse to predict\n\u201cno\u201d when the answer is really \u201cyes\u201d, than the other way around.", "Any given prediction rule will usually be evaluated based on multiple predictions\nand the loss of each one. At this level, we might be interested in:", "Minimizing expected loss over all the predictions (also known as risk)\nMinimizing maximum loss: the loss of the worst prediction", "Minimizing or bounding regret: how much worse this predictor performs than\nthe best one drawn from some classL(g,a)\ng a\nL(g,a)={0if\u00a0g=a\n1otherwise\nL(g,a)=(g\u2212a)2\nL(g,a)=|g\u2212a|\nL(g,a)=\u23a7\n\u23aa\u23a8", "L(g,a)=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91 if\u00a0g=1\u00a0and\u00a0a=0\n10if\u00a0g=0\u00a0and\u00a0a=1\n0 otherwise Characterizing asymptotic behavior: how well the predictor will perform in the\nlimit of in\ufb01nite training data", "Finding algorithms that are probably approximately correct: they probably\ngenerate a hypothesis that is right most of the time.", "There is a theory of rational agency that argues that you should always select the\naction that minimizes the expected loss. This strategy will, for example, make you the", "most money in the long run, in a gambling setting. As mentioned above, expected\nloss is also sometimes called risk in ML literature, but that term means other things", "in economics or other parts of decision theory, so be careful...it\u2019s risky to use it. We\nwill, most of the time, concentrate on this criterion.\n1.4 Model type", "1.4 Model type\nRecall that the goal of a ML system is typically to estimate or generalize, based on\ndata provided. Below, we examine the role of model-making in machine learning.", "In some simple cases, in response to queries, we can generate predictions directly\nfrom the training data, without the construction of any intermediate model, or more", "precisely, without the learning of any parameters.\nFor example, in regression or classi\ufb01cation, we might generate an answer to a new", "query by averaging answers to recent queries, as in the nearest neighbor method.\nThis two-step process is more typical:", "1. \u201cFit\u201d a model (with some a-prior chosen parameterization) to the training data\n2. Use the model directly to make predictions", "In the parametric models setting of regression or classi\ufb01cation, the model will be\nsome hypothesis or prediction rule  for some functional form . The", "term hypothesis has its roots in statistical learning and the scienti\ufb01c method, where\nmodels or hypotheses about the world are tested against real data, and re\ufb01ned with", "more evidence, observations, or insights. Note that the parameters themselves are\nonly part of the assumptions that we\u2019re making about the world. The model itself is", "a hypothesis that will be re\ufb01ned with more evidence.\nThe idea is that  is a set of one or more parameter values that will be determined", "by \ufb01tting the model to the training data and then be held \ufb01xed during testing.\nGiven a new , we would then make the prediction .\n1.4.1 Non-parametric models\n1.4.2 Parametric modelsy=h(x;\u0398) h\n\u0398", "\u0398\nx(n+1)h(x(n+1);\u0398) The \ufb01tting process is often articulated as an optimization problem: Find a value of\n that minimizes some criterion involving  and the data. An optimal strategy, if", "we knew the actual underlying distribution on our data, \u00a0would be to\npredict the value of  that minimizes the expected loss, which is also known as the", "test error. If we don\u2019t have that actual underlying distribution, or even an estimate of\nit, we can take the approach of minimizing the training error: that is, \ufb01nding the", "prediction rule  that minimizes the average loss on our training data set. So, we\nwould seek  that minimizes\nwhere the loss function  measures how bad it would be to make a guess of", "when the actual value is .\nWe will \ufb01nd that minimizing training error alone is often not a good choice: it is\npossible to emphasize \ufb01tting the current data too strongly and end up with a", "hypothesis that does not generalize well when presented with new  values.\n1.5 Model class and parameter \ufb01tting\nA model class  is a set of possible models, typically parameterized by a vector of", "parameters . What assumptions will we make about the form of the model? When\nsolving a regression problem using a prediction-rule approach, we might try to \ufb01nd", "a linear function  that \ufb01ts our data well. In this example, the\nparameter vector .\nFor problem types such as classi\ufb01cation, there are huge numbers of model classes", "that have been considered...we\u2019ll spend much of this course exploring these model\nclasses, especially neural networks models. We will almost completely restrict our", "attention to model classes with a \ufb01xed, \ufb01nite number of parameters. Models that\nrelax this assumption are called \u201cnon-parametric\u201d models.", "How do we select a model class? In some cases, the ML practitioner will have a\ngood idea of what an appropriate model class is, and will specify it directly. In other", "cases, we may consider several model classes and choose the best based on some\nobjective function. In such situations, we are solving a model selection problem:", "model-selection is to pick a model class  from a (usually \ufb01nite) set of possible\nmodel classes, whereas model \ufb01tting is to pick a particular model in that class,", "speci\ufb01ed by (usually continuous) parameters .\n1.6 Algorithm\nOnce we have described a class of models and a way of scoring a model given data,", "we have an algorithmic problem: what sequence of computational instructions\nshould we run in order to \ufb01nd a good model from our class? For example,\u0398 \u0398\nPr(X,Y)\ny\nh\n\u0398\nEtrain(h;\u0398)=1\nnn\n\u2211", "Etrain(h;\u0398)=1\nnn\n\u2211\ni=1L(h(x(i);\u0398),y(i)),\nL(g,a) g\na\nx\nM\n\u0398\nh(x;\u03b8,\u03b80)=\u03b8Tx+\u03b80\n\u0398=(\u03b8,\u03b80)\nM\n\u0398 determining the parameter vector which minimizes the training error might be", "done using a familiar least-squares minimization algorithm, when the model  is a\nfunction being \ufb01t to some data .\nSometimes we can use software that was designed, generically, to perform", "optimization. In many other cases, we use algorithms that are specialized for ML\nproblems, or for particular hypotheses classes. Some algorithms are not easily seen", "as trying to optimize a particular criterion. In fact, a historically important method\nfor \ufb01nding linear classi\ufb01ers, the perceptron algorithm, has this character.h", "x Th is page contains all content from the legacy PDF  notes; gradient descent chapter.\nAs we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .", "In the previous chapter, we showed how to describe an interesting objective\nfunction for machine learning, but we need a way to \ufb01nd the optimal", ", particularly when the objective function is not amenable to\nanalytical optimization. For example, this can be the case when  involves a", "more complex loss function, or more general forms of regularization. It can also be\nthe case when there are simply too many parameters to learn for it to be\ncomputationally feasible.", "There is an enormous and fascinating literature on the mathematical and\nalgorithmic foundations of optimization, but for this class, we will consider one of", "the simplest methods, called gradient descent.\nIntuitively, in one or two dimensions, we can easily think of  as de\ufb01ning a", "surface over ; that same idea extends to higher dimensions. Now, our objective is\nto \ufb01nd the  value at the lowest point on that surface. One way to think about", "gradient descent is that you start at some arbitrary point on the surface, look to see\nin which direction the \u201chill\u201d goes down most steeply, take a small step in that", "direction, determine the direction of steepest descent from where you are, take\nanother small step, etc.\nBelow, we explicitly give gradient descent algorithms for one and multidimensional", "objective functions (Section 3.1 and\u00a0Section 3.2). We then illustrate the application of\ngradient descent to a loss function which is not merely mean squared loss", "(Section 3.3). And we present an important method known as stochastic gradient\ndescent (Section 3.4), which is especially useful when datasets are too large for", "descent in a single batch, and has some important behaviors of its own.\n3.1 Gradient descent in one dimension\nWe start by considering gradient descent in one dimension. Assume , and", "that we know both  and its \ufb01rst derivative with respect to , . Here is\npseudo-code for gradient descent on an arbitrary function . Along with  and its", "gradient  (which, in the case of a scalar , is the same as its derivative ), we\nhave to specify some hyper-parameters. These hyper-parameters include the initial", "value for parameter , a step-size hyper-parameter , and an accuracy hyper-\nparameter\u00a0.3\u00a0 Gradient Descent\nNote\n\u0398\u2217=argmin\u0398J(\u0398)\nJ(\u0398)\nJ(\u0398)\n\u0398\n\u0398\n\u0398\u2208R\nJ(\u0398) \u0398J\u2032(\u0398)\nf f\n\u2207\u0398f \u0398 f\u2032\n\u0398 \u03b7", "f f\n\u2207\u0398f \u0398 f\u2032\n\u0398 \u03b7\n\u03f5You might want to consider\nstudying optimization some day!\nIt\u2019s one of the fundamental tools\nenabling machine learning, and it\u2019s", "a beautiful and deep \ufb01eld.\uf461 3\u00a0 Gradient Descent \uf52a The hyper-parameter  is often called learning rate when gradient descent is applied", "in machine learning. For simplicity,  may be taken as a constant, as is the case in\nthe pseudo-code below; and we\u2019ll see adaptive (non-constant) step-sizes soon.", "What\u2019s important to notice though, is that even when  is constant, the actual\nmagnitude of the change to  may not be constant, as that change depends on the\nmagnitude of the gradient itself too.", "procedure 1D-G RADIENT -D ESCENT( )\nrepeat\nuntil \nreturn \nend procedure\nNote that this algorithm terminates when the derivative of the function  is", "suf\ufb01ciently small. There are many other reasonable ways to decide to terminate,\nincluding:\nStop after a \ufb01xed number of iterations , i.e.,\u00a0when . Practically, this is the\nmost common choice.", "most common choice.\nStop when the change in the value of the parameter  is suf\ufb01ciently small,\ni.e.,\u00a0when .\n\u2753 Study Question", "\u2753 Study Question\nConsider all of the potential stopping criteria for 1D-Gradient-Descent, both\nin the algorithm as it appears and listed separately later. Can you think of ways", "that any two of the criteria relate to each other?\nTheorem 3.1 Choose any small distance . If we assume that  has a minimum, is", "suf\ufb01ciently \u201csmooth\u201d and convex, and if the learning rate  is suf\ufb01ciently small, gradient\ndescent will reach a point within  of a global optimum point .", "However, we must be careful when choosing the learning rate to prevent slow\nconvergence, non-converging oscillation around the minimum, or divergence.", "The following plot illustrates a convex function , starting gradient\ndescent at  with a step-size of . It is very well-behaved!\u03b7\n\u03b7\n\u03b7\n\u0398\n1: \u0398init,\u03b7,f,f\u2032,\u03f5\n2:\u0398(0)\u2190\u0398init\n3:t\u21900\n4:\n5:t\u2190t+1", "3:t\u21900\n4:\n5:t\u2190t+1\n6: \u0398(t)=\u0398(t\u22121)\u2212\u03b7f\u2032(\u0398(t\u22121))\n7: |f\u2032(\u0398(t))|<\u03f5\n8: \u0398(t)\n9:\nf\nT t=T\n\u0398\n\u0398(t)\u2212\u0398(t\u22121)<\u03f5\n\u2223\u2223~\u03f5>0 f\n\u03b7\n~\u03f5 \u0398\nf(x)=(x\u22122)2\nxinit=4.0 1/2 \u2212 1 1 2 3 4 5 624\nxf (x )", "xf (x )\nIf  is non-convex, where gradient descent converges to depends on . First, let\u2019s\nestablish some de\ufb01nitions. Let  be a real-valued function de\ufb01ned over some", "domain . A point  is called a global minimum point of  if  for\nall other . A point  is instead called a local minimum point of a function", "if there exists some constant  such that for all  within the interval de\ufb01ned\nby  , where  is some distance metric, e.g.,\n A global minimum point is also a local minimum point, but a", "local minimum point does not have to be a global minimum point.\n\u2753 Study Question\nWhat happens in this example with very small ? With very big ?", "If  is non-convex (and suf\ufb01ciently smooth), one expects that gradient descent (run\nlong enough with small enough learning rate) will get very close to a point at which", "the gradient is zero, though we cannot guarantee that it will converge to a global\nminimum point.\nThere are two notable exceptions to this common sense expectation: First, gradient", "descent can get stagnated while approaching a point  which is not a local\nminimum or maximum, but satis\ufb01es . For example, for , starting", "gradient descent from the initial guess , while using learning rate \nwill lead to  converging to zero as . Second, there are functions (even", "convex ones) with no minimum points, like , for which gradient\ndescent with a positive learning rate converges to .\nThe plot below shows two different , and how gradient descent started from", "each point heads toward two different local optimum points.f xinit\nf\nD x0\u2208D ff(x0)\u2264f(x)\nx\u2208D x0\u2208D\nf \u03f5>0 x\nd(x,x0)<\u03f5,f(x0)\u2264f(x)d\nd(x,x0)=||x\u2212x0||.\n\u03b7 \u03b7\nf\nx\nf\u2032(x)=0 f(x)=x3\nxinit=1 \u03b7<1/3\nx(k)k\u2192\u221e", "x(k)k\u2192\u221e\nf(x)=exp(\u2212x)\n+\u221e\nxinit \u2212 2 \u2212 1 1 2 3 44681 0\nxf (x )\n3.2 Multiple dimensions\nThe extension to the case of multi-dimensional  is straightforward. Let\u2019s assume\n, so .", ", so .\nThe gradient of  with respect to  is\nThe algorithm remains the same, except that the update step in line 5 becomes", "and any termination criteria that depended on the dimensionality of  would have\nto change. The easiest thing is to keep the test in line 6 as ,\nwhich is sensible no matter the dimensionality of .", "\u2753 Study Question\nWhich termination criteria from the 1D case were de\ufb01ned in a way that assumes\n is one dimensional?\n3.3 Application to regression\u0398\n\u0398\u2208Rmf:Rm\u2192R\nf \u0398\n\u2207\u0398f=\u23a1\n\u23a2\u23a3\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m\u23a4\n\u23a5\u23a6", "\u22ee\n\u2202f/\u2202\u0398m\u23a4\n\u23a5\u23a6\n\u0398(t)=\u0398(t\u22121)\u2212\u03b7\u2207\u0398f(\u0398(t\u22121))\n\u0398\nf(\u0398(t))\u2212f(\u0398(t\u22121))<\u03f5\n\u2223\u2223\u0398\n\u0398 Recall from the previous chapter that choosing a loss function is the \ufb01rst step in", "formulating a machine-learning problem as an optimization problem, and for\nregression we studied the mean square loss, which captures losws as\n. This leads to the ordinary least squares objective", "We use the gradient of the objective with respect to the parameters,\nto obtain an analytical solution to the linear regression problem. Gradient descent", "could also be applied to numerically compute a solution, using the update rule\nNow, let\u2019s add in the regularization term, to get the ridge-regression objective:", "Recall that in ordinary least squares, we \ufb01nessed handling  by adding an extra\ndimension of all 1\u2019s. In ridge regression, we really do need to separate the", "parameter vector  from the offset , and so, from the perspective of our general-\npurpose gradient descent method, our whole parameter set  is de\ufb01ned to be", ". We will go ahead and \ufb01nd the gradients separately for each one:\nNote that  will be of shape  and  will be a scalar since we\nhave separated  from  here.\n\u2753 Study Question(guess\u2212actual)2\nJ(\u03b8)=1\nnn\n\u2211", "J(\u03b8)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)\u2212y(i))2\n.\n\u2207\u03b8J=2\nnXT\nd\u00d7n(X\u03b8\u2212Y)\nn\u00d71,\ue152 \ue154 \ue151\ue150 \ue154 \ue153\ue152 \ue154 \ue151\ue150 \ue154 \ue153(3.1)\n\u03b8(t)=\u03b8(t\u22121)\u2212\u03b72\nnn\n\u2211\ni=1([\u03b8(t\u22121)]T\nx(i)\u2212y(i))x(i).\n3.3.1 Ridge regression\nJridge(\u03b8,\u03b80)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))2", "+\u03bb\u2225\u03b8\u22252.\n\u03b80\n\u03b8 \u03b80\n\u0398\n\u0398=(\u03b8,\u03b80)\n\u2207\u03b8Jridge(\u03b8,\u03b80)=2\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))x(i)+2\u03bb\u03b8\n\u2202Jridge(\u03b8,\u03b80)\n\u2202\u03b80=2\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i)).\n\u2207\u03b8Jridge d\u00d71 \u2202Jridge/\u2202\u03b80", "\u03b80\u03b8 Convince yourself that the dimensions of all these quantities are correct, under\nthe assumption that  is . How does  relate to  as discussed for  in the\nprevious section?\n\u2753 Study Question", "\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives\n. What is the shape of ?\n\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives\n.\n\u2753 Study Question", ".\n\u2753 Study Question\nUse these last two results to verify our derivation above.\nPutting everything together, our gradient descent algorithm for ridge regression\nbecomes", "becomes\nprocedure RR-G RADIENT -D ESCENT( )\nrepeat\nuntil \nreturn \nend procedure\n\u2753 Study Question\nIs it okay that  doesn\u2019t appear in line 8?\n\u2753 Study Question", "\u2753 Study Question\nIs it okay that the 2\u2019s from the gradient de\ufb01nitions don\u2019t appear in the\nalgorithm?\u03b8d\u00d71 d m \u0398\n\u2207\u03b8||\u03b8||2\n(\u2202||\u03b8||2/\u2202\u03b81,\u2026,\u2202||\u03b8||2/\u2202\u03b8d) \u2207\u03b8||\u03b8||2\n\u2207\u03b8Jridge(\u03b8Tx+\u03b80,y)", "\u2207\u03b8Jridge(\u03b8Tx+\u03b80,y)\n(\u2202Jridge(\u03b8Tx+\u03b80,y)/\u2202\u03b81,\u2026,\u2202Jridge(\u03b8Tx+\u03b80,y)/\u2202\u03b8d)\n1: \u03b8init,\u03b80init,\u03b7,\u03f5\n2:\u03b8(0)\u2190\u03b8init\n3:\u03b8(0)\n0\u2190\u03b80init\n4:t\u21900\n5:\n6:t\u2190t+1\n7:\u03b8(t)=\u03b8(t\u22121)\u2212\u03b7(1\nn\u2211n\ni=1(\u03b8(t\u22121)Tx(i)+\u03b80(t\u22121)\u2212y(i))x(i)+\u03bb\u03b8(t\u22121))", "8:\u03b8(t)\n0=\u03b8(t\u22121)\n0\u2212\u03b7(1\nn\u2211n\ni=1(\u03b8(t\u22121)Tx(i)+\u03b80(t\u22121)\u2212y(i)))\n9: Jridge(\u03b8(t),\u03b8(t)\n0)\u2212Jridge(\u03b8(t\u22121),\u03b8(t\u22121)\n0)<\u03f5\n\u2223\u222310: \u03b8(t),\u03b8(t)\n0\n11:\n\u03bbBeware double superscripts!  is\nthe transpose of the vector .[\u03b8]T", "\u03b8 3.4 Stochastic gradient descent\nWhen the form of the gradient is a sum, rather than take one big(ish) step in the", "direction of the gradient, we can, instead, randomly select one term of the sum, and\ntake a very small step in that direction. This seems sort of crazy, but remember that", "all the little steps would average out to the same direction as the big step if you\nwere to stay in one place. Of course, you\u2019re not staying in that place, so you move,", "in expectation, in the direction of the gradient.\nMost objective functions in machine learning can end up being written as an", "average over data points, in which case, stochastic gradient descent (sgd) is\nimplemented by picking a data point randomly out of the data set, computing the", "gradient as if there were only that one point in the data set, and taking a small step\nin the negative direction.\nLet\u2019s assume our objective has the form", "where  is the number of data points used in the objective (and this may be\ndifferent from the number of points available in the whole data set).", "Here is pseudocode for applying sgd to such an objective ; it assumes we know the\nform of  for all  in :\nprocedure STOCHAS TIC -G RADIENT -D ESCENT( )\nfor  do\nrandomly select \nend for\nend procedure", "end procedure\nNote that now instead of a \ufb01xed value of ,  is indexed by the iteration of the\nalgorithm, . Choosing a good stopping criterion can be a little trickier for sgd than", "traditional gradient descent. Here we\u2019ve just chosen to stop after a \ufb01xed number of\niterations .\nFor sgd to converge to a local optimum point as  increases, the learning rate has to", "decrease as a function of time. The next result shows one learning rate sequence that\nworks.\nTheorem 3.2 If  is convex, and  is a sequence satisfyingf(\u0398)=1\nnn\n\u2211\ni=1fi(\u0398),\nn\nf\n\u2207\u0398fii1\u2026n", "n\nf\n\u2207\u0398fii1\u2026n\n1: \u0398init,\u03b7,f,\u2207\u0398f1,...,\u2207\u0398fn,T\n2:\u0398(0)\u2190\u0398init\n3:t\u21901\n4: i\u2208{1,2,\u2026,n}\n5: \u0398(t)=\u0398(t\u22121)\u2212\u03b7(t)\u2207\u0398fi(\u0398(t\u22121))\n6:\n7:\n\u03b7\u03b7\nt\nT\nt\nf \u03b7(t)\n\u221e\n\u2211\nt=1\u03b7(t)=\u221eand\u221e\n\u2211\nt=1\u03b7(t)2<\u221e,Sometimes you will see that the", "objective being written as a sum,\ninstead of an average. In the \u201csum\u201d\nconvention, the  normalizing\nconstant is getting \u201cabsorbed\u201d into\nindividual .1\nn\nfi\nf(\u0398)=n\n\u2211", "n\nfi\nf(\u0398)=n\n\u2211\ni=1fi(\u0398). then SGD converges with probability one* to the optimal .*\nWhy these two conditions? The intuition is that the \ufb01rst condition, on , is", "needed to allow for the possibility of an unbounded potential range of exploration,\nwhile the second condition, on , ensures that the learning rates get smaller\nand smaller as  increases.", "One \u201clegal\u201d way of setting the learning rate is to make  but people often\nuse rules that decrease more slowly, and so don\u2019t strictly satisfy the criteria for\nconvergence.\n\u2753 Study Question", "\u2753 Study Question\nIf you start a long way from the optimum, would making  decrease more\nslowly tend to make you move more quickly or more slowly to the optimum?", "There are multiple intuitions for why sgd might be a better choice algorithmically\nthan regular gd (which is sometimes called batch gd (bgd)):", "bgd typically requires computing some quantity over every data point in a data\nset. sgd may perform well after visiting only some of the data. This behavior", "can be useful for very large data sets \u2013 in runtime and memory savings.\nIf your  is actually non-convex, but has many shallow local optimum points", "that might trap bgd, then taking samples from the gradient at some point \nmight \u201cbounce\u201d you around the landscape and away from the local optimum\npoints.", "points.\nSometimes, optimizing  really well is not what we want to do, because it\nmight over\ufb01t the training set; so, in fact, although sgd might not get lower", "training error than bgd, it might result in lower test error.\u0398\n\u2211\u03b7(t)\n\u2211\u03b7(t)2\nt\n\u03b7(t)=1/t\n\u03b7(t)\nf\n\u0398\nf We had legacy PDF  notes that us ed mixed conventions for data matrices: \u201ceach row as a", "data point\u201d and \u201ceach colum n as a data point\u201d.\nWe are standardizing to \u201ceach row as a data point.\u201d Th us ,  aligns with  in the PDF", "notes if you\u2019ve read those. If you spot inconsistencies or experience any confus ion, please\nraise an issue . Th anks!", "Regression is an important machine-learning problem that provides a good starting\npoint for diving deeply into the \ufb01eld.\n2.1 Problem formulation", "A hypothesis  is employed as a model for solving the regression problem, in that it\nmaps inputs  to outputs ,\nwhere  (i.e., a length  column vector of real numbers), and  (i.e., a real", "number). Real life rarely gives us vectors of real numbers; the  we really want to\ntake as input is usually something like a song, image, or person. In that case, we\u2019ll", "have to de\ufb01ne a function , whose range is , where  represents features of ,\nlike a person\u2019s height or the amount of bass in a song, and then let the", ". In much of the following, we\u2019ll omit explicit mention of  and assume that the \nare in , but you should always have in mind that some additional process was", "almost surely required to go from the actual input examples to their feature\nrepresentation, and we\u2019ll talk a lot more about features later in the course.", "Regression is a supervised learning problem, in which we are given a training dataset\nof the form\nwhich gives examples of input values  and the output values  that should be", "associated with them. Because  values are real-valued, our hypotheses will have\nthe form\nThis is a good framework when we want to predict a numerical quantity, like", "height, stock value, etc., rather than to divide the inputs into discrete categories.2\u00a0 Regre ssion\nWarning\nX~X\nh\nx y\nx\u2192 \u2192y,h\nx\u2208Rdd y\u2208R\nx\n\u03c6(x) Rd\u03c6 x\nh:\u03c6(x)\u2192R\n\u03c6 x(i)\nRd", "h:\u03c6(x)\u2192R\n\u03c6 x(i)\nRd\nDtrain={(x(1),y(1)),\u2026,(x(n),y(n))},\nx(i)y(i)\ny\nh:Rd\u2192R.\u201cRegression,\u201d in common parlance,\nmeans moving backwards. But this\nis forward progress!\nReal life rarely gives us  vectors of", "real num bers. Th e  we really want\nto take as input is us ua lly\nsomething like a song, image, or\nperson. In that case, we\u2019ll have to\nde\ufb01ne a fun ction  whose\nrange is , where  represents", "features of  (e.g., a person\u2019s height\nor the amoun t of bass in a song).x\n\u03c6(x)\nRd\u03c6\nx\uf461 2\u00a0 Regression \uf52a\n1 What makes a hypothesis useful? That it works well on new data\u2014that is, it makes", "good predictions on examples it hasn\u2019t seen.\nHowever, we don\u2019t know exactly what data this hypothesis might be tested on in", "the real world. So, we must assume a connection between the training data and\ntesting data. Typically, the assumption is that they are drawn independently from\nthe same probability distribution.", "To make this discussion more concrete, we need a loss function to express how\nunhappy we are when we guess an output  given an input  for which the desired\noutput was .", "output was .\nGiven a training set  and a hypothesis  with parameters , the training error\nof  can be de\ufb01ned as the average loss on the training data:", "The training error of  gives us some idea of how well it characterizes the\nrelationship between  and  values in our data, but it isn\u2019t the quantity we most", "care about. What we most care about is test error:\non  new examples that were not used in the process of \ufb01nding the hypothesis.", "It might be worthwhile to stare at the two errors and think about what\u2019s the difference.\nFor example, notice how  is no longer a variable in the testing error? Th is is becaus e, in", "evalua ting the testing error, the parameters will have been \u201cpicked\u201d or \u201c\ufb01xed\u201d already.\nFor now, we will try to \ufb01nd a hypothesis with small training error (later, with some", "added criteria) and try to make some design choices so that it generalizes well to new\ndata, meaning that it also has a small test error.\n2.2 Regression as an optimization problem", "Given data, a loss function, and a hypothesis class, we need a method for \ufb01nding a\ngood hypothesis in the class. One of the most general ways to approach this", "problem is by framing the machine learning problem as an optimization problem.\nOne reason for taking this approach is that there is a rich area of math and", "algorithms studying and developing ef\ufb01cient methods for solving optimizationg x\na\nDtrain h \u0398\nh\nEtrain(h;\u0398)=1\nnn\n\u2211\ni=1L(h(x(i);\u0398),y(i)). (2.1)\nh\nxy\nEtest(h)=1\nn\u2032n+n\u2032\n\u2211\ni=n+1L(h(x(i)),y(i)),\nn\u2032\nNote", "n\u2032\nNote\n\u0398Th is process of converting our  data\ninto a num erical form is often\nreferred to as data pre-processing.\nTh en  maps  to .\nIn muc h of the following, we\u2019ll\nomit explicit mention of  and", "assum e that the  are in .\nHowever, you should always\nremember that some additional\nprocess was almost sur ely required\nto go from the actua l input\nexamples to their featur e", "representation. We will discus s\nfeatur es more later in the cour se.h\u03c6(x)R\n\u03c6\nx(i)Rd\nM y favorite analogy is to problem\nsets. We evalua te a stud ent\u2019s ability\nto generalize by putting que stions", "on the exam that were not on the\nhomework (training set). problems, and lots of very good software implementations of these methods. So, if", "we can turn our problem into one of these problems, then there will be a lot of work\nalready done for us!\nWe begin by writing down an objective function , where  stands for all the", "parameters in our model (i.e., all possible choices over parameters). We often write\n to make clear the dependence on the data .", "The objective function describes how we feel about possible hypotheses . We\ngenerally look for parameter values  that minimize the objective function:", "In the most general case, there is not a guarantee that there exists a unique set of\nparameters which minimize the objective function. However, we will ignore that for", "now. A very common form for a machine-learning objective is:\nThe loss measures how unhappy we are about the prediction  for the pair", ". Minimizing this loss improves prediction accuracy. The regularizer \nis an additional term that encourages the prediction to remain general, and the", "constant  adjusts the balance between \ufb01tting the training examples and\ngeneralizing to unseen examples. We will discuss this balance and the idea of\nregularization further in Section 2.7.", "2.3 Linear regression\nTo make this discussion more concrete, we need to provide a hypothesis class and a\nloss function.\nWe begin by picking a class of hypotheses  that might provide a good set of", "possible models for the relationship between  and  in our data. We start with a\nvery simple class of linear hypotheses for regression:\nwhere the model parameters are . In one dimension ( ), this", "corresponds to the familiar slope-intercept form  of a line. In two\ndimesions ( ), this corresponds to a plane. In higher dimensions, this model", "describes a hyperplane. This hypothesis class is both simple to study and very\npowerful, and will serve as the basis for many other important techniques (even\nneural networks!).J(\u0398) \u0398\nJ(\u0398;D) D\n\u0398\n\u0398", "J(\u0398;D) D\n\u0398\n\u0398\n\u0398\u2217=argmin\n\u0398J(\u0398).\nJ(\u0398)=1\nnn\n\u2211\ni=1L(h(x(i);\u0398),y(i))\nloss+ \u03bb\nnon-negativeconstantR(\u0398).\u239b\n\u239c\u239d\ue152 \ue154 \ue151\ue150 \ue154 \ue153\u239e\n\u239f\u23a0\ue152 \ue154 \ue151\ue150 \ue154 \ue153(2.2)\nh(x(i);\u0398)\n(x(i),y(i)) R(\u0398)\n\u03bb\nH\nxy\ny=h(x;\u03b8,\u03b80)=\u03b8Tx+\u03b80, (2.3)", "\u0398=(\u03b8,\u03b80) d=1\ny=mx+b\nd=2Do n\u2019t be too pertur bed by the\nsemicolon where you expected to\nsee a comma! It\u2019s a mathematical\nway of saying that we are mostly\ninterested in this as a fun ction of", "the argum ents before the ;, but we\nshould remember there\u2019s a\ndependence on the stuff after it as\nwell. For now, our objective in linear regression is to \ufb01nd a hypothesis that goes as close", "as possible, on average, to all of our training data. We de\ufb01ne a loss function to\ndescribe how to evaluate the quality of the predictions our hypothesis is making,", "when compared to the \u201ctarget\u201d  values in the data set. The choice of loss function\nis part of modeling your domain. In the absence of additional information about a", "regression problem, we typically use squared loss:\nwhere  is our \u201cguess\u201d from the hypothesis, or the hypothesis\u2019 prediction,", "and  is the \u201cactual\u201d observation (in other words, here  is being used equivalently\nas ). With this choice of squared loss, the average loss as generally de\ufb01ned in", "Equation\u00a02.1 will become the so-called mean squared error (MSE).\nApplying the general optimization framework to the linear regression hypothesis", "class of Equation\u00a02.3 with squared loss and no regularization, our objective is to \ufb01nd\nvalues for  that minimize the MSE:\nresulting in the solution:", "For one-dimensional data ( ), this corresponds to \ufb01tting a line to data. For\n, this hypothesis represents a -dimensional hyperplane embedded in a", "-dimensional space (the input dimension plus the  dimension).\nFor example, in the left plot below, we can see data points with labels  and input", "dimensions  and . In the right plot below, we see the result of \ufb01tting these\npoints with a two-dimensional plane that resides in three dimensions. We interpret", "the plane as representing a function that provides a  value for any input .\ny\nL(g,a)=(g\u2212a)2.\ng=h(x)\na a\ny\n\u0398=(\u03b8,\u03b80)\nJ(\u03b8,\u03b80)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))2\n, (2.4)\n\u03b8\u2217,\u03b8\u2217\n0=argmin\n\u03b8,\u03b80J(\u03b8,\u03b80). (2.5)\nd=1", "d=1\nd>1 d\n(d+1) y\ny\nx1x2\ny (x1,x2)Th e squa red loss penalizes gue sses\nthat are too high the same amoun t\nas it penalizes gue sses that are too\nlow, and has a good mathematical", "jus ti\ufb01cation in the case that your\ndata are generated from an\nun derlying linear hypothesis with\nthe so-called Gaus sian-\ndistributed\u00a0noise added to the \nvalue s. But there are applications", "in which other losses would be\nbetter, and muc h of the framework\nwe discus s can be applied to\ndifferent loss fun ctions, althoug h\nthis one has a form that also makes", "it particularly computationally\nconvenient.\nWe won\u2019t get into the details of\nGaus sian distribution in our  class;\nbut it\u2019s one of the most important\ndistributions and well-worth", "stud ying closely at some point.\nOn e obvious  fact about Gaus sian is\nthat it\u2019s symmetric; this is in fact", "one of the reasons squa red lossy A richer class of hypotheses can be obtained by performing a non-linear feature", "transformation before doing the regression, as we will later see, but it will still end\nup that we have to solve a linear regression problem.\n2.4 A gloriously simple linear regression\nalgorithm", "algorithm\nOkay! Given the objective in Equation\u00a02.4, how can we \ufb01nd good values of  and \n? We\u2019ll study several general-purpose, ef\ufb01cient, interesting algorithms. But before", "we do that, let\u2019s start with the simplest one we can think of: guess a whole bunch ()\nof different values of  and , see which one has the smallest error on the training set,\nand return it.", "and return it.\nAlgorithm 2.1 Random-Regression\nRequire: Data , integer \nfor  to  do\nRandomly generate hypothesis \nend for\nLet \nreturn", "Let \nreturn \nThis seems kind of silly, but it\u2019s a learning algorithm, and it\u2019s not completely\nuseless.\n\u2753 Study Question", "\u2753 Study Question\nIf your data set has  data points, and the dimension of the  values is , what is\nthe size of an individual ?\n\u2753 Study Question", "\u2753 Study Question\nHow do you think increasing the number of guesses  will change the training\nerror of the resulting hypothesis?\n2.5 Analytical solution: ordinary least squares", "One very interesting aspect of the problem of \ufb01nding a linear hypothesis that\nminimizes mean squared error is that we can \ufb01nd a closed-form formula for the", "answer! This general problem is often called the ordinary least squares (ols).\nEverything is easier to deal with if we \ufb01rst ignore the offset . So, suppose for now,\nwe have, simply,\u03b8\u03b80\nk\n\u03b8\u03b80\nD k", "k\n\u03b8\u03b80\nD k\n1:i=1k\n2: \u03b8i,\u03b80(i)\n3:\n4:i=argminjJ(\u03b8(j),\u03b80(j);D)\n5: \u03b8(i),\u03b80(i)\nn x d\n\u03b8(i)\nk\n\u03b80\ny=\u03b8Tx. (2.6)works well un der Gaus sian\nsettings, as the loss is also\nsymmetric.", "symmetric.\nthis corresponds to a hyperplane\nthat goes throug h the origin. In this case, the objective becomes\nWe approach this just like a minimization problem from calculus homework: take", "the derivative of  with respect to , set it to zero, and solve for . There are\nadditional steps required, to check that the resulting  is a minimum (rather than a", "maximum or an in\ufb02ection point) but we won\u2019t work through that here. It is possible\nto approach this problem by:\nFinding  for  in ,\nConstructing a set of  equations of the form , and", "Solving the system for values of .\nThat works just \ufb01ne. To get practice for applying techniques like this to more\ncomplex problems, we will work through a more compact (and cool!) matrix view.", "Along the way, it will be helpful to collect all of the derivatives in one vector. In\nparticular, the gradient of  with respect to  is following column vector of length :\n\u2753 Study Question", "\u2753 Study Question\nWork through the next steps and check your answer against ours below.\nWe can think of our training data in terms of matrices  and , where each row of", "is an example, and each row (or rather, element) of  is the corresponding target\noutput value:\n\u2753 Study Question\nWhat are the dimensions of  and ?J(\u03b8)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)\u2212y(i))2\n. (2.7)\nJ \u03b8 \u03b8\n\u03b8", ". (2.7)\nJ \u03b8 \u03b8\n\u03b8\n\u2202J/\u2202\u03b8kk1,\u2026,d\nk \u2202J/\u2202\u03b8k=0\n\u03b8k\nJ \u03b8 d\n\u2207\u03b8J= .\u23a1\n\u23a2\u23a3\u2202J/\u2202\u03b81\n\u22ee\n\u2202J/\u2202\u03b8d\u23a4\n\u23a5\u23a6\nXY\nX Y\nX= Y= .\u23a1\n\u23a2\u23a3x(1)\n1\u2026x(1)\nd\n\u22ee \u22f1 \u22ee\nx(n)\n1 \u2026x(n)\nd\u23a4\n\u23a5\u23a6\u23a1\n\u23a2\u23a3y(1)\n\u22ee\ny(n)\u23a4\n\u23a5\u23a6\nXY Now we can write", "XY Now we can write\nand using facts about matrix/vector calculus, we get\nSetting this equal to zero and solving for  yields the \ufb01nal closed-form solution:", "and the dimensions work out! So, given our data, we can directly compute the\nlinear regression that minimizes mean squared error. That\u2019s pretty awesome!", "Now, how do we deal with the offset? We augment the original feature vector with\na \u201cfake\u201d feature of value 1, and add a corresponding parameter  to the  vector.", "That is, we de\ufb01ne columns vectors  such that,\nwhere the \u201caug\u201d denotes that  have been augmented.\nThen we can now write the linear hypothesis as if there is no offset,", "We can do this \u201cappending a fake feature of 1\u201d to all data points to form the\naugmented data matrix \nwhere  as an -by vector of all one. Then use the formula in Equation\u00a02.8 to \ufb01nd", "the  that minimizes the mean squared error.J(\u03b8)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)\u2212y(i))2=1\nn(X\u03b8\u2212Y)T(X\u03b8\u2212Y).\n\u2207\u03b8J(\u03b8)=1\nn\u2207\u03b8[(X\u03b8)TX\u03b8\u2212YTX\u03b8\u2212(X\u03b8)TY+YTY]\n=2\nn(XTX\u03b8\u2212XTY).\n\u03b8\n\u03b8\u2217=(XTX)\u22121XTY (2.8)\n\u03b80\u03b8\nxaug,\u03b8aug\u2208Rd+1\nxaug= ,\u03b8aug=\u23a1", "xaug= ,\u03b8aug=\u23a1\n\u23a2\u23a3x1\nx2\n\u22ee\nxd\n1\u23a4\n\u23a5\u23a6\u23a1\n\u23a2\u23a3\u03b81\n\u03b82\n\u22ee\n\u03b8d\n\u03b80\u23a4\n\u23a5\u23a6\n\u03b8,x\ny=h(xaug;\u03b8aug)=\u03b8T\naugxaug (2.9)\nXaug\nXaug= =[ ]\u23a1\n\u23a2\u23a3x(1)\n1\u2026x(1)\nd1\n\u22ee \u22f1 \u22ee \u22ee\nx(n)\n1 \u2026x(n)\nd1\u23a4\n\u23a5\u23a6X \ud835\udfd9\n\ud835\udfd9n 1\n\u03b8augSe e Appendix A if you need some", "help \ufb01nding this gradient.\nHere are two related alternate\nangles to view this formula, for\nintuition\u2019s sake:\n1. Note that\n is the\npseudo-inverse of . Th us , \n\u201cpseud o-solves\u201d", "\u201cpseud o-solves\u201d \n(multiply both sides of this on\nthe left by ).\n2. Note that\nis the projection matrix onto\nthe colum n space of . Th us ,\n solves .(XTX)\u22121XT=X+:\nX\u03b8\u2217\nX\u03b8=Y\nX+\nX(XTX)\u22121XT=projcol(X)\nX", "X\n\u03b8\u2217X\u03b8=projcol(X)Y Th is is a very special case where\nwe can \ufb01nd the solution in closed\nform. In general, we will need to\nus e iterative optimization\nalgorithms to \ufb01nd the best", "parameters. Also, this process of\nsetting the graident/derivatives to\nzero and solving for the\nparameters works out in this\nproblem. But there can be\nexceptions to this rule, and we will", "discus s them later in the cour se.But of cour se, the constant offset is\nnot really gone, it\u2019s jus t hidden in\nthe aug mentation. \u2753 Study Question", "Stop and prove to yourself that adding that extra feature with value 1 to every\ninput vector and getting rid of the  parameter, as done in Equation\u00a02.9 is", "equivalent to our original model Equation\u00a02.3.\n2.6 Centering\nIn fact, augmenting a \u201cfake\u201d feature of 1, as described above, is also useful for an", "important idea: namely, why utilizing the so-called centering eliminates the need\nfor \ufb01tting an intercept, and thereby offers an alternative way to avoid dealing with\n directly.", "directly.\nBy centering, we mean subtracting the average (mean) of each feature from all data\npoints, and we apply the same operation to the labels. For an example of a dataset", "before and after centering, see here\nThe idea is that, with centered dataset, even if we were to search for an offset term", ", it would naturally fall out to be 0. Intuitively, this makes sense \u2013 if a dataset is\ncentered around the origin, it seems natural that the best \ufb01tting plane would go\nthrough the origin.", "through the origin.\nLet\u2019s see how this works out mathematically. First, for a centered dataset, two claims\nimmediately follow (recall that  is an -by-1 vector of all ones):", "1. Each column of  sums up to zero, that is, .\n2. Similarly, the mean of the labels is 0, so .\nRecall that our ultimate goal is to \ufb01nd an optimal \ufb01tting hyperplane, parameterized", "by  and . In other words, we aim to \ufb01nd  which at this point, involves\nsimply plugging  into Equation\u00a02.8.\u03b80\n\u03b80\n\u03b80\n\ud835\udfd9n\nX XT\ud835\udfd9=0\nYT\ud835\udfd9=\ud835\udfd9TY=0\n\u03b8\u03b80 \u03b8aug,\nXaug=[ ]X \ud835\udfd9", "Xaug=[ ]X \ud835\udfd9\n1 Indeed, the optimal  naturally falls out to be 0.\n2.7 Regularization\nThe objective function of Equation\u00a02.2 balances (training-data) memorization,", "induced by the loss term, with generalization, induced by the regularization term.\nHere, we address the need for regularization speci\ufb01cally for linear regression, and", "show how this can be realized using one popular regularization technique called\nridge regression.\nIf all we cared about was \ufb01nding a hypothesis with small loss on the training data,", "we would have no need for regularization, and could simply omit the second term\nin the objective. But remember that our ultimate goal is to perform well on input", "values that we haven\u2019t trained on! It may seem that this is an impossible task, but\nhumans and machine-learning methods do this successfully all the time. What", "allows generalization to new input values is a belief that there is an underlying\nregularity that governs both the training and testing data. One way to describe an", "assumption about such a regularity is by choosing a limited class of possible\nhypotheses. Another way to do this is to provide smoother guidance, saying that,", "within a hypothesis class, we prefer some hypotheses to others. The regularizer\narticulates this preference and the constant  says how much we are willing to trade", "off loss on the training data versus preference over hypotheses.\nFor example, consider what happens when  and  is highly correlated with", ", meaning that the data look like a line, as shown in the left panel of the \ufb01gure\nbelow. Thus, there isn\u2019t a unique best hyperplane. Such correlations happen often in", "real-life data, because of underlying common causes; for example, across a\npopulation, the height of people may depend on both age and amount of food\u03b8\u2217\naug=([][ ])\u22121\n[]Y\n=[ ]\u22121\n[]Y\n=[ ]\u22121\n[]Y\n=[ ]\u22121", "=[ ]\u22121\n[]Y\n=[ ]\u22121\n[]Y\n=[ ]\n=[ ]\n=[]XT\n\ud835\udfd9TX \ud835\udfd9XT\n\ud835\udfd9T\nXTXXT\ud835\udfd9\n\ud835\udfd9TX \ud835\udfd9T\ud835\udfd9XT\n\ud835\udfd9T\nXTXXT\ud835\udfd9\n\ud835\udfd9TX \ud835\udfd9T\ud835\udfd9XT\n\ud835\udfd9T\nXTX0\n0nXT\n\ud835\udfd9T\n(XTX)\u22121XTY\nn\ud835\udfd9TY\n(XTX)\u22121XTY\n0\n\u03b8\u2217\n\u03b8\u2217\n0\n\u03b80\n2.7.1 Regularization and linear regression\n\u03bb\nd=2,x2", "\u03bb\nd=2,x2\nx1 intake in the same way. This is especially the case when there are many feature\ndimensions used in the regression. Mathematically, this leads to  close to", "singularity, such that  is unde\ufb01ned or has huge values, resulting in\nunstable models (see the middle panel of \ufb01gure and note the range of the  values\u2014\nthe slope is huge!):", "A common strategy for specifying a regularizer is to use the form\nwhen we have some idea in advance that  ought to be near some value .", "Here, the notion of distance is quanti\ufb01ed by squaring the  norm of the parameter\nvector: for any -dimensional vector  the  norm of  is de\ufb01ned as,", "In the absence of such knowledge a default is to regularize toward zero:\nWhen this is done in the example depicted above, the regression model becomes", "stable, producing the result shown in the right-hand panel in the \ufb01gure. Now the\nslope is much more sensible.\nThere are some kinds of trouble we can get into in regression problems. What if", "is not invertible?\nAnother kind of problem is over\ufb01tting: we have formulated an objective that is just\nabout \ufb01tting the data as well as possible, but we might also want to regularize to", "keep the hypothesis from getting too attached to the data.\nWe address both the problem of not being able to invert  and the problem", "of over\ufb01tting using a mechanism called ridge regression. We add a regularization\nterm  to the OLS objective, with a non-negative scalar value  to control theXTX\n(XTX)\u22121\ny\nR(\u0398)=\u2225\u0398\u2212\u0398prior\u22252\n\u0398 \u0398prior\nl2", "\u0398 \u0398prior\nl2\nd v\u2208Rd,l2 v\n\u2225v\u2225=d\n\u2211\ni=1|vi|2.\ue001\n\ue000\u23b7\nR(\u0398)=\u2225\u0398\u22252.\n2.7.2 Ridge regression\n(XTX)\n(XTX)\u22121\n\u2225\u03b8\u22252\u03bb tradeoff between the training error and the regularization term. Here is the ridge", "regression objective function:\nLarger  values (in magnitude) pressure  values to be near zero.\nNote that, when data isn\u2019t centered, we don\u2019t penalize ; intuitively,  is what", "\u201c\ufb02oats\u201d the regression surface to the right level for the data you have, and so we\nshouldn\u2019t make it harder to \ufb01t a data set where the  values tend to be around one", "million than one where they tend to be around one. The other parameters control\nthe orientation of the regression surface, and we prefer it to have a not-too-crazy\norientation.", "orientation.\nThere is an analytical expression for the  values that minimize , even when\nthe data isn\u2019t centered, but it\u2019s a more complicated to derive than the solution for", "OLS, even though the process is conceptually similar: taking the gradient, setting it\nto zero, and solving for the parameters.", "The good news is, when the dataset is centered, we again have very clean set up and\nderivation. In particular, the objective can be written as:\nand the solution is:", "One other great news is that in Equation\u00a02.13, the matrix we are trying to invert can\nalways be inverted! Why is the term  invertible? Explaining this", "requires some linear algebra. The matrix  is positive semide\ufb01nite, which\nimplies that its eigenvalues  are greater than or equal to 0. The matrix\n has eigenvalues  which are guaranteed to be strictly", "positive since . Recalling that the determinant of a matrix is simply the\nproduct of its eigenvalues, we get that  and conclude that\n is invertible.\n2.8 Evaluating learning algorithmsJridge(\u03b8,\u03b80)=1", "nn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))2\n+\u03bb\u2225\u03b8\u22252(2.10)\n\u03bb \u03b8\n\u03b80 \u03b80\ny\n\u03b8,\u03b80 Jridge\nJridge(\u03b8)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)\u2212y(i))2\n+\u03bb\u2225\u03b8\u22252(2.11)\n\u03b8ridge=(XTX+n\u03bbI)\u22121XTY (2.12)", "Derivation of the Ridge Regression Solution for Centered Data Set\n(XTX+n\u03bbI)\nXTX\n{\u03b3i}i\nXTX+n\u03bbI {\u03b3i+n\u03bb}i\n\u03bb>0\ndet(XTX+n\u03bbI)>0\nXTX+n\u03bbICompare Equation\u00a02.10 and\nEquation\u00a02.11. W hat is the", "difference between the two? How\nis it possible to drop the offset\nhere? In this section, we will explore how to evaluate supervised machine-learning", "algorithms. We will study the special case of applying them to regression problems,\nbut the basic ideas of validation, hyper-parameter selection, and cross-validation\napply much more broadly.", "We have seen how linear regression is a well-formed optimization problem, which\nhas an analytical solution when ridge regularization is applied. But how can one", "choose the best amount of regularization, as parameterized by ? Two key ideas\ninvolve the evaluation of the performance of a hypothesis, and a separate", "evaluation of the algorithm used to produce hypotheses, as described below.\nThe performance of a given hypothesis  may be evaluated by measuring test error", "on data that was not used to train it. Given a training set  a regression\nhypothesis , and if we choose squared loss, we can de\ufb01ne the OLS training error of", "to be the mean square error between its predictions and the expected outputs:\nTest error captures the performance of  on unseen data, and is the mean square", "error on the test set, with a nearly identical expression as that above, differing only\nin the range of index :\non  new examples that were not used in the process of constructing .", "In machine learning in general, not just regression, it is useful to distinguish two\nways in which a hypothesis  might contribute to test error. Two are:", "Structural error: This is error that arises because there is no hypothesis  that\nwill perform well on the data, for example because the data was really generated by", "a sine wave but we are trying to \ufb01t it with a line.\nEstimation error: This is error that arises because we do not have enough data (or", "the data are in some way unhelpful) to allow us to choose a good , or because\nwe didn\u2019t solve the optimization problem well enough to \ufb01nd the best  given the\ndata that we had.", "data that we had.\nWhen we increase , we tend to increase structural error but decrease estimation\nerror, and vice versa.", "Note that this section is relevant to learning algorithms generally\u2014we are just introducing\nthe topic here since we now have an algorithm that can be evaluated!\u03bb\n2.8.1 Evaluating hypotheses\nh\nDn,\nh\nh", "h\nDn,\nh\nh\nEtrain(h)=1\nnn\n\u2211\ni=1[h(x(i))\u2212y(i)]2\n.\nh\ni\nEtest(h)=1\nn\u2032n+n\u2032\n\u2211\ni=n+1[h(x(i))\u2212y(i)]2\nn\u2032h\nh\u2208H\nh\u2208H\nh\u2208H\nh\n\u03bb", "n\u2032h\nh\u2208H\nh\u2208H\nh\u2208H\nh\n\u03bb\n2.8.2 Evaluating learning algorithms A learning algorithm is a procedure that takes a data set  as input and returns an\nhypothesis  from a hypothesis class ; it looks like", "Keep in mind that  has parameters. The learning algorithm itself may have its own\nparameters, and such parameters are often called hyperparameters. The analytical", "solutions presented above for linear regression, e.g., Equation\u00a02.12, may be thought\nof as learning algorithms, where  is a hyperparameter that governs how the", "learning algorithm works and can strongly affect its performance.\nHow should we evaluate the performance of a learning algorithm? This can be", "tricky. There are many potential sources of variability in the possible result of\ncomputing test error on a learned hypothesis :\nWhich particular training examples occurred in", "Which particular testing examples occurred in \nRandomization inside the learning algorithm itself\nGenerally, to evaluate how well a learning algorithm works, given an unlimited data", "source, we would like to execute the following process multiple times:\nTrain on a new training set (subset of our big data source)", "Evaluate resulting  on a validation set that does not overlap the training set\n(but is still a subset of our same big data source)", "Running the algorithm multiple times controls for possible poor choices of training\nset or unfortunate randomization inside the algorithm itself.", "One concern is that we might need a lot of data to do this, and in many applications\ndata is expensive or dif\ufb01cult to acquire. We can re-use data with cross validation (but", "it\u2019s harder to do theoretical analysis).\nAlgorithm 2.1 Cross-Validate\nRequire: Data , integer \nDivide  into  chunks  (of roughly equal size)\nfor  to  do", "for  to  do\nTrain  on  (withholding chunk  as the validation set)\nCompute \"test\" error  on withheld data \nend for\nreturn", "end for\nreturn \nIt\u2019s very important to understand that (cross-)validation neither delivers nor\nevaluates a single particular hypothesis . It evaluates the learning algorithm that", "produces hypotheses.Dn\nh H\nDtrain\u27f6 \u27f6h learningalg(H)\nh\n\u03bb\nh\nDtrain\nDtest\n2.8.2.1 Validation\nh\n2.8.2.2 Cross validation\nD k\n1: Dk D1,D2,\u2026,Dk\n2:i=1k\n3: hiD\u2216Di Di\n4: Ei(hi) Di\n5:\n6:1\nk\u2211k\ni=1Ei(hi)", "6:1\nk\u2211k\ni=1Ei(hi)\nh The hyper-parameters of a learning algorithm affect how the algorithm works but\nthey are not part of the resulting hypothesis. So, for example,  in ridge regression", "affects which hypothesis will be returned, but  itself doesn\u2019t show up in the\nhypothesis (the hypothesis is speci\ufb01ed using parameters  and ).", "You can think about each different setting of a hyper-parameter as specifying a\ndifferent learning algorithm.\nIn order to pick a good value of the hyper-parameter, we often end up just trying a", "lot of values and seeing which one works best via validation or cross-validation.\n\u2753 Study Question\nHow could you use cross-validation to decide whether to use analytic ridge", "regression or our random-regression algorithm and to pick  for random\nregression or  for ridge regression?\n2.8.2.3 Hyperparameter tuning\u03bb\n\u03bb\n\u03b8\u03b80\nk\n\u03bb"]