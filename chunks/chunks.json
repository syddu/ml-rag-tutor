["Th is page contains all content from the legacy PDF  notes; gradient descent chapter.\nAs we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .", "In the previous chapter, we showed how to describe an interesting objective\nfunction for machine learning, but we need a way to \ufb01nd the optimal", ", particularly when the objective function is not amenable to\nanalytical optimization. For example, this can be the case when  involves a", "more complex loss function, or more general forms of regularization. It can also be\nthe case when there are simply too many parameters to learn for it to be\ncomputationally feasible.", "There is an enormous and fascinating literature on the mathematical and\nalgorithmic foundations of optimization, but for this class, we will consider one of", "the simplest methods, called gradient descent.\nIntuitively, in one or two dimensions, we can easily think of  as de\ufb01ning a", "surface over ; that same idea extends to higher dimensions. Now, our objective is\nto \ufb01nd the  value at the lowest point on that surface. One way to think about", "gradient descent is that you start at some arbitrary point on the surface, look to see\nin which direction the \u201chill\u201d goes down most steeply, take a small step in that", "direction, determine the direction of steepest descent from where you are, take\nanother small step, etc.\nBelow, we explicitly give gradient descent algorithms for one and multidimensional", "objective functions (Section 3.1 and Section 3.2). We then illustrate the application of\ngradient descent to a loss function which is not merely mean squared loss", "(Section 3.3). And we present an important method known as stochastic gradient\ndescent (Section 3.4), which is especially useful when datasets are too large for", "descent in a single batch, and has some important behaviors of its own.\n3.1 Gradient descent in one dimension\nWe start by considering gradient descent in one dimension. Assume , and", "that we know both  and its \ufb01rst derivative with respect to , . Here is\npseudo-code for gradient descent on an arbitrary function . Along with  and its", "gradient  (which, in the case of a scalar , is the same as its derivative ), we\nhave to specify some hyper-parameters. These hyper-parameters include the initial", "value for parameter , a step-size hyper-parameter , and an accuracy hyper-\nparameter .3  Gradient Descent\nNote\n\u0398\u2217=argmin\u0398J(\u0398)\nJ(\u0398)\nJ(\u0398)\n\u0398\n\u0398\n\u0398\u2208R\nJ(\u0398) \u0398J\u2032(\u0398)\nf f\n\u2207\u0398f \u0398 f\u2032\n\u0398 \u03b7", "f f\n\u2207\u0398f \u0398 f\u2032\n\u0398 \u03b7\n\u03f5You might want to consider\nstudying optimization some day!\nIt\u2019s one of the fundamental tools\nenabling machine learning, and it\u2019s", "a beautiful and deep \ufb01eld.\uf461 3  Gradient Descent \uf52a The hyper-parameter  is often called learning rate when gradient descent is applied", "in machine learning. For simplicity,  may be taken as a constant, as is the case in\nthe pseudo-code below; and we\u2019ll see adaptive (non-constant) step-sizes soon.", "What\u2019s important to notice though, is that even when  is constant, the actual\nmagnitude of the change to  may not be constant, as that change depends on the\nmagnitude of the gradient itself too.", "procedure 1D-G RADIENT -D ESCENT( )\nrepeat\nuntil \nreturn \nend procedure\nNote that this algorithm terminates when the derivative of the function  is", "suf\ufb01ciently small. There are many other reasonable ways to decide to terminate,\nincluding:\nStop after a \ufb01xed number of iterations , i.e., when . Practically, this is the\nmost common choice.", "most common choice.\nStop when the change in the value of the parameter  is suf\ufb01ciently small,\ni.e., when .\n\u2753 Study Question", "\u2753 Study Question\nConsider all of the potential stopping criteria for 1D-Gradient-Descent, both\nin the algorithm as it appears and listed separately later. Can you think of ways", "that any two of the criteria relate to each other?\nTheorem 3.1 Choose any small distance . If we assume that  has a minimum, is", "suf\ufb01ciently \u201csmooth\u201d and convex, and if the learning rate  is suf\ufb01ciently small, gradient\ndescent will reach a point within  of a global optimum point .", "However, we must be careful when choosing the learning rate to prevent slow\nconvergence, non-converging oscillation around the minimum, or divergence.", "The following plot illustrates a convex function , starting gradient\ndescent at  with a step-size of . It is very well-behaved!\u03b7\n\u03b7\n\u03b7\n\u0398\n1: \u0398init,\u03b7,f,f\u2032,\u03f5\n2:\u0398(0)\u2190\u0398init\n3:t\u21900\n4:\n5:t\u2190t+1", "3:t\u21900\n4:\n5:t\u2190t+1\n6: \u0398(t)=\u0398(t\u22121)\u2212\u03b7f\u2032(\u0398(t\u22121))\n7: |f\u2032(\u0398(t))|<\u03f5\n8: \u0398(t)\n9:\nf\nT t=T\n\u0398\n\u0398(t)\u2212\u0398(t\u22121)<\u03f5\n\u2223\u2223~\u03f5>0 f\n\u03b7\n~\u03f5 \u0398\nf(x)=(x\u22122)2\nxinit=4.0 1/2 \u2212 1 1 2 3 4 5 624\nxf (x )", "xf (x )\nIf  is non-convex, where gradient descent converges to depends on . First, let\u2019s\nestablish some de\ufb01nitions. Let  be a real-valued function de\ufb01ned over some", "domain . A point  is called a global minimum point of  if  for\nall other . A point  is instead called a local minimum point of a function", "if there exists some constant  such that for all  within the interval de\ufb01ned\nby  , where  is some distance metric, e.g.,\n A global minimum point is also a local minimum point, but a", "local minimum point does not have to be a global minimum point.\n\u2753 Study Question\nWhat happens in this example with very small ? With very big ?", "If  is non-convex (and suf\ufb01ciently smooth), one expects that gradient descent (run\nlong enough with small enough learning rate) will get very close to a point at which", "the gradient is zero, though we cannot guarantee that it will converge to a global\nminimum point.\nThere are two notable exceptions to this common sense expectation: First, gradient", "descent can get stagnated while approaching a point  which is not a local\nminimum or maximum, but satis\ufb01es . For example, for , starting", "gradient descent from the initial guess , while using learning rate \nwill lead to  converging to zero as . Second, there are functions (even", "convex ones) with no minimum points, like , for which gradient\ndescent with a positive learning rate converges to .\nThe plot below shows two different , and how gradient descent started from", "each point heads toward two different local optimum points.f xinit\nf\nD x0\u2208D ff(x0)\u2264f(x)\nx\u2208D x0\u2208D\nf \u03f5>0 x\nd(x,x0)<\u03f5,f(x0)\u2264f(x)d\nd(x,x0)=||x\u2212x0||.\n\u03b7 \u03b7\nf\nx\nf\u2032(x)=0 f(x)=x3\nxinit=1 \u03b7<1/3\nx(k)k\u2192\u221e", "x(k)k\u2192\u221e\nf(x)=exp(\u2212x)\n+\u221e\nxinit \u2212 2 \u2212 1 1 2 3 44681 0\nxf (x )\n3.2 Multiple dimensions\nThe extension to the case of multi-dimensional  is straightforward. Let\u2019s assume\n, so .", ", so .\nThe gradient of  with respect to  is\nThe algorithm remains the same, except that the update step in line 5 becomes", "and any termination criteria that depended on the dimensionality of  would have\nto change. The easiest thing is to keep the test in line 6 as ,\nwhich is sensible no matter the dimensionality of .", "\u2753 Study Question\nWhich termination criteria from the 1D case were de\ufb01ned in a way that assumes\n is one dimensional?\n3.3 Application to regression\u0398\n\u0398\u2208Rmf:Rm\u2192R\nf \u0398\n\u2207\u0398f=\u23a1\n\u23a2\u23a3\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m\u23a4\n\u23a5\u23a6", "\u22ee\n\u2202f/\u2202\u0398m\u23a4\n\u23a5\u23a6\n\u0398(t)=\u0398(t\u22121)\u2212\u03b7\u2207\u0398f(\u0398(t\u22121))\n\u0398\nf(\u0398(t))\u2212f(\u0398(t\u22121))<\u03f5\n\u2223\u2223\u0398\n\u0398 Recall from the previous chapter that choosing a loss function is the \ufb01rst step in", "formulating a machine-learning problem as an optimization problem, and for\nregression we studied the mean square loss, which captures losws as\n. This leads to the ordinary least squares objective", "We use the gradient of the objective with respect to the parameters,\nto obtain an analytical solution to the linear regression problem. Gradient descent", "could also be applied to numerically compute a solution, using the update rule\nNow, let\u2019s add in the regularization term, to get the ridge-regression objective:", "Recall that in ordinary least squares, we \ufb01nessed handling  by adding an extra\ndimension of all 1\u2019s. In ridge regression, we really do need to separate the", "parameter vector  from the offset , and so, from the perspective of our general-\npurpose gradient descent method, our whole parameter set  is de\ufb01ned to be", ". We will go ahead and \ufb01nd the gradients separately for each one:\nNote that  will be of shape  and  will be a scalar since we\nhave separated  from  here.\n\u2753 Study Question(guess\u2212actual)2\nJ(\u03b8)=1\nnn\n\u2211", "J(\u03b8)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)\u2212y(i))2\n.\n\u2207\u03b8J=2\nnXT\nd\u00d7n(X\u03b8\u2212Y)\nn\u00d71,\ue152 \ue154 \ue151\ue150 \ue154 \ue153\ue152 \ue154 \ue151\ue150 \ue154 \ue153(3.1)\n\u03b8(t)=\u03b8(t\u22121)\u2212\u03b72\nnn\n\u2211\ni=1([\u03b8(t\u22121)]T\nx(i)\u2212y(i))x(i).\n3.3.1 Ridge regression\nJridge(\u03b8,\u03b80)=1\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))2", "+\u03bb\u2225\u03b8\u22252.\n\u03b80\n\u03b8 \u03b80\n\u0398\n\u0398=(\u03b8,\u03b80)\n\u2207\u03b8Jridge(\u03b8,\u03b80)=2\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i))x(i)+2\u03bb\u03b8\n\u2202Jridge(\u03b8,\u03b80)\n\u2202\u03b80=2\nnn\n\u2211\ni=1(\u03b8Tx(i)+\u03b80\u2212y(i)).\n\u2207\u03b8Jridge d\u00d71 \u2202Jridge/\u2202\u03b80", "\u03b80\u03b8 Convince yourself that the dimensions of all these quantities are correct, under\nthe assumption that  is . How does  relate to  as discussed for  in the\nprevious section?\n\u2753 Study Question", "\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives\n. What is the shape of ?\n\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives\n.\n\u2753 Study Question", ".\n\u2753 Study Question\nUse these last two results to verify our derivation above.\nPutting everything together, our gradient descent algorithm for ridge regression\nbecomes", "becomes\nprocedure RR-G RADIENT -D ESCENT( )\nrepeat\nuntil \nreturn \nend procedure\n\u2753 Study Question\nIs it okay that  doesn\u2019t appear in line 8?\n\u2753 Study Question", "\u2753 Study Question\nIs it okay that the 2\u2019s from the gradient de\ufb01nitions don\u2019t appear in the\nalgorithm?\u03b8d\u00d71 d m \u0398\n\u2207\u03b8||\u03b8||2\n(\u2202||\u03b8||2/\u2202\u03b81,\u2026,\u2202||\u03b8||2/\u2202\u03b8d) \u2207\u03b8||\u03b8||2\n\u2207\u03b8Jridge(\u03b8Tx+\u03b80,y)", "\u2207\u03b8Jridge(\u03b8Tx+\u03b80,y)\n(\u2202Jridge(\u03b8Tx+\u03b80,y)/\u2202\u03b81,\u2026,\u2202Jridge(\u03b8Tx+\u03b80,y)/\u2202\u03b8d)\n1: \u03b8init,\u03b80init,\u03b7,\u03f5\n2:\u03b8(0)\u2190\u03b8init\n3:\u03b8(0)\n0\u2190\u03b80init\n4:t\u21900\n5:\n6:t\u2190t+1\n7:\u03b8(t)=\u03b8(t\u22121)\u2212\u03b7(1\nn\u2211n\ni=1(\u03b8(t\u22121)Tx(i)+\u03b80(t\u22121)\u2212y(i))x(i)+\u03bb\u03b8(t\u22121))", "8:\u03b8(t)\n0=\u03b8(t\u22121)\n0\u2212\u03b7(1\nn\u2211n\ni=1(\u03b8(t\u22121)Tx(i)+\u03b80(t\u22121)\u2212y(i)))\n9: Jridge(\u03b8(t),\u03b8(t)\n0)\u2212Jridge(\u03b8(t\u22121),\u03b8(t\u22121)\n0)<\u03f5\n\u2223\u222310: \u03b8(t),\u03b8(t)\n0\n11:\n\u03bbBeware double superscripts!  is\nthe transpose of the vector .[\u03b8]T", "\u03b8 3.4 Stochastic gradient descent\nWhen the form of the gradient is a sum, rather than take one big(ish) step in the", "direction of the gradient, we can, instead, randomly select one term of the sum, and\ntake a very small step in that direction. This seems sort of crazy, but remember that", "all the little steps would average out to the same direction as the big step if you\nwere to stay in one place. Of course, you\u2019re not staying in that place, so you move,", "in expectation, in the direction of the gradient.\nMost objective functions in machine learning can end up being written as an", "average over data points, in which case, stochastic gradient descent (sgd) is\nimplemented by picking a data point randomly out of the data set, computing the", "gradient as if there were only that one point in the data set, and taking a small step\nin the negative direction.\nLet\u2019s assume our objective has the form", "where  is the number of data points used in the objective (and this may be\ndifferent from the number of points available in the whole data set).", "Here is pseudocode for applying sgd to such an objective ; it assumes we know the\nform of  for all  in :\nprocedure STOCHAS TIC -G RADIENT -D ESCENT( )\nfor  do\nrandomly select \nend for\nend procedure", "end procedure\nNote that now instead of a \ufb01xed value of ,  is indexed by the iteration of the\nalgorithm, . Choosing a good stopping criterion can be a little trickier for sgd than", "traditional gradient descent. Here we\u2019ve just chosen to stop after a \ufb01xed number of\niterations .\nFor sgd to converge to a local optimum point as  increases, the learning rate has to", "decrease as a function of time. The next result shows one learning rate sequence that\nworks.\nTheorem 3.2 If  is convex, and  is a sequence satisfyingf(\u0398)=1\nnn\n\u2211\ni=1fi(\u0398),\nn\nf\n\u2207\u0398fii1\u2026n", "n\nf\n\u2207\u0398fii1\u2026n\n1: \u0398init,\u03b7,f,\u2207\u0398f1,...,\u2207\u0398fn,T\n2:\u0398(0)\u2190\u0398init\n3:t\u21901\n4: i\u2208{1,2,\u2026,n}\n5: \u0398(t)=\u0398(t\u22121)\u2212\u03b7(t)\u2207\u0398fi(\u0398(t\u22121))\n6:\n7:\n\u03b7\u03b7\nt\nT\nt\nf \u03b7(t)\n\u221e\n\u2211\nt=1\u03b7(t)=\u221eand\u221e\n\u2211\nt=1\u03b7(t)2<\u221e,Sometimes you will see that the", "objective being written as a sum,\ninstead of an average. In the \u201csum\u201d\nconvention, the  normalizing\nconstant is getting \u201cabsorbed\u201d into\nindividual .1\nn\nfi\nf(\u0398)=n\n\u2211", "n\nfi\nf(\u0398)=n\n\u2211\ni=1fi(\u0398). then SGD converges with probability one* to the optimal .*\nWhy these two conditions? The intuition is that the \ufb01rst condition, on , is", "needed to allow for the possibility of an unbounded potential range of exploration,\nwhile the second condition, on , ensures that the learning rates get smaller\nand smaller as  increases.", "One \u201clegal\u201d way of setting the learning rate is to make  but people often\nuse rules that decrease more slowly, and so don\u2019t strictly satisfy the criteria for\nconvergence.\n\u2753 Study Question", "\u2753 Study Question\nIf you start a long way from the optimum, would making  decrease more\nslowly tend to make you move more quickly or more slowly to the optimum?", "There are multiple intuitions for why sgd might be a better choice algorithmically\nthan regular gd (which is sometimes called batch gd (bgd)):", "bgd typically requires computing some quantity over every data point in a data\nset. sgd may perform well after visiting only some of the data. This behavior", "can be useful for very large data sets \u2013 in runtime and memory savings.\nIf your  is actually non-convex, but has many shallow local optimum points", "that might trap bgd, then taking samples from the gradient at some point \nmight \u201cbounce\u201d you around the landscape and away from the local optimum\npoints.", "points.\nSometimes, optimizing  really well is not what we want to do, because it\nmight over\ufb01t the training set; so, in fact, although sgd might not get lower", "training error than bgd, it might result in lower test error.\u0398\n\u2211\u03b7(t)\n\u2211\u03b7(t)2\nt\n\u03b7(t)=1/t\n\u03b7(t)\nf\n\u0398\nf Th is page contains all content from the legacy PDF  notes; classi\ufb01cation chapter.", "As we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .\n4.1 Classi\ufb01cation\nClassi\ufb01cation is a machine learning problem seeking to map from inputs  to", "outputs in an unordered set.\nExamples of classi\ufb01cation output sets could be  if we\u2019re\ntrying to \ufb01gure out what type of fruit we have, or  if", "we\u2019re working in an emergency room and trying to give the best medical care to a\nnew patient. We focus on an essential simple case, binary classi\ufb01cation, where we aim", "to \ufb01nd a mapping from  to two outputs. While we should think of the outputs as\nnot having an order, it\u2019s often convenient to encode them as . As before, let", "the letter  (for hypothesis) represent a classi\ufb01er, so the classi\ufb01cation process looks\nlike:\nLike regression, classi\ufb01cation is a supervised learning problem, in which we are given", "a training data set of the form\nWe will assume that each  is a  column vector. The intended use of this data\nis that, when given an input , the learned hypothesis should generate output \n.", ".\nWhat makes a classi\ufb01er useful? As in regression, we want it to work well on new\ndata, making good predictions on examples it hasn\u2019t seen. But we don\u2019t know", "exactly what data this classi\ufb01er might be tested on when we use it in the real world.\nSo, we have to assume a connection between the training data and testing data;", "typically, they are drawn independently from the same probability distribution.\nIn classi\ufb01cation, we will often use 0-1 loss for evaluation (as discussed in", "Section 1.3). For that choice, we can write the training error and the testing error. In\nparticular, given a training set  and a classi\ufb01er , we de\ufb01ne the training error of \nto be4  Classi\ufb01cation\nNote", "Note\nRd\n{apples,oranges,pears}\n{heartattack,noheartattack}\nRd\n{+1,0}\nh\nx\u2192 \u2192y.h\nDtrain={(x(1),y(1)),\u2026,(x(n),y(n))}.\nx(i)d\u00d71\nx(i)y(i)\nDn h hTh is is in contrast to a continuo us", "real-value d output, as we saw for\nlinear regression.\uf461 4  Classi\ufb01cation \uf52a\n3 For now, we will try to \ufb01nd a classi\ufb01er with small training error (later, with some", "added criteria) and hope it generalizes well to new data, and has a small test error\non  new examples that were not used in the process of \ufb01nding the classi\ufb01er.", "We begin by introducing the hypothesis class of linear classi\ufb01ers (Section 4.2) and\nthen de\ufb01ne an optimization framework to learn linear logistic classi\ufb01ers (Section 4.3).\n4.2 Linear classi\ufb01ers", "We start with the hypothesis class of linear classi\ufb01ers. They are (relatively) easy to\nunderstand, simple in a mathematical sense, powerful on their own, and the basis", "for many other more sophisticated methods. Following their de\ufb01nition, we present\na simple learning algorithm for classi\ufb01ers.\nA linear classi\ufb01er in  dimensions is de\ufb01ned by a vector of parameters  and", "scalar . So, the hypothesis class  of linear classi\ufb01ers in  dimensions is\nparameterized by the set of all vectors in . We\u2019ll assume that  is a \ncolumn vector.", "column vector.\nGiven particular values for  and , the classi\ufb01er is de\ufb01ned by\nRemember that we can think of  as specifying a -dimensional hyperplane", "(compare the above with Equation 2.3). But this time, rather than being interested in\nthat hyperplane\u2019s values at particular points , we will focus on the separator that it", "induces. The separator is the set of  values such that . This is also a\nhyperplane, but in  dimensions! We can interpret  as a vector that is", "perpendicular to the separator. (We will also say that  is normal to the separator.)\nBelow is an embedded demo illustrating the separator and normal vector. Open\ndemo in full screen.Etrain(h)=1\nnn\n\u2211", "nn\n\u2211\ni=1{ .1h(x(i))\u2260y(i)\n0otherwise(4.1)\nEtest(h)=1\nn\u2032n+n\u2032\n\u2211\ni=n+1{1h(x(i))\u2260y(i)\n0otherwise\nn\u2032\n4.2.1 Linear classi\ufb01ers: de\ufb01nition\nd \u03b8\u2208Rd\n\u03b80\u2208R H d\nRd+1\u03b8d\u00d71\n\u03b8\u03b80\nh(x;\u03b8,\u03b80)=step(\u03b8Tx+\u03b80)={ .+1if \u03b8Tx+\u03b80>0", "0 otherwise\n\u03b8,\u03b80 d\nx\nx \u03b8Tx+\u03b80=0\nd\u22121 \u03b8\n\u03b8\nDemo: Linear classi\ufb01er separator \u03b8\u2081:\n0.5\u03b8\u2082:\n0.5\u03b8\u2080:\n0.0Toggle z=0\nSurface\nBuilt with \u2764  by Shen\u00b2  | Report a Bug", "Features (x \u2081 , x \u2082 ) & z = \u03b8 \u2081 x \u2081  + \u03b8 \u2082 x \u2082  + \u03b8 \u2080\u22122\u22121012\u2212505Separ ator\nNormal v ecto\nPrediction: P\nPrediction: NFeature space (x \u2081 , x \u2082\nx \u2081x \u2082", "x \u2081x \u2082  \nFor example, in two dimensions ( ) the separator has dimension 1, which\nmeans it is a line, and the two components of  give the orientation of", "the separator, as illustrated in the following example.\nLet  be the linear classi\ufb01er de\ufb01ned by . Th e diagram below shows the \nvector (in green) and the separator it de\ufb01nes:d=2\n\u03b8=[\u03b81,\u03b82]T", "\u03b8=[\u03b81,\u03b82]T\n4.2.2 Linear classi\ufb01ers: examples\nExample:\nh \u03b8=[],\u03b80=11\n\u22121\u03b8 \u03b8Tx +\u03b8 0 = 0\nx 1x 2\n\u03b8 \u03b8 2\n\u03b8 1\nW hat is ? We can solve for it by plug ging a point on the line into the equa tion for the", "line. It is often convenient to choose a point on one of the axes, e.g., in this case,\n, for which , giving .\nIn this example, the separator divides , the space our  points live in, into two", "half-spaces. The one that is on the same side as the normal vector is the positive half-\nspace, and we classify all points in that space as positive. The half-space on the", "other side is negative and all points in it are classi\ufb01ed as negative.\nNote that we will call a separator a linear separator of a data set if all of the data with", "one label falls on one side of the separator and all of the data with the other label\nfalls on the other side of the separator. For instance, the separator in the next", "example is a linear separator for the illustrated data. If there exists a linear separator\non a dataset, we call this dataset linearly separable.\nLet  be the linear classi\ufb01er de\ufb01ned by .", "Th e diagram below shows several points classi\ufb01ed by . In particular, let  and\n.\u03b80\nx=[0,1]T\u03b8T[]+\u03b80=00\n1\u03b80=1\nRdx(i)\nExample:\nh \u03b8=[],\u03b80=3\u22121\n1.5\nh x(1)=[]3\n2\nx(2)=[]4\n\u22121", "2\nx(2)=[]4\n\u22121\n( [] ) Th us ,  and  are given positive (label +1) and negative (label 0) classi\ufb01cations,\nrespectively.\n\u2753 Study Question", "\u2753 Study Question\nWhat is the green vector normal to the separator? Specify it as a column vector.\n\u2753 Study Question\nWhat change would you have to make to  if you wanted to have the", "separating hyperplane in the same place, but to classify all the points labeled \u2018+\u2019\nin the diagram as negative and all the points labeled \u2018-\u2019 in the diagram as\npositive?", "positive?\n4.3 Linear logistic classi\ufb01ers\nGiven a data set and the hypothesis class of linear classi\ufb01ers, our goal will be to \ufb01nd", "the linear classi\ufb01er that optimizes an objective function relating its predictions to\nthe training data. To make this problem computationally reasonable, we will need", "to take care in how we formulate the optimization problem to achieve this goal.\nFor classi\ufb01cation, it is natural to make predictions in  and use the 0-1 loss", "function, , as introduced in Chapter 1:h(x(1);\u03b8,\u03b80)=step([ ][]+3)=step(3)=+1\nh(x(2);\u03b8,\u03b80)=step([ ][]+3)=step(\u22122.5)=0\u221211.53\n2\n\u221211.54\n\u22121\nx(1)x(2)\n\u03b8,\u03b80\n{+1,0}\nL01\nL01(g,a)={ .0if g=a", "L01(g,a)={ .0if g=a\n1otherwise However, even for simple linear classi\ufb01ers, it is very dif\ufb01cult to \ufb01nd values for \nthat minimize simple 0-1 training error", "This problem is NP-hard, which probably implies that solving the most dif\ufb01cult\ninstances of this problem would require computation time exponential in the number\nof training examples, .", "What makes this a dif\ufb01cult optimization problem is its lack of \u201csmoothness\u201d:\nThere can be two hypotheses,  and , where one is closer in", "parameter space to the optimal parameter values , but they make the\nsame number of misclassi\ufb01cations so they have the same  value.", "All predictions are categorical: the classi\ufb01er can\u2019t express a degree of certainty\nabout whether a particular input  should have an associated value .", "For these reasons, if we are considering a hypothesis  that makes \ufb01ve incorrect\npredictions, it is dif\ufb01cult to see how we might change  so that it will perform", "better, which makes it dif\ufb01cult to design an algorithm that searches in a sensible\nway through the space of hypotheses for a good one. For these reasons, we", "investigate another hypothesis class: linear logistic classi\ufb01ers, providing their\nde\ufb01nition, then an approach for learning such classi\ufb01ers using optimization.", "The hypotheses in a linear logistic classi\ufb01er (LLC) are parameterized by a -\ndimensional vector  and a scalar , just as is the case for linear classi\ufb01ers.", "However, instead of making predictions in , LLC hypotheses generate real-\nvalued outputs in the interval . An LLC has the form\nThis looks familiar! What\u2019s new?", "The logistic function, also known as the sigmoid function, is de\ufb01ned as\nand is plotted below, as a function of its input . Its output can be interpreted as a", "probability, because for any value of  the output is in .\u03b8,\u03b80\nJ(\u03b8,\u03b80)=1\nnn\n\u2211\ni=1L01(step(\u03b8Tx(i)+\u03b80),y(i)).\nn\n(\u03b8,\u03b80) (\u03b8\u2032,\u03b8\u2032\n0)\n(\u03b8\u2217,\u03b8\u2217\n0)\nJ\nx y\n\u03b8,\u03b80\n\u03b8,\u03b80\n4.3.1 Linear logistic classi\ufb01ers: de\ufb01nition\nd", "d\n\u03b8 \u03b80\n{+1,0}\n(0,1)\nh(x;\u03b8,\u03b80)=\u03c3(\u03b8Tx+\u03b80).\n\u03c3(z)=1\n1+e\u2212z,\nz\nz (0,1)Th e \u201cprobably\u201d here is not becaus e\nwe\u2019re too lazy to look it up , but\nactua lly becaus e of a fun damental", "un solved problem in computer-\nscience theory, known as \u201cP\nvs. NP.\u201d \u2212 4 \u2212 2 2 40. 51\nz\u03c3 (z )\n\u2753 Study Question\nConvince yourself the output of  is always in the interval . Why can\u2019t it", "equal 0 or equal 1? For what value of  does ?\nWhat does an LLC look like? Let\u2019s consider the simple case where , so our", "input points simply lie along the  axis. Classi\ufb01ers in this case have dimension ,\nmeaning that they are points. The plot below shows LLCs for three different\nparameter settings: , , and", "\u2212 4 \u2212 2 2 40. 51\nx\u03c3 (\u03b8Tx +\u03b8 0 )\n\u2753 Study Question\nWhich plot is which? What governs the steepness of the curve? What governs\nthe  value where the output is equal to 0.5?", "But wait! Remember that the de\ufb01nition of a classi\ufb01er is that it\u2019s a mapping from\n or to some other discrete set. So, then, it seems like an LLC is actually\nnot a classi\ufb01er!", "not a classi\ufb01er!\nGiven an LLC, with an output value in , what should we do if we are forced to\nmake a prediction in ? A default answer is to predict  if\u03c3 (0,1)\nz\u03c3(z)=0.5", "z\u03c3(z)=0.5\n4.3.2 Linear logistic classi\ufb01er: examples\nd=1\nx 0\n\u03c3(10x+1)\u03c3(\u22122x+1)\u03c3(2x\u22123).\nx\nRd\u2192{+1,0}\n(0,1)\n{+1,0} +1  and  otherwise. The value  is sometimes called a prediction\nthreshold.", "threshold.\nIn fact, for different problem settings, we might prefer to pick a different prediction\nthreshold. The \ufb01eld of decision theory considers how to make this choice. For", "example, if the consequences of predicting  when the answer should be  are\nmuch worse than the consequences of predicting  when the answer should be", ", then we might set the prediction threshold to be greater than .\n\u2753 Study Question\nUsing a prediction threshold of 0.5, for what values of  do each of the LLCs\nshown in the \ufb01gure above predict ?", "When , then our inputs  lie in a two-dimensional space with axes  and ,\nand the output of the LLC is a surface, as shown below, for .\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that the set of points for which , that is, the\n``boundary\u2019\u2019 between positive and negative predictions with prediction", "threshold , is a line in  space. What particular line is it for the case in\nthe \ufb01gure above? How would the plot change for , but now with\n? For ?", "? For ?\nOptimization is a key approach to solving machine learning problems; this also\napplies to learning linear logistic classi\ufb01ers (LLCs) by de\ufb01ning an appropriate loss", "function for optimization. A \ufb01rst attempt might be to use the simple 0-1 loss\u03c3(\u03b8Tx+\u03b80)>0.5 0 0.5\n+1 \u22121\n\u22121 +1\n0.5\nx\n+1\nd=2 x x1x2\n\u03b8=(1,1),\u03b80=2\n\u03c3(\u03b8Tx+\u03b80)=0.5\n0.5 (x1,x2)\n\u03b8=(1,1)\n\u03b80=\u22122\u03b8=(\u22121,\u22121),\u03b80=2", "\u03b80=\u22122\u03b8=(\u22121,\u22121),\u03b80=2\n4.3.3 Learning linear logistic classi\ufb01ers function  that gives a value of 0 for a correct prediction, and a 1 for an incorrect", "prediction. As noted earlier, however, this gives rise to an objective function that is\nvery dif\ufb01cult to optimize, and so we pursue another strategy for de\ufb01ning our\nobjective.", "objective.\nFor learning LLCs, we\u2019d have a class of hypotheses whose outputs are in , but\nfor which we have training data with  values in . How can we de\ufb01ne an", "appropriate loss function? We start by changing our interpretation of the output to\nbe the probability that the input should map to output value 1 (we might also say that", "this is the probability that the input is in class 1 or that the input is \u2018positive.\u2019)\n\u2753 Study Question\nIf  is the probability that  belongs to class , what is the probability that", "belongs to the class , assuming there are only these two classes?\nIntuitively, we would like to have low loss if we assign a high probability to the correct", "class. We\u2019ll de\ufb01ne a loss function, called negative log-likelihood (NLL), that does just\nthis. In addition, it has the cool property that it extends nicely to the case where we", "would like to classify our inputs into more than two classes.\nIn order to simplify the description, we assume that (or transform our data so that)\nthe labels in the training data are .", "We would like to pick the parameters of our classi\ufb01er to maximize the probability\nassigned by the LLC to the correct  values, as speci\ufb01ed in the training set. Letting\nguess , that probability is", "under the assumption that our predictions are independent. This can be cleverly\nrewritten, when , as\n\u2753 Study Question\nBe sure you can see why these two expressions are the same.", "The big product above is kind of hard to deal with in practice, though. So what can\nwe do? Because the log function is monotonic, the  that maximize the quantityL01\n(0,1)\ny {+1,0}\nh(x) x +1\nx \u22121", "h(x) x +1\nx \u22121\ny\u2208{0,1}\ny\ng(i)=\u03c3(\u03b8Tx(i)+\u03b80)\nn\n\u220f\ni=1{ ,g(i)if y(i)=1\n1\u2212g(i)otherwise\ny(i)\u2208{0,1}\nn\n\u220f\ni=1g(i)y(i)\n(1\u2212g(i))1\u2212y(i).\n\u03b8,\u03b80Remember to be sur e your   value s", "have this form if you try to learn an\nLLC us ing NLL!y\nTh at crazy hug e  represents\ntaking the produc t over a bun ch of\nfactors jus t as hug e  represents\ntaking the sum  over a bun ch of\nterms.\u03a0", "terms.\u03a0\n\u03a3 above will be the same as the  that maximize its log, which is the following:\nFinally, we can turn the maximization problem above into a minimization problem", "by taking the negative of the above expression, and writing in terms of minimizing\na loss\nwhere  is the negative log-likelihood loss function:", "This loss function is also sometimes referred to as the log loss or cross entropy. and it\nwon\u2019t make any real difference. If we ask you for numbers, use log base .", "What is the objective function for linear logistic classi\ufb01cation? We can \ufb01nally put\nall these pieces together and develop an objective function for optimizing", "regularized negative log-likelihood for a linear logistic classi\ufb01er. In fact, this process\nis usually called \u201clogistic regression,\u201d so we\u2019ll call our objective , and de\ufb01ne it as\n\u2753 Study Question", "\u2753 Study Question\nConsider the case of linearly separable data. What will the  values that\noptimize this objective be like if ? What will they be like if  is very big?", "Try to work out an example in one dimension with two data points.\nWhat role does regularization play for classi\ufb01ers? This objective function has the", "same structure as the one we used for regression, Equation 2.2, where the \ufb01rst term\n(in parentheses) is the average loss, and the second term is for regularization.", "Regularization is needed for building classi\ufb01ers that can generalize well (just as was\nthe case for regression). The parameter  governs the trade-off between the two", "terms as illustrated in the following example.\nSuppose we wish to obtain a linear logistic classi\ufb01er for this one-dimensional\ndataset:\u03b8,\u03b80\nn\n\u2211\ni=1(y(i)logg(i)+(1\u2212y(i))log(1\u2212g(i))).\nn\n\u2211", "n\n\u2211\ni=1Lnll(g(i),y(i))\nLnll\nLnll(guess,actual)=\u2212(actual\u22c5log(guess)+(1\u2212actual)\u22c5log(1\u2212guess)).\ne\nJlr\nJlr(\u03b8,\u03b80;D)=(1\nnn\n\u2211\ni=1Lnll(\u03c3(\u03b8Tx(i)+\u03b80),y(i)))+\u03bb\u2225\u03b8\u22252. (4.2)\n\u03b8\n\u03bb=0 \u03bb", "\u03b8\n\u03bb=0 \u03bb\n\u03bb Clearly, this can be \ufb01t very nicely by a hypothesis , but what is the best\nvalue for ? Evidently, when there is no regularization ( ), the objective", "function  will approach zero for large values of , as shown in the plot on the\nleft, below. However, would the best hypothesis really have an in\ufb01nite (or very", "large) value for ? Such a hypothesis would suggest that the data indicate strong\ncertainty that a sharp transition between  and  occurs exactly at ,\ndespite the actual data having a wide gap around .", "In absence of other beliefs about the solution, we might prefer that our linear\nlogistic classi\ufb01er not be overly certain about its predictions, and so we might prefer", "a smaller  over a large  By not being overcon\ufb01dent, we might expect a somewhat\nsmaller  to perform better on future examples drawn from this same distribution.", "This preference can be realized using a nonzero value of the regularization trade-off\nparameter, as illustrated in the plot on the right, above, with .", "Another nice way of thinking about regularization is that we would like to prevent\nour hypothesis from being too dependent on the particular training data that we", "were given: we would like for it to be the case that if the training data were changed\nslightly, the hypothesis would not change by much.\n4.4 Gradient descent for logistic regression", "Now that we have a hypothesis class (LLC) and a loss function (NLL), we need to\ntake some data and \ufb01nd parameters! Sadly, there is no lovely analytical solution like", "the one we obtained for regression, in Section 2.7.2. Good thing we studied gradient\nh(x)=\u03c3(\u03b8x)\n\u03b8 \u03bb=0\nJlr(\u03b8) \u03b8\n\u03b8\ny=0y=1 x=0\nx=0\n\u03b8 \u03b8.\n\u03b8", "x=0\n\u03b8 \u03b8.\n\u03b8\n\u03bb=0.2 descent! We can perform gradient descent on the  objective, as we\u2019ll see next. We\ncan also apply stochastic gradient descent to this problem.", "Luckily,  has enough nice properties that gradient descent and stochastic\ngradient descent should generally \u201cwork\u201d. We\u2019ll soon see some more challenging", "optimization problems though \u2013 in the context of neural networks, in Section 6.7.\nFirst we need derivatives with respect to both  (the scalar component) and  (the", "vector component) of . Explicitly, they are:\nNote that  will be of shape  and  will be a scalar since we have\nseparated  from  here.", "Putting everything together, our gradient descent algorithm for logistic regression\nbecomes:\n\u2753 Study Question\nConvince yourself that the dimensions of all these quantities are correct, under", "the assumption that  is .\n\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives .\nWhat is the shape of ?\n\u2753 Study Question\nCompute  by \ufb01nding the vector of partial derivatives\n.", ".\n\u2753 Study Question\nUse these last two results to verify our derivation above.\nAlgorithm 4.1 LR-Gradient-Descent( )\nrepeatJlr\nJlr\n\u03b80 \u03b8\n\u0398\n\u2207\u03b8Jlr(\u03b8,\u03b80)=1\nnn\n\u2211\ni=1(g(i)\u2212y(i))x(i)+2\u03bb\u03b8\n\u2202Jlr(\u03b8,\u03b80)\n\u2202\u03b80=1\nnn\n\u2211", "\u2202\u03b80=1\nnn\n\u2211\ni=1(g(i)\u2212y(i)).\n\u2207\u03b8Jlr d\u00d71\u2202Jlr\n\u2202\u03b80\n\u03b80\u03b8\n\u03b8d\u00d71\n\u2207\u03b8\u2225\u03b8\u22252(\u2202\u2225\u03b8\u22252\n\u2202\u03b81,\u2026,\u2202\u2225\u03b8\u22252\n\u2202\u03b8d)\n\u2207\u03b8\u2225\u03b8\u22252\n\u2207\u03b8Lnll(\u03c3(\u03b8Tx+\u03b80),y)\n(\u2202Lnll(\u03c3(\u03b8Tx+\u03b80),y)\n\u2202\u03b81,\u2026,\u2202Lnll(\u03c3(\u03b8Tx+\u03b80),y)\n\u2202\u03b8d)\n\u03b8init,\u03b80init,\u03b7,\u03f5\n1:\u03b8(0)\u2190\u03b8init\n2:\u03b8(0)", "1:\u03b8(0)\u2190\u03b8init\n2:\u03b8(0)\n0\u2190\u03b80init\n3:t\u21900\n4: until \nreturn \nLogistic regression, implemented using batch or stochastic gradient descent, is a", "useful and fundamental machine learning technique. We will also see later that it\ncorresponds to a one-layer neural network with a sigmoidal activation function,", "and so is an important step toward understanding neural networks.\nMuch like the squared-error loss function that we saw for linear regression, the NLL", "loss function for linear logistic regression is a convex function of the parameters \nand  (below is a proof if you\u2019re interested). This means that running gradient", "descent with a reasonable set of hyperparameters will behave nicely.\n4.5 Handling multiple classes\nSo far, we have focused on the binary classi\ufb01cation case, with only two possible", "classes. But what can we do if we have multiple possible classes (e.g., we want to\npredict the genre of a movie)? There are two basic strategies:", "Train multiple binary classi\ufb01ers using different subsets of our data and\ncombine their outputs to make a class prediction.\nDirectly train a multi-class classi\ufb01er using a hypothesis class that is a", "generalization of logistic regression, using a one-hot output encoding and NLL\nloss.\nThe method based on NLL is in wider use, especially in the context of neural", "networks, and is explored here. In the following, we will assume that we have a\ndata set  in which the inputs  but the outputs  are drawn from a set of", "classes . Next, we extend the idea of NLL directly to multi-class\nclassi\ufb01cation with  classes, where the training label is represented with what is", "called a one-hot vector , where  if the example is of class \nand  otherwise. Now, we have a problem of mapping an input  that is in", "into a -dimensional output. Furthermore, we would like this output to be\ninterpretable as a discrete probability distribution over the possible classes, which5:t\u2190t+1\n6:\u03b8(t)\u2190\u03b8(t\u22121)\u2212\u03b7(1\nn\u2211n", "n\u2211n\ni=1(\u03c3(\u03b8(t\u22121)Tx(i)+\u03b8(t\u22121)\n0)\u2212y(i))x(i)+2\u03bb\u03b8(t\u22121))\n7:\u03b8(t)\n0\u2190\u03b8(t\u22121)\n0\u2212\u03b7(1\nn\u2211n\ni=1(\u03c3(\u03b8(t\u22121)Tx(i)+\u03b8(t\u22121)\n0)\u2212y(i)))\n8:Jlr(\u03b8(t),\u03b8(t)\n0)\u2212Jlr(\u03b8(t\u22121),\u03b8(t\u22121)\n0)<\u03f5\n\u2223\u22239: \u03b8(t),\u03b8(t)\n0", "\u2223\u22239: \u03b8(t),\u03b8(t)\n0\n4.4.1 Convexity of the NLL Loss Function\n\u03b8\n\u03b80\nProof of convexity of the NLL loss function\nD x(i)\u2208Rdy(i)\nK {c1,\u2026,cK}\nK\ny=[ ]Ty1,\u2026,yK yk=1 k\nyk=0 x(i)", "yk=0 x(i)\nRdK means the elements of the output vector have to be non-negative (greater than or\nequal to 0) and sum to 1.", "We will do this in two steps. First, we will map our input  into a vector value\n by letting  be a whole  matrix of parameters, and  be a \nvector, so that", "vector, so that\nNext, we have to extend our use of the sigmoid function to the multi-dimensional\nsoftmax function, that takes a whole vector  and generates", "which can be interpreted as a probability distribution over  items. To make the\n\ufb01nal prediction of the class label, we can then look at  \ufb01nd the most likely", "probability over these  entries in  (i.e. \ufb01nd the largest entry in ) and return the\ncorresponding index as the \u201cone-hot\u201d element of  in our prediction.\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that the vector of  values will be non-negative and sum to 1.\nPutting these steps together, our hypotheses will be", "Now, we retain the goal of maximizing the probability that our hypothesis assigns\nto the correct output  for each input . We can write this probability, letting", "stand for our \u201cguess\u201d, , for a single example  as .\n\u2753 Study Question\nHow many elements that are not equal to 1 will there be in this product?", "The negative log of the probability that we are making a correct guess is, then, for\none-hot vector  and probability distribution vector ,", "We\u2019ll call this nllm for negative log likelihood multiclass. It is also worth noting that the\nNLLM loss function is also convex; however, we will omit the proof.x(i)\nz(i)\u2208RK\u03b8 d\u00d7K \u03b80K\u00d71\nz=\u03b8Tx+\u03b80.\nz\u2208RK", "z=\u03b8Tx+\u03b80.\nz\u2208RK\ng=softmax(z)= .\u23a1\n\u23a2\u23a3exp(z1)/\u2211iexp(zi)\n\u22ee\nexp(zK)/\u2211iexp(zi)\u23a4\n\u23a5\u23a6\nK\ng,\nK g, g,\n1\ng\nh(x;\u03b8,\u03b80)=softmax(\u03b8Tx+\u03b80).\nyk x g\nh(x) (x,y)\u220fK\nk=1gyk\nk\ny g\nLnllm(g,y)=\u2212K\n\u2211", "y g\nLnllm(g,y)=\u2212K\n\u2211\nk=1yk\u22c5log(gk).Let\u2019s check dimensions!  is\n and  is , and  is\n, so  is  and we\u2019re\ngood!\u03b8T\nK\u00d7dxd\u00d71\u03b80\nK\u00d71zK\u00d71 \u2753 Study Question", "Be sure you see that is  is minimized when the guess assigns high\nprobability to the true class.\n\u2753 Study Question\nShow that  for  is the same as .\n4.6 Prediction accuracy and validation", "In order to formulate classi\ufb01cation with a smooth objective function that we can\noptimize robustly using gradient descent, we changed the output from discrete", "classes to probability values and the loss function from 0-1 loss to NLL. However,\nwhen time comes to actually make a prediction we usually have to make a hard", "choice: buy stock in Acme or not? And, we get rewarded if we guessed right,\nindependent of how sure or not we were when we made the guess.", "The performance of a classi\ufb01er is often characterized by its accuracy, which is the\npercentage of a data set that it predicts correctly in the case of 0-1 loss. We can see", "that accuracy of hypothesis  on data  is the fraction of the data set that does not\nincur any loss:\nwhere  is the \ufb01nal guess for one class or the other that we make from ,", "e.g., after thresholding. It\u2019s noteworthy here that we use a different loss function for\noptimization than for evaluation. This is a compromise we make for computational\nease and ef\ufb01ciency.Lnllm", "LnllmK=2 Lnll\nh D\nA(h;D)=1\u22121\nnn\n\u2211\ni=1L01(g(i),y(i)),\ng(i)h(x(i)) Th is page contains all content from the legacy PDF  notes; featur es chapter.", "As we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .\nLinear regression and classi\ufb01cation are powerful tools, but in the real world, data", "often exhibit non-linear behavior that cannot immediately be captured by the linear\nmodels which we have built so far. For example, suppose the true behavior of a", "system (with ) looks like this wavelet:\nSuch behavior is actually ubiquitous in physical systems, e.g., in the vibrations of", "the surface of a drum, or scattering of light through an aperture. However, no single\nhyperplane would be a very good \ufb01t to such peaked responses!", "A richer class of hypotheses can be obtained by performing a non-linear feature\ntransformation  before doing the regression. That is,  is a linear", "function of , but  is a non-linear function of  if  is a non-linear\nfunction of .\nThere are many different ways to construct . Some are relatively systematic and", "domain independent. Others are directly related to the semantics (meaning) of the\noriginal features, and we construct them deliberately with our application (goal) in\nmind.", "mind.\n5.1 Gaining intuition about feature\ntransformations\nIn this section, we explore the effects of non-linear feature transformations on\nsimple classi\ufb01cation problems, to gain intuition.", "Let\u2019s look at an example data set that starts in 1-D:5  Feature Representation\nNote\nd=2\n\u03d5(x) \u03b8Tx+\u03b80\nx\u03b8T\u03d5(x)+\u03b80 x,\u03d5\nx\n\u03d5\uf461 5  Feature Representation \uf52a x\n0", "0\nThese points are not linearly separable, but consider the transformation\n. Plotting this transformed data (in two-dimensional space, since", "there are now two features), we see that it is now separable. There are lots of\npossible separators; we have just shown one of them here.\nxx2\ns e p a r a t o r", "s e p a r a t o r\nA linear separator in  space is a nonlinear separator in the original space! Let\u2019s see\nhow this plays out in our simple example. Consider the separator", "(which corresponds to  and  in our transformed space), which\nlabels the half-plane  as positive. What separator does it correspond to in", "the original 1-D space? We have to ask the question: which  values have the\nproperty that . The answer is  and , so those two points constitute", "our separator, back in the original space. Similarly, by evaluating where \nand where , we can \ufb01nd the regions of 1D space that are labeled positive\nand negative (respectively) by this separator.", "Example\u03d5(x)=[x,x2]T\nExample\n\u03d5\nx2\u22121=0\n\u03b8=[0,1]T\u03b80=\u22121\nx2\u22121>0\nx\nx2\u22121=0 +1 \u22121\nx2\u22121>0\nx2\u22121<0\nExample x\n01 - 1\n5.2 Systematic feature construction", "Here are two different ways to systematically construct features in a problem\nindependent way.\nIf the features in your problem are already naturally numerical, one systematic", "strategy for constructing a new feature space is to use a polynomial basis. The idea is\nthat, if you are using the th-order basis (where  is a positive integer), you include", "a feature for every possible product of  different dimensions in your original input.\nHere is a table illustrating the th order polynomial basis for different values of ,", "calling out the cases when  and :\nOrder in general ( )\n0\n1\n2\n3\n\u22ee \u22ee \u22ee\nThis transformation can be used in combination with linear regression or logistic", "regression (or any other regression or classi\ufb01cation model). When we\u2019re using a\nlinear regression or classi\ufb01cation model, the key insight is that a linear regressor or", "separator in the transformed space is a non-linear regressor or separator in the\noriginal space.\nTo give a regression example, the wavelet pictured at the start of this chapter can be", "\ufb01t much better using a polynomial feature representation up to order ,\ncompared to just using a simple hyperplane in the original (single-dimensional)\nfeature space:\n5.2.1 Polynomial basisk k\nk\nk k", "k\nk k\nd=1d>1\nd=1 d>1\n[1] [1]\n[1,x]T[1,x1,\u2026,xd]T\n[1,x,x2]T[1,x1,\u2026,xd,x2\n1,x1x2,\u2026]T\n[1,x,x2,x3]T[1,x1,\u2026,xd,x2\n1,x1x2,\u2026,x3\n1,x1x2\n2,x1x2x3,\u2026]T", "1,x1x2\n2,x1x2x3,\u2026]T\nk=8 The raw data (with  random samples) is plotted on the left, and the\nregression result (curved surface) is on the right.", "Now let\u2019s look at a classi\ufb01cation example and see how polynomial feature\ntransformation may help us.\nOne well-known example is the \u201cexclusive or\u201d (xor) data set, the drosophila of", "machine-learning data sets:\nClearly, this data set is not linearly separable. So, what if we try to solve the xor\nclassi\ufb01cation problem using a polynomial basis as the feature transformation? We", "can just take our two-dimensional data and transform it into a higher-dimensional\ndata set, by applying some feature transformation . Now, we have a classi\ufb01cation\nproblem as usual.", "problem as usual.\nLet\u2019s try it for  on our xor problem. The feature transformation is\n\u2753  Study Question\nIf we train a classi\ufb01er after performing this feature transformation, would we", "lose any expressive power if we let  (i.e., trained without offset instead of\nwith offset)?\nWe might run a classi\ufb01cation learning algorithm and \ufb01nd a separator with", "coef\ufb01cients  and . This corresponds to\nn=1000\nExample\n\u03d5\nk=2\n\u03d5([x1,x2]T)=[1,x1,x2,x2\n1,x1x2,x2\n2]T.\n\u03b80=0\n\u03b8=[0,0,0,0,4,0]T\u03b80=0\n2 2D. Melanogaster is a species of\nfruit \ufb02y, used as a simple system in", "which to study genetics, since 1910. and is plotted below, with the gray shaded region classi\ufb01ed as negative and the\nwhite region classi\ufb01ed as positive:\n\u2753  Study Question", "\u2753  Study Question\nBe sure you understand why this high-dimensional hyperplane is a separator,\nand how it corresponds to the \ufb01gure.", "For fun, we show some more plots below. Here is another result for a linear\nclassi\ufb01er on xor generated with logistic regression and gradient descent, using a", "random initial starting point and second-order polynomial basis:\nHere is a harder data set. Logistic regression with gradient descent failed to", "separate it with a second, third, or fourth-order basis feature representation, but0+0x1+0x2+0x2\n1+4x1x2+0x2\n2+0=0\nExample\nExample", "Example\nExample\n succeeded with a \ufb01fth-order basis. Shown below are some results after \ngradient descent iterations (from random starting points) for bases of order 2", "(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).\n\u2753  Study Question\nPercy Eptron has a domain with four numeric input features, . He\ndecides to use a representation of the form", "where  means the vector  concatenated with the vector .\nWhat is the dimension of Percy\u2019s representation? Under what assumptions\nabout the original features is this a reasonable choice?", "Another cool idea is to use the training data itself to construct a feature space. The\nidea works as follows. For any particular point  in the input space , we can\u223c1000\nExample\n(x1,\u2026,x4)", "Example\n(x1,\u2026,x4)\n\u03d5(x)=PolyBasis((x1,x2),3)\u2322PolyBasis((x3,x4),3)\na\u2322b a b\n5.2.2 (Optional) Radial basis functions\np X construct a feature  which takes any element  and returns a scalar value", "that is related to how far  is from the  we started with.\nLet\u2019s start with the basic case, in which . Then we can de\ufb01ne\nThis function is maximized when  and decreases exponentially as  becomes", "more distant from .\nThe parameter  governs how quickly the feature value decays as we move away\nfrom the center point . For large values of , the  values are nearly 0 almost", "everywhere except right near ; for small values of , the features have a high value\nover a larger part of the space.\nNow, given a dataset  containing  points, we can make a feature transformation", "that maps points in our original space, , into points in a new space, . It is\nde\ufb01ned as follows:\nSo, we represent a new datapoint  in terms of how far it is from each of the", "datapoints in our training set.\nThis idea can be generalized in several ways and is the fundamental concept\nunderlying kernel methods, that are not directly covered in this class but we", "recommend you read about some time. This idea of describing objects in terms of\ntheir similarity to a set of reference objects is very powerful and can be applied to", "cases where  is not a simple vector space, but where the inputs are graphs or\nstrings or other types of objects, as long as there is a distance metric de\ufb01ned on the\ninput space.", "input space.\n5.3 (Optional) Hand-constructing features for real\ndomains\nIn many machine-learning applications, we are given descriptions of the inputs", "with many different types of attributes, including numbers, words, and discrete\nfeatures. An important factor in the success of an ML application is the way that the", "features are chosen to be encoded by the human who is framing the learning\nproblem.\nGetting a good encoding of discrete features is particularly important. You want to", "create \u201copportunities\u201d for the ML system to \ufb01nd the underlying patterns. Although\nthere are machine-learning methods that have special mechanisms for handling", "discrete inputs, most of the methods we consider in this class will assume the inputfp x\u2208X\nx p\nX=Rd\nfp(x)=e\u2212\u03b2\u2225p\u2212x\u22252.\np=x x\np\n\u03b2\np \u03b2fp\np \u03b2\nD n\n\u03d5 RdRn\n\u03d5(x)=[fx(1)(x),fx(2)(x),\u2026,fx(n)(x)]T.\nx\nX", "x\nX\n5.3.1 Discrete features vectors  are in . So, we have to \ufb01gure out some reasonable strategies for turning\ndiscrete values into (vectors of) real numbers.", "We\u2019ll start by listing some encoding strategies, and then work through some\nexamples. Let\u2019s assume we have some feature in our raw data that can take on one\nof  discrete values.", "Numeric: Assign each of these values a number, say . We\nmight want to then do some further processing, as described in Section 1.3.3.", "This is a sensible strategy only when the discrete values really do signify some\nsort of numeric quantity, so that these numerical values are meaningful.", "Thermometer code: If your discrete values have a natural ordering, from\n, but not a natural mapping into real numbers, a good strategy is to use", "a vector of length  binary variables, where we convert discrete input value\n into a vector in which the \ufb01rst  values are  and the rest are .", "This does not necessarily imply anything about the spacing or numerical\nquantities of the inputs, but does convey something about ordering.", "Factored code: If your discrete values can sensibly be decomposed into two\nparts (say the \u201cmaker\u201d and \u201cmodel\u201d of a car), then it\u2019s best to treat those as two", "separate features, and choose an appropriate encoding of each one from this\nlist.\nOne-hot code: If there is no obvious numeric, ordering, or factorial structure,", "then the best strategy is to use a vector of length , where we convert discrete\ninput value  into a vector in which all values are , except for the \nth, which is .", "th, which is .\nBinary code: It might be tempting for the computer scientists among us to use\nsome binary code, which would let us represent  values using a vector of", "length . This is a bad idea! Decoding a binary code takes a lot of work, and\nby encoding your inputs this way, you\u2019d be forcing your system to learn the\ndecoding algorithm.", "decoding algorithm.\nAs an example, imagine that we want to encode blood types, that are drawn from\nthe set . There is no obvious linear", "numeric scaling or even ordering to this set. But there is a reasonable factoring, into\ntwo features:  and . And, in fact, we can further reasonably", "factor the \ufb01rst group into , . So, here are two plausible\nencodings of the whole set:\nUse a 6-D vector, with two components of the vector each encoding the", "corresponding factor using a one-hot encoding.\nUse a 3-D vector, with one dimension for each factor, encoding its presence as\n and absence as  (this is sometimes better than ). In this case,", "would be  and  would be .x Rd\nk\n1.0/k,2.0/k,\u2026,1.0\n1,\u2026,k\nk\n0<j\u2264k j 1.0 0.0\nk\n0<j\u2264k 0.0 j\n1.0\nk\nlogk\n{A+,A\u2212,B+,B\u2212,AB+,AB\u2212,O+,O\u2212}\n{A,B,AB,O} {+,\u2212}\n{A,notA}{B,notB}\n1.0 \u22121.0 0.0 AB+", "1.0 \u22121.0 0.0 AB+\n[1.0,1.0,1.0]TO\u2212 [\u22121.0,\u22121.0,\u22121.0]T \u2753  Study Question\nHow would you encode  in both of these approaches?", "The problem of taking a text (such as a tweet or a product review, or even this\ndocument!) and encoding it as an input for a machine-learning algorithm is", "interesting and complicated. Much later in the class, we\u2019ll study sequential input\nmodels, where, rather than having to encode a text as a \ufb01xed-length feature vector,", "we feed it into a hypothesis word by word (or even character by character!).\nThere are some simple encodings that work well for basic applications. One of them", "is the bag of words (bow) model, which can be used to encode documents. The idea is\nto let  be the number of words in our vocabulary (either computed from the", "training set or some other body of text or dictionary). We will then make a binary\nvector (with values  and ) of length , where element  has value  if word \noccurs in the document, and  otherwise.", "If some feature is already encoded as a numeric value (heart rate, stock price,\ndistance, etc.) then we should generally keep it as a numeric value. An exception", "might be a situation in which we know there are natural \u201cbreakpoints\u201d in the\nsemantics: for example, encoding someone\u2019s age in the US, we might make an", "explicit distinction between under and over 18 (or 21), depending on what kind of\nthing we are trying to predict. It might make sense to divide into discrete bins", "(possibly spacing them closer together for the very young) and to use a one-hot\nencoding for some sorts of medical situations in which we don\u2019t expect a linear (or", "even monotonic) relationship between age and some physiological features.\n\u2753  Study Question\nConsider using a polynomial basis of order  as a feature transformation  on", "our data. Would increasing  tend to increase or decrease structural error? What\nabout estimation error?A+\n5.3.2 Text\nd\n1.0 0.0 d j 1.0 j\n0.0\n5.3.3 Numeric values\nk \u03d5", "k \u03d5\nk Th is page contains all content from the legacy PDF  notes; neur al networks chapter.\nAs we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .", "You\u2019ve probably been hearing a lot about \u201cneural networks.\u201d Now that we have\nseveral useful machine-learning concepts (hypothesis classes, classi\ufb01cation,", "regression, gradient descent, regularization, etc.), we are well equipped to\nunderstand neural networks in detail.", "This is, in some sense, the \u201cthird wave\u201d of neural nets. The basic idea is founded on\nthe 1943 model of neurons of McCulloch and Pitts and the learning ideas of Hebb.", "There was a great deal of excitement, but not a lot of practical success: there were\ngood training methods (e.g., perceptron) for linear functions, and interesting", "examples of non-linear functions, but no good way to train non-linear functions\nfrom data. Interest died out for a while, but was re-kindled in the 1980s when", "several people came up with a way to train neural networks with \u201cback-\npropagation,\u201d which is a particular style of implementing gradient descent, that we\nwill study here.", "will study here.\nAs with many good ideas in science, the basic idea for how to train non-linear\nneural networks with gradient descent was independently developed by more than\none researcher.", "one researcher.\nBy the mid-90s, the enthusiasm waned again, because although we could train non-\nlinear networks, the training tended to be slow and was plagued by a problem of", "getting stuck in local optima. Support vector machines (SVMs) that use\nregularization of high-dimensional hypotheses by seeking to maximize the margin,", "alongside kernel methods that provide an ef\ufb01cient and beautiful way of using\nfeature transformations to non-linearly transform data into a higher-dimensional", "space, provided reliable learning methods with guaranteed convergence and no\nlocal optima.\nHowever, during the SVM enthusiasm, several groups kept working on neural", "networks, and their work, in combination with an increase in available data and\ncomputation, has made neural networks rise again. They have become much more", "reliable and capable, and are now the method of choice in many applications. There\nare many, many variations of neural networks, which we can\u2019t even begin to survey.", "We will study the core \u201cfeed-forward\u201d networks with \u201cback-propagation\u201d training,\nand then, in later chapters, address some of the major advances beyond this core.", "We can view neural networks from several different perspectives:6  Neural Networks\nNote\nThe number of neural network\nvariants increases daily, as may be\nseen on arxiv.org.", "seen on arxiv.org.\n\uf2296  Ne ural Ne tworks\uf461 6  Neural Networks \uf52a View 1: An application of stochastic gradient descent for classi\ufb01cation and\nregression with a potentially very rich hypothesis class.", "View 2: A brain-inspired network of neuron-like computing elements that learn\ndistributed representations.\nView 3: A method for building applications that make predictions based on huge", "amounts of data in very complex domains.\nWe will mostly take view 1, with the understanding that the techniques we develop", "will enable the applications in view 3. View 2 was a major motivation for the early\ndevelopment of neural networks, but the techniques we will study do not seem to", "actually account for the biological learning processes in brains.\n6.1 Basic element\nThe basic element of a neural network is a \u201cneuron,\u201d pictured schematically below.", "We will also sometimes refer to a neuron as a \u201cunit\u201d or \u201cnode.\u201d\n\ue050x 1\n.. .\nxmf ( \u00b7 ) aw 1\nwm\nw 0z\ni n p u tp r e - a c t i v a t i o no u t p u t\na c t i v a t i o n f u n c t i o n", "It is a (generally non-linear) function of an input vector  to a single output\nvalue .\nIt is parameterized by a vector of weights  and an offset or\nthreshold .", "threshold .\nWe also specify an activation function . In general, this is chosen to be a\nnon-linear function, which means the neuron is non-linear. In the case that the", "activation function is the identity ( ) or another linear function, then the\nneuron is a linear function of ). The activation can theoretically be any function,", "though we will only be able to work with it if it is differentiable.\nThe function represented by the neuron is expressed as:x\u2208Rm\na\u2208R\n(w1,\u2026,wm)\u2208Rm\nw0\u2208R\nf:R\u2192R\nf(x)=x\nx\na=f(z)=f((m\n\u2211", "x\na=f(z)=f((m\n\u2211\nj=1xjwj)+w0)=f(wTx+w0).Some prominent researchers are, in\nfact, working hard to \ufb01nd\nanalogues of these methods in the\nbrain.\nSorry for changing our notation", "here. We were using  as the\ndimension of the input, but we are\ntrying to be consistent here with\nmany other accounts of neural\nnetworks. It is impossible to be\nconsistent with all of them though", "\u2014there are many different ways of\ntelling this story.d\nThis should remind you of our \nand  for linear models.\u03b8\n\u03b80", "\u03b80\n\uf2296  Ne ural Ne tworks Before thinking about a whole network, we can consider how to train a single unit.\nGiven a loss function  and a dataset ,", "we can do (stochastic) gradient descent, adjusting the weights  to minimize\nwhere  is the output of our single-unit neural net for a given input.", "We have already studied two special cases of the neuron: linear logistic classi\ufb01ers\n(LLCs) with NLL loss and regressors with quadratic loss! The activation function for", "the LLC is  and for linear regression it is simply .\n\u2753 Study Question\nJust for a single neuron, imagine for some reason, that we decide to use\nactivation function  and loss function", ". Derive a gradient descent update for \nand .\n6.2 Networks\nNow, we\u2019ll put multiple neurons together into a network. A neural network in", "general takes in an input  and generates an output . It is constructed\nout of multiple neurons; the inputs of each neuron might be elements of  and/or", "outputs of other neurons. The outputs of the neural network are generated by \noutput units.\nIn this chapter, we will only consider feed-forward networks. In a feed-forward", "network, you can think of the network as de\ufb01ning a function-call graph that is\nacyclic: that is, the input to a neuron can never depend on that neuron\u2019s output.", "Data \ufb02ows one way, from the inputs to the outputs, and the function computed by\nthe network is just a composition of the functions computed by the individual\nneurons.", "neurons.\nAlthough the graph structure of a feed-forward neural network can really be\nanything (as long as it satis\ufb01es the feed-forward constraint), for simplicity in", "software and analysis, we usually organize them into layers. A layer is a group of\nneurons that are essentially \u201cin parallel\u201d: their inputs are the outputs of neurons in", "the previous layer, and their outputs are the inputs to the neurons in the next layer.", "We\u2019ll start by describing a single layer, and then go on to the case of multiple layers.L(guess,actual) {(x(1),y(1)),\u2026,(x(n),y(n))}\nw,w0\nJ(w,w0)=\u2211\niL(NN(x(i);w,w0),y(i)),\nNN\nf(x)=\u03c3(x) f(x)=x\nf(z)=ez", "f(z)=ez\nL(guess,actual)=(guess\u2212actual)2w\nw0\nx\u2208Rma\u2208Rn\nx\nn\n6.2.1 Single layer\n\uf2296  Ne ural Ne tworks A layer is a set of units that, as we have just described, are not connected to each", "other. The layer is called fully connected if, as in the diagram below, all of the inputs\n(i.e.,  in this case) are connected to every unit in the layer. A layer has", "input  and output (also known as activation) .\n\ue050\n\ue050\n\ue050\n.. .\n\ue050x 1\nx 2\n.. .\nxmf\nf\nf\n.. .\nfa 1\na 2\na 3\n.. .\nanW, W 0\nSince each unit has a vector of weights and a single offset, we can think of the", "weights of the whole layer as a matrix, , and the collection of all the offsets as a\nvector . If we have  inputs,  units, and  outputs, then\n is an  matrix,\n is an  column vector,", ", the input, is an  column vector,\n, the pre-activation, is an  column vector,\n, the activation, is an  column vector,\nand the output vector is", "The activation function  is applied element-wise to the pre-activation values .\nA single neural network generally combines multiple layers, most typically by", "feeding the outputs of one layer into the inputs of another layer.x1,x2,\u2026xm\nx\u2208Rma\u2208Rn\nW\nW0 m n n\nWm\u00d7n\nW0n\u00d71\nX m\u00d71\nZ=WTX+W0 n\u00d71\nA n\u00d71\nA=f(Z)=f(WTX+W0).\nf Z\n6.2.2 Many layers", "6.2.2 Many layers\n\uf2296  Ne ural Ne tworks We have to start by establishing some nomenclature. We will use  to name a layer,\nand let  be the number of inputs to the layer and  be the number of outputs", "from the layer. Then,  and  are of shape  and , respectively.\nNote that the input to layer  is the output from layer , so we have ,\nand as a result  is of shape , or equivalently . Let  be the", "activation function of layer . Then, the pre-activation outputs are the  vector\nand the activation outputs are simply the  vector", "Here\u2019s a diagram of a many-layered network, with two blocks for each layer, one\nrepresenting the linear part of the operation and one representing the non-linear", "activation function. We will use this structural decomposition to organize our\nalgorithmic thinking and implementation.\nW1\nW1\n0f1W2\nW2\n0f2 \u00b7 \u00b7 \u00b7WL\nWL\n0fLX =A0Z1A1Z2A2AL \u2212 1ZLAL", "l a y e r 1 l a y e r 2 l a y e rL\n6.3 Choices of activation function\nThere are many possible choices for the activation function. We will start by", "thinking about whether it\u2019s really necessary to have an  at all.\nWhat happens if we let  be the identity? Then, in a network with  layers (we\u2019ll", "leave out  for simplicity, but keeping it wouldn\u2019t change the form of this\nargument),\nSo, multiplying out the weight matrices, we \ufb01nd that", "which is a linear function of ! Having all those layers did not change the\nrepresentational capacity of the network: the non-linearity of the activation function\nis crucial.\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that any function representable by any number of linear\nlayers (where  is the identity function) can be represented by a single layer.l\nmlnl\nWlWl\n0ml\u00d7nlnl\u00d71", "WlWl\n0ml\u00d7nlnl\u00d71\nl l\u22121 ml=nl\u22121\nAl\u22121ml\u00d71 nl\u22121\u00d71fl\nl nl\u00d71\nZl=WlTAl\u22121+Wl\n0\nnl\u00d71\nAl=fl(Zl).\nf\nf L\nW0\nAL=WLTAL\u22121=WLTWL\u22121T\u22efW1TX.\nAL=WtotalX,\nX\nfIt is technically possible to have", "different activation functions\nwithin the same layer, but, again,\nfor convenience in speci\ufb01cation\nand implementation, we generally\nhave the same activation function\nwithin a layer.", "within a layer.\n\uf2296  Ne ural Ne tworks Now that we are convinced we need a non-linear activation, let\u2019s examine a few\ncommon choices. These are shown mathematically below, followed by plots of these", "functions.\nStep function:\nRecti\ufb01ed linear unit (ReLU):\nSigmoid function: Also known as a logistic function. This can sometimes be", "interpreted as probability, because for any value of  the output is in :\nHyperbolic tangent: Always in the range :\nSoftmax function: Takes a whole vector  and generates as output a vector", "with the property that , which means we can interpret it as\na probability distribution over  items:\n\u2212 2 \u2212 1 1 2\n\u2212 0. 50. 511. 5\nzs t e p (z )\n\u2212 2 \u2212 1 1 2\n\u2212 0. 50. 511. 5\nzR e L U (z )\n\u2212 4 \u2212 2 2 4", "\u2212 4 \u2212 2 2 4\n\u2212 1\u2212 0. 50. 51\nz\u03c3 (z )\n\u2212 4 \u2212 2 2 4\n\u2212 1\u2212 0. 50. 51\nzt a n h (z )step(z)={0if z<0\n1otherwise\nReLU(z)={ =max(0,z)0if z<0\nzotherwise\nz (0,1)\n\u03c3(z)=1\n1+e\u2212z\n(\u22121,1)\ntanh(z)=ez\u2212e\u2212z\nez+e\u2212z\nZ\u2208Rn", "ez+e\u2212z\nZ\u2208Rn\nA\u2208(0,1)n\u2211n\ni=1Ai=1\nn\nsoftmax(z)=\u23a1\n\u23a2\u23a3exp(z1)/\u2211iexp(zi)\n\u22ee\nexp(zn)/\u2211iexp(zi)\u23a4\n\u23a5\u23a6\n\uf2296  Ne ural Ne tworks The original idea for neural networks involved using the step function as an", "activation, but because the derivative of the step function is zero everywhere except\nat the discontinuity (and there it is unde\ufb01ned), gradient-descent methods won\u2019t be", "useful in \ufb01nding a good setting of the weights, and so we won\u2019t consider the step\nfunction further. Step functions have been replaced, in a sense, by the sigmoid,\nReLU, and tanh activation functions.", "\u2753 Study Question\nConsider sigmoid, ReLU, and tanh activations. Which one is most like a step\nfunction? Is there an additional parameter you could add to a sigmoid that", "would make it be more like a step function?\n\u2753 Study Question\nWhat is the derivative of the ReLU function? Are there some values of the input\nfor which the derivative vanishes?", "ReLUs are especially common in internal (\u201chidden\u201d) layers, sigmoid activations are\ncommon for the output for binary classi\ufb01cation, and softmax activations are", "common for the output for multi-class classi\ufb01cation (see Section 4.3.3 for an\nexplanation).\n6.4 Loss functions and activation functions", "At layer  which is the output layer, we need to specify a loss function, and\npossibly an activation function as well. Different loss functions make different", "assumptions about the range of values they will get as input and, as we have seen,\ndifferent activation functions will produce output values in different ranges. When", "you are designing a neural network, it\u2019s important to make these things \ufb01t together\nwell. In particular, we will think about matching loss functions with the activation", "function in the last layer, . Here is a table of loss functions and activations that\nmake sense for them:\nLoss task\nsquared linear regression\nnll sigmoid binary classi\ufb01cation", "nllm softmax multi-class classi\ufb01cation\nWe explored squared loss in Chapter 2 and (nll and nllm) in Chapter 4.L,\nfL\nfL\n\uf2296  Ne ural Ne tworks 6.5 Error back-propagation", "We will train neural networks using gradient descent methods. It\u2019s possible to use\nbatch gradient descent, in which we sum up the gradient over all the points (as in", "Section 3.2 of Chapter 3) or stochastic gradient descent (SGD), in which we take a\nsmall step with respect to the gradient considering a single point at a time (as in\nSection 3.4 of Chapter 3).", "Our notation is going to get pretty hairy pretty quickly. To keep it as simple as we\ncan, we\u2019ll focus on computing the contribution of one data point  to the gradient", "of the loss with respect to the weights, for SGD; you can simply sum up these\ngradients over all the data points if you wish to do batch descent.", "So, to do SGD for a training example , we need to compute\n, where  represents all weights  in all the layers\n. This seems terrifying, but is actually quite easy to do using the chain\nrule.", "rule.\nRemember that we are always computing the gradient of the loss function with\nrespect to the weights for a particular value of . That tells us how much we want", "to change the weights, in order to reduce the loss incurred on this particular\ntraining example.\nTo get some intuition for how these derivations work, we\u2019ll \ufb01rst suppose everything", "in our neural network is one-dimensional. In particular, we\u2019ll assume there are\n inputs and  outputs at every layer. So layer  looks like:\nIn the equation above, we\u2019re using the lowercase letters  to", "emphasize that all of these quantities are scalars just for the moment. We\u2019ll look at\nthe more general matrix case below.\nTo use SGD, then, we want to compute  and", "for each layer  and each data point . Below we\u2019ll write\n\u201closs\u201d as an abbreviation for . Then our \ufb01rst quantity of interest is\n. The chain rule gives us the following.", "First, let\u2019s look at the case :x(i)\n(x,y)\n\u2207WL(NN(x;W),y)W Wl,Wl\n0\nl=(1,\u2026,L)\n(x,y)\n6.5.1 First, suppose everything is one-dimensional\nml=1 nl=1 l\nal=fl(zl),zl=wlal\u22121+wl\n0.\nal,zl,wl,al\u22121,wl\n0", "al,zl,wl,al\u22121,wl\n0\n\u2202L(NN(x;W),y)/\u2202wl\n\u2202L(NN(x;W),y)/\u2202wl\n0 l (x,y)\nL(NN(x;W),y)\n\u2202loss/\u2202wl\nl=L\n\u2202loss\n\u2202wL=\u2202loss\n\u2202aL\u22c5\u2202aL\n\u2202zL\u22c5\u2202zL\n\u2202wL\n=\u2202loss\n\u2202aL\u22c5(fL)\u2032(zL)\u22c5aL\u22121.Remember the chain rule! If\n and , so that", "and , so that\n, thena=f(b)b=g(c)\na=f(g(c))\nda\ndc=da\ndb\u22c5db\ndc\n=f\u2032(b)g\u2032(c)\n=f\u2032(g(c))g\u2032(c)\nCheck your understanding: why\ndo we need exactly these quantities\nfor SGD?", "for SGD?\n\uf2296  Ne ural Ne tworks Now we can look at the case of general :\nNote that every multiplication above is scalar multiplication because every term in", "every product above is a scalar. And though we solved for all the other terms in the\nproduct, we haven\u2019t solved for  because the derivative will depend on", "which loss function you choose. Once you choose a loss function though, you\nshould be able to compute this derivative.\n\u2753 Study Question\nSuppose you choose squared loss. What is ?\n\u2753 Study Question", "\u2753 Study Question\nCheck the derivations above yourself. You should use the chain rule and also\nsolve for the individual derivatives that arise in the chain rule.\n\u2753 Study Question", "\u2753 Study Question\nCheck that the \ufb01nal layer ( ) case is a special case of the general layer  case\nabove.\n\u2753 Study Question\nDerive  for yourself, for both the \ufb01nal layer ( ) and\ngeneral .", "general .\n\u2753 Study Question\nDoes the  case remind you of anything from earlier in this course?\n\u2753 Study Question\nWrite out the full SGD algorithm for this neural network.l\n\u2202loss\n\u2202wl=\u2202loss\n\u2202aL\u22c5\u2202aL", "\u2202wl=\u2202loss\n\u2202aL\u22c5\u2202aL\n\u2202zL\u22c5\u2202zL\n\u2202aL\u22121\u22c5\u2202aL\u22121\n\u2202zL\u22121\u22ef\u2202zl+1\n\u2202al\u22c5\u2202al\n\u2202zl\u22c5\u2202zl\n\u2202wl\n=\u2202loss\n\u2202aL\u22c5(fL)\u2032(zL)\u22c5wL\u22c5(fL\u22121)\u2032(zL\u22121)\u22ef\u22c5wl+1\u22c5(fl)\u2032(zl)\u22c5al\u22121\n=\u2202loss\n\u2202zl\u22c5al\u22121.\n\u2202loss/\u2202aL\n\u2202loss/\u2202aL\nl=L l\n\u2202L(NN(x;W),y)/\u2202wl\n0 l=L\nl", "0 l=L\nl\nL=1\n\uf2296  Ne ural Ne tworks It\u2019s pretty typical to run the chain rule from left to right like we did above. But, for", "where we\u2019re going next, it will be useful to notice that it\u2019s completely equivalent to\nwrite it in the other direction. So we can rewrite our result from above as follows:", "Next we\u2019re going to do everything that we did above, but this time we\u2019ll allow any\nnumber of inputs  and outputs  at every layer. First, we\u2019ll tell you the results", "that correspond to our derivations above. Then we\u2019ll talk about why they make\nsense. And \ufb01nally we\u2019ll derive them carefully.\nOK, let\u2019s start with the results! Again, below we\u2019ll be using \u201closs\u201d as an", "abbreviation for . Then,\nwhere\nor equivalently,\nFirst, compare each equation to its one-dimensional counterpart, and make sure", "you see the similarities. That is, compare the general weight derivatives in\nEquation 6.4 to the one-dimensional case in Equation 6.1. Compare the intermediate", "derivative of loss with respect to the pre-activations  in Equation 6.5 to the one-\ndimensional case in Equation 6.2. And \ufb01nally compare the version where we\u2019ve", "substituted in some of the derivatives in Equation 6.6 to Equation 6.3. Hopefully\u2202loss\n\u2202wl=al\u22121\u22c5\u2202loss\n\u2202zl(6.1)\n\u2202loss\n\u2202zl=\u2202al\n\u2202zl\u22c5\u2202zl+1\n\u2202al\u22ef\u2202aL\u22121\n\u2202zL\u22121\u22c5\u2202zL\n\u2202aL\u22121\u22c5\u2202aL\n\u2202zL\u22c5\u2202loss\n\u2202aL(6.2)\n=\u2202al", "\u2202aL(6.2)\n=\u2202al\n\u2202zl\u22c5wl+1\u22ef\u2202aL\u22121\n\u2202zL\u22121\u22c5wL\u22c5\u2202aL\n\u2202zL\u22c5\u2202loss\n\u2202aL. (6.3)\n6.5.2 The general case\nmlnl\nL(NN(x;W),y)\n\u2202loss\n\u2202Wl\nml\u00d7nl=Al\u22121\nml\u00d71(\u2202loss\n\u2202Zl)T\n1\u00d7nl\ue152 \ue154 \ue151\ue150 \ue154 \ue153\ue152 \ue154 \ue151\ue150 \ue154 \ue153\n\ue152 \ue154 \ue151\ue150 \ue154 \ue153(6.4)\n\u2202loss\n\u2202Zl=\u2202Al", "\u2202loss\n\u2202Zl=\u2202Al\n\u2202Zl\u22c5\u2202Zl+1\n\u2202Al\u22ef\u22c5\u2202AL\u22121\n\u2202ZL\u22121\u22c5\u2202ZL\n\u2202AL\u22121\u22c5\u2202AL\n\u2202ZL\u22c5\u2202loss\n\u2202AL(6.5)\n\u2202loss\n\u2202Zl=\u2202Al\n\u2202Zl\u22c5Wl+1\u22ef\u22c5\u2202AL\u22121\n\u2202ZL\u22121\u22c5WL\u22c5\u2202AL\n\u2202ZL\u22c5\u2202loss\n\u2202AL. (6.6)\nZlEven though we have reordered\nthe gradients for notational", "convenience, when actually\ncomputing the product in\nEquation 6.3, it is computationally\nmuch cheaper to run the\nmultiplications from right-to-left\nthan from left-to-right. Convince", "yourself of this, by reasoning\nthrough the cost of the matrix\nmultiplications in each case.\nThere are lots of weights in a\nneural network, which means we\nneed to compute a lot of gradients.", "Luckily, as we can see, the\ngradients associated with weights\nin earlier layers depend on the\nsame terms as the gradients\nassociated with weights in later\nlayers. This means we can reuse", "terms and save ourselves some\ncomputation!\n\uf2296  Ne ural Ne tworks you see how the forms are very analogous. But in the matrix case, we now have to", "be careful about the matrix dimensions. We\u2019ll check these matrix dimensions below. Let\u2019s start by talking through each of the terms in the matrix version of these", "equations. Recall that loss is a scalar, and  is a matrix of size . You can\nread about the conventions in the course for derivatives starting in this chapter in", "Appendix A. By these conventions (not the only possible conventions!), we have\nthat  will be a matrix of size  whose  entry is the scalar", ". In some sense, we\u2019re just doing a bunch of traditional scalar\nderivatives, and the matrix notation lets us write them all simultaneously and", "succinctly. In particular, for SGD, we need to \ufb01nd the derivative of the loss with\nrespect to every scalar component of the weights because these are our model\u2019s", "parameters and therefore are the things we want to update in SGD.\nThe next quantity we see in Equation 6.4 is , which we recall has size  (or", "equivalently  since it represents the outputs of the  layer). Finally, we\nsee . Again, loss is a scalar, and  is a  vector. So by the\nconventions in Appendix A, we have that  has size . The transpose", "then has size . Now you should be able to check that the dimensions all make\nsense in Equation 6.4; in particular, you can check that inner dimensions agree in", "the matrix multiplication and that, after the multiplication, we should be left with\nsomething that has the dimensions on the lefthand side.", "Now let\u2019s look at Equation 6.6. We\u2019re computing  so that we can use it in\nEquation 6.4. The weights are familiar. The one part that remains is terms of the", "form . Checking out Appendix A, we see that this term should be a matrix\nof size  since  and  both have size . The  entry of this matrix", "is . This scalar derivative is something that you can compute when you\nknow your activation function. If you\u2019re not using a softmax activation function,", "typically is a function only of , which means that  should equal 0\nwhenever , and that .\n\u2753 Study Question\nCompute the dimensions of every term in Equation 6.5 and Equation 6.6 using", "Appendix A. After you\u2019ve done that, check that all the matrix multiplications\nwork; that is, check that the inner dimensions agree and that the lefthand side", "and righthand side of these equations have the same dimensions.\n\u2753 Study Question\nIf I use the identity activation function, what is  for any ? What is the\nfull matrix ?Wlml\u00d7nl\n\u2202loss/\u2202Wlml\u00d7nl(i,j)", "\u2202loss/\u2202Wlml\u00d7nl(i,j)\n\u2202loss/\u2202Wl\ni,j\nAl\u22121ml\u00d71\nnl\u22121\u00d71 l\u22121\n\u2202loss/\u2202ZlZlnl\u00d71\n\u2202loss/\u2202Zlnl\u00d71\n1\u00d7nl\n\u2202loss/\u2202Zl\n\u2202Al/\u2202Zl\nnl\u00d7nlAlZlnl\u00d71 (i,j)\n\u2202Al\nj/\u2202Zl\ni\nAl\nj\nZl\nj\u2202Al\nj/\u2202Zl\ni\ni\u2260j \u2202Al\nj/\u2202Zl\nj=(fl)\u2032(Zl\nj)\n\u2202Al\nj/\u2202Zl", "j)\n\u2202Al\nj/\u2202Zl\njj\n\u2202Al/\u2202Zl\n\uf2296  Ne ural Ne tworks You can use everything above without deriving it yourself. But if you want to \ufb01nd", "the gradients of loss with respect to  (which we need for SGD!), then you\u2019ll want\nto know how to actually do these derivations. So next we\u2019ll work out the\nderivations.", "derivations.\nThe key trick is to just break every equation down into its scalar meaning. For\ninstance, the  element of  is . If you think about it for a", "moment (and it might help to go back to the one-dimensional case), the loss is a\nfunction of the elements of , and the elements of  are a function of the .", "There are  elements of , so we can use the chain rule to write\nTo \ufb01gure this out, let\u2019s remember that . We can write one\nelement of the  vector, then, as . It follows that", "will be zero except when  (check you agree!). So we can rewrite\nEquation 6.7 as\nFinally, then, we match entries of the matrices on both sides of the equation above\nto recover Equation 6.4.", "\u2753 Study Question\nCheck that Equation 6.8 and Equation 6.4 say the same thing.\n\u2753 Study Question\nConvince yourself that  by comparing the entries of the\nmatrices on both sides on the equality sign.", "\u2753 Study Question\nConvince yourself that Equation 6.5 is true.\n\u2753 Study Question\nApply the same reasoning to \ufb01nd the gradients of  with respect to .\n6.5.3 Derivations for the general caseWl\n0", "0\n(i,j) \u2202loss/\u2202Wl\u2202loss/\u2202Wl\ni,j\nZlZlWl\ni,j\nnlZl\n\u2202loss\n\u2202Wl\ni,j=nl\n\u2211\nk=1\u2202loss\n\u2202Zl\nk\u2202Zl\nk\n\u2202Wl\ni,j. (6.7)\nZl=(Wl)\u22a4Al\u22121+Wl\n0\nZlZl\nb=\u2211ml\na=1Wl\na,bAl\u22121\na+(Wl\n0)b\n\u2202Zl\nk/\u2202Wl\ni,jk=j\n\u2202loss\n\u2202Wl\ni,j=\u2202loss\n\u2202Zl\nj\u2202Zl", "i,j=\u2202loss\n\u2202Zl\nj\u2202Zl\nj\n\u2202Wl\ni,j=\u2202loss\n\u2202Zl\njAl\u22121\ni. (6.8)\n\u2202Zl/\u2202Al\u22121=Wl\nloss Wl\n0\n\uf2296  Ne ural Ne tworks This general process of computing the gradients of the loss with respect to the", "weights is called error back-propagation.\nThe idea is that we \ufb01rst do a forward pass to compute all the  and  values at all the", "layers, and \ufb01nally the actual loss. Then, we can work backward and compute the\ngradient of the loss with respect to the weights in each layer, starting at layer  and\ngoing back to layer 1.\nW1\nW1", "W1\nW1\n0f1W2\nW2\n0f2 \u00b7 \u00b7 \u00b7WL\nWL\n0fLL o s sX =A0Z1A1Z2A2AL \u2212 1ZLALy\n\u2202 l o s s\n\u2202AL\u2202 l o s s\n\u2202ZL\u2202 l o s s\n\u2202AL \u2212 1\u2202 l o s s\n\u2202A2\u2202 l o s s\n\u2202Z2\u2202 l o s s\n\u2202A1\u2202 l o s s\n\u2202Z1", "\u2202A1\u2202 l o s s\n\u2202Z1\nIf we view our neural network as a sequential composition of modules (in our work\nso far, it has been an alternation between a linear transformation with a weight", "matrix, and a component-wise application of a non-linear activation function), then\nwe can de\ufb01ne a simple API for a module that will let us compute the forward and", "backward passes, as well as do the necessary weight updates for gradient descent.\nEach module has to provide the following \u201cmethods.\u201d We are already using letters", "with particular meanings, so here we will use  as the vector input to the\nmodule and  as the vector output:\nforward: \nbackward: \nweight grad:  only needed for modules that have weights", "In homework we will ask you to implement these modules for neural network\ncomponents, and then use them to construct a network and train it as described in\nthe next section.\n6.6 Training", "6.6 Training\nHere we go! Here\u2019s how to do stochastic gradient descent training on a feed-\nforward neural network. After this pseudo-code, we motivate the choice of", "initialization in lines 2 and 3. The actual computation of the gradient values (e.g.,\n) is not directly de\ufb01ned in this code, because we want to make the\nstructure of the computation clear.", "\u2753 Study Question\n6.5.4 Re\ufb02ecting on backpropagationaz\nL\na,x,y,z u\nv\nu\u2192v\nu,v,\u2202L/\u2202v\u2192\u2202L/\u2202u\nu,\u2202L/\u2202v\u2192\u2202L/\u2202W\nW\n\u2202loss/\u2202ALNotice that the backward pass does\nnot output , even though the", "forward pass maps from  to . In\nthe backward pass, we are always\ndirectly computing and ``passing\naround\u2019\u2019 gradients of the loss.\u2202v/\u2202u\nuv\n\uf2296  Ne ural Ne tworks What is ?\n\u2753 Study Question", "\u2753 Study Question\nWhich terms in the code below depend on ?\nprocedure SGD-NEURAL-NET( )\nfor  to  do\nend for\nfor  to  do\n//forward pass to compute \nfor  to  do\nend for", "for  to  do\nend for\nfor  down to  do//error back-propagation\n//SGD update\nend for\nend for\nend procedure\nInitializing  is important; if you do it badly there is a good chance the neural", "network training won\u2019t work well. First, it is important to initialize the weights to\nrandom values. We want different parts of the network to tend to \u201caddress\u201d", "different aspects of the problem; if they all start at the same weights, the symmetry\nwill often keep the values from moving in useful directions. Second, many of our", "activation functions have (near) zero slope when the pre-activation  values have\nlarge magnitude, so we generally want to keep the initial weights small so we will", "be in a situation where the gradients are non-zero, so that gradient descent will\nhave some useful signal about which way to go.", "One good general-purpose strategy is to choose each weight at random from a\nGaussian (normal) distribution with mean 0 and standard deviation  where\n is the number of inputs to the unit.", "\u2753 Study Question\u2202Zl/\u2202Wl\nfL\n1: Dn,T,L,(m1,\u2026,mL),(f1,\u2026,fL),Loss\n2:l\u21901L\n3:Wl\nij\u223cGaussian(0,1/ml)\n4:Wl\n0j\u223cGaussian(0,1)\n5:\n6:t\u21901T\n7:i\u2190randomsamplefrom {1,\u2026,n}\n8:A0\u2190x(i)AL\n9: l\u21901L\n10: Zl\u2190WlTAl\u22121+Wl\n0", "10: Zl\u2190WlTAl\u22121+Wl\n0\n11: Al\u2190fl(Zl)\n12:\n13: loss\u2190Loss(AL, y(i))\n14: l\u2190L 1\n15:\u2202loss\n\u2202Al\u2190{\u2202Zl+1\n\u2202Al\u22c5\u2202loss\n\u2202Zl+1if l<L,\n\u2202loss\n\u2202AL otherwise\n16:\u2202loss\n\u2202Zl\u2190\u2202Al\n\u2202Zl\u22c5\u2202loss\n\u2202Al\n17:\u2202loss\n\u2202Wl\u2190Al\u22121(\u2202loss\n\u2202Zl)\u22a4", "\u2202Zl)\u22a4\n18:\u2202loss\n\u2202Wl\n0\u2190\u2202loss\n\u2202Zl\n19: Wl\u2190Wl\u2212\u03b7(t)\u2202loss\n\u2202Wl\n20: Wl\n0\u2190Wl\n0\u2212\u03b7(t)\u2202loss\n\u2202Wl\n0\n21:\n22:\n23:\nW\nz\n(1/m)\nm", "22:\n23:\nW\nz\n(1/m)\nm\n\uf2296  Ne ural Ne tworks If the input  to this unit is a vector of 1\u2019s, what would the expected pre-\nactivation  value be with these initial weights?", "We write this choice (where  means \u201cis drawn randomly from the distribution\u201d) as\nIt will often turn out (especially for fancier activations and loss functions) that", "computing  is easier than computing  and  So, we may instead ask for\nan implementation of a loss function to provide a backward method that computes\n directly.", "directly.\n6.7 Optimizing neural network parameters\nBecause neural networks are just parametric functions, we can optimize loss with", "respect to the parameters using standard gradient-descent software, but we can take\nadvantage of the structure of the loss function and the hypothesis class to improve", "optimization. As we have seen, the modular function-composition structure of a\nneural network hypothesis makes it easy to organize the computation of the", "gradient. As we have also seen earlier, the structure of the loss function as a sum\nover terms, one per training data point, allows us to consider stochastic gradient", "methods. In this section we\u2019ll consider some alternative strategies for organizing\ntraining, and also for making it easier to handle the step-size parameter.", "Assume that we have an objective of the form\nwhere  is the function computed by a neural network, and  stands for all the\nweight matrices and vectors in the network.", "Recall that, when we perform batch (or the vanilla) gradient descent, we use the\nupdate rule\nwhich is equivalent to\nSo, we sum up the gradient of loss at each training point, with respect to , and", "then take a step in the negative direction of the gradient.x\nz\n\u223c\nWl\nij\u223cGaussian(0,1\nml).\n\u2202loss\n\u2202ZL\u2202loss\n\u2202AL\u2202AL\n\u2202ZL.\n\u2202loss/\u2202ZL\n6.7.1 Batches\nJ(W)=1\nnn\n\u2211\ni=1L(h(x(i);W),y(i)),\nh W\nWt=Wt\u22121\u2212\u03b7\u2207WJ(Wt\u22121),", "Wt=Wt\u22121\u2212\u03b7\u2207WJ(Wt\u22121),\nWt=Wt\u22121\u2212\u03b7n\n\u2211\ni=1\u2207WL(h(x(i);Wt\u22121),y(i)).\nW\n\uf2296  Ne ural Ne tworks In stochastic gradient descent, we repeatedly pick a point  at random from", "the data set, and execute a weight update on that point alone:\nAs long as we pick points uniformly at random from the data set, and decrease  at", "an appropriate rate, we are guaranteed, with high probability, to converge to at least\na local optimum.\nThese two methods have offsetting virtues. The batch method takes steps in the", "exact gradient direction but requires a lot of computation before even a single step\ncan be taken, especially if the data set is large. The stochastic method begins moving", "right away, and can sometimes make very good progress before looking at even a\nsubstantial fraction of the whole data set, but if there is a lot of variability in the", "data, it might require a very small  to effectively average over the individual steps\nmoving in \u201ccompeting\u201d directions.", "An effective strategy is to \u201caverage\u201d between batch and stochastic gradient descent\nby using mini-batches. For a mini-batch of size , we select  distinct data points", "uniformly at random from the data set and do the update based just on their\ncontributions to the gradient\nMost neural network software packages are set up to do mini-batches.\n\u2753 Study Question", "\u2753 Study Question\nFor what value of  is mini-batch gradient descent equivalent to stochastic\ngradient descent? To batch gradient descent?", "Picking  unique data points at random from a large data-set is potentially\ncomputationally dif\ufb01cult. An alternative strategy, if you have an ef\ufb01cient procedure", "for randomly shuf\ufb02ing the data set (or randomly shuf\ufb02ing a list of indices into the\ndata set) is to operate in a loop, roughly as follows:\nprocedure MINI -B ATCH -SGD(NN, data, K)\nwhile not done do", "while not done do\nRANDOM -S HUFFLE (data)\nfor  to  do\nBATCH -G RADIENT -U PDATE(NN, data[(i-1)K : iK])\nend for\nend while\nend procedure(x(i),y(i))\nWt=Wt\u22121\u2212\u03b7\u2207WL(h(x(i);Wt\u22121),y(i)).\n\u03b7\n\u03b7\nK K\nWt=Wt\u22121\u2212\u03b7\nKK", "\u03b7\nK K\nWt=Wt\u22121\u2212\u03b7\nKK\n\u2211\ni=1\u2207WL(h(x(i);Wt\u22121),y(i)).\nK\nK\n1:\n2:n\u2190length(data)\n3:\n4:\n5: i\u21901\u2308n\nK\u2309\n6:\n7:\n8:\n9:In line 4 of the algorithm above, \nis known as the ceiling function; it", "returns the smallest integer greater\nthan or equal to its input. E.g.,\n and .\u2308\u22c5\u2309\n\u23082.5\u2309=3 \u23083\u2309=3\n\uf2296  Ne ural Ne tworks Picking a value for  is dif\ufb01cult and time-consuming. If it\u2019s too small, then", "convergence is slow and if it\u2019s too large, then we risk divergence or slow\nconvergence due to oscillation. This problem is even more pronounced in stochastic", "or mini-batch mode, because we know we need to decrease the step size for the\nformal guarantees to hold.\nIt\u2019s also true that, within a single neural network, we may well want to have", "different step sizes. As our networks become deep (with increasing numbers of\nlayers) we can \ufb01nd that magnitude of the gradient of the loss with respect the", "weights in the last layer, , may be substantially different from the\ngradient of the loss with respect to the weights in the \ufb01rst layer . If you", "look carefully at Equation 6.6, you can see that the output gradient is multiplied by\nall the weight matrices of the network and is \u201cfed back\u201d through all the derivatives", "of all the activation functions. This can lead to a problem of exploding or vanishing\ngradients, in which the back-propagated gradient is much too big or small to be", "used in an update rule with the same step size.\nSo, we can consider having an independent step-size parameter for each weight, and", "updating it based on a local view of how the gradient updates have been going.\nSome common strategies for this include momentum (\u201caveraging\u201d recent gradient", "updates), Adadelta (take larger steps in parts of the space where  is nearly \ufb02at),\nand Adam (which combines these two previous ideas). Details of these approaches\nare described in Section B.1.", "6.8 Regularization\nSo far, we have only considered optimizing loss on the training data as our objective\nfor neural network training. But, as we have discussed before, there is a risk of", "over\ufb01tting if we do this. The pragmatic fact is that, in current deep neural networks,\nwhich tend to be very large and to be trained with a large amount of data,", "over\ufb01tting is not a huge problem. This runs counter to our current theoretical\nunderstanding and the study of this question is a hot area of research. Nonetheless,", "there are several strategies for regularizing a neural network, and they can\nsometimes be important.\nOne group of strategies can, interestingly, be shown to have similar effects to each", "other: early stopping, weight decay, and adding noise to the training data.\nEarly stopping is the easiest to implement and is in fairly common use. The idea is", "to train on your training set, but at every epoch (a pass through the whole training\n6.7.2 Adaptive step-size\u03b7\n\u2202loss/\u2202WL\n\u2202loss/\u2202W1\nJ(W)", "\u2202loss/\u2202W1\nJ(W)\n6.8.1 Methods related to ridge regressionThis section is very strongly\nin\ufb02uenced by Sebastian Ruder\u2019s\nexcellent blog posts on the topic:\n{ruder.io/optimizing-gradient-\ndescent}", "descent}\nResult is due to Bishop, described\nin his textbook and here.\nWarning: If you use your\nvalidation set in this way \u2013 i.e., to", "\uf2296  Ne ural Ne tworks set, or possibly more frequently), evaluate the loss of the current  on a validation\nset. It will generally be the case that the loss on the training set goes down fairly", "consistently with each iteration, the loss on the validation set will initially decrease,\nbut then begin to increase again. Once you see that the validation loss is", "systematically increasing, you can stop training and return the weights that had the\nlowest validation error.\nAnother common strategy is to simply penalize the norm of all the weights, as we", "did in ridge regression. This method is known as weight decay, because when we\ntake the gradient of the objective\nwe end up with an update of the form", "This rule has the form of \ufb01rst \u201cdecaying\u201d  by a factor of  and then\ntaking a gradient step.\nFinally, the same effect can be achieved by perturbing the  values of the training", "data by adding a small amount of zero-mean normally distributed noise before each\ngradient computation. It makes intuitive sense that it would be more dif\ufb01cult for", "the network to over\ufb01t to particular training data if they are changed slightly on\neach training step.\nDropout is a regularization method that was designed to work with deep neural", "networks. The idea behind it is, rather than perturbing the data every time we train,\nwe\u2019ll perturb the network! We\u2019ll do this by randomly, on each training step,", "selecting a set of units in each layer and prohibiting them from participating. Thus,\nall of the units will have to take a kind of \u201ccollective\u201d responsibility for getting the", "answer right, and will not be able to rely on any small subset of the weights to do\nall the necessary computation. This tends also to make the network more robust to\ndata perturbations.", "data perturbations.\nDuring the training phase, for each training example, for each unit, randomly with\nprobability  temporarily set . There will be no contribution to the output and", "no gradient update for the associated unit.\nWhen we are done training and want to use the network to make predictions, we\nmultiply all weights by  to achieve the same average activation levels.W", "J(W)=n\n\u2211\ni=1L(NN(x(i)),y(i);W)+\u03bb\u2225W\u22252\nWt=Wt\u22121\u2212\u03b7((\u2207WL(NN(x(i)),y(i);Wt\u22121))+2\u03bbWt\u22121)\n=Wt\u22121(1\u22122\u03bb\u03b7)\u2212\u03b7(\u2207WL(NN(x(i)),y(i);Wt\u22121)).\nWt\u22121 (1\u22122\u03bb\u03b7)\nx(i)\n6.8.2 Dropout\np a\u2113\nj=0\npset the number of epochs (or any", "other hyperparameter associated\nwith your learning algorithm) \u2013\nthen error on the validation set no\nlonger provides a \u201cpure\u201d estimate\nof error on the test set (i.e.,\ngeneralization error). This is", "because information about the\nvalidation set has \u201cleaked\u201d into the\ndesign of your algorithm. See also\nthe discussion on Validation and\nCross-Validation in Chapter 2.", "\uf2296  Ne ural Ne tworks Implementing dropout is easy! In the forward pass during training, we let\nwhere  denotes component-wise product and  is a vector of \u2019s and \u2019s drawn", "randomly with probability . The backwards pass depends on , so we do not need\nto make any further changes to the algorithm.", "It is common to set  to , but this is something one might experiment with to get\ngood results on your problem and data.", "Another strategy that seems to help with regularization and robustness in training\nis batch normalization.\nIt was originally developed to address a problem of covariate shift: that is, if you", "consider the second layer of a two-layer neural network, the distribution of its input\nvalues is changing over time as the \ufb01rst layer\u2019s weights change. Learning when the", "input distribution is changing is extra dif\ufb01cult: you have to change your weights to\nimprove your predictions, but also just to compensate for a change in your inputs", "(imagine, for instance, that the magnitude of the inputs to your layer is increasing\nover time\u2014then your weights will have to decrease, just to keep your predictions\nthe same).", "the same).\nSo, when training with mini-batches, the idea is to standardize the input values for\neach mini-batch, just in the way that we did it in Section 5.3.3 of Chapter 5,", "subtracting off the mean and dividing by the standard deviation of each input\ndimension. This means that the scale of the inputs to each layer remains the same,", "no matter how the weights in previous layers change. However, this somewhat\ncomplicates matters, because the computation of the weight updates will need to", "take into account that we are performing this transformation. In the modular view,\nbatch normalization can be seen as a module that is applied to , interposed after", "the product with  and before input to .\nAlthough batch-norm was originally justi\ufb01ed based on the problem of covariate", "shift, it\u2019s not clear that that is actually why it seems to improve performance. Batch\nnormalization can also end up having a regularizing effect for similar reasons that", "adding noise and dropout do: each mini-batch of data ends up being mildly\nperturbed, which prevents the network from exploiting very particular values of", "the data points. For those interested, the equations for batch normalization,\nincluding a derivation of the forward pass and backward pass, are described in\nSection B.2.a\u2113=f(z\u2113)\u2217d\u2113\n\u2217 d\u21130 1\np a\u2113\np0.5", "\u2217 d\u21130 1\np a\u2113\np0.5\n6.8.3 Batch normalization\nzl\nWlflFor more details see\narxiv.org/abs/1502.03167.\nWe follow here the suggestion from\nthe original paper of applying\nbatch normalization before the", "activation function. Since then it\nhas been shown that, in some\ncases, applying it after works a bit\nbetter. But there aren\u2019t any de\ufb01nite\n\ufb01ndings on which works better\nand when.", "and when.\n\uf2296  Ne ural Ne tworks \uf2296  Ne ural Ne tworks Th is page contains all content from the legacy PDF  notes; convolutional neur al networks\nchapter.", "chapter.\nAs we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .\nSo far, we have studied what are called fully connected neural networks, in which all", "of the units at one layer are connected to all of the units in the next layer. This is a\ngood arrangement when we don\u2019t know anything about what kind of mapping", "from inputs to outputs we will be asking the network to learn to approximate. But if\nwe do know something about our problem, it is better to build it into the structure", "of our neural network. Doing so can save computation time and signi\ufb01cantly\ndiminish the amount of training data required to arrive at a solution that\ngeneralizes robustly.", "One very important application domain of neural networks, where the methods\nhave achieved an enormous amount of success in recent years, is signal processing.", "Signals might be spatial (in two-dimensional camera images or three-dimensional\ndepth or CAT scans) or temporal (speech or music). If we know that we are", "addressing a signal-processing problem, we can take advantage of invariant\nproperties of that problem. In this chapter, we will focus on two-dimensional spatial", "problems (images) but use one-dimensional ones as a simple example. In a later\nchapter, we will address temporal problems.", "Imagine that you are given the problem of designing and training a neural network\nthat takes an image as input, and outputs a classi\ufb01cation, which is positive if the", "image contains a cat and negative if it does not. An image is described as a two-\ndimensional array of pixels, each of which may be represented by three integer", "values, encoding intensity levels in red, green, and blue color channels.\nThere are two important pieces of prior structural knowledge we can bring to bear\non this problem:", "on this problem:\nSpatial locality: The set of pixels we will have to take into consideration to \ufb01nd\na cat will be near one another in the image.", "Translation invariance: The pattern of pixels that characterizes a cat is the\nsame no matter where in the image the cat occurs.", "We will design neural network structures that take advantage of these properties.\n7.1 Filters7  Convolutional Neural Networks\nNote\nA pixel is a \u201cpictur e element.\u201d\nSo , for example, we won\u2019t have to", "consider some combination of\npixels in the four  corners of the\nimage, in order to see if they\nencode cat-ness.Cats don\u2019t look different if they\u2019re\non the left or the right side of the", "image.\uf461 7  Convolutional Neural Networks \uf52a We begin by discussing image \ufb01lters.\nAn image \ufb01lter is a function that takes in a local spatial neighborhood of pixel", "values and detects the presence of some pattern in that data.\nLet\u2019s consider a very simple case to start, in which we have a 1-dimensional binary", "\u201cimage\u201d and a \ufb01lter  of size two. The \ufb01lter is a vector of two numbers, which we\nwill move along the image, taking the dot product between the \ufb01lter values and the", "image values at each step, and aggregating the outputs to produce a new image.\nLet  be the original image, of size ; then pixel  of the the output image is\nspeci\ufb01ed by", "speci\ufb01ed by\nTo ensure that the output image is also of dimension , we will generally \u201cpad\u201d the\ninput image with 0 values if we need to access pixels that are beyond the bounds of", "the input image. This process of applying the \ufb01lter to the image to create a new\nimage is called \u201cconvolution.\u201d\nIf you are already familiar with what a convolution is, you might notice that this", "de\ufb01nition corresponds to what is often called a correlation and not to a convolution.\nIndeed, correlation and convolution refer to different operations in signal", "processing. However, in the neural networks literature, most libraries implement\nthe correlation (as described in this chapter) but call it convolution. The distinction", "is not signi\ufb01cant; in principle, if convolution is required to solve the problem, the\nnetwork could learn the necessary weights. For a discussion of the difference", "between convolution and correlation and the conventions used in the literature you\ncan read Section 9.1 in this excellent book: Deep Learning.", "Here is a concrete example. Let the \ufb01lter . Then given the image in\nthe \ufb01rst line below, we can convolve it with \ufb01lter  to obtain the second image.", "You can think of this \ufb01lter as a detector for \u201cleft edges\u201d in the original image\u2014to see\nthis, look at the places where there is a  in the output image, and see what pattern", "exists at that position in the input image. Another interesting \ufb01lter is\n. The third image (the last line below) shows the result of", "convolving the \ufb01rst image with , where we see that the output pixel \ncorresponds to when the center of  is aligned at input pixel .\n\u2753  Study Question", "\u2753  Study Question\nConvince yourself that \ufb01lter  can be understood as a detector for isolated\npositive pixels in the binary image.F\nX d i\nYi=F\u22c5(Xi\u22121,Xi).\nd\nF1=(\u22121,+1)\nF1\n1\nF2=(\u22121,+1,\u22121)\nF2 i\nF2 i", "F2 i\nF2 i\nF2Unfortun ately in\nAI/ M L/CS/ M ath, the word\n``\ufb01lter\u2019\u2019 gets us ed in many ways: in\naddition to the one we describe\nhere, it can describe a temporal\nprocess (in fact, our  moving", "averages are a kind of \ufb01lter) and\neven a somewhat esoteric algebraic\nstruc tur e.\nAnd \ufb01lters are also sometimes\ncalled convolutional kernels. 0 0 1 1 1 0 1 0 0 0 I m a g e :\nF 1 : - 1 + 1", "F 1 : - 1 + 1\n0 0 1 0 0 - 1 1 - 1 0 0 A f t e r c o n v o l u t i o n ( w i t hF 1 ) :\n0 - 1 0 - 1 0 - 2 1 - 1 0 0 A f t e r c o n v o l u t i o n ( w i t hF 2 ) :F 2 - 1 + 1 - 1", "Two-dimensional versions of \ufb01lters like these are thought to be found in the visual\ncortex of all mammalian brains. Similar patterns arise from statistical analysis of", "natural images. Computer vision people used to spend a lot of time hand-designing\n\ufb01lter banks. A \ufb01lter bank is a set of sets of \ufb01lters, arranged as shown in the diagram\nbelow.\nI m a g e", "below.\nI m a g e\nAll of the \ufb01lters in the \ufb01rst group are applied to the original image; if there are \nsuch \ufb01lters, then the result is  new images, which are called channels. Now imagine", "stacking all these new images up so that we have a cube of data, indexed by the\noriginal row and column indices of the image, as well as by the channel. The next", "set of \ufb01lters in the \ufb01lter bank will generally be three-dimensional: each one will be\napplied to a sub-range of the row and column indices of the image and to all of the\nchannels.", "channels.\nThese 3D chunks of data are called tensors. The algebra of tensors is fun, and a lot\nlike matrix algebra, but we won\u2019t go into it in any detail.", "Here is a more complex example of two-dimensional \ufb01ltering. We have two \n\ufb01lters in the \ufb01rst layer,  and . You can think of each one as \u201clooking\u201d for three", "pixels in a row,  vertically and  horizontally. Assuming our input image is\n, then the result of \ufb01ltering with these two \ufb01lters is an  tensor. Nowk\nk\n3\u00d73\nf1f2\nf1 f2", "k\n3\u00d73\nf1f2\nf1 f2\nn\u00d7n n\u00d7n\u00d72Th ere are now many us eful neur al-\nnetwork software packages, suc h\nas TensorFlow and PyTorch that", "make operations on tensors easy. we apply a tensor \ufb01lter (hard to draw!) that \u201clooks for\u201d a combination of two\nhorizontal and two vertical bars (now represented by individual pixels in the two", "channels), resulting in a single \ufb01nal  image.\nWhen we have a color image as input, we treat it as having three channels, and\nhence as an  tensor.\nf 2\nf 1t e n s o r\n\ufb01 l t e r", "\ufb01 l t e r\nWe are going to design neural networks that have this structure. Each \u201cbank\u201d of the\n\ufb01lter bank will correspond to a neural-network layer. The numbers in the individual", "\ufb01lters will be the \u201cweights\u201d (plus a single additive bias or offset value for each\n\ufb01lter) of the network, that we will train using gradient descent. What makes this", "interesting and powerful (and somewhat confusing at \ufb01rst) is that the same weights\nare used many many times in the computation of each layer. This weight sharing", "means that we can express a transformation on a large image with relatively few\nparameters; it also means we\u2019ll have to take care in \ufb01guring out exactly how to train\nit!", "it!\nWe will de\ufb01ne a \ufb01lter layer  formally with:\nnumber of \ufb01lters ;\nsize of one \ufb01lter is  plus  bias value (for this one \ufb01lter);", "stride  is the spacing at which we apply the \ufb01lter to the image; in all of our\nexamples so far, we have used a stride of 1, but if we were to \u201cskip\u201d and apply", "the \ufb01lter only at odd-numbered indices of the image, then it would have a\nstride of two (and produce a resulting image of half the size);\ninput tensor size", "input tensor size \npadding:  is how many extra pixels \u2013 typically with value 0 \u2013 we add around\nthe edges of the input. For an input of size , our new\neffective input size with padding becomes\n.n\u00d7n", ".n\u00d7n\nn\u00d7n\u00d73\nl\nml\nkl\u00d7kl\u00d7ml\u221211\nsl\nnl\u22121\u00d7nl\u22121\u00d7ml\u22121\npl\nnl\u22121\u00d7nl\u22121\u00d7ml\u22121\n(nl\u22121+2\u22c5pl)\u00d7(nl\u22121+2\u22c5pl)\u00d7ml\u22121For simplicity, we are assum ing\nthat all images and \ufb01lters are\nsqua re (having the same num ber of", "rows and colum ns). Th at is in no\nway necessary, but is us ua lly \ufb01ne\nand de\ufb01nitely simpli\ufb01es our\nnotation. This layer will produce an output tensor of size , where", ". The weights are the values de\ufb01ning the \ufb01lter:\nthere will be  different  tensors of weight values; plus each \ufb01lter\nmay have a bias term, which means there is one more weight value per \ufb01lter. A \ufb01lter", "with a bias operates just like the \ufb01lter examples above, except we add the bias to the\noutput. For instance, if we incorporated a bias term of 0.5 into the \ufb01lter  above,", "the output would be  instead of\n.\nThis may seem complicated, but we get a rich class of mappings that exploit image\nstructure and have many fewer weights than a fully connected layer would.", "\u2753  Study Question\nHow many weights are in a convolutional layer speci\ufb01ed as above?\n\u2753  Study Question\nIf we used a fully-connected layer with the same size inputs and outputs, how", "many weights would it have?\n7.2 Max pooling\nIt is typical (both in engineering and in natrure) to structure \ufb01lter banks into a", "pyramid, in which the image sizes get smaller in successive layers of processing. The\nidea is that we \ufb01nd local patterns, like bits of edges in the early layers, and then", "look for patterns in those patterns, etc. This means that, effectively, we are looking\nfor patterns in larger pieces of the image as we apply successive \ufb01lters. Having a", "stride greater than one makes the images smaller, but does not necessarily\naggregate information over that spatial range.", "Another common layer type, which accomplishes this aggregation, is max pooling. A\nmax pooling layer operates like a \ufb01lter, but has no weights. You can think of it as", "purely functional, like a ReLU in a fully connected network. It has a \ufb01lter size, as in a\n\ufb01lter layer, but simply returns the maximum value in its \ufb01eld.", "Usually, we apply max pooling with the following traits:\n, so that the resulting image is smaller than the input image; and\n, so that the whole image is covered.", "As a result of applying a max pooling layer, we don\u2019t keep track of the precise\nlocation of a pattern. This helps our \ufb01lters to learn to recognize patternsnl\u00d7nl\u00d7ml\nnl=\u2308(nl\u22121+2\u22c5pl\u2212(kl\u22121))/sl\u2309", "mlkl\u00d7kl\u00d7ml\u22121\nF2\n(\u22120.5,0.5,\u22120.5,0.5,\u22121.5,1.5,\u22120.5,0.5)\n(\u22121,0,\u22121,0,\u22122,1,\u22121,0)\nstride>1\nk\u2265strideRecall that  is the fun ction; it\nretur ns the smallest integer greater\nthan or equa l to its input. E.g.,", "and .\u2308\u22c5\u2309\n\u23082.5\u2309=3 \u23083\u2309=3\nWe sometimes us e the term\nreceptive \ufb01eld or jus t \ufb01eld to mean\nthe area of an input image that a\n\ufb01lter is being applied to. independent of their location.", "Consider a max pooling layer where both the strides and  are set to be 2. This\nwould map a  image to a  image. Note that max pooling\nlayers do not have additional bias or offset values.", "\u2753  Study Question\nMaximilian Poole thinks it would be a good idea to add two max pooling layers\nof size , one right after the other, to their network. What single layer would be\nequivalent?", "equivalent?\nOne potential concern about max-pooling layers is that they actually don\u2019t\ncompletely preserve translation invariance. If you do max-pooling with a stride", "other than 1 (or just pool over the whole image size), then shifting the pattern you\nare hoping to detect within the image by a small amount can change the output of", "the max-pooling layer substantially, just because there are discontinuities induced\nby the way the max-pooling window matches up with its input image. Here is an", "interesting paper that illustrates this phenomenon clearly and suggests that one\nshould \ufb01rst do max-pooling with a stride of 1, then do \u201cdownsampling\u201d by\naveraging over a window of outputs.", "7.3 Typical architecture\nHere is the form of a typical convolutional network:\nAt the end of each \ufb01lter layer, we typically apply a ReLU activation function. There", "may be multiple \ufb01lter plus ReLU layers. Then we have a max pooling layer. Then\nwe have some more \ufb01lter + ReLU layers. Then we have max pooling again. Once", "the output is down to a relatively small size, there is typically a last fully-connected\nlayer, leading into an activation function such as softmax that produces the \ufb01nal", "output. The exact design of these structures is an art\u2014there is not currently any\nclear theoretical (or even systematic empirical) understanding of how these various", "design choices affect overall performance of the network.\nThe critical point for us is that this is all just a big neural network, which takes an", "input and computes an output. The mapping is a differentiable function of the\nweights, which means we can adjust the weights to decrease the loss by performingk\n64\u00d764\u00d73 32\u00d732\u00d73\nk", "64\u00d764\u00d73 32\u00d732\u00d73\nk\nTh e \u201cdepth\u201d dimension in the\nlayers shown as cub oids\ncorresponds to the num ber of\nchannels in the output tensor.\n(Figur e sour ce: M athworks)\nWell, technically the derivative", "does not exist at every point, both\nbecaus e of the ReLU and the max gradient descent, and we can compute the relevant gradients using back-\npropagation!\n7.4 Backpropagation in a simple CNN", "Let\u2019s work through a very simple example of how back-propagation can work on a\nconvolutional network. The architecture is shown below. Assume we have a one-", "dimensional single-channel image  of size , and a single \ufb01lter  of size\n (where we omit the \ufb01lter bias) for the \ufb01rst convolutional operation", "denoted \u201cconv\u201d in the \ufb01gure below. Then we pass the intermediate result \nthrough a ReLU layer to obtain the activation , and \ufb01nally through a fully-", "connected layer with weights , denoted \u201cfc\u201d below, with no additional\nactivation function, resulting in the output .\nX =A000\np a d wit h 0 \u2019 s \n( t o g e t o u t p u t \no f s a m e s h a p e )W1", "Z1A1Z2=A2W2c o n v R e L U f c\nFor simplicity assume  is odd, let the input image , and assume we are\nusing squared loss. Then we can describe the forward pass as follows:\n\u2753  Study Question", "\u2753  Study Question\nAssuming a stride of  for a \ufb01lter of size , how much padding do we need to\nadd to the top and bottom of the image? We see one zero at the top and bottom", "in the \ufb01gure just above; what \ufb01lter size is implicitly being shown in the \ufb01gure?\n(Recall the padding is for the sake of getting an output the same size as the\ninput.)X n\u00d71\u00d71 W1\nk\u00d71\u00d71\nZ1\nA1\nW2\nA2", "k\u00d71\u00d71\nZ1\nA1\nW2\nA2\nk X=A0\nZ1\ni=W1TA0\n[i\u2212\u230ak/2\u230b:i+\u230ak/2\u230b]\nA1=ReLU(Z1)\nA2=Z2=W2TA1\nLsquare(A2,y)=(A2\u2212y)2\n1, k\n7.4.1 Weight updatepooling operations, but we ignore", "that fact. How do we update the weights in \ufb01lter ?\n is the  matrix such that . So, for\nexample, if , which corresponds to column 10 in this matrix, which", "illustrates the dependence of pixel 10 of the output image on the weights, and\nif , then the elements in column 10 will be .\n is the  diagonal matrix such that\n, an  vector", ", an  vector\nMultiplying these components yields the desired gradient, of shape .\nOne last point is how to handle back-propagation through a max-pooling operation.", "Let\u2019s study this via a simple example. Imagine\nwhere  and  are each computed by some network. Consider doing back-\npropagation through the maximum. First consider the case where . Then the", "error value at  is propagated back entirely to the network computing the value .\nThe weights in the network computing  will ultimately be adjusted, and the\nnetwork computing  will be untouched.", "\u2753  Study Question\nWhat is  ?W1\n\u2202loss\n\u2202W1=\u2202Z1\n\u2202W1\u2202A1\n\u2202Z1\u2202loss\n\u2202A1\n\u2202Z1/\u2202W1k\u00d7n \u2202Z1\ni/\u2202W1\nj=Xi\u2212\u230ak/2\u230b+j\u22121\ni=10\nk=5 X8,X9,X10,X11,X12\n\u2202A1/\u2202Z1n\u00d7n\n\u2202A1\ni/\u2202Z1\ni={1if Z1\ni>0\n0otherwise", "i>0\n0otherwise\n\u2202loss/\u2202A1=(\u2202loss/\u2202A2)(\u2202A2/\u2202A1)=2(A2\u2212y)W2n\u00d71\nk\u00d71\n7.4.2 Max pooling\ny=max(a1,a2),\na1a2\na1>a2\ny a1\na1\na2", "a1>a2\ny a1\na1\na2\n\u2207(x,y)max(x,y) Th is page contains all content from the legacy PDF  notes; autoencoders chapter.", "As we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .\nIn previous chapters, we have largely focused on classi\ufb01cation and regression", "problems, where we use supervised learning with training samples that have both\nfeatures/inputs and corresponding outputs or labels, to learn hypotheses or models", "that can then be used to predict labels for new data.\nIn contrast to supervised learning paradigm, we can also have an unsupervised", "learning setting, where we only have features but no corresponding outputs or\nlabels for our dataset. On natural question aries then: if there are no labels, what are\nwe learning?", "we learning?\nOne canonical example of unsupervised learning is clustering, which is discussed in\nSection 12.3. In clustering, the goal is to develop algorithms that can reason about", "\u201csimilarity\u201d among data points\u2019s features, and group the data points into clusters.\nAutoencoders are another family of unsupervised learning algorithms, in this case", "seeking to obtain insights about our data by learning compressed versions of the\noriginal data, or, in other words, by \ufb01nding a good lower-dimensional feature", "representations of the same data set. Such insights might help us to discover and\ncharacterize underlying factors of variation in data, which can aid in scienti\ufb01c", "discovery; to compress data for ef\ufb01cient storage or communication; or to pre-\nprocess our data prior to supervised learning, perhaps to reduce the amount of data", "that is needed to learn a good classi\ufb01er or regressor.\n8.1 Autoencoder structure\nAssume that we have input data , where . We seek to\nlearn an autoencoder that will output a new dataset , where", "with . We can think about  as the new representation of data point\n. For example, in Figure 8.1 we show the learned representations of a dataset of", "MNIST digits with . We see, after inspecting the individual data points, that\nunsupervised learning has found a compressed (or latent) representation where", "images of the same digit are close to each other, potentially greatly aiding\nsubsequent clustering or classi\ufb01cation tasks.8  Representation Learning (Autoencoders)\nNote\nD={x(1),\u2026,x(n)}x(i)\u2208Rd", "Dout={a(1),\u2026,a(n)}\na(i)\u2208Rkk<d a(i)\nx(i)\nk=2\uf461 8  Representation Learning (Autoencoders) \uf52a Formally, an autoencoder consists of two functions, a vector-valued encoder", "that deterministically maps the data to the representation space \n, and a decoder  that maps the representation space back into the\noriginal data space.", "In general, the encoder and decoder functions might be any functions appropriate\nto the domain. Here, we are particularly interested in neural network embodiments", "of encoders and decoders. The basic architecture of one such autoencoder,\nconsisting of only a single layer neural network in each of the encoder and decoder,", "is shown in Figure 8.2; note that bias terms  and  into the summation nodes\nexist, but are omitted for clarity in the \ufb01gure. In this example, the original -", "dimensional input is compressed into  dimensions via the encoder\n with  and , and where the\nnon-linearity  is applied to each dimension of the vector. To recover (an", "approximation to) the original instance, we then apply the decoder\n, where  denotes a different non-linearity\n(activation function). In general, both the decoder and the encoder could involve", "multiple layers, as opposed to the single layer shown here. Learning seeks\nparameters  and  such that the reconstructed instances,\n, are close to the original input .\n8.2 Autoencoder Learning", "g:Rd\u2192Rka\u2208Rk\nh:Rk\u2192Rd\nW1\n0W2\n0\nd\nk=3\ng(x;W1,W1\n0)=f1(W1Tx+W1\n0)W1\u2208Rd\u00d7kW1\n0\u2208Rk\nf1\nh(a;W2,W2\n0)=f2(W2Ta+W2\n0)f2\nW1,W1\n0W2,W2\n0\nh(g(x(i);W1,W1\n0);W2,W2\n0) x(i)\nFigur e 8.1: Compression of digits", "dataset into two dimensions. Th e\ninput , an image of a\nhandwritten digit, is shown at the\nnew low-dimensional\nrepresentation .x(i)\n(a1,a2)\nFigur e 8.2: Autoencoder struc tur e,", "showing the encoder (left half,\nlight green), and the decoder (right\nhalf, light blue ), encoding inputs \nto the representation , and\ndecoding the representation to\nproduc e , the reconstruc tion. In", "this speci\ufb01c example, the\nrepresentation ( , , ) only has\nthree dimensions.x\na\n~x\na1a2a3 We learn the weights in an autoencoder using the same tools that we previously", "used for supervised learning, namely (stochastic) gradient descent of a multi-layer\nneural network to minimize a loss function. All that remains is to specify the loss", "function , which tells us how to measure the discrepancy between the\nreconstruction  and the original input . For\nexample, for continuous-valued  it might make sense to use squared loss, i.e.,\n.", ".\nLearning then seeks to optimize the parameters of  and  so as to minimize the\nreconstruction error, measured according to this loss function:\n8.3 Evaluating an autoencoder", "What makes a good learned representation in an autoencoder? Notice that, without\nfurther constraints, it is always possible to perfectly reconstruct the input. For", "example, we could let  and  and  be the identity functions. In this case, we\nwould not obtain any compression of the data.", "To learn something useful, we must create a bottleneck by making  to be smaller\n(often much smaller) than . This forces the learning algorithm to seek", "transformations that describe the original data using as simple a description as\npossible. Thinking back to the digits dataset, for example, an example of a", "compressed representation might be the digit label (i.e., 0\u20139), rotation, and stroke\nthickness. Of course, there is no guarantee that the learning algorithm will discover", "precisely this representation. After learning, we can inspect the learned\nrepresentations, such as by arti\ufb01cially increasing or decreasing one of the", "dimensions (e.g., ) and seeing how it affects the output , to try to better\nunderstand what it has learned.\nAs with clustering, autoencoders can be a preliminary step toward building other", "models, such as a regressor or classi\ufb01er. For example, once a good encoder has been\nlearned, the decoder might be replaced with another neural network that is then", "trained with supervised learning (perhaps using a smaller dataset that does include\nlabels).\n8.4 Linear encoders and decoders\nWe close by mentioning that even linear encoders and decoders can be very", "powerful. In this case, rather than minimizing the above objective with gradient\ndescent, a technique called principal components analysis (PCA) can be used to obtainL(~x,x)\n~x=h(g(x;W1,W1\n0);W2,W2", "0);W2,W2\n0) x\nx\nLSE(~x,x)=\u2211d\nj=1(xj\u2212~xj)2\nhg\nmin\nW1,W1\n0,W2,W2\n0n\n\u2211\ni=1LSE(h(g(x(i);W1,W1\n0);W2,W2\n0),x(i))\nk=dhg\nk\nd\na1 h(a)Alternatively, you could think of\nthis as multi-task learning, where", "the goal is to predict each\ndimension of . On e can mix-and-\nmatch loss fun ctions as appropriate", "for each dimension\u2019s data type.x a closed-form solution to the optimization problem using a singular value\ndecomposition (SVD). Just as a multilayer neural network with nonlinear", "activations for regression (learned by gradient descent) can be thought of as a\nnonlinear generalization of a linear regressor (\ufb01t by matrix algebraic operations),", "the neural network based autoencoders discussed above (and learned with gradient\ndescent) can be thought of as a generalization of linear PCA (as solved with matrix\nalgebra by SVD).", "algebra by SVD).\n8.5 Advanced encoders and decoders\nAdvanced neural networks built on encoder-decoder architectures have become", "increasingly powerful. One prominent example is generative networks, designed to\ncreate new outputs that resemble\u2014but differ from\u2014existing training examples. A", "notable type, variational autoencoders, learns a compressed representation\ncapturing statistical properties (such as mean and variance) of training data. These", "latent representations can then be sampled to generate novel outputs using the\ndecoder.\nAnother in\ufb02uential encoder-decoder architecture is the Transformer, covered in", "Chapter 9. Transformers consist of multiple encoder and decoder layers combined\nwith self-attention mechanisms, which excel at predicting sequential data, such as", "words and sentences in natural language processing (NLP).\nCentral to autoencoders and Transformers is the idea of learning representations.", "Autoencoders compress data into ef\ufb01cient, informative representations, while NLP\nmodels encode language\u2014words, phrases, sentences\u2014into numerical forms. This", "numerical encoding leads us to the concept of vector embeddings.\n8.6 Embeddings\nIn NLP, words are represented as vectors, commonly known as word embeddings. A", "key property of good embeddings is that their numerical closeness mirrors semantic\nsimilarity. For instance, semantically related words such as \u201cdog\u201d and \u201ccat\u201d should", "have vectors close together, while unrelated words like \u201ccat\u201d and \u201ctable\u201d should be\nfarther apart.\nSimilarity between embeddings is frequently measured using the inner product:", "The inner product indicates how aligned two vectors are: highly positive values\nimply strong similarity, negative values indicate opposition, and values near zero", "suggest no similarity (up to a scaling factor related to the magnitude).aTb=a\u22c5b A groundbreaking embedding method, word2vec (2012), signi\ufb01cantly advanced NLP", "by producing embeddings where vector arithmetic corresponded to real-world\nsemantic relationships. For instance:\nSuch embeddings revealed meaningful semantic relationships like analogies across", "diverse vocabulary (e.g., uncle \u2013 man + woman \u2248 aunt).\nImportantly, embeddings don\u2019t need exact coordinates\u2014it\u2019s their relative", "positioning within the vector space that matters. Embeddings are considered\neffective if they facilitate downstream NLP tasks, such as predicting missing words,", "classifying texts, or language translation.\nFor example, effective embeddings allow models to accurately predict a missing\nword in a sentence:\nAfter the rain, the grass was ____.", "Or a model could be built that tries to correctly predict words in the middle of\nsentences:\nThe child fell __ __ during the long car ride", "This task exempli\ufb01es self-supervision, a training approach where models generate\nlabels directly from the data itself, eliminating the need for manual labeling.", "Training neural networks through self-supervision involves optimizing their ability\nto predict words accurately from large text corpora (e.g., Wikipedia). Through such", "optimization, embeddings capture subtle semantic and syntactic nuances, greatly\nenhancing NLP capabilities.\nThe idea of good embeddings will play a central role when we discuss attention", "mechanisms in Chapter 9, where embeddings dynamically adjust based on context\n(via the so-called attention mechanism), enabling a more nuanced understanding of", "language.embeddingparis\u2212embeddingfrance+embeddingitaly\u2248embeddingrome We are actively overhauling the Transformers chapter from the legacy PDF  notes to", "enhance clarity and presentation. Please feel free to raise issue s or reque st more\nexplanation on speci\ufb01c topics.\nTransformers are a very recent family of architectures that were originally", "introduced in the \ufb01eld of natural language processing (NLP) in 2017, as an\napproach to process and understand human language. Since then, they have", "revolutionized not only NLP but also other domains such as image processing and\nmulti-modal generative AI. Their scalability and parallelizability have made them", "the backbone of large-scale foundation models, such as GPT, BERT, and Vision\nTransformers (ViT), powering many state-of-the-art applications.", "Like CNNs, transformers factorize signal processing into stages, each involving\nindependently and identically processed chunks. Transformers have many intricate", "components; however, we\u2019ll focus on their most crucial innovation: a new type of\nlayer called the attention layer. Attention layers enable transformers to effectively", "mix information across chunks, allowing the entire transformer pipeline to model\nlong-range dependencies among these chunks. To help make Transformers more", "digestible, in this chapter, we will \ufb01rst succinctly motivate and describe them in an\noverview Section 9.1. Then, we will dive into the details following the \ufb02ow of data \u2013", "\ufb01rst describing how to represent inputs Section 9.2, and then describe the attention\nmechanism Section 9.3, and \ufb01nally we then assemble all these ideas together to", "arrive at the full transformer architecture in Section 9.5.\n9.1 Transformers Overview\nTransformers are powerful neural architectures designed primarily for sequential", "data, such as text. At their core, transformers are typically auto-regressive, meaning\nthey generate sequences by predicting each token sequentially, conditioned on", "previously generated tokens. This auto-regressive property ensures that the\ntransformer model inherently captures temporal dependencies, making them", "especially suited for language modeling tasks like text generation and completion.\nSuppose our training data contains this sentence: \u201cTo date, the cleverest thinker was", "Issac.\u201d The transformer model will learn to predict the next token in the sequence,\ngiven the previous tokens. For example, when predicting the token \u201ccleverest,\u201d the", "model will condition its prediction on the tokens \u201cTo,\u201d \u201cdate,\u201d and \u201cthe.\u201d This9  Transformers\nCaution\nHum an langua ge is inherently\nseque ntial in natur e (e.g.,\ncharacters form words, words form", "sentences, and sentences form\nparagraphs and docum ents). Prior\nto the advent of the transformers\narchitectur e, recur rent neur al\nnetworks (RNNs) brie\ufb02y\ndominated the \ufb01eld for their ability", "to process seque ntial information.\nHowever, RNNs, like many other\narchitectur es, processed seque ntial\ninformation in an\niterative/seque ntial fashion,\nwhereby each item of a seque nce", "was individua lly processed one\nafter another. Transformers offer\nmany advantages over RNNs,\ninclud ing their ability to process all\nitems in a seque nce in a parallel", "fashion (as do CNNs).\uf461 9  Transformers \uf52a process continues until the entire sequence is generated.\nThe animation above illustrates the auto-regressive nature of transformers.", "Below is another example. Suppose the sentence is the 2nd law of robotics: \u201cA robot\nmust obey the orders given it by human beings\u2026\u201d The training objective of a", "transformer would be to make each token\u2019s prediction, conditioning on previously\ngenerated tokens, forming a step-by-step probability distribution over the\nvocabulary.", "vocabulary.\nThe transformer architecture processes inputs by applying multiple identical\nbuilding blocks stacked in layers. Each block performs a transformation that", "progressively re\ufb01nes the internal representation of the data.\nSpeci\ufb01cally, each block consists of two primary sub-layers: an attention layer", "Section 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6.\nAttention layers mix information across different positions (or \"chunks\") in the", "sequence, allowing the model to effectively capture dependencies regardless of\ndistance. Meanwhile, the feed-forward network signi\ufb01cantly enhances the", "expressiveness of these representations by applying non-linear transformations\nindependently to each position.\nA notable strength of transformers is their capacity for parallel processing.", "Transformers process entire sequences simultaneously rather than sequentially\ntoken-by-token. This parallelization signi\ufb01cantly boosts computational ef\ufb01ciency", "and makes it feasible to train larger and deeper models.\nIn this overview, we emphasize the auto-regressive nature of transformers, their", "layered approach to transforming representations, the parallel processing advantage, and the critical role of the feed-forward layers in enhancing their\nexpressive power.", "expressive power.\nThere are additional essential components and enhancements\u2014such as causal\nattention mechanisms and positional encoding\u2014that further empower", "transformers. We\u2019ll explore these \"bells and whistles\" in greater depth in subsequent\ndiscussions.\n9.2 Embedding and Representations", "We start by describing how language is commonly represented, then we provide a\nbrief explanation of why it can be useful to predict subsequent items (e.g.,\nwords/tokens) in a sequence.", "As a reminder, two key components of any ML system are: (1) the representation of\nthe data; and (2) the actual modelling to perform some desired task. Computers, by", "default, have no natural way of representing human language. Modern computers\nare based on the Von Neumann architecture and are essentially very powerful", "calculators, with no natural understanding of what any particular string of\ncharacters means to us humans. Considering the rich complexities of language (e.g.,", "humor, sarcasm, social and cultural references and implications, slang, homonyms,\netc), you can imagine the innate dif\ufb01culties of appropriately representing", "languages, along with the challenges for computers to then model and\n\u201cunderstand\u201d language.\nThe \ufb01eld of NLP aims to represent words with vectors of \ufb02oating-point numbers", "(aka word embeddings) such that they capture semantic meaning. More precisely,\nthe degree to which any two words are related in the \u2018real-world\u2019 to us humans", "should be re\ufb02ected by their corresponding vectors (in terms of their numeric\nvalues). So, words such as \u2018dog\u2019 and \u2018cat\u2019 should be represented by vectors that are", "more similar to one another than, say, \u2018cat\u2019 and \u2018table\u2019 are.\nTo measure how similar any two word embeddings are (in terms of their numeric", "values) it is common to use some similarity as the metric, e.g. the dot-product\nsimilarity we saw in Chapter 8.\nThus, one can imagine plotting every word embedding in -dimensional space and", "observing natural clusters to form, whereby similar words (e.g., synonyms) are\nlocated near each other. The problem of determining how to parse (aka tokenize)", "individual words is known as tokenization. This is an entire topic of its own, so we\nwill not dive into the full details here. However, the high-level idea of tokenization", "is straightforward: the individual inputs of data that are represented and processed\nby a model are referred to as tokens. And, instead of processing each word as a", "whole, words are typically split into smaller, meaningful pieces (akin to syllables).\nFor example, the word \u201cevaluation\u201d may be input into a model as 3 individuald How can we de\ufb01ne an optimal", "vocabulary of suc h tokens? How\nmany distinct tokens should we\nhave in our  vocabulary? How\nshould we handle digits or other\npun ctua tion? How does this work\nfor non-English langua ges, in", "particular, script-based langua ges\nwhere word boun daries are less\nobvious  (e.g., Chinese or", "Japanese)? All of these are open tokens (eval + ua + tion). Thus, when we refer to tokens, know that we\u2019re referring\nto these sub-word units. For any given application/model, all of the language data", "must be prede\ufb01ned by a \ufb01nite vocabulary of valid tokens (typically on the order of\n40,000 distinct tokens).\n9.3 Query, Key, Value, and Attention Output", "Attention mechanisms ef\ufb01ciently process global information by selectively focusing\non the most relevant parts of the input. Given an input sentence, each token is", "processed sequentially to predict subsequent tokens. As more context (previous\ntokens) accumulates, this context ideally becomes increasingly bene\ufb01cial\u2014provided", "the model can appropriately utilize it. Transformers employ a mechanism known as\nattention, which enables models to identify and prioritize contextually relevant\ntokens.", "tokens.\nFor example, consider the partial sentence: \u201cAnqi forgot ___\u201c. At this point, the\nmodel has processed tokens\u201dAnqi\u201d and \u201cforgot,\u201d and aims to predict the next", "token. Numerous valid completions exist, such as articles (\u201cthe,\u201d \u201can\u201d),\nprepositions (\u201cto,\u201d \u201cabout\u201d), or possessive pronouns (\u201cher,\u201d \u201chis,\u201d \u201ctheir\u201d). A well-", "trained model should assign higher probabilities to contextually relevant tokens,\nsuch as \u201cher,\u201d based on the feminine-associated name \u201cAnqi.\u201d Attention", "mechanisms guide the model to selectively focus on these relevant contextual cues\nusing query, key, and value vectors.", "Our goal is for each input token to learn how much attention it should give to every\nother token in the sequence. To achieve this, each token is assigned a unique query", "vector used to \u201cprobe\u201d or assess other tokens\u2014including itself\u2014to determine\nrelevance.\nA token\u2019s query vector  is computed by multiplying the input token  (a -", "dimensional vector) by a learnable query weight matrix  (of dimension ,\n is a hyperparameter typically chosen such that ):\nThus, for a sequence of  tokens, we generate  distinct query vectors.", "To complement query vectors, we introduce key vectors, which tokens use to\n\u201canswer\u201d queries about their relevance. Speci\ufb01cally, when evaluating token , its", "query vector  is compared to each token\u2019s key vector  to determine the attention\nweight. Each key vector  is computed similarly using a learnable key weight\nmatrix :\n9.3.1 Query Vectorsqi xid\nWq d\u00d7dk", "Wq d\u00d7dk\ndk dk<d\nqi=WT\nqxi\nn n\n9.3.2 Key Vectors\nx3\nq3 kj\nki\nWk\nTNLP research problems receiving\nincreased attention lately. The attention mechanism calculates similarity using the dot product, which", "ef\ufb01ciently measures vector similarity:\nThe vector  (softmax\u2019d attention scores) quanti\ufb01es how much attention token \nshould pay to each token in the sequence, normalized so that elements sum to 1.", "Normalizing by  prevents large dot-product magnitudes, stabilizing training.\nTo incorporate meaningful contributions from attended tokens, we use value", "vectors (), providing distinct representations for contribution to attention outputs.\nEach token\u2019s value vector is computed with another learnable matrix :", "Finally, attention outputs are computed as weighted sums of value vectors, using\nthe softmax\u2019d attention scores:\nThis vector  represents token \u2019s enriched embedding, incorporating context", "from across the sequence, weighted by learned attention.\n9.4 Self-attention Layer\nSelf-attention is an attention mechanism where the keys, values, and queries are all\ngenerated from the same input.", "At a very high level, typical transformer with self-attention layers maps\n. In particular, the transformer takes in  tokens, each having feature", "dimension  and through many layers of transformation (most important of which\nare self-attention layers); the transformer \ufb01nally outputs a sequence of  tokens,\neach of which -dimensional still.", "With a self-attention layer, there can be multiple attention head. We start with\nunderstanding a single head.\nA single self-attention head is largely the same as our discussion in Section 9.3. The", "main additional info introduced in this part is a compact matrix form. The layerki=WT\nkxi\nai=softmax([qT\nik1,qT\nik2,\u2026,qT\nikn]\n\u221adk)T\n\u2208R1\u00d7n\nai qi\n\u221adk\n9.3.3 Value Vectors\nvi\nWv\nvi=WT\nvxi", "vi\nWv\nvi=WT\nvxi\n9.3.4 Attention Output\nzi=n\n\u2211\nj=1aijvj\u2208Rdk\nzi xi\nRn\u00d7d\u27f6 Rn\u00d7dn\nd,\nn\nd\n9.4.1 A Single Self-attention Head takes in  tokens, each having feature dimension . Thus, all tokens can be", "collectively written as , where the -th row of  stores the -th token,\ndenoted as . For each token , self-attention computes (via learned", "projection matrices, discussed in Section 9.3), a query , key , and\nvalue , and overall, we will have  queries,  keys, and  values; all of", "these vectors live in the same dimension in practice, and we often denote all three\nembedding dimension via a uni\ufb01ed .\nThe self-attention output is calculated as a weighted sum:", "where  is the th element in .\nSo far, we\u2019ve discussed self-attention focusing on a single token input-output.\nActually, we can calculate all outputs  ( ) at the same time using a", "matrix form. For clearness, we \ufb01rst introduce the  query matrix,\n key matrix, and  value matrix:\nIt should be straightforward to understand that the , ,  matrices simply stack", ", , and  in a row-wise manner, respectively. Now, the the full attention matrix\n is:\nwhich often time is shorten as:\nNote that the Softmax operation is applied in a row-wise manner. The th row  of", "this matrix corresponds to the softmax\u2019d attention scores computed for query \nover all keys (i.e., ). The full output of the self-attention layer can then be written\ncompactly as:n d\nX\u2208Rn\u00d7di X i", "X\u2208Rn\u00d7di X i\nxi\u2208R1\u00d7dxi\nqi\u2208Rdqki\u2208Rdk\nvi\u2208Rdv n n n\ndk\nzi=n\n\u2211\nj=1aijvj\u2208Rdk\naijj ai\nzii=1,2,\u2026,n\nQ\u2208Rn\u00d7dk\nK\u2208Rn\u00d7dk V\u2208Rn\u00d7dk\nQ= \u2208Rn\u00d7d,K= \u2208Rn\u00d7d,V= \u2208Rn\u00d7dv\u23a1\n\u23a2\u23a3q\u22a4\n1\nq\u22a4\n2\n\u22ee\nq\u22a4\nn\u23a4\n\u23a5\u23a6\u23a1\n\u23a2\u23a3k\u22a4\n1\nk\u22a4\n2\n\u22ee\nk\u22a4\nn\u23a4\n\u23a5\u23a6\u23a1\n\u23a2\u23a3v\u22a4\n1", "\u22ee\nk\u22a4\nn\u23a4\n\u23a5\u23a6\u23a1\n\u23a2\u23a3v\u22a4\n1\nv\u22a4\n2\n\u22ee\nv\u22a4\nn\u23a4\n\u23a5\u23a6\nQKV\nqikivi\nA\u2208Rn\u00d7n\nA=\u23a1\n\u23a2\u23a3softmax([ ]/\u221adk)\nsoftmax([ ]/\u221adk)\n\u22ee\nsoftmax([ ]/\u221adk)q\u22a4\n1k1q\u22a4\n1k2\u22efq\u22a4\n1kn\nq\u22a4\n2k1q\u22a4\n2k2\u22efq\u22a4\n2kn\nq\u22a4\nnk1q\u22a4\nnk2\u22efq\u22a4\nnkn\u23a4\n\u23a5\u23a6(9.1)\n=softmax(QK\u22a4", "=softmax(QK\u22a4\n\u221adk) A=softmax1\n\u221adk\u239b\n\u239c\u239d\u23a1\n\u23a2\u23a3q\u22a4\n1k1q\u22a4\n1k2\u22efq\u22a4\n1kn\nq\u22a4\n2k1q\u22a4\n2k2\u22efq\u22a4\n2kn\n\u22ee \u22ee \u22f1 \u22ee\nq\u22a4\nnk1q\u22a4\nnk2\u22efq\u22a4\nnkn\u23a4\n\u23a5\u23a6\u239e\n\u239f\u23a0\niA\nqi\n\u03b1i\n\u22a4 where  is the matrix of value vectors stacked row-wise, and  is", "the output, whose th row corresponds to the attention output for the th query (i.e.,\n).\nYou will also see this compact notation Attention in the literature, which is an", "operation of three arguments , , and  (and we add an emphasis that the\nsoftmax is performed on each row):\nHuman language can be very nuanced. There are many properties of language that", "collectively contribute to a human\u2019s understanding of any given sentence. For\nexample, words have different tenses (past, present, future, etc), genders,", "abbreviations, slang references, implied words or meanings, cultural references,\nsituational relevance, etc. While the attention mechanism allows us to appropriately", "focus on tokens in the input sentence, it\u2019s unreasonable to expect a single set of\n matrices to fully represent \u2013 and for a model to capture \u2013 the meaning of\na sentence with all of its complexities.", "To address this limitation, the idea of multi-head attention is introduced. Instead of\nrelying on just one attention head (i.e., a single set of  matrices), the", "model uses multiple attention heads, each with its own independently learned set\nof  matrices. This allows each head to attend to different parts of the", "input tokens and to model different types of semantic relationships. For instance,\none head might focus on syntactic structure and another on verb tense or sentiment.", "These different \u201cperspectives\u201d are then concatenated and projected to produce a\nricher, more expressive representation of the input.", "Now, we introduce the formal math notations. Let us denote the number of head as\n. For the th head, the input  is linearly projected into query, key, and", "value matrices using the projection matrices , , and\n (recall that usually ):\nThe output of the -th head is : . After", "computing all  heads, we concatenate their outputs and apply a \ufb01nal linearZ= =AV\u2208Rn\u00d7dk\u23a1\n\u23a2\u23a3z\u22a4\n1\nz\u22a4\n2\n\u22ee\nz\u22a4\nn\u23a4\n\u23a5\u23a6\nV\u2208Rn\u00d7dk Z\u2208Rn\u00d7dk\ni i\nzi\nQKV\nAttention(Q,K,V)=softmaxrow(QK\u22a4\n\u221adk)V", "\u221adk)V\n9.4.2 Multi-head Self-attention\n{Q,K,V}\n{Q,K,V}\n{Q,K,V}\nH h X\u2208Rn\u00d7d\nWh\nq\u2208Rd\u00d7dqWh\nk\u2208Rd\u00d7dk\nWh\nv\u2208Rd\u00d7dv dq=dk=dv\nQh=XWh\nq\nKh=XWh\nk\nVh=XWh\nv\ni ZhZh=Attention(Qh,Kh,Vh)\u2208Rn\u00d7dk\nh projection:", "h projection:\nwhere the concatenation operation concatenates  horizontally, yielding a matrix\nof size , and  is a \ufb01nal linear projection matrix.\n9.5 Transformers Architecture Details", "An extremely observant reader might have been suspicious of a small but very\nimportant detail that we have not yet discussed: the attention mechanism, as", "introduced so far, does not encode the order of the input tokens. For instance, when\ncomputing softmax\u2019d attention scores and building token representations, the", "model is fundamentally permutation-equivariant \u2014 the same set of tokens, even if\nscrambled into a different order, would result in identical outputs permuted in the", "same order \u2014 Formally, when we \ufb01x  and switch the input  with \n, then the output  and  will be switched. However, natural language is not a bag\nof words: meaning is tied closely to word order.", "To address this, transformers incorporate positional embeddings \u2014 additional\ninformation that encodes the position of each token in the sequence. These", "embeddings are added to the input token embeddings before any attention layers\nare applied, effectively injecting ordering information into the model.", "There are two main strategies for positional embeddings: (i) learned positional\nembeddings, where a trainable vector  is assigned to each position (i.e.,", "token index) . These vectors are learned alongside all other model\nparameters and allow the model to discover how best to encode position for a given", "task, (ii) \ufb01xed positional embeddings, such as sinusoidal positional embedding\nproposed in the original Transformer paper:\nwhere  is the token index, while  is the dimension index.", "Namely, this sinusoidal positional embedding uses sine for the even dimension and\ncosine for the odd dimension. Regardless of learnable or \ufb01xed positional", "embedding, it will enter the computation of attention at the input place:\n  where  is the th original input token, and  is its positional", "embedding. The  will now be what we really feed into the attention layer, so that\nthe input to the attention mechanism now carries information about both what the", "token is and where it appears in the sequence.MultiHead(X)=Concat(Z1,\u2026,ZH)(WO)T\nZh\nn\u00d7HdkWO\u2208Rd\u00d7Hdk\n9.5.1 Positional Embeddings\n{Wq,Wk,Wv} xixj\nzizj\npi\u2208Rd\ni=0,1,2,...,n\np(i,2k)=sin(i\n100002k/d)", "100002k/d)\np(i,2k+1)=cos(i\n100002k/d)\ni=1,2,..,n k=1,2,...,d\nx\u2217\ni=xi+pi ,xii pi\nx\u2217\ni This simple additive design enables attention layers to leverage both semantic", "content and ordering structure when deciding where to focus. In practice, this\naddition occurs at the very \ufb01rst layer of the transformer stack, and all subsequent", "layers operate on position-aware representations. This is a key design choice that\nallows transformers to work effectively with sequences of text, audio, or even image", "patches (as in Vision Transformers).\nMore generally, a mask may be applied to limit which tokens are used in the\nattention computation. For example, one common mask limits the attention", "computation to tokens that occur previously in time to the one being used for the\nquery. This prevents the attention mechanism from \u201clooking ahead\u201d in scenarios", "where the transformer is being used to generate one token at a time. This causal\nmasking is done by introducing a mask matrix  that restricts attention to", "only current and previous positions. A typical causal mask is a lower-triangular\nmatrix:\nand we now have the masked attention matrix:", "The softmax is performed to each row independently. The attention output is still\n. Essentially, the lower-triangular property of  ensures that the self-", "attention operation for the -th query only considers tokens . Note that we\nshould apply the masking before performing softmax, so that the attention matrix", "can be properly normalized (i.e., each row sum to 1).\nEach self-attention stage is trained to have key, value, and query embeddings that", "lead it to pay speci\ufb01c attention to some particular feature of the input. We generally\nwant to pay attention to many different kinds of features in the input; for example,", "in translation one feature might be be the verbs, and another might be objects or\nsubjects. A transformer utilizes multiple instances of self-attention, each known as", "an \u201cattention head,\u201d to allow combinations of attention paid to many different\nfeatures.\n9.5.2 Causal Self-attentionM\u2208Rn\u00d7n\nM=\u23a1\n\u23a2\u23a30\u2212\u221e\u2212\u221e\u22ef \u2212\u221e\n00\u2212\u221e\u22ef \u2212\u221e\n00 0 \u22ef \u2212\u221e\n\u22ee\u22ee \u22ee\u22f1 \u22ee\n00 0 \u22ef 0\u23a4\n\u23a5\u23a6\nA=softmax1\n\u221adk+M\u239b", "A=softmax1\n\u221adk+M\u239b\n\u239c\u239d\u23a1\n\u23a2\u23a3q\u22a4\n1k1q\u22a4\n1k2\u22efq\u22a4\n1kn\nq\u22a4\n2k1q\u22a4\n2k2\u22efq\u22a4\n2kn\n\u22ee \u22ee \u22f1 \u22ee\nq\u22a4\nnk1q\u22a4\nnk2\u22efq\u22a4\nnkn\u23a4\n\u23a5\u23a6\u239e\n\u239f\u23a0\nY=AV M", "nkn\u23a4\n\u23a5\u23a6\u239e\n\u239f\u23a0\nY=AV M\nj 0,1,...,j  This page contains all content from the legacy PDF notes; markov decision processes\nchapter.", "chapter.\nAs we phase out the PDF, this page may receive updates not re\ufb02ected in the static PDF.\nConsider a robot learning to navigate through a maze, a game-playing AI", "developing strategies through self-play, or a self-driving car making driving\ndecisions in real-time. These problems share a common challenge: the agent must", "make a sequence of decisions where each choice affects future possibilities and\nrewards. Unlike static prediction tasks where we learn a one-time mapping from", "inputs to outputs, these problems require reasoning about the consequences of\nactions over time.\nThis sequential and dynamical nature demands mathematical tools beyond the", "more static supervised or unsupervised learning approaches. The most general\nframework for such problems is reinforcement learning (RL), where an agent learns to", "take actions in an unknown environment to maximize cumulative rewards over\ntime.\nIn this chapter, we\u2019ll \ufb01rst study Markov decision processes (MDPs), which provide the", "mathematical foundation for understanding and solving sequential decision\nmaking problems like RL. MDPs formalize the interaction between an agent and its", "environment, capturing the key elements of states, actions, rewards, and transitions.\n10.1 De\ufb01nition and value functions\nFormally, a Markov decision process is  where  is the state space,", "is the action space, and:\n is a transition model, where\nspecifying a conditional probability distribution;\n is a reward function, where  speci\ufb01es an immediate", "reward for taking action  when in state ; and\n is a discount factor, which we\u2019ll discuss in Section 10.1.2.\nIn this class, we assume the rewards are deterministic functions. Further, in this", "MDP chapter, we assume the state space and action space are discrete and \ufb01nite.10  Markov Decision Processes\nNote\n\u27e8S,A,T,R,\u03b3\u27e9 S A\nT:S\u00d7A\u00d7S\u2192R\nT(s,a,s\u2032)=Pr(St=s\u2032|St\u22121=s,At\u22121=a),\nR:S\u00d7A\u2192R R(s,a)\na s", "R:S\u00d7A\u2192R R(s,a)\na s\n\u03b3\u2208[0,1]Th e notation  us es a capital\nletter  to stand for a random\nvariable, and small letter  to stand\nfor a concrete value . So   here is a\nrandom variable that can take on", "elements of  as value s.St=s\u2032\nS\ns\nSt\nS\uf46110  Markov Decision Processes \uf52a The following description of a simple machine as Markov decision process provides a\nconcrete example of an MDP.", "The machine has three possible operations (actions): wash, paint, and eject (each with\na corresponding button). Objects are put into the machine, and each time you push a", "button, something is done to the object. However, it\u2019s an old machine, so it\u2019s not very\nreliable. The machine has a camera inside that can clearly detect what is going on with the", "object and will output the state of the object: dirty, clean, painted, or ejected.\nFor each action, this is what is done to the object:\nWash", "Wash\nIf you perform the wash operation on any object\u2014whether it\u2019s dirty, clean, or\npainted\u2014it will end up clean with probability 0.9 and dirty otherwise.\nPaint", "Paint\nIf you perform the paint operation on a clean object, it will become nicely painted\nwith probability 0.8. With probability 0.1, the paint misses but the object stays clean,", "and with probability 0.1, the machine dumps rusty dust all over the object, making it\ndirty.\nIf you perform the paint operation on a painted object, it stays painted with\nprobability 1.0.", "probability 1.0.\nIf you perform the paint operation on a dirty object, it stays dirty with\nprobability 1.0.\nEject\nIf you perform an eject operation on any object, the object comes out of the", "machine and the process is terminated. The object remains ejected regardless of\nany further actions.\nThese descriptions specify the transition model , and the transition function for each", "action can be depicted as a state machine diagram. For example, here is the diagram for\nwash:\nExampleT d i r t y c l e a n\np a i n t e d e j e c t e d0 . 1\n0 . 90 . 9\n0 . 1\n0 . 1 0 . 9 1 . 0", "0 . 1 0 . 9 1 . 0\nYou get reward +10 for ejecting a painted object, reward 0 for ejecting a non-painted\nobject, reward 0 for any action on an \u201cejected\u201d object, and reward -3 otherwise. The MDP", "description would be completed by also specifying a discount factor.\nA policy is a function  that speci\ufb01es what action to take in each state. The policy is", "what we will want to learn; it is akin to the strategy that a player employs to win a\ngiven game. Below, we take just the initial steps towards this eventual goal. We", "describe how to evaluate how good a policy is, \ufb01rst in the \ufb01nite horizon case\nSection 10.1.1 when the total number of transition steps is \ufb01nite. In the \ufb01nite", "horizon case, we typically denote the policy as , where  is a non-negative\ninteger denoting the number of steps remaining and  is the current state. Then", "we consider the in\ufb01nite horizon case Section 10.1.2, when you don\u2019t know when the\ngame will be over.\nThe goal of a policy is to maximize the expected total reward, averaged over the", "stochastic transitions that the domain makes. Let\u2019s \ufb01rst consider the case where\nthere is a \ufb01nite horizon , indicating the total number of steps of interaction that the\nagent will have with the MDP.", "We seek to measure the goodness of a policy. We do so by de\ufb01ning for a given\nhorizon  and MDP policy , the \u201chorizon  value\u201d of a state, . We do this by", "induction on the horizon, which is the number of steps left to go.\nThe base case is when there are no steps remaining, in which case, no matter what\nstate we\u2019re in, the value is 0, so", "Then, the value of a policy in state  at horizon  is equal to the reward it will\nget in state  plus the next state\u2019s expected horizon  value, discounted by a factor \u03c0\n\u03c0h(s) h\ns\u2208S", "\u03c0h(s) h\ns\u2208S\n10.1.1 Finite-horizon value functions\nh\nh \u03c0h h V\u03c0\nh(s)\nV\u03c0\n0(s)=0. (10.1)\ns h+1\ns h \u03b3 . So, starting with horizons 1 and 2, and then moving to the general case, we have:", "The sum over  is an expectation: it considers all possible next states , and\ncomputes an average of their -horizon values, weighted by the probability", "that the transition function from state  with the action chosen by the policy \nassigns to arriving in state , and discounted by .\n\u2753  Study Question\nWhat is the value of", "for any given state\u2013action pair ?\n\u2753  Study Question\nConvince yourself that the de\ufb01nitions in Equation 10.1 and Equation 10.3 are\nspecial cases of the more general formulation in Equation 10.4.", "Then we can say that a policy  is better than policy  for horizon  if and only if\nfor all ,  and there exists at least one  such that\n.", ".\nMore typically, the actual \ufb01nite horizon is not known, i.e., when you don\u2019t know\nwhen the game will be over! This is called the in\ufb01nite horizon version of the problem.", "How does one evaluate the goodness of a policy in the in\ufb01nite horizon case?\nIf we tried to simply take our de\ufb01nitions above and use them for an in\ufb01nite", "horizon, we could get in trouble. Imagine we get a reward of 1 at each step under\none policy and a reward of 2 at each step under a different policy. Then the reward", "as the number of steps grows in each case keeps growing to become in\ufb01nite in the\nlimit of more and more steps. Even though it seems intuitive that the second policy", "should be better, we can\u2019t justify that by saying .V\u03c0\n1(s)=R(s,\u03c01(s))+0 (10.2)\nV\u03c0\n2(s)=R(s,\u03c02(s))+\u03b3\u2211\ns\u2032T(s,\u03c02(s),s\u2032)V\u03c0\n1(s\u2032) (10.3)\n\u22ee\nV\u03c0\nh(s)=R(s,\u03c0h(s))+\u03b3\u2211\ns\u2032T(s,\u03c0h(s),s\u2032)V\u03c0\nh\u22121(s\u2032) (10.4)\ns\u2032s\u2032\n(h\u22121)", "s\u2032s\u2032\n(h\u22121)\ns \u03c0h(s)\ns\u2032\u03b3\n\u2211\ns\u2032T(s,a,s\u2032)\n(s,a)\n\u03c0 \u00af\u03c0 h\ns\u2208SV\u03c0\nh(s)\u2265V\u00af\u03c0\nh(s) s\u2208S\nV\u03c0\nh(s)>V\u00af\u03c0\nh(s)\n10.1.2 In\ufb01nite-horizon value functions", "\u221e<\u221e One standard approach to deal with this problem is to consider the discounted\nin\ufb01nite horizon. We will generalize from the \ufb01nite-horizon case by adding a\ndiscount factor.", "discount factor.\nIn the \ufb01nite-horizon case, we valued a policy based on an expected \ufb01nite-horizon\nvalue:\nwhere  is the reward received at time .", "What is ? This mathematical notation indicates an expectation, i.e., an average taken\nover all the random possibilities which may occur for the argument. Here, the expectation", "is taken over the conditional probability , where  is the random variable\nfor the reward, subject to the policy being  and the state being . Since  is a function,", "this notation is shorthand for conditioning on all of the random variables implied by\npolicy  and the stochastic transitions of the MDP.", "A very important point is that  is always deterministic (in this class) for any given\n and . Here  represents the set of all possible  at time step ; this  is a", "random variable because the state we\u2019re in at step  is itself a random variable, due to\nprior stochastic state transitions up to but not including at step  and prior (deterministic)", "actions dictated by policy \nNow, for the in\ufb01nite-horizon case, we select a discount factor , and\nevaluate a policy based on its expected in\ufb01nite horizon value:", "Note that the  indices here are not the number of steps to go, but actually the\nnumber of steps forward from the starting state (there is no sensible notion of \u201csteps", "to go\u201d in the in\ufb01nite horizon case).\nEquation 10.5 and Equation 10.6 are a conceptual stepping stone. Our main objective is to", "get to Equation 10.8, which can also be viewed as including  in Equation 10.4, with the\nappropriate de\ufb01nition of the in\ufb01nite-horizon value.", "There are two good intuitive motivations for discounting. One is related to\neconomic theory and the present value of money: you\u2019d generally rather have some", "money today than that same amount of money next week (because you could use it\nnow or invest it). The other is to think of the whole process terminating, with", "probability  on each step of the interaction. (At every step, your expected\nfuture lifetime, given that you have survived until now, is .) This value is", "the expected amount of reward the agent would gain under this terminating model.E[h\u22121\n\u2211\nt=0\u03b3tRt\u2223\u03c0,s0], (10.5)\nRt t\nNote\nE[\u22c5]\nPr(Rt=r\u2223\u03c0,s0) Rt\n\u03c0 s0 \u03c0\n\u03c0\nR(s,a)\ns a Rt R(st,a) t Rt\nt\nt\n\u03c0.\n0\u2264\u03b3\u22641\nE[\u221e\n\u2211", "t\nt\n\u03c0.\n0\u2264\u03b3\u22641\nE[\u221e\n\u2211\nt=0\u03b3tRt\u2223\u03c0,s0]=E[R0+\u03b3R1+\u03b32R2+\u2026\u2223\u03c0,s0]. (10.6)\nt\nNote\n\u03b3\n1\u2212\u03b3\n1/(1\u2212\u03b3) \u2753  Study Question\nVerify this fact: if, on every day you wake up, there is a probability of  that", "today will be your last day, then your expected lifetime is  days.\nLet us now evaluate a policy in terms of the expected discounted in\ufb01nite-horizon", "value that the agent will get in the MDP if it executes that policy. We de\ufb01ne the\nin\ufb01nite-horizon value of a state  under policy  as", "Because the expectation of a linear combination of random variables is the linear\ncombination of the expectations, we have\nThe equation de\ufb01ned in Equation 10.8 is known as the Bellman Equation, which", "breaks down the value function into the immediate reward and the (discounted)\nfuture value function. You could write down one of these equations for each of the", "states. There are  unknowns . These are linear equations, and\nstandard software (e.g., using Gaussian elimination or other linear algebraic", "methods) will, in most cases, enable us to \ufb01nd the value of each state under this\npolicy.\n10.2 Finding policies for MDPs", "Given an MDP, our goal is typically to \ufb01nd a policy that is optimal in the sense that\nit gets as much total reward as possible, in expectation over the stochastic", "transitions that the domain makes. We build on what we have learned about\nevaluating the goodness of a policy (Section 10.1.2), and \ufb01nd optimal policies for the", "\ufb01nite horizon case (Section 10.2.1), then the in\ufb01nite horizon case (Section 10.2.2).\nHow can we go about \ufb01nding an optimal policy for an MDP? We could imagine", "enumerating all possible policies and calculating their value functions as in the\nprevious section and picking the best one \u2013 but that\u2019s too much work!", "The \ufb01rst observation to make is that, in a \ufb01nite-horizon problem, the best action to\ntake depends on the current state, but also on the horizon: imagine that you are in a", "situation where you could reach a state with reward 5 in one step or a state with\nreward 100 in two steps. If you have at least two steps to go, then you\u2019d move1\u2212\u03b3\n1/(1\u2212\u03b3)\ns \u03c0\nV\u03c0", "1/(1\u2212\u03b3)\ns \u03c0\nV\u03c0\n\u221e(s)=E[R0+\u03b3R1+\u03b32R2+\u22ef\u2223\u03c0,S0=s]\n=E[R0+\u03b3(R1+\u03b3(R2+\u03b3\u2026)))\u2223\u03c0,S0=s].(10.7)\nV\u03c0\n\u221e(s)=E[R0\u2223\u03c0,S0=s]+\u03b3E[R1+\u03b3(R2+\u03b3\u2026)))\u2223\u03c0,S0=s]\n=R(s,\u03c0(s))+\u03b3\u2211\ns\u2032T(s,\u03c0(s),s\u2032)V\u03c0\n\u221e(s\u2032). (10.8)\nn=|S| n V\u03c0(s)", "n=|S| n V\u03c0(s)\n10.2.1 Finding optimal \ufb01nite-horizon policies toward the reward 100 state, but if you only have one step left to go, you should go", "in the direction that will allow you to gain 5!For the \ufb01nite-horizon case, we de\ufb01ne  to be the expected value of\nstarting in state ,\nexecuting action , and", "continuing for  more steps executing an optimal policy for the\nappropriate horizon on each step.\nSimilar to our de\ufb01nition of  for evaluating a policy, we de\ufb01ne the  function", "recursively according to the horizon. The only difference is that, on each step with\nhorizon , rather than selecting an action speci\ufb01ed by a given policy, we select the", "value of  that will maximize the expected  value of the next state.\nwhere  denotes the next time-step state/action pair. We can solve for the", "values of  with a simple recursive algorithm called \ufb01nite-horizon value iteration\nthat just computes  starting from horizon 0 and working backward to the desired", "horizon . Given , an optimal  can be found as follows:\nwhich gives the immediate best action(s) to take when there are  steps left; then", "gives the best action(s) when there are  steps left, and so on. In the\ncase where there are multiple best actions, we typically can break ties randomly.", "Additionally, it is worth noting that in order for such an optimal policy to be\ncomputed, we assume that the reward function  is bounded on the set of all", "possible (state, action) pairs. Furthermore, we will assume that the set of all possible\nactions is \ufb01nite.\n\u2753  Study Question", "\u2753  Study Question\nThe optimal value function is unique, but the optimal policy is not. Think of a\nsituation in which there is more than one optimal policy.Q\u2217\nh(s,a)\ns\na\nh\u22121\nV\u2217\nh Q\u2217\nh\nh\na Q\u2217\nh\nQ\u2217", "h Q\u2217\nh\nh\na Q\u2217\nh\nQ\u2217\n0(s,a)=0\nQ\u2217\n1(s,a)=R(s,a)+0\nQ\u2217\n2(s,a)=R(s,a)+\u03b3\u2211\ns\u2032T(s,a,s\u2032)max\na\u2032Q\u2217\n1(s\u2032,a\u2032)\n\u22ee\nQ\u2217\nh(s,a)=R(s,a)+\u03b3\u2211\ns\u2032T(s,a,s\u2032)max\na\u2032Q\u2217\nh\u22121(s\u2032,a\u2032)\n(s\u2032,a\u2032)\nQ\u2217\nh\nQ\u2217\nh\nh Q\u2217\nh \u03c0\u2217\nh\n\u03c0\u2217\nh(s)=argmax\naQ\u2217", "\u03c0\u2217\nh(s)=argmax\naQ\u2217\nh(s,a).\nh\n\u03c0\u2217\nh\u22121(s) (h\u22121)\nR(s,a)\n10.2.2 Finding optimal in\ufb01nite-horizon policiesWe can also de\ufb01ne the action-value\nfun ction for a \ufb01xed policy ,\ndenoted by . Th is qua ntity", "represents the expected sum  of\ndiscoun ted rewards obtained by\ntaking action  in state  and\nthereafter following the policy \nover the remaining horizon of\n steps.\nSimilar to ,  satis\ufb01es", "the Bellman recur sion/equa tions\nintroduc ed earlier. In fact, for a\ndeterministic policy :\nHowever, since our  primary goal in\ndealing with action value s is\ntypically to identify an optimal", "policy, we will not dwell\nextensively on ( ). Instead,\nwe will place more emphasis on\nthe optimal action-value  fun ctions\n.\u03c0\nQ\u03c0\nh(s,a)\na s\n\u03c0\nh\u22121\nV\u03c0\nh(s)Q\u03c0\nh(s,a)\n\u03c0\nQ\u03c0\nh(s,\u03c0(s))=V\u03c0\nh(s).\nQ\u03c0\nh(s,a)\nQ\u2217", "h(s).\nQ\u03c0\nh(s,a)\nQ\u2217\nh(s,a) In contrast to the \ufb01nite-horizon case, the best way of behaving in an in\ufb01nite-horizon\ndiscounted MDP is not time-dependent. That is, the decisions you make at time", "looking forward to in\ufb01nity, will be the same decisions that you make at time\n for any positive , also looking forward to in\ufb01nity.", "An important theorem about MDPs is: in the in\ufb01nite-horizon case, there exists a\nstationary optimal policy  (there may be more than one) such that for all \nand all other policies , we have", "There are many methods for \ufb01nding an optimal policy for an MDP. We have already\nseen the \ufb01nite-horizon value iteration case. Here we will study a very popular and", "useful method for the in\ufb01nite-horizon case, in\ufb01nite-horizon value iteration. It is also\nimportant to us, because it is the basis of many reinforcement-learning methods.", "We will again assume that the reward function  is bounded on the set of all\npossible (state, action) pairs and additionally that the number of actions in the", "action space is \ufb01nite. De\ufb01ne  to be the expected in\ufb01nite-horizon value of\nbeing in state , executing action , and executing an optimal policy  thereafter.", "Using similar reasoning to the recursive de\ufb01nition of  we can express this value\nrecursively as\nThis is also a set of equations, one for each  pair. This time, though, they are", "not linear (due to the  operation), and so they are not easy to solve. But there is\na theorem that says they have a unique solution!", "Once we know the optimal action-value function , then we can extract an\noptimal policy  as\nWe can iteratively solve for the  values with the in\ufb01nite-horizon value iteration\nalgorithm, shown below:", "Algorithm 10.1 Infinite-Horizon-Value-Iteration\nRequire: , , , , , \nInitialization:\nfor each  and  do\nend for\nwhile not converged do\nfor each  and  do\nend for\nif  then\nreturn \nend ift=0\nt=T T\n\u03c0\u2217s\u2208S\n\u03c0", "t=T T\n\u03c0\u2217s\u2208S\n\u03c0\nV\u03c0(s)\u2264V\u03c0\u2217(s).\nR(s,a)\nQ\u2217\n\u221e(s,a)\ns a \u03c0\u2217\nV\u03c0,\nQ\u2217\n\u221e(s,a)=R(s,a)+\u03b3\u2211\ns\u2032T(s,a,s\u2032)max\na\u2032Q\u2217\n\u221e(s\u2032,a\u2032).\n(s,a)\nmax\nQ\u2217\n\u221e(s,a)\n\u03c0\u2217\n\u03c0\u2217(s)=argmax\naQ\u2217\n\u221e(s,a)\nQ\u2217\nSATR\u03b3\u03f5\n1:\n2: s\u2208S a\u2208A\n3:Qold(s,a)\u21900\n4:\n5:", "3:Qold(s,a)\u21900\n4:\n5:\n6: s\u2208S a\u2208A\n7: Qnew(s,a)\u2190R(s,a)+\u03b3\u2211\ns\u2032T(s,a,s\u2032)max\na\u2032Qold(s\u2032,a\u2032)\n8:\n9: max\ns,a|Qold(s,a)\u2212Qnew(s,a)|<\u03f5\n10: Qnew\n11: end while", "11: end while\nThere are a lot of nice theoretical results about in\ufb01nite-horizon value iteration. For\nsome given (not necessarily optimal)  function, de\ufb01ne .", "After executing in\ufb01nite-horizon value iteration with convergence hyper-\nparameter ,\nThere is a value of  such that\nAs the algorithm executes,  decreases monotonically on each\niteration.", "iteration.\nThe algorithm can be executed asynchronously, in parallel: as long as all \npairs are updated in\ufb01nitely often in an in\ufb01nite run, it still converges to the\noptimal value.12:Qold\u2190Qnew\n13:", "13:\nTheory\nQ \u03c0Q(s)=argmaxaQ(s,a)\n\u03f5\n\u2225V\u03c0Qnew\u2212V\u03c0\u2217\u2225max<\u03f5.\n\u03f5\n\u2225Qold\u2212Qnew\u2225max<\u03f5\u27f9\u03c0Qnew=\u03c0\u2217\n\u2225V\u03c0Qnew\u2212V\u03c0\u2217\u2225max\n(s,a) Th is page contains all content from the legacy PDF  notes; reinforcement learning chapter.", "As we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .\nReinforcement learning (RL) is a type of machine learning where an agent learns to", "make decisions by interacting with an environment. Unlike other learning\nparadigms, RL has several distinctive characteristics:", "The agent interacts directly with an environment, receiving feedback in the\nform of rewards or penalties\nThe agent can choose actions that in\ufb02uences what information it gains from the\nenvironment", "environment\nThe agent updates its decision-making strategy incrementally as it gains more\nexperience\nIn a reinforcement learning problem, the interaction between the agent and", "environment follows a speci\ufb01c pattern:\nL e a r n e r\nE n v i r o n m e n tr e w a r ds t a t e a c t i o n\nThe interaction cycle proceeds as follows:\n1. Agent observes the current state", "2. Agent selects and executes an action \n3. Agent receives a reward  from the environment\n4. Agent observes the new state \n5. Agent selects and executes a new action \n6. Agent receives a new reward", "7. This cycle continues\u2026\nSimilar to MDP Chapter 10, in an RL problem, the agent\u2019s goal is to learn a policy - a\nmapping from states to actions - that maximizes its expected cumulative reward", "over time. This policy guides the agent\u2019s decision-making process, helping it choose\nactions that lead to the most favorable outcomes.11  Reinforcement Learning\nNote\ns(i)\na(i)\nr(i)\ns(i+1)\na(i+1)", "r(i)\ns(i+1)\na(i+1)\nr(i+1)\uf461 11  Reinforcement Learning \uf52a 11.1 Reinforcement learning algorithms overview\nApproaches to reinforcement learning differ signi\ufb01cantly according to what kind of", "hypothesis or model is being learned. Roughly speaking, RL methods can be\ncategorized into model-free methods and model-based methods. The main", "distinction is that model-based methods explicitly learn the transition and reward\nmodels to assist the end-goal of learning a policy; model-free methods do not. We", "will start our discussion with the model-free methods, and introduce two of the\narguably most popular types of algorithms, Q-learning Section 11.1.2 and policy", "gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,\nwe brie\ufb02y consider \u201cbandit\u201d problems Section 11.5, which differ from our MDP", "learning context by having probabilistic rewards.\nModel-free methods are methods that do not explicitly learn transition and reward", "models. Depending on what is explicitly being learned, model-free methods are\nsometimes further categorized into value-based methods (where the goal is to", "learn/estimate a value function) and policy-based methods (where the goal is to\ndirectly learn an optimal policy). It\u2019s important to note that such categorization is", "approximate and the boundaries are blurry. In fact, current RL research tends to\ncombine the learning of value functions, policies, and transition and reward models", "all into a complex learning algorithm, in an attempt to combine the strengths of\neach approach.\nQ-learning is a frequently used class of RL algorithms that concentrates on learning", "(estimating) the state-action value function, i.e., the  function. Speci\ufb01cally, recall\nthe MDP value-iteration update:", "The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,\nwhere we do not know the transition function  or reward function , and instead", "rely on samples to perform the updates.\nprocedure Q-L EARNING( )\nfor all  do\nend for\nwhile  do\n11.1.1 Model-free methods\n11.1.2 Q-learningQ\nQ(s,a)=R(s,a)+\u03b3\u2211\ns\u2032T(s,a,s\u2032)max\na\u2032Q(s\u2032,a\u2032)\nT R", "a\u2032Q(s\u2032,a\u2032)\nT R\n1: S,A,\u03b3,\u03b1,s0,max_iter\n2:i\u21900\n3: s\u2208S,a\u2208A\n4: Qold(s,a)\u21900\n5:\n6:s\u2190s0\n7: i<max_iterTh e thing that most stud ents seem\nto get confus ed about is when we\ndo value  iteration and when we do", "Q- learning. Value  iteration\nassum es you know  and  and\njus t need to compute . In Q-\nlearning, we don\u2019t know or even\ndirectly estimate  and : we\nestimate  directly from\nexperience!T R\nQ\nT R", "Q\nT R\nQ end while\nreturn \nend procedure\nWith the pseudo\u2011code provided for Q\u2011Learning, there are a few key things to note.", "First, we must determine which state to initialize the learning from. In the context of\na game, this initial state may be well de\ufb01ned. In the context of a robot navigating an", "environment, one may consider sampling the initial state at random. In any case,\nthe initial state is necessary to determine the trajectory the agent will experience as\nit navigates the environment.", "Second, different contexts will in\ufb02uence how we want to choose when to stop\niterating through the while loop. Again, in some games there may be a clear", "terminating state based on the rules of how it is played. On the other hand, a robot\nmay be allowed to explore an environment ad in\ufb01nitum. In such a case, one may", "consider either setting a \ufb01xed number of transitions (as done explicitly in the\npseudo\u2011code) to take; or we may want to stop iterating in the example once the", "values in the Q\u2011table are not changing, after the algorithm has been running for a\nwhile.\nFinally, a single trajectory through the environment may not be suf\ufb01cient to", "adequately explore all state\u2011action pairs. In these instances, it becomes necessary to\nrun through a number of iterations of the Q\u2011Learning algorithm, potentially with", "different choices of initial state .\nOf course, we would then want to modify Q\u2011Learning such that the Q table is not\nreset with each call.", "Now, let\u2019s dig into what is happening in Q\u2011Learning. Here,  represents the\nlearning rate, which needs to decay for convergence purposes, but in practice is often", "set to a constant. It\u2019s also worth mentioning that Q-learning assumes a discrete state\nand action space where states and actions take on discrete values like  etc.", "In contrast, a continuous state space would allow the state to take values from, say,\na continuous range of numbers; for example, the state could be any real number in", "the interval . Similarly, a continuous action space would allow the action to be\ndrawn from, e.g., a continuous range of numbers. There are now many extensions", "developed based on Q-learning that can handle continuous state and action spaces\n(we\u2019ll look at one soon), and therefore the algorithm above is also sometimes", "referred to more speci\ufb01cally as tabular Q-learning.\nIn the Q-learning update rule8: a\u2190select_action(s,Qold(s,a))\n9: (r,s\u2032)\u2190execute(a)\n10: Qnew(s,a)\u2190(1\u2212\u03b1)Qold(s,a)+\u03b1(r+\u03b3maxa\u2032Qold(s\u2032,a\u2032))\n11: s\u2190s\u2032", "11: s\u2190s\u2032\n12: i\u2190i+1\n13: Qold\u2190Qnew\n14:\n15: Qnew\n16:\ns0\n\u03b1\u2208(0,1]\n1,2,3,\u2026\n[1,3]\nQ[s,a]\u2190(1\u2212\u03b1)Q[s,a]+\u03b1(r+\u03b3max\na\u2032Q[s\u2032,a\u2032]) (11.1)Th is notion of run ning a num ber of\ninstances of Q\u2011 Learning is often", "referred to as experiencing\nmultiple episodes. the term  is often referred to as the one-step look-ahead target.\nThe update can be viewed as a combination of two different iterative processes that", "we have already seen: the combination of an old estimate with the target using a\nrunning average with a learning rate \nEquation 11.1 can also be equivalently rewritten as", "which allows us to interpret Q-learning in yet another way: we make an update (or\ncorrection) based on the temporal difference between the target and the current\nestimated value", "estimated value \nThe Q-learning algorithm above includes a procedure called select_action, that,\ngiven the current state  and current  function, has to decide which action to take.", "If the  value is estimated very accurately and the agent is deployed to \u201cbehave\u201d in\nthe world (as opposed to \u201clearn\u201d in the world), then generally we would want to", "choose the apparently optimal action .\nBut, during learning, the  value estimates won\u2019t be very good and exploration is\nimportant. However, exploring completely at random is also usually not the best", "strategy while learning, because it is good to focus your attention on the parts of the\nstate space that are likely to be visited when executing a good policy (not a bad or\nrandom one).", "random one).\nA typical action-selection strategy that attempts to address this exploration versus\nexploitation dilemma is the so-called -greedy strategy:\nwith probability , choose ;", "with probability , choose the action  uniformly at random.\nwhere the  probability of choosing a random action helps the agent to explore and", "try out actions that might not seem so desirable at the moment.\nQ-learning has the surprising property that it is guaranteed to converge to the actual", "optimal  function! The conditions speci\ufb01ed in the theorem are: visit every state-\naction pair in\ufb01nitely often, and the learning rate  satis\ufb01es a scheduling condition.", "This implies that for exploration strategy speci\ufb01cally, any strategy is okay as long as\nit tries every state-action in\ufb01nitely often on an in\ufb01nite run (so that it doesn\u2019t", "converge prematurely to a bad action choice).\nQ-learning can be very inef\ufb01cient. Imagine a robot that has a choice between", "moving to the left and getting a reward of 1, then returning to its initial state, or\nmoving to the right and walking down a 10-step hallway in order to get a reward of", "1000, then returning to its initial state.(r+\u03b3maxa\u2032Q[s\u2032,a\u2032])\n\u03b1.\nQ[s,a]\u2190Q[s,a]+\u03b1((r+\u03b3max\na\u2032Q[s\u2032,a\u2032])\u2212Q[s,a]), (11.2)\nQ[s,a].\ns Q\nQ\nargmaxa\u2208AQ(s,a)\nQ\n\u03f5\n1\u2212\u03f5 argmaxa\u2208AQ(s,a)\n\u03f5 a\u2208A\n\u03f5\nQ", "\u03f5 a\u2208A\n\u03f5\nQ\n\u03b1 ro b o t 1 2 3 4 5 6 7 8 9 1 0+ 1 0 0 0 + 1\n- 1\nThe \ufb01rst time the robot moves to the right and goes down the hallway, it will", "update the  value just for state 9 on the hallway and action ``right\u2019\u2019 to have a high\nvalue, but it won\u2019t yet understand that moving to the right in the earlier steps was a", "good choice. The next time it moves down the hallway it updates the value of the\nstate before the last one, and so on. After 10 trips down the hallway, it now can see", "that it is better to move to the right than to the left.\nMore concretely, consider the vector of Q values , representing\nthe Q values for moving right at each of the positions . Position index 0", "is the starting position of the robot as pictured above.\nThen, for  and , Equation 11.2 becomes\nStarting with Q values of 0,\nSince the only nonzero reward from moving right is , after our", "robot makes it down the hallway once, our new Q vector is\nAfter making its way down the hallway again,\nupdates:\nSimilarly,Q\nQ(i=0,\u2026,9;right)\ni=0,\u2026,9\n\u03b1=1 \u03b3=0.9\nQ(i,right)=R(i,right)+0.9max\naQ(i+1,a).", "aQ(i+1,a).\nQ(0)(i=0,\u2026,9;right)=[ ]. 0000000000\nR(9,right)=1000\nQ(1)(i=0,\u2026,9;right)=[ ]. 0000000001000\nQ(8,right)=0+0.9Q(9,right)=900\nQ(2)(i=0,\u2026,9;right)=[ ]. 000000009001000", "Q(3)(i=0,\u2026,9;right)=[ ], 00000008109001000\nQ(4)(i=0,\u2026,9;right)=[ ], 0000007298109001000We are violating our  us ua l\nnotational conventions here, and\nwriting  to mean the Q value", "fun ction that results after the robot\nrun s all the way to the end of the\nhallway, when executing the policy", "that always moves to the right.Qi and the robot \ufb01nally sees the value of moving right from position 0.\n\u2753  Study Question\nDetermine the Q value functions that result from always executing the \u201cmove", "left\u201d policy.\n11.2 Function approximation: Deep Q learning\nIn our Q-learning algorithm above, we essentially keep track of each  value in a", "table, indexed by  and . What do we do if  and/or  are large (or continuous)?\nWe can use a function approximator like a neural network to store Q values. For", "example, we could design a neural network that takes inputs  and , and outputs\n. We can treat this as a regression problem, optimizing this loss:\nwhere  is now the output of the neural network.", "There are several different architectural choices for using a neural network to\napproximate  values:\nOne network for each action , that takes  as input and produces  as\noutput;", "output;\nOne single network that takes  as input and produces a vector ,\nconsisting of the  values for each action; or\nOne single network that takes  concatenated into a vector (if  is discrete,", "we would probably use a one-hot encoding, unless it had some useful internal\nstructure) and produces  as output.\nThe \ufb01rst two choices are only suitable for discrete (and not too big) action sets. The", "last choice can be applied for continuous actions, but then it is dif\ufb01cult to \ufb01nd\n.\nThere are not many theoretical guarantees about Q-learning with function", "approximation and, indeed, it can sometimes be fairly unstable (learning to perform\nwell for a while, and then suddenly getting worse, for example). But neural", "network Q-learning has also had some signi\ufb01cant successes.\u2026\nQ(10)(i=0,\u2026,9;right)=[ ], 387.4420.5478.3531.4590.5656.17298109001000\nQ\ns a S A\ns a\nQ(s,a)\n(Q(s,a)\u2212(r+\u03b3max\na\u2032Q(s\u2032,a\u2032)))2\nQ(s,a)\nQ", "Q(s,a)\nQ\na s Q(s,a)\ns Q(s,\u22c5)\nQ\ns,a a\nQ(s,a)\nargmaxa\u2208AQ(s,a)Here, we can see the\nexploration/exploitation dilemma\nin action: from the perspective of\n, it will seem that getting the", "immediate reward of  is a better\nstrategy without exploring the long\nhallway.s0=0\n1\nTh is is the so-called squa red\nBellman error; as the name\nsug gests, it\u2019s closely related to the", "Bellman equa tion we saw in MDPs\nin Chapter Chapter 10. Roug hly\nspeaking, this error measur es how\nmuc h the Bellman equa lity is\nviolated.\nFor continuo us  action spaces, it is", "popular to us e a class of methods\ncalled actor-critic methods, which\ncombine policy and value -fun ction\nlearning. We won\u2019t get into them in", "detail here, thoug h. One form of instability that we do know how to guard against is catastrophic\nforgetting. In standard supervised learning, we expect that the training  values", "were drawn independently from some distribution.\nBut when a learning agent, such as a robot, is moving through an environment, the", "sequence of states it encounters will be temporally correlated. For example, the\nrobot might spend 12 hours in a dark environment and then 12 in a light one. This", "can mean that while it is in the dark, the neural-network weight-updates will make\nthe  function \"forget\" the value function for when it\u2019s light.", "One way to handle this is to use experience replay, where we save our \nexperiences in a replay buffer. Whenever we take a step in the world, we add the", "to the replay buffer and use it to do a Q-learning update. Then we also\nrandomly select some number of tuples from the replay buffer, and do Q-learning", "updates based on them as well. In general, it may help to keep a sliding window of\njust the 1000 most recent experiences in the replay buffer. (A larger buffer will be", "necessary for situations when the optimal policy might visit a large part of the state\nspace, but we like to keep the buffer size small for memory reasons and also so that", "we don\u2019t focus on parts of the state space that are irrelevant for the optimal policy.)\nThe idea is that it will help us propagate reward values through our state space", "more ef\ufb01ciently if we do these updates. We can see it as doing something like value\niteration, but using samples of experience rather than a known model.", "An alternative strategy for learning the  function that is somewhat more robust\nthan the standard -learning algorithm is a method called \ufb01tted Q.\nprocedure FITTED -Q-L EARNING( )", "//e.g.,  can be drawn randomly from \ninitialize neural-network representation of \nwhile True do\n experience from executing -greedy policy based on  for  steps\n represented as tuples", "for each tuple  do\nend for\nre-initialize neural-network representation of \nend while\nend procedure\nHere, we alternate between using the policy induced by the current  function to", "gather a batch of data , adding it to our overall data set , and then using\nsupervised neural-network training to learn a representation of the  value", "function on the whole data set. This method does not mix the dynamic-x\nQ\n(s,a,s\u2032,r)\n(s,a,s\u2032,r)\n11.2.1 Fitted Q-learning\nQ\nQ\n1: A,s0,\u03b3,\u03b1,\u03f5,m\n2:s\u2190s0 s0 S\n3:D\u2190\u2205\n4: Q\n5:\n6: Dnew\u2190 \u03f5 Q m", "5:\n6: Dnew\u2190 \u03f5 Q m\n7: D\u2190D\u222aDnew (s,a,s\u2032,r)\n8: Dsupervised\u2190\u2205\n9: (s,a,s\u2032,r)\u2208D\n10: x\u2190(s,a)\n11: y\u2190r+\u03b3maxa\u2032\u2208AQ(s\u2032,a\u2032)\n12: Dsupervised\u2190Dsupervised\u222a{(x,y)}\n13:\n14: Q", "13:\n14: Q\n15: Q\u2190supervised-NN-regression(Dsupervised)\n16:\n17:\nQ\nDnew D\nQAnd, in fact, we routinely shuf\ufb02e", "their order in the data \ufb01le, anyway. programming phase (computing new  values based on old ones) with the function\napproximation phase (supervised training of the neural network) and avoids", "catastrophic forgetting. The regression training in line 10 typically uses squared\nerror as a loss function and would be trained until the \ufb01t is good (possibly\nmeasured on held-out data).", "11.3 Policy gradient\nA different model-free strategy is to search directly for a good policy. The strategy\nhere is to de\ufb01ne a functional form  for the policy, where  represents the", "parameters we learn from experience. We choose  to be differentiable, and often\nde\ufb01ne\n, a conditional probability distribution over our possible actions.", "Now, we can train the policy parameters using gradient descent:\nWhen  has relatively low dimension, we can compute a numeric estimate of", "the gradient by running the policy multiple times for different values of , and\ncomputing the resulting rewards.\nWhen  has higher dimensions (e.g., it represents the set of parameters in a", "complicated neural network), there are more clever algorithms, e.g., one called\nREINFORCE, but they can often be dif\ufb01cult to get to work reliably.", "Policy search is a good choice when the policy has a simple known form, but the\nMDP would be much more complicated to estimate.\n11.4 Model-based RL", "11.4 Model-based RL\nThe conceptually simplest approach to RL is to model  and  from the data we\nhave gotten so far, and then use those models, together with an algorithm for", "solving MDPs (such as value iteration) to \ufb01nd a policy that is near-optimal given\nthe current models.\nAssume that we have had some set of interactions with the environment, which can", "be characterized as a set of tuples of the form .\nBecause the transition function  speci\ufb01es probabilities, multiple\nobservations of  may be needed to model the transition function. One", "approach to building a model  for the true  is to estimate it using\na simple counting strategy:Q\nf(s;\u03b8)=a \u03b8\nf\nf(s,a;\u03b8)=Pr(a|s)\n\u03b8\n\u03b8\n\u03b8\nR T\n(s(t),a(t),s(t+1),r(t))\nT(s,a,s\u2032)\n(s,a,s\u2032)", "T(s,a,s\u2032)\n(s,a,s\u2032)\n^T(s,a,s\u2032) T(s,a,s\u2032)\n^T(s,a,s\u2032)=#(s,a,s\u2032)+1\n#(s,a)+|S|.Th is means the chance of choosing\nan action depends on which state\nthe agent is in. Sup pose, e.g., a", "robot is trying to get to a goal and\ncan go left or right. An\nun conditional policy can say: I go\nleft 99% of the time; a conditional\npolicy can consider the robot\u2019s", "state, and say: if I\u2019m to the right of\nthe goal, I go left 99% of the time. Here,  represents the number of times in our data set we have the\nsituation where , , , and  represents the number of", "times in our data set we have the situation where , .\nAdding 1 and  to the numerator and denominator, respectively, is a form of", "smoothing called the Laplace correction. It ensures that we never estimate that a\nprobability is 0, and keeps us from dividing by 0. As the amount of data we gather", "increases, the in\ufb02uence of this correction fades away.\nIn contrast, the reward function  is a deterministic function, such that", "knowing the reward  for a given  is suf\ufb01cient to fully determine the function\nat that point. Our model  can simply be a record of observed rewards, such that\n.", ".\nGiven empirical models  and  for the transition and reward functions, we can\nnow solve the MDP  to \ufb01nd an optimal policy using value iteration, or", "use a search algorithm to \ufb01nd an action to take for a particular state.\nThis approach is effective for problems with small state and action spaces, where it", "is not too hard to get enough experience to model  and  well; but it is dif\ufb01cult to\ngeneralize this method to handle continuous (or very large discrete) state spaces,", "and is a topic of current research.\n11.5 Bandit problems\nBandit problems are a subset of reinforcement learning problems. A basic bandit\nproblem is given by:\nA set of actions ;", "A set of actions ;\nA set of reward values ; and\nA probabilistic reward function , i.e.,  is a function that\ntakes an action and a reward and returns the probability of getting that reward", "conditioned on that action being taken,\n. Each time the agent takes an action, a\nnew value is drawn from this distribution.\nThe most typical bandit problem has  and . This is called a -", "armed bandit problem, where the decision is which \u201carm\u201d (action ) to select, and the\nreward is either getting a payoff () or not ().", "The important question is usually one of exploration versus exploitation. Imagine you\nhave tried each action 10 times, and now you have estimates  for the", "probabilities . Which arm should you pick next? You could:\nexploit your knowledge, choosing the arm with the highest value of expected\nreward; or#(s,a,s\u2032)\ns(t)=sa(t)=as(t+1)=s\u2032#(s,a)\ns(t)=sa(t)=a\n|S|", "s(t)=sa(t)=a\n|S|\nR(s,a)\nr (s,a)\n^R\n^R(s,a)=r=R(s,a)\n^T ^R\n(S,A,^T,^R)\nT R\nA\nR\nRp:A\u00d7R\u2192R Rp\nRp(a,r)=Pr(reward=r\u2223action=a)\nR={0,1} |A|=k k\na\n1 0\n^Rp(a,r)\nRp(a,r)Conceptua lly, this is similar to", "having \u201cinitialized\u201d our  estimate\nfor the transition fun ction with\nun iform random probabilities\nbefore making any observations.\nNotice that this probablistic\nrewards set up  in bandits differs", "from the \u201crewards are\ndeterministic\u201d assum ptions we\nmade so far.\nW hy \u201cbandit\u201d? In English slang,\n\u201cone-armed bandit\u201d refers to a slot\nmachine becaus e it has one arm\nand takes your  money! Here, we", "have a similar machine but with \narms.k explore further, trying some or all actions more times to get better estimates of\nthe  values.", "the  values.\nThe theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1\nour discount factor), the more time we should spend exploring, so that we don\u2019t", "converge prematurely on a bad choice of action.\nBandit problems are reinforcement learning problems (and very different from\nbatch supervised learning) in that:", "The agent gets to in\ufb02uence what data it obtains (selecting  gives it another\nsample from ), and\nThe agent is penalized for mistakes it makes while it is learning (trying to", "maximize the expected reward it gets while behaving).\nIn a contextual bandit problem, you have multiple possible states from some set ,\nand a separate bandit problem associated with each one.", "Bandit problems are an essential subset of reinforcement learning. It\u2019s important to\nbe aware of the issues, but we will not study solutions to them in this class.Rp(a,r)\nh\na\nR(a,r)", "h\na\nR(a,r)\nS Th is page contains all content from the legacy PDF  notes; non-parametric models chapter.\nAs we phase out the PDF , this page may receive up dates not re\ufb02e cted in the static PDF .", "Neural networks have adaptable complexity, in the sense that we can try different\nstructural models and use cross validation to \ufb01nd one that works well on our data.", "Beyond neural networks, we may further broaden the class of models that we can\n\ufb01t to our data, for example as illustrated by the techniques introduced in this\nchapter.", "chapter.\nHere, we turn to models that automatically adapt their complexity to the training\ndata. The name non-parametric methods is misleading: it is really a class of methods", "that does not have a \ufb01xed parameterization in advance. Rather, the complexity of\nthe parameterization can grow as we acquire more data.", "Some non-parametric models, such as nearest-neighbor, rely directly on the data to\nmake predictions and do not compute a model that summarizes the data. Other", "non-parametric methods, such as decision trees, can be seen as dynamically\nconstructing something that ends up looking like a more traditional parametric", "model, but where the actual training data affects exactly what the form of the model\nwill be.\nThe non-parametric methods we consider here tend to have the form of a\ncomposition of simple models:", "Nearest neighbor models: Section 12.1 where we don\u2019t process data at training\ntime, but do all the work when making predictions, by looking for the closest", "training example(s) to a given new data point.\nTree models: Section 12.2 where we partition the input space and use different", "simple predictions on different regions of the space; the hypothesis space can\nbecome arbitrarily large allowing \ufb01ner and \ufb01ner partitions of the input space.", "Ensemble models: Section 12.2.3 in which we train several different classi\ufb01ers on\nthe whole space and average the answers; this decreases the estimation error. In", "particular, we will look at bootstrap aggregation, or bagging of trees.\nBoosting is a way to construct a model composed of a sequence of component", "models (e.g., a model consisting of a sequence of trees, each subsequent tree\nseeking to correct errors in the previous trees) that decreases both estimation", "and structural error. We won\u2019t consider this in detail in this class.12  Non-parametric methods", "Note\uf46112  Non-parametric methods \uf52a *-means clustering methods, Section 12.3 where we partition data into groups\nbased on similarity without prede\ufb01ned labels, adapting complexity by", "adjusting the number of clusters.\nWhy are we studying these methods, in the heyday of complicated models such as\nneural networks ?", "neural networks ?\nThey are fast to implement and have few or no hyperparameters to tune.\nThey often work as well as or better than more complicated methods.", "Predictions from some of these models can be easier to explain to a human\nuser: decision trees are fairly directly human-interpretable, and nearest", "neighbor methods can justify their decisions to some extent by showing a few\ntraining examples that the predictions were based on.\n12.1 Nearest Neighbor", "In nearest-neighbor models, we don\u2019t do any processing of the data at training time\n\u2013 we just remember it! All the work is done at prediction time.", "Input values  can be from any domain  (, documents, tree-structured objects,\netc.). We just need a distance metric, , which satis\ufb01es the following,\nfor all :", "for all :\nGiven a data-set , our predictor for a new  is\nthat is, the predicted output associated with the training point that is closest to the", "query point . Tie breaking is typically done at random.\nThis same algorithm works for regression and classi\ufb01cation!\nThe nearest neighbor prediction function can be described by dividing the space up", "into regions whose closest point is each individual training point as shown below :k\nx XRd\nd:X\u00d7X\u2192R+\nx,x\u2032,x\u2032\u2032\u2208X\nd(x,x)=0\nd(x,x\u2032)=d(x\u2032,x)\nd(x,x\u2032\u2032)\u2264d(x,x\u2032)+d(x\u2032,x\u2032\u2032)\nD={(x(i),y(i))}n\ni=1x\u2208X", "i=1x\u2208X\nh(x)=y(i)wherei=argmin\nid(x,x(i)),\nx In each region, we predict the associated  value.\nThere are several useful variations on this method. In -nearest-neighbors, we \ufb01nd", "the  training points nearest to the query point  and output the majority  value\nfor classi\ufb01cation or the average for regression. We can also do locally weighted", "regression in which we \ufb01t locally linear regression models to the  nearest points,\npossibly giving less weight to those that are farther away. In large data-sets, it is", "important to use good data structures (e.g., ball trees) to perform the nearest-\nneighbor look-ups ef\ufb01ciently (without looking at all the data points each time).\n12.2 Tree Models", "12.2 Tree Models\nThe idea here is that we would like to \ufb01nd a partition of the input space and then \ufb01t\nvery simple models to predict the output in each piece. The partition is described", "using a (typically binary) \u201ctree\u201d that recursively splits the space.\nTree methods differ by:\nThe class of possible ways to split the space at each node; these are typically", "linear splits, either aligned with the axes of the space, or sometimes using more\ngeneral classi\ufb01ers.\nThe class of predictors within the partitions; these are often simply constants,", "but may be more general classi\ufb01cation or regression models.\nThe way in which we control the complexity of the hypothesis: it would be", "within the capacity of these methods to have a separate partition element for\neach individual training example.y\nk\nk x y\nk The algorithm for making the partitions and \ufb01tting the models.", "One advantage of tree models is that they are easily interpretable by humans. This\nis important in application domains, such as medicine, where there are human", "experts who often ultimately make critical decisions and who need to feel con\ufb01dent\nin their understanding of recommendations made by an algorithm. Below is an", "example decision tree, illustrating how one might be able to understand the\ndecisions made by the tree.\n#Example Here is a sample tree (reproduc ed from Breiman, Friedman, Ol shen, Stone\n(1984)):", "(1984)):\nThese methods are most appropriate for domains where the input space is not very\nhigh-dimensional and where the individual input features have some substantially", "useful information individually or in small groups. Trees would not be good for\nimage input, but might be good in cases with, for example, a set of meaningful", "measurements of the condition of a patient in the hospital, as in the example above.\nWe\u2019ll concentrate on the CART/ID3 (\u201cclassi\ufb01cation and regression trees\u201d and", "\u201citerative dichotomizer 3\u201d, respectively) family of algorithms, which were invented\nindependently in the statistics and the arti\ufb01cial intelligence communities. They", "work by greedily constructing a partition, where the splits are axis aligned and by\n\ufb01tting a constant model in the leaves. The interesting questions are how to select the", "splits and how to control complexity. The regression and classi\ufb01cation versions are\nvery similar.\nAs a concrete example, consider the following images:\nNote", "Note  \nThe left image depicts a set of labeled data points in a two-dimensional feature\nspace. The right shows a partition into regions by a decision tree, in this case having", "no classi\ufb01cation errors in the \ufb01nal partitions.\nThe predictor is made up of\na partition function, , mapping elements of the input space into exactly one of\n regions, , and", "regions, , and\na collection of  output values, , one for each region.\nIf we already knew a division of the space into regions, we would set , the", "constant output for region , to be the average of the training output values in\nthat region. For a training data set , we let  be an", "indicator set of all of the elements within , so that  for our whole\ndata set. We can de\ufb01ne  as the subset of data set samples that are in region , so\nthat . Then", "that . Then\nWe can de\ufb01ne the error in a region as . For example,  as the sum of squared\nerror would be expressed as\nIdeally, we should select the partition to minimize", "for some regularization constant . It is enough to search over all partitions of the\ntraining data (not all partitions of the input space!) to optimize this, but the problem\nis NP-complete.", "is NP-complete.\n12.2.1 Regression\u03c0\nM R1,\u2026,RM\nM Om\nOm\nRm\nD={(x(i),y(i))},i=1,\u2026nI\nD I={1,\u2026,n}\nIm Rm\nIm={i\u2223x(i)\u2208Rm}\nOm=averagei\u2208Im y(i).\nEm Em\nEm=\u2211\ni\u2208Im(y(i)\u2212Om)2.\n\u03bbM+M\n\u2211\nm=1Em,\n\u03bb", "\u03bbM+M\n\u2211\nm=1Em,\n\u03bb\n12.2.1.1 Building a tree So, we\u2019ll be greedy. We establish a criterion, given a set of data, for \ufb01nding the best", "single split of that data, and then apply it recursively to partition the space. For the\ndiscussion below, we will select the partition of the data that minimizes the sum of the", "sum of squared errors of each partition element. Then later, we will consider other\nsplitting criteria.\nGiven a data set , we now consider  to be an", "indicator of the subset of elements within  that we wish to build a tree (or subtree)\nfor. That is,  may already indicate a subset of data set , based on prior splits in", "constructing our overall tree. We de\ufb01ne terms as follows:\n indicates the set of examples (subset of ) whose feature value in dimension\n is greater than or equal to split point ;", "indicates the set of examples (subset of ) whose feature value in dimension\n is less than ;\n is the average  value of the data points indicated by set ; and", "is the average  value of the data points indicated by set .\nHere is the pseudocode. In what follows,  is the largest leaf size that we will allow", "in the tree, and is a hyperparameter of the algorithm.\nprocedure BUILD T REE()\nif  then\nreturn \nelse\nfor all split dimension , split value  do\nend for\nreturn \nend if\nend procedure", "end procedure\nIn practice, we typically start by calling BuildTree with the \ufb01rst input equal to our\nwhole data set (that is, with ). But then that call of BuildTree can", "recursively lead to many other calls of BuildTree.\nLet\u2019s think about how long each call of BuildTree takes to run. We have to", "consider all possible splits. So we consider a split in each of the  dimensions. In\neach dimension, we only need to consider splits between two data points (any otherD={(x(i),y(i))},i=1,\u2026n I\nD\nI D\nI+", "D\nI D\nI+\nj,s I\nj s\nI\u2212\nj,sI\nj s\n^y+\nj,s y I+\nj,s\n^y\u2212\nj,s y I\u2212\nj,s\nk\n1: I,k\n2: |I|\u2264k\n3: ^y\u21901\n|I|\u2211i\u2208Iy(i)\n4: Leaf(value=^y)\n5:\n6: j s\n7: I+\nj,s\u2190{i\u2208I\u2223x(i)\nj\u2265s}\n8: I\u2212\nj,s\u2190{i\u2208I\u2223x(i)\nj<s}\n9: ^y+\nj,s\u21901\n|I+", "9: ^y+\nj,s\u21901\n|I+\nj,s|\u2211i\u2208I+\nj,sy(i)\n10: ^y\u2212\nj,s\u21901\n|I\u2212\nj,s|\u2211i\u2208I\u2212\nj,sy(i)\n11: Ej,s\u2190\u2211i\u2208I+\nj,s(y(i)\u2212^y+\nj,s)2+\u2211i\u2208I\u2212\nj,s(y(i)\u2212^y\u2212\nj,s)2\n12:\n13: (j\u2217,s\u2217)\u2190argminj,sEj,s\n14:\n15: Node(j\u2217,s\u2217,BuildTree(I\u2212", "j\u2217,s\u2217,k),BuildTree(I+\nj\u2217,s\u2217,k))\n16:\n17:\nI={1,\u2026,n}\nd split will give the same error on the training data). So, in total, we consider \nsplits in each call to BuildTree.", "It might be tempting to regularize by using a somewhat large value of , or by\nstopping when splitting a node does not signi\ufb01cantly decrease the error. One", "problem with short-sighted stopping criteria is that they might not see the value of\na split that will require one more split before it seems useful. So, we will tend to", "build a tree that is too large, and then prune it back.\nWe de\ufb01ne cost complexity of a tree , where  ranges over its leaves, as", "and  is the number of leaves. For a \ufb01xed , we can \ufb01nd a  that (approximately)\nminimizes  by \u201cweakest-link\u201d pruning:\nCreate a sequence of trees by successively removing the bottom-level split that", "minimizes the increase in overall error, until the root is reached.\nReturn the  in the sequence that minimizes the cost complexity.\nWe can choose an appropriate  using cross validation.", "The strategy for building and pruning classi\ufb01cation trees is very similar to the\nstrategy for regression trees.\nGiven a region  corresponding to a leaf of the tree, we would pick the output", "class  to be the value that exists most frequently (the majority value) in the data\npoints whose  values are in that region, i.e., data points indicated by :", "Let\u2019s now de\ufb01ne the error in a region as the number of data points that do not have\nthe value :\nWe de\ufb01ne the empirical probability of an item from class  occurring in region  as:", "where  is the number of training points in region ; that is,  For later\nuse, we\u2019ll also de\ufb01ne the empirical probabilities of split values, , as the", "fraction of points with dimension  in split  occurring in region  (one branch ofO(dn)\n12.2.1.2 Pruning\nk\nT m\nC\u03b1(T)=|T|\n\u2211\nm=1Em(T)+\u03b1|T|,\n|T| \u03b1 T\nC\u03b1(T)\nT\n\u03b1\n12.2.2 Classi\ufb01cation\nRm\ny\nx Im", "Rm\ny\nx Im\nOm=majorityi\u2208Im y(i).\nOm\nEm={i\u2223i\u2208Imandy(i)\u2260Om}.\n\u2223\u2223k m\n^Pm,k=^P(Im,k)={i\u2223i\u2208Imandy(i)=k}\nNm,\n\u2223\u2223Nm m Nm=|Im|.\n^Pm,j,s", "^Pm,j,s\njs m the tree), and  as the complement (the fraction of points in the other\nbranch).\nIn our greedy algorithm, we need a way to decide which split to make next. There", "are many criteria that express some measure of the \u201cimpurity\u201d in child nodes. Some\nmeasures include:\nMisclassi\ufb01cation error:\nGini index:\nEntropy:", "Entropy:\nSo that the entropy  is well-de\ufb01ned when , we will stipulate that\n.\nThese splitting criteria are very similar, and it\u2019s not entirely obvious which one is", "better. We will focus on entropy, just to be concrete.\nAnalogous to how for regression we choose the dimension  and split  that", "minimizes the sum of squared error , for classi\ufb01cation, we choose the dimension\n and split  that minimizes the weighted average entropy over the \u201cchild\u201d data", "points in each of the two corresponding splits,  and . We calculate the entropy\nin each split based on the empirical probabilities of class memberships in the split,", "and then calculate the weighted average entropy  as\nChoosing the split that minimizes the entropy of the children is equivalent to\nmaximizing the information gain of the test , de\ufb01ned by", "In the two-class case (with labels 0 and 1), all of the splitting criteria mentioned\nabove have the values1\u2212^Pm,j,s\nSp litting criteria\nQm(T)=Em\nNm=1\u2212^Pm,Om\nQm(T)=\u2211\nk^Pm,k(1\u2212^Pm,k)\nQm(T)=H(Im)=\u2212\u2211", "Qm(T)=H(Im)=\u2212\u2211\nk^Pm,klog2^Pm,k\nH ^P=0\n0log20=0\nj s\nEj,s\nj s\nI+\nj,sI\u2212\nj,s\n^H\n^H=(fractionofpointsinleftdataset)\u22c5H(I\u2212\nj,s)\n+(fractionofpointsinrightdataset)\u22c5H(I+\nj,s)\n=(1\u2212^Pm,j,s)H(I\u2212\nj,s)+^Pm,j,sH(I+", "j,s)+^Pm,j,sH(I+\nj,s)\n=|I\u2212\nj,s|\nNm\u22c5H(I\u2212\nj,s)+|I+\nj,s|\nNm\u22c5H(I+\nj,s).\nxj=s\ninfoGain(xj=s,Im)=H(Im)\u2212(|I\u2212\nj,s|\nNm\u22c5H(I\u2212\nj,s)+|I+\nj,s|\nNm\u22c5H(I+\nj,s))", "j,s|\nNm\u22c5H(I+\nj,s))\n{ The respective impurity curves are shown below, where ; the vertical axis\nplots  for each of the three criteria.", "There used to be endless haggling about which impurity function one should use. It\nseems to be traditional to use entropy to select which node to split while growing the", "tree, and misclassi\ufb01cation error in the pruning criterion.\nOne important limitation or drawback in conventional trees is that they can have", "high estimation error: small changes in the data can result in very big changes in the\nresulting tree.\nBootstrap aggregation is a technique for reducing the estimation error of a non-linear", "predictor, or one that is adaptive to the data. The key idea applied to trees, is to\nbuild multiple trees with different subsets of the data, and then create an ensemble", "model that combines the results from multiple trees to make a prediction.\nConstruct  new data sets of size . Each data set is constructed by sampling", "data points with replacement from . A single data set is called bootstrap sample\nof .\nTrain a predictor  on each bootstrap sample.\nRegression case: bagged predictor is", "Classi\ufb01cation case: Let  be the number of classes. We \ufb01nd a majority bagged\npredictor as follows. We let  be a \u201cone-hot\u201d vector with a single 1 and{ .0.0when ^Pm,0=0.0\n0.0when ^Pm,0=1.0\np=^Pm,0\nQm(T)", "p=^Pm,0\nQm(T)\n12.2.3 Bagging\nB n n\nD\nD\n^fb(x)\n^fbag(x)=1\nBB\n\u2211\nb=1^fb(x).\nK\n^fb(x)  zeros, and de\ufb01ne the predicted output  for predictor  as\n. Then", ". Then\nwhich is a vector containing the proportion of classi\ufb01ers that predicted each\nclass  for input . Then the overall predicted output is", "There are theoretical arguments showing that bagging does, in fact, reduce\nestimation error. However, when we bag a model, any simple intrepetability is lost.", "Random forests are collections of trees that are constructed to be de-correlated, so\nthat using them to vote gives maximal advantage. In competitions, they often have", "excellent classi\ufb01cation performance among large collections of much fancier\nmethods.\nIn what follows, , , and  are hyperparameters of the algorithm.\nprocedure RANDOM F OREST( )\nfor  to  do", "for  to  do\nDraw a bootstrap sample  of size  from \nGrow tree  on :\nwhile there are splittable nodes do\nSelect  variables at random from the  total variables", "Pick the best variable and split point among those \nSplit the current node\nend while\nend for\nreturn \nend procedure\nGiven the ensemble of trees, vote to make a prediction on a new .", "There are many variations on the tree theme. One is to employ different regression\nor classi\ufb01cation methods in each leaf. For example, a linear regression might be", "used to model the examples in each leaf, rather than using a constant value.\nIn the relatively simple trees that we\u2019ve considered, splits have been based on only", "a single feature at a time, and with the resulting splits being axis-parallel. Other\nmethods for splitting are possible, including consideration of multiple features and", "linear classi\ufb01ers based on those, potentially resulting in non-axis-parallel splits.\nComplexity is a concern in such cases, as many possible combinations of featuresK\u22121 ^y fb\n^yb(x)=argmaxk^fb(x)k", "^fbag(x)=1\nBB\n\u2211\nb=1^fb(x),\nk x\n^ybag(x)=argmax\nk^fbag(x)k.\n12.2.4 Random Forests\nBmn\n1: B,m,n\n2:b=1B\n3: Dbn D\n4: TbDb\n5:\n6: m d\n7: m\n8:\n9:\n10:\n11:\n12: {Tb}B\nb=1\n13:\nx", "12: {Tb}B\nb=1\n13:\nx\n12.2.5 Tree variants and tradeoffs may need to be considered, to select the best variable combination (rather than a\nsingle split variable).", "Another generalization is a hierarchical mixture of experts, where we make a \u201csoft\u201d\nversion of trees, in which the splits are probabilistic (so every point has some degree", "of membership in every leaf). Such trees can be trained using a form of gradient\ndescent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,", "gradient boosted trees) and implementations are readily available (e.g., XGBoost).\nTrees have a number of strengths, and remain a valuable tool in the machine", "learning toolkit. Some bene\ufb01ts include being relatively easy to interpret, fast to\ntrain, and ability to handle multi-class classi\ufb01cation in a natural way. Trees can", "easily handle different loss functions; one just needs to change the predictor and\nloss being applied in the leaves. Methods also exist to identify which features are", "particularly important or in\ufb02uential in forming the tree, which can aid in human\nunderstanding of the data set. Finally, in many situations, trees perform", "surprisingly well, often comparable to more complicated regression or classi\ufb01cation\nmodels. Indeed, in some settings it is considered good practice to start with trees", "(especially random forest or boosted trees) as a \u201cbaseline\u201d machine learning model,\nagainst which one can evaluate performance of more sophisticated models.", "While tree-based methods excel at supervised learning tasks, we now turn to\nanother important class of non-parametric methods that focus on discovering", "structure in unlabeled data. These clustering methods share some conceptual\nsimilarities with tree-based approaches - both aim to partition the input space into", "meaningful regions - but clustering methods operate without supervision, making\nthem particularly valuable for exploratory data analysis and pattern discovery.\n12.3 -means Clustering", "Clustering is an unsupervised learning method where we aim to discover\nmeaningful groupings or categories in a dataset based on patterns or similarities", "within the data itself, without relying on pre-assigned labels. It is widely used for\nexploratory data analysis, pattern recognition, and segmentation tasks, allowing us", "to interpret and manage complex datasets by uncovering hidden structures and\nrelationships.\nOftentimes a dataset can be partitioned into different categories. A doctor may", "notice that their patients come in cohorts and different cohorts respond to different\ntreatments. A biologist may gain insight by identifying that bats and whales,", "despite outward appearances, have some underlying similarity, and both should be\nconsidered members of the same category, i.e., \u201cmammal\u201d. The problem of", "automatically identifying meaningful groupings in datasets is called clustering.\nOnce these groupings are found, they can be leveraged toward interpreting the data", "and making optimal decisions for each group.k Mathematically, clustering looks a bit like classi\ufb01cation: we wish to \ufb01nd a mapping", "from datapoints, , to categories, . However, rather than the categories being\nprede\ufb01ned labels, the categories in clustering are automatically discovered partitions\nof an unlabeled dataset.", "Because clustering does not learn from labeled examples, it is an example of an\nunsupervised learning algorithm. Instead of mimicking the mapping implicit in", "supervised training pairs , clustering assigns datapoints to categories\nbased on how the unlabeled data  is distributed in data space.", "Intuitively, a \u201ccluster\u201d is a group of datapoints that are all nearby to each other and\nfar away from other clusters. Let\u2019s consider the following scatter plot. How many", "clusters do you think there are?\nThere seem to be about \ufb01ve clumps of datapoints and those clumps are what we\nwould like to call clusters. If we assign all datapoints in each clump to a cluster", "corresponding to that clump, then we might desire that nearby datapoints are\nassigned to the same cluster, while far apart datapoints are assigned to different\nclusters.", "clusters.\nIn designing clustering algorithms, three critical things we need to decide are:\nHow do we measure distance between datapoints? What counts as \u201cnearby\u201d\nand \u201cfar apart\u201d?", "and \u201cfar apart\u201d?\nHow many clusters should we look for?\nHow do we evaluate how good a clustering is?\nWe will see how to begin making these decisions as we work through a concrete", "clustering algorithm in the next section.\nOne of the simplest and most commonly used clustering algorithms is called k-", "means. The goal of the k-means algorithm is to assign datapoints to  clusters in\nsuch a way that the variance within clusters is as small as possible. Notice that this\n12.3.1 Clustering formalismsx y", "{x(i),y(i)}n\ni=1\n{x(i)}n\ni=1\n12.3.2 The k-means formulation\nkFigure 12.1: A dataset we would\nlike to cluster. How many clusters", "do you think there are? matches our intuitive idea that a cluster should be a tightly packed set of\ndatapoints.\nSimilar to the way we showed that supervised learning could be formalized", "mathematically as the minimization of an objective function (loss function +\nregularization), we will show how unsupervised learning can also be formalized as", "minimizing an objective function. Let us denote the cluster assignment for a\ndatapoint  as , i.e.,  means we are assigning datapoint", "to cluster number 1. Then the k-means objective can be quanti\ufb01ed with the\nfollowing objective function (which we also call the \u201ck-means loss\u201d):\nwhere  and , so that  is the", "mean of all datapoints in cluster , and using  to denote the indicator function\n(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum", "(over data points) of the loss is the variance of datapoints within cluster . We sum\nup the variance of all  clusters to get our overall loss.", "The k-means algorithm minimizes this loss by alternating between two steps: given\nsome initial cluster assignments: 1) compute the mean of all data in each cluster and", "assign this as the \u201ccluster mean\u201d, and 2) reassign each datapoint to the cluster with\nnearest cluster mean. Figure 12.2 shows what happens when we repeat these steps\non the dataset from above.", "Each time we reassign the data to the nearest cluster mean, the k-means loss\ndecreases (the datapoints end up closer to their assigned cluster mean), or stays the", "same. And each time we recompute the cluster means the loss also decreases (the\nmeans end up closer to their assigned datapoints) or stays the same. Overall then,", "the clustering gets better and better, according to our objective \u2013 until it stops\nimproving.\nAfter four iterations of cluster assignment + update means in our example, the k-", "means algorithm stops improving. We say it has converged, and its \ufb01nal solution is\nshown in Figure 12.3.x(i)y(i)\u2208{1,2,\u2026,k}y(i)=1\nx(i)\nk\n\u2211\nj=1n\n\u2211\ni=1\ud835\udfd9(y(i)=j)x(i)\u2212\u03bc(j)2,\n\u2225\u2225(12.1)\n\u03bc(j)=1\nNj\u2211n", "\u03bc(j)=1\nNj\u2211n\ni=1\ud835\udfd9(y(i)=j)x(i)Nj=\u2211n\ni=1\ud835\udfd9(y(i)=j)\u03bc(j)\nj \ud835\udfd9(\u22c5)\nj\nk\n12.3.2.0.0.1 K-means algorithm\nFigure 12.2: The \ufb01rst three steps of\nrunning the k-means algorithm on\nthis data. Datapoints are colored", "according to the cluster to which\nthey are assigned. Cluster means\nare the larger X\u2019s with black\noutlines. It seems to converge to something reasonable! Now let\u2019s write out the algorithm in", "complete detail:\nprocedure KM EANS( )\nInitialize centroids  and assignments  randomly\nfor  to  do\nfor  to  do\nend for\nfor  to  do\nend for\nif  then\nbreak//convergence\nend if\nend for\nreturn", "end for\nreturn \nend procedure\nThe for-loop over the  datapoints assigns each datapoint to the nearest cluster\ncenter. The for-loop over the k clusters updates the cluster center to be the mean of", "all datapoints currently assigned to that cluster. As suggested above, it can be\nshown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it", "converges to a local minimum of the loss.\nIt\u2019s like classi\ufb01cation except it picked what the classes are rather than being given\nexamples of what the classes are.", "We can also use gradient descent to optimize the k-means objective. To show how to\napply gradient descent, we \ufb01rst rewrite the objective as a differentiable function\nonly of :", "only of :\n is the value of the k-means loss given that we pick the optimal assignments of\nthe datapoints to cluster means (that\u2019s what the  does). Now we can use the", "gradient  to \ufb01nd the values for  that achieve minimum loss when cluster\n1: k,\u03c4,{x(i)}n\ni=1\n2: \u03bc(1),\u2026,\u03bc(k)y(1),\u2026,y(n)\n3:t=1\u03c4\n4:yold\u2190y\n5: i=1n\n6: y(i)\u2190argminj\u2208{1,\u2026,k}x(i)\u2212\u03bc(j)2\n\u2225\u22257:\n8: j=1k\n9: Nj\u2190\u2211n", "8: j=1k\n9: Nj\u2190\u2211n\ni=1\ud835\udfd9(y(i)=j)\n10: \u03bc(j)\u21901\nNj\u2211n\ni=1\ud835\udfd9(y(i)=j)x(i)\n11:\n12:y=yold\n13:\n14:\n15:\n16:\n17: \u03bc,y\n18:\nn\n12.3.2.0.0.2 Using gradient descent to minimize k-means objective\n\u03bc\nL(\u03bc)=n\n\u2211\ni=1min", "\u03bc\nL(\u03bc)=n\n\u2211\ni=1min\njx(i)\u2212\u03bc(j)2.\n\u2225\u2225L(\u03bc)\nminj\n\u2202L(\u03bc)\n\u2202\u03bc\u03bcFigure 12.3: Converged result. assignments are optimal. Finally, we read off the optimal cluster assignments, given", "the optimized , just by assigning datapoints to their nearest cluster mean:\nThis procedure yields a local minimum of Equation 12.1, as does the standard k-", "means algorithm we presented (though they might arrive at different solutions). It\nmight not be the global optimum since the objective is not convex (due to , as", "the minimum of multiple convex functions is not necessarily convex).\nThe standard k-means algorithm, as well as the variant that uses gradient descent,", "both are only guaranteed to converge to a local minimum, not necessarily the global\nminimum of the loss. Thus the answer we get out depends on how we initialize the", "cluster means. Figure 12.4 is an example of a different initialization on our toy data,\nwhich results in a worse converged clustering:", "A variety of methods have been developed to pick good initializations (see, for\nexample, the k-means++ algorithm). One simple option is to run the standard k-", "means algorithm multiple times, with different random initial conditions, and then\npick from these the clustering that achieves the lowest k-means loss.", "A very important parameter in cluster algorithms is the number of clusters we are\nlooking for. Some advanced algorithms can automatically infer a suitable number of", "clusters, but most of the time, like with k-means, we will have to pick  \u2013 it\u2019s a\nhyperparameter of the algorithm.\nFigure 12.5 shows an example of the effect. Which result looks more correct? It can", "be hard to say! Using higher k we get more clusters, and with more clusters we can\nachieve lower within-cluster variance \u2013 the k-means objective will never increase,\u03bc\ny(i)=argmin\njx(i)\u2212\u03bc(j)2.\n\u2225\u2225minj", "jx(i)\u2212\u03bc(j)2.\n\u2225\u2225minj\n12.3.2.0.0.3 Importance of initialization\n12.3.2.0.0.4 Importance of k\nk\nFigure 12.4: With the initialization\nof the means to the left, the yellow\nand red means end up splitting", "what perhaps should be one cluster\nin half.\nFigure 12.5: Example of k-means\nrun on our toy data, with two\ndifferent values of k. Setting k=4,\non the left, results in one cluster", "being merged, compared to setting\nk=5, on the right. Which clustering\ndo you think is better? How could", "you decide? and will typically strictly decrease as we increase k. Eventually, we can increase k to\nequal the total number of datapoints, so that each datapoint is assigned to its own", "cluster. Then the k-means objective is zero, but the clustering reveals nothing.\nClearly, then, we cannot use the k-means objective itself to choose the best value for", "k. In Section 1.3, we will discuss some ways of evaluating the success of clustering\nbeyond its ability to minimize the k-means objective, and it\u2019s with these sorts of", "methods that we might decide on a proper value of k.\nAlternatively, you may be wondering: why bother picking a single k? Wouldn\u2019t it be", "nice to reveal a hierarchy of clusterings of our data, showing both coarse and \ufb01ne\ngroupings? Indeed hierarchical clustering is another important class of clustering", "algorithms, beyond k-means. These methods can be useful for discovering tree-like\nstructure in data, and they work a bit like this: initially a coarse split/clustering of", "the data is applied at the root of the tree, and then as we descend the tree we split\nand cluster the data in ever more \ufb01ne-grained ways. A prototypical example of", "hierarchical clustering is to discover a taxonomy of life, where creatures may be\ngrouped at multiple granularities, from species to families to kingdoms. You may", "\ufb01nd a suite of clustering algorithms in SKLEARN\u2019s cluster module.\nClustering algorithms group data based on a notion of similarity, and thus we need", "to de\ufb01ne a distance metric between datapoints. This notion will also be useful in\nother machine learning approaches, such as nearest-neighbor methods that we see", "in Chapter 12. In k-means and other methods, our choice of distance metric can\nhave a big impact on the results we will \ufb01nd.\nOur k-means algorithm uses the Euclidean distance, i.e., , with a loss", "function that is the square of this distance. We can modify k-means to use different\ndistance metrics, but a more common trick is to stick with Euclidean distance but", "measured in a feature space. Just like we did for regression and classi\ufb01cation\nproblems, we can de\ufb01ne a feature map from the data to a nicer feature", "representation, , and then apply k-means to cluster the data in the feature\nspace.\nAs a simple example, suppose we have two-dimensional data that is very stretched", "out in the \ufb01rst dimension and has less dynamic range in the second dimension.\nThen we may want to scale the dimensions so that each has similar dynamic range,", "prior to clustering. We could use standardization, like we did in Chapter 5.\nIf we want to cluster more complex data, like images, music, chemical compounds,", "etc., then we will usually need more sophisticated feature representations. One\ncommon practice these days is to use feature representations learned with a neural", "network. For example, we can use an autoencoder to compress images into feature\nvectors, then cluster those feature vectors.\n12.3.2.0.0.5 k-means in feature spacex(i)\u2212\u03bc(j)\n\u2225\u2225\u03d5(x)", "\u2225\u2225\u03d5(x)\n12.3.3 How to evaluate clustering algorithms One of the hardest aspects of clustering is knowing how to evaluate it. This is", "actually a big issue for all unsupervised learning methods, since we are just looking\nfor patterns in the data, rather than explicitly trying to predict target values (which", "was the case with supervised learning).\nRemember, evaluation metrics are not the same as loss functions, so we can\u2019t just", "measure success by looking at the k-means loss. In prediction problems, it is critical\nthat the evaluation is on a held-out test set, while the loss is computed over training", "data. If we evaluate on training data we cannot detect over\ufb01tting. Something\nsimilar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too", "large can precisely \u201c\ufb01t\u201d the data (minimize the loss), but yields no general insight.\nOne way to evaluate our clusters is to look at the consistency with which they are", "found when we run on different subsamples of our training data, or with different\nhyperparameters of our clustering algorithm (e.g., initializations). For example, if", "running on several bootstrapped samples (random subsets of our data) results in\nvery different clusters, it should call into question the validity of any of the\nindividual results.", "individual results.\nIf we have some notion of what ground truth clusters should be, e.g., a few data\npoints that we know should be in the same cluster, then we can measure whether or", "not our discovered clusters group these examples correctly.\nClustering is often used for visualization and interpretability, to make it easier for", "humans to understand the data. Here, human judgment may guide the choice of\nclustering algorithm. More quantitatively, discovered clusters may be used as input", "to downstream tasks. For example, as we saw in the lab, we may \ufb01t a different\nregression function on the data within each cluster. Figure 12.6 gives an example", "where this might be useful. In cases like this, the success of a clustering algorithm\ncan be indirectly measured based on the success of the downstream application", "(e.g., does it make the downstream predictions more accurate).\nFigure 12.6: Averaged across the\nwhole population, risk of heart\ndisease positively correlates with\nhours of exercise. However, if we", "cluster the data, we can observe\nthat there are four subgroups of the\npopulation which correspond to\ndifferent age groups, and within\neach subgroup the correlation is\nnegative. We can make better", "predictions, and better capture the\npresumed true effect, if we cluster\nthis data and then model the trend", "in each cluster separately.  What are some conventions for derivatives of matrices and vectors? It will always\nwork to explicitly write all indices and treat everything as scalars, but we", "introduce here some shortcuts that are often faster to use and helpful for\nunderstanding.\nThere are at least two consistent but different systems for describing shapes and", "rules for doing matrix derivatives. In the end, they all are correct, but it is\nimportant to be consistent.\nWe will use what is often called the \u2018Hessian\u2019 or denominator layout, in which we", "say that for\n of size  and  of size ,  is a matrix of size  with the \nentry . This denominator layout convention has been adopted by the \ufb01eld", "of machine learning to ensure that the shape of the gradient is the same as the\nshape of the respective derivative. This is somewhat controversial at large, but alas,", "we shall continue with denominator layout.\nThe discussion below closely follows the Wikipedia on matrix derivatives.\nA.1 The shapes of things\nHere are important special cases of the rule above:", "Scalar-by-scalar: For  of size  and  of size ,  is the (scalar)\npartial derivative of  with respect to .\nScalar-by-vector: For  of size  and  of size ,  (also written", ", the gradient of  with respect to ) is a column vector of size  with\nthe  entry :\nVector-by-scalar: For  of size  and  of size ,  is a row\nvector of size  with the  entry :", "Vector-by-vector: For  of size  and  of size ,  is a matrix of\nsize  with the  entry :Appendix A \u2014 Matrix derivative common\ncases\nxn\u00d71 ym\u00d71\u2202y/\u2202x n\u00d7m (i,j)\n\u2202yj/\u2202xi\nx 1\u00d71y 1\u00d71\u2202y/\u2202x\ny x\nxn\u00d71y 1\u00d71\u2202y/\u2202x", "y x\nxn\u00d71y 1\u00d71\u2202y/\u2202x\n\u2207xy y x n\u00d71\nith\u2202y/\u2202xi\n\u2202y/\u2202x= .\u23a1\n\u23a2\u23a3\u2202y/\u2202x1\n\u2202y/\u2202x2\n\u22ee\n\u2202y/\u2202xn\u23a4\n\u23a5\u23a6\nx 1\u00d71 ym\u00d71\u2202y/\u2202x\n1\u00d7m jth\u2202yj/\u2202x\n\u2202y/\u2202x=[ ]. \u2202y1/\u2202x\u2202y2/\u2202x\u22ef\u2202ym/\u2202x\nxn\u00d71 ym\u00d71\u2202y/\u2202x", "xn\u00d71 ym\u00d71\u2202y/\u2202x\nn\u00d7m (i,j)\u2202yj/\u2202xi\uf461 Appendices>A  M atrix derivative common cases \uf52a\n1 Scalar-by-matrix: For  of size  and  of size ,  (also written", ", the gradient of  with respect to ) is a matrix of size  with the\n entry :\nYou may notice that in this list, we have not included matrix-by-matrix, matrix-by-", "vector, or vector-by-matrix derivatives. This is because, generally, they cannot be\nexpressed nicely in matrix form and require higher order objects (e.g., tensors) to", "represent their derivatives. These cases are beyond the scope of this course.\nAdditionally, notice that for all cases, you can explicitly compute each element of", "the derivative object using (scalar) partial derivatives. You may \ufb01nd it useful to\nwork through some of these by hand as you are reviewing matrix derivatives.\nA.2 Some vector-by-vector identities", "Here are some examples of . In each case, assume  is ,  is ,  is\na scalar constant,  is a vector that does not depend on  and  is a matrix that", "does not depend on ,  and  are scalars that do depend on , and  and  are\nvectors that do depend on . We also have vector-valued functions  and .", "First, we will cover a couple of fundamental cases: suppose that  is an \nvector which is not a function of , an  vector. Then,", "is an  matrix of 0s. This is similar to the scalar case of differentiating a\nconstant. Next, we can consider the case of differentiating a vector with respect to\nitself:", "itself:\nThis is the  identity matrix, with 1\u2019s along the diagonal and 0\u2019s elsewhere. It\nmakes sense, because  is 1 for  and 0 otherwise. This identity is also\nsimilar to the scalar case.\u2202y/\u2202x= .\u23a1", "\u23a2\u23a3\u2202y1/\u2202x1\u2202y2/\u2202x1\u22ef\u2202ym/\u2202x1\n\u2202y1/\u2202x2\u2202y2/\u2202x2\u22ef\u2202ym/\u2202x2\n\u22ee \u22ee\u22f1 \u22ee\n\u2202y1/\u2202xn\u2202y2/\u2202xn\u22ef\u2202ym/\u2202xn\u23a4\n\u23a5\u23a6\nX n\u00d7my 1\u00d71\u2202y/\u2202X\n\u2207Xy y X n\u00d7m\n(i,j)\u2202y/\u2202Xi,j\n\u2202y/\u2202X= .\u23a1\n\u23a2\u23a3\u2202y/\u2202X1,1\u22ef\u2202y/\u2202X1,m\n\u22ee\u22f1 \u22ee\n\u2202y/\u2202Xn,1\u22ef\u2202y/\u2202Xn,m\u23a4\n\u23a5\u23a6\n\u2202y/\u2202x xn\u00d71ym\u00d71a", "\u23a5\u23a6\n\u2202y/\u2202x xn\u00d71ym\u00d71a\na x A\nxuv x u v\nx f g\nA.2.1 Some fundamental cases\nam\u00d71\nxn\u00d71\n\u2202a\n\u2202x=0, (A.1)\nn\u00d7m\n\u2202x\n\u2202x=I\nn\u00d7n\n\u2202xj/xii=j Let the dimensions of  be . Then the object  is an  vector. We can", "then compute the derivative of  with respect to  as:\nNote that any element of the column vector  can be written as, for :\nThus, computing the  entry of  requires computing the partial derivative", "Therefore, the  entry of  is the  entry of :\nSimilarly, for objects  of the same shape, one can obtain,\nSuppose that  are both vectors of size . Then,", "Suppose that  is a scalar constant and  is an  vector that is a function of .\nThen,\nOne can extend the previous identity to vector- and matrix-valued constants.", "Suppose that  is a vector with shape  and  is a scalar which depends on .\nThen,\nFirst, checking dimensions,  is  and  is  so  is  and our", "answer is  as it should be. Now, checking a value, element  of the\nA.2.2 Derivatives involving a constant matrixAm\u00d7n Axm\u00d71\nAx x\n\u2202Ax\n\u2202x=\u23a1\n\u23a2\u23a3\u2202(Ax)1/\u2202x1\u2202(Ax)2/\u2202x1\u22ef\u2202(Ax)m/\u2202x1", "\u2202(Ax)1/\u2202x2\u2202(Ax)2/\u2202x2\u22ef\u2202(Ax)m/\u2202x2\n\u22ee \u22ee \u22f1 \u22ee\n\u2202(Ax)1/\u2202xn\u2202(Ax)2/\u2202xn\u22ef\u2202(Ax)m/\u2202xn\u23a4\n\u23a5\u23a6\nAx j=1,\u2026,m\n(Ax)j=n\n\u2211\nk=1Aj,kxk.\n(i,j)\u2202Ax\n\u2202x\n\u2202(Ax)j/\u2202xi:\n\u2202(Ax)j/\u2202xi=\u2202(n\n\u2211\nk=1Aj,kxk)/\u2202xi=Aj,i\n(i,j)\u2202Ax\n\u2202x(j,i) A\n\u2202Ax", "\u2202x(j,i) A\n\u2202Ax\n\u2202x=AT(A.2)\nx,A\n\u2202xTA\n\u2202x=A (A.3)\nA.2.3 Linearity of derivatives\nu,v m\u00d71\n\u2202(u+v)\n\u2202x=\u2202u\n\u2202x+\u2202v\n\u2202x(A.4)\na um\u00d71 x\n\u2202au\n\u2202x=a\u2202u\n\u2202x\na m\u00d71v x\n\u2202va\n\u2202x=\u2202v\n\u2202xaT\n\u2202v/\u2202xn\u00d71 am\u00d71aT1\u00d7m", "\u2202v/\u2202xn\u00d71 am\u00d71aT1\u00d7m\nn\u00d7m (i,j) answer is  =  which corresponds to element  of\n.\nSimilarly, suppose that  is a matrix which does not depend on  and  is a\ncolumn vector which does depend on . Then,", "Suppose that  is a scalar which depends on , while  is a column vector of shape\n and  is a column vector of shape . Then,\nOne can see this relationship by expanding the derivative as follows:", "Then, one can use the product rule for scalar-valued functions,\nto obtain the desired result.\nSuppose that  is a vector-valued function with output vector of shape , and", "the argument to  is a column vector  of shape  which depends on . Then,\none can obtain the chain rule as,\nFollowing \u201cthe shapes of things,\u201d  is  and  is , where", "element  is . The same chain rule applies for further compositions\nof functions:\nA.3 Some other identities\nYou can get many scalar-by-vector and vector-by-scalar cases as special cases of the", "rules above, making one of the relevant vectors just be 1 x 1. Here are some other\nones that are handy. For more, see the Wikipedia article on Matrix derivatives (for", "consistency, only use the ones in denominator layout).\u2202vaj/\u2202xi(\u2202v/\u2202xi)aj (i,j)\n(\u2202v/\u2202x)aT\nA x u\nx\n\u2202Au\n\u2202x=\u2202u\n\u2202xAT\nA.2.4 Product rule (vector-valued numerator)\nv x u\nm\u00d71 x n\u00d71\n\u2202vu\n\u2202x=v\u2202u\n\u2202x+\u2202v\n\u2202xuT\n\u2202vu", "\u2202x+\u2202v\n\u2202xuT\n\u2202vu\n\u2202x= .\u23a1\n\u23a2\u23a3\u2202(vu1)/\u2202x1\u2202(vu2)/\u2202x1\u22ef\u2202(vum)/\u2202x1\n\u2202(vu1)/\u2202x2\u2202(vu2)/\u2202x2\u22ef\u2202(vum)/\u2202x2\n\u22ee \u22ee \u22f1 \u22ee\n\u2202(vu1)/\u2202xn\u2202(vu2)/\u2202xn\u22ef\u2202(vum)/\u2202xn\u23a4\n\u23a5\u23a6\n\u2202(vuj)/\u2202xi=v(\u2202uj/\u2202xi)+(\u2202v/\u2202xi)uj,\nA.2.5 Chain rule\ng m\u00d71\ng u d\u00d71 x", "g m\u00d71\ng u d\u00d71 x\n\u2202g(u)\n\u2202x=\u2202u\n\u2202x\u2202g(u)\n\u2202u\n\u2202u/\u2202xn\u00d7d\u2202g(u)/\u2202ud\u00d7m\n(i,j)\u2202g(u)j/\u2202ui\n\u2202f(g(u))\n\u2202x=\u2202u\n\u2202x\u2202g(u)\n\u2202u\u2202f(g)\n\u2202g\nT A.4 Derivation of gradient for linear regression", "Recall here that  is a matrix of of size  and  is an  vector.\nApplying identities Equation A.3, Equation A.5,Equation A.4, Equation A.2,\nEquation A.1\nA.5 Matrix derivatives using Einstein summation", "You do not have to read or learn this! But you might \ufb01nd it interesting or helpful.\nConsider the objective function for linear regression, written out as products of\nmatrices:", "matrices:\nwhere  is ,  is , and  is . How does one show, with no\nshortcuts, that\nOne neat way, which is very explicit, is to simply write all the matrices as variables", "with row and column indices, e.g.,  is the row , column  entry of the matrix\n. Furthermore, let us use the convention that in any product, all indices which", "appear more than once get summed over; this is a popular convention in\ntheoretical physics, and lets us suppress all the summation symbols which would", "otherwise clutter the following expresssions. For example,  would be the\nimplicit summation notation giving the element at the  row of the matrix-vector\nproduct .", "product .\nUsing implicit summation notation with explicit indices, we can rewrite  as\u2202uTv\n\u2202x=\u2202u\n\u2202xv+\u2202v\n\u2202xu (A.5)\n\u2202uT\n\u2202x=(\u2202u\n\u2202x)T(A.6)\nX n\u00d7d Yn\u00d71\n\u2202(X\u03b8\u2212Y)T(X\u03b8\u2212Y)/n\n\u2202\u03b8=2\nn\u2202(X\u03b8\u2212Y)\n\u2202\u03b8(X\u03b8\u2212Y)\n=2\nn(\u2202X\u03b8\n\u2202\u03b8\u2212\u2202Y", "=2\nn(\u2202X\u03b8\n\u2202\u03b8\u2212\u2202Y\n\u2202\u03b8)(X\u03b8\u2212Y)\n=2\nn(XT\u22120)(X\u03b8\u2212Y)\n=2\nnXT(X\u03b8\u2212Y)\nJ(\u03b8)=1\nn(X\u03b8\u2212Y)T(X\u03b8\u2212Y),\nXn\u00d7dYn\u00d71\u03b8d\u00d71\n\u2207\u03b8J=2\nnXT(X\u03b8\u2212Y)?\nXab a b\nX\nXab\u03b8b\nath\nX\u03b8\nJ(\u03b8)\nJ(\u03b8)=1", "ath\nX\u03b8\nJ(\u03b8)\nJ(\u03b8)=1\nn(Xab\u03b8b\u2212Ya)(Xac\u03b8c\u2212Ya). Note that we no longer need the transpose on the \ufb01rst term, because all that\ntranspose accomplished was to take a dot product between the vector given by the", "left term, and the vector given by the right term. With implicit summation, this is\naccomplished by the two terms sharing the repeated index .", "Taking the derivative of  with respect to the  element of  thus gives, using the\nchain rule for (ordinary scalar) multiplication:", "where the second line follows from the \ufb01rst, with the de\ufb01nition that  only\nwhen  (and similarly for ). And the third line follows from the second by", "recognizing that the two terms in the second line are identical. Now note that in\nthis implicit summation notation, the  element of the matrix product of  and", "is . That is, ordinary matrix multiplication sums over indices\nwhich are adjacent to each other, because a row of  times a column of  becomes", "a scalar number. So the term in the above equation with  is not a matrix\nproduct of  with . However, taking the transpose  switches row and column", "indices, so . And  is a matrix product of  with ! Thus, we\nhave that\nwhich is the desired result.a\nJ dth\u03b8\ndJ\nd\u03b8d=1\nn[Xab\u03b4bd(Xac\u03b8c\u2212Ya)+(Xab\u03b8b\u2212Ya)Xac\u03b4cd]\n=1\nn[Xad(Xac\u03b8c\u2212Ya)+(Xab\u03b8b\u2212Ya)Xad]\n=2", "=2\nnXad(Xab\u03b8b\u2212Ya),\n\u03b4bd=1\nb=d \u03b4cd\na,b A\nB(AB)ac=AabBbc\nA B\nXadXab\nXX XT\nXad=XT\ndaXT\ndaXab XTX\ndJ\nd\u03b8d=2\nnXT\nda(Xab\u03b8b\u2212Ya)\n=2\nn[XT(X\u03b8\u2212Y)]d, B.1 Strategies towards adaptive step-size", "We\u2019ll start by looking at the notion of a running average. It\u2019s a computational\nstrategy for estimating a possibly weighted average of a sequence of data. Let our", "data sequence be ; then we de\ufb01ne a sequence of running average values,\n using the equations\nwhere . If  is a constant, then this is a moving average, in which", "So, you can see that inputs  closer to the end of the sequence have more effect on\n than early inputs.\nIf, instead, we set , then we get the actual average.\n\u2753  Study Question", "\u2753  Study Question\nProve to yourself that the previous assertion holds.\nNow, we can use methods that are a bit like running averages to describe strategies", "for computing . The simplest method is momentum, in which we try to \u201caverage\u201d\nrecent gradient updates, so that if they have been bouncing back and forth in some", "direction, we take out that component of the motion. For momentum, we haveAppendix B \u2014 Optimizing Neural\nNetworks\nB.1.1 Running averages\nc1,c2,\u2026\nC0,C1,C2,\u2026\nC0=0,\nCt=\u03b3tCt\u22121+(1\u2212\u03b3t)ct,\n\u03b3t\u2208(0,1)\u03b3t", "\u03b3t\u2208(0,1)\u03b3t\nCT=\u03b3CT\u22121+(1\u2212\u03b3)cT\n=\u03b3(\u03b3CT\u22122+(1\u2212\u03b3)cT\u22121)+(1\u2212\u03b3)cT\n=T\n\u2211\nt=1\u03b3T\u2212t(1\u2212\u03b3)ct.\nct\nCT\n\u03b3t=t\u22121\nt\nB.1.2 Momentum\n\u03b7\nV0=0,\nVt=\u03b3Vt\u22121+\u03b7\u2207WJ(Wt\u22121),", "Wt=Wt\u22121\u2212Vt.\uf461 Appendices>B  Optimizing Neural Networks \uf52a This doesn\u2019t quite look like an adaptive step size. But what we can see is that, if we", "let , then the rule looks exactly like doing an update with step size \non a moving average of the gradients with parameter :\n\u2753  Study Question", "\u2753  Study Question\nProve to yourself that these formulations are equivalent.\nWe will \ufb01nd that  will be bigger in dimensions that consistently have the same", "sign for  and smaller for those that don\u2019t. Of course we now have two\nparameters to set ( and ), but the hope is that the algorithm will perform better", "overall, so it will be worth trying to \ufb01nd good values for them. Often  is set to be\nsomething like .\nThe red arrows show the update after each successive step of mini-batch gradient", "descent with momentum. The blue points show the direction of the gradient with\nrespect to the mini-batch at each step. Momentum smooths the path taken towards", "the local minimum and leads to faster convergence.\n\u2753  Study Question\nIf you set , would momentum have more of an effect or less of an effect\nthan if you set it to ?", "Another useful idea is this: we would like to take larger steps in parts of the space\nwhere  is nearly \ufb02at (because there\u2019s no risk of taking too big a step due to the", "gradient being large) and smaller steps when it is steep. We\u2019ll apply this idea to\neach weight independently, and end up with a method called adadelta, which is a\u03b7=\u03b7\u2032(1\u2212\u03b3) \u03b7\u2032\n\u03b3\nM0=0,", "\u03b3\nM0=0,\nMt=\u03b3Mt\u22121+(1\u2212\u03b3)\u2207WJ(Wt\u22121),\nWt=Wt\u22121\u2212\u03b7\u2032Mt.\nVt\n\u2207W\n\u03b7 \u03b3\n\u03b3\n0.9\n\u03b3=0.1\n0.9\nB.1.3 Adadelta\nJ(W)M omentum variant on adagrad (for adaptive gradient). Even though our weights are indexed by", "layer, input unit, and output unit, for simplicity here, just let  be any weight in\nthe network (we will do the same thing for all of them).", "The sequence  is a moving average of the square of the th component of the\ngradient. We square it in order to be insensitive to the sign\u2014we want to know", "whether the magnitude is big or small. Then, we perform a gradient update to\nweight , but divide the step size by , which is larger when the surface is", "steeper in direction  at point  in weight space; this means that the step size\nwill be smaller when it\u2019s steep and larger when it\u2019s \ufb02at.", "Adam has become the default method of managing step sizes in neural networks.\nIt combines the ideas of momentum and adadelta. We start by writing moving", "averages of the gradient and squared gradient, which re\ufb02ect estimates of the mean\nand variance of the gradient for weight :\nA problem with these estimates is that, if we initialize , they will", "always be biased (slightly too small). So we will correct for that bias by de\ufb01ning\nNote that  is  raised to the power , and likewise for . To justify these", "corrections, note that if we were to expand  in terms of  and\n, the coef\ufb01cients would sum to 1. However, the coef\ufb01cient behind\n is  and since , the sum of coef\ufb01cients of nonzero terms is ;", "hence the correction. The same justi\ufb01cation holds for .\n\u2753  Study QuestionWj\ngt,j=\u2207WJ(Wt\u22121)j,\nGt,j=\u03b3Gt\u22121,j+(1\u2212\u03b3)g2\nt,j,\nWt,j=Wt\u22121,j\u2212\u03b7\n\u221aGt,j+\u03f5gt,j.\nGt,j j\nj \u221aGt,j+\u03f5\nj Wt\u22121\nB.1.4 Adam\nj\ngt,j=\u2207WJ(Wt\u22121)j,", "j\ngt,j=\u2207WJ(Wt\u22121)j,\nmt,j=B1mt\u22121,j+(1\u2212B1)gt,j,\nvt,j=B2vt\u22121,j+(1\u2212B2)g2\nt,j.\nm0=v0=0\n^mt,j=mt,j\n1\u2212Bt\n1,\n^vt,j=vt,j\n1\u2212Bt\n2,\nWt,j=Wt\u22121,j\u2212\u03b7\n\u221a^vt,j+\u03f5^mt,j.\nBt\n1B1 t Bt\n2\nmt,j m0,j\ng0,j,g1,j,\u2026,gt,j\nm0,jBt", "m0,jBt\n1 m0,j=0 1\u2212Bt\n1\nvt,jAlthoug h, interestingly, it may\nactua lly violate the convergence\nconditions of SG D:\narxiv.org/abs/1705.08292 De\ufb01ne  directly as a moving average of . What is the decay (", "parameter)?\nEven though we now have a step size for each weight, and we have to update\nvarious quantities on each iteration of gradient descent, it\u2019s relatively easy to", "implement by maintaining a matrix for each quantity (, , , ) in each layer\nof the network.\nB.2 Batch Normalization Details", "Let\u2019s think of the batch-normalization layer as taking  as input and producing an\noutput . But now, instead of thinking of  as an  vector, we have to", "explicitly think about handling a mini-batch of data of size  all at once, so  will\nbe an  matrix, and so will the output .", "Our \ufb01rst step will be to compute the batchwise mean and standard deviation. Let \nbe the  vector where\nand let  be the  vector where", "The basic normalized version of our data would be a matrix, element  of which\nis\nwhere  is a very small constant to guard against division by zero.", "However, if we let these be our  values, we really are forcing something too\nstrong on our data\u2014our goal was to normalize across the data batch, but not", "necessarily force the output values to have exactly mean 0 and standard deviation 1.\nSo, we will give the layer the opportunity to shift and scale the outputs by adding", "new weights to the layer. These weights are  and , each of which is an \nvector. Using the weights, we de\ufb01ne the \ufb01nal output to be\nThat\u2019s the forward pass. Whew!", "Now, for the backward pass, we have to do two things: given ,^mt,j gt,j \u03b3\nm\u2113\ntv\u2113\ntg\u2113\ntg2\nt\u2113\nZl\n\u02c6ZlZlnl\u00d71\nK Zl\nnl\u00d7K \u02c6Zl\n\u03bcl\nnl\u00d71\n\u03bcl\ni=1\nKK\n\u2211\nj=1Zl\nij,\n\u03c3lnl\u00d71\n\u03c3l\ni=1\nKK\n\u2211\nj=1(Zl\nij\u2212\u03bcl\ni)2\n.\ue001\n\ue000\u23b7\n(i,j)\nZl", "i)2\n.\ue001\n\ue000\u23b7\n(i,j)\nZl\nij=Zl\nij\u2212\u03bcl\ni\n\u03c3l\ni+\u03f5, \u2013\n\u03f5\n\u02c6Zl\nGlBlnl\u00d71\n\u02c6Zl\nij=Gl\niZl\nij+Bl\ni. \u2013\n\u2202L\n\u2202\u02c6Zl Compute  for back-propagation, and\nCompute  and  for gradient updates of the weights in this layer.", "Schematically, we have\nIt\u2019s hard to think about these derivatives in matrix terms, so we\u2019ll see how it works\nfor the components.  contributes to  for all data points  in the batch. So,", "Similarly,  contributes to  for all data points  in the batch. Thus,\nNow, let\u2019s \ufb01gure out how to do backprop. We can start schematically:", "And because dependencies only exist across the batch, but not across the unit\noutputs,\nThe next step is to note that\nAnd now that\nwhere  if  and 0 otherwise. We need two more pieces:", "Putting the whole thing together, we get\u2202L\n\u2202Zl\n\u2202L\n\u2202Gl\u2202L\n\u2202Bl\n\u2202L\n\u2202B=\u2202L\n\u2202\u02c6Z\u2202\u02c6Z\n\u2202B.\nBi\u02c6Zij j\n\u2202L\n\u2202Bi=\u2211\nj\u2202L\n\u2202\u02c6Zij\u2202\u02c6Zij\n\u2202Bi=\u2211\nj\u2202L\n\u2202\u02c6Zij.\nGi\u02c6Zij j\n\u2202L\n\u2202Gi=\u2211\nj\u2202L\n\u2202\u02c6Zij\u2202\u02c6Zij\n\u2202Gi=\u2211\nj\u2202L\n\u2202\u02c6ZijZij. \u2013\n\u2202L\n\u2202Z=\u2202L", "\u2202L\n\u2202Z=\u2202L\n\u2202\u02c6Z\u2202\u02c6Z\n\u2202Z.\n\u2202L\n\u2202Zij=K\n\u2211\nk=1\u2202L\n\u2202\u02c6Zik\u2202\u02c6Zik\n\u2202Zij.\n\u2202\u02c6Zik\n\u2202Zij=\u2202\u02c6Zik\n\u2202Zik\u2202Zik\n\u2202Zij=Gi\u2202Zik\n\u2202Zij. \u2013\u2013\u2013\n\u2202Zik\n\u2202Zij=(\u03b4jk\u2212\u2202\u03bci\n\u2202Zij)1\n\u03c3i\u2212Zik\u2212\u03bci\n\u03c32\ni\u2202\u03c3i\n\u2202Zij, \u2013\n\u03b4jk=1j=k\n\u2202\u03bci\n\u2202Zij=1\nK,\u2202\u03c3i\n\u2202Zij=Zij\u2212\u03bci\nK\u03c3i.", "\u2202Zij=Zij\u2212\u03bci\nK\u03c3i.\n\u2202L\n\u2202Zij=K\n\u2211\nk=1\u2202L\n\u2202\u02c6ZikGi1\nK\u03c3i(K\u03b4jk\u22121\u2212(Zik\u2212\u03bci)(Zij\u2212\u03bci)\n\u03c32\ni).  In which we try to describe the outlines of the \u201clifecycle\u201d of supervised learning,", "including hyperparameter tuning and evaluation of the \ufb01nal product.\nC.1 General case\nWe start with a very generic setting.", "Given: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()\nsuch that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning", "algorithm () takes as input a data set of the form\nwhere  and  and returns an .\nGiven a problem speci\ufb01cation and a set of data , we evaluate hypothesis \naccording to average loss, or error,", "If the data used for evaluation were not used during learning of the hypothesis then this\nis a reasonable estimate of how well the hypothesis will make additional", "predictions on new data from the same source.\nA validation strategy  takes an algorithm , a loss function , and a data source", "and produces a real number which measures how well  performs on data from\nthat distribution.\nIn the simplest case, we can divide  into two sets,  and , train on the", "\ufb01rst, and then evaluate the resulting hypothesis on the second. In that case,Appendix C \u2014 Supervised learning in a\nnutshell\nC.1.1 Minimal problem speci\ufb01cation \ue9cb\nD={(x(1),y(1)),\u2026,(x(n),y(n))}", "x(i)\u2208Xy(i)\u2208y h\u2208H\nC.1.2 Evaluating a hypothesis\nD h\nE(h,L,D)=1\n|D|D\n\u2211\ni=1L(h(x(i)),y(i))\nC.1.3 Evaluating a supervised learning algorithm\nV A L D\nA\nC.1.3.1 Using a validation set\nD Dtrain Dval", "D Dtrain Dval \nV(A,L,D)=E(A(Dtrain ),L,Dval )\uf461 Appendices>C  Su pervised learning in a nutshell \uf52a We can\u2019t reliably evaluate an algorithm based on a single application to a single", "training and test set, because there are many aspects of the training and testing\ndata, as well as, sometimes, randomness in the algorithm itself, that cause variance", "in the performance of the algorithm. To get a good idea of how well an algorithm\nperforms, we need to, multiple times, train it and evaluate the resulting hypothesis,", "and report the average over  executions of the algorithm of the error of the\nhypothesis it produced each time.\nWe divide the data into 2 K random non-overlapping subsets:\n.\nThen,", ".\nThen,\nIn cross validation, we do a similar computation, but allow data to be re-used in the\n different iterations of training and testing the algorithm (but never share training", "and testing data for a single iteration!). See Se ction 2.8.2.2 for details.\nNow, if we have two different algorithms  and , we might be interested in", "knowing which one will produce hypotheses that generalize the best, using data\nfrom a particular source. We could compute  and , and", "prefer the algorithm with lower validation error. More generally, given algorithms\n, we would prefer\nNow what? We have to deliver a hypothesis to our customer. We now know how to", "\ufb01nd the algorithm, , that works best for our type of data. We can apply it to all of\nour data to get the best hypothesis we know how to create, which would be", "and deliver this resulting hypothesis as our best product.\nA majority of learning algorithms have the form of optimizing some objective\ninvolving the training data and a loss function.", "C.1.3.2 Using multiple training/evaluation runsK\nDtrain \n1,Dval \n1,\u2026,Dtrain \nK,Dval \nK\nV(A,L,D)=1\nKK\n\u2211\nk=1E(A(Dtrain\nk),L,Dval\nk).\nC.1.3.3 Cross validation\nK", "K\nC.1.4 Comparing supervised learning algorithms\nA1A2\nV(A1,L,D)V(A\u2208,L,D)\nA1,\u2026,AM\nA\u2217=argmin\nmV(AM,L,D)\nC.1.5 Fielding a hypothesis\nA\u2217\nh\u2217=A\u2217(D)\nC.1.6 Learning algorithms as optimizers", "Interestingly, this loss fun ction is\nnot always the same as the loss\nfun ction that is us ed for So for example, (assuming a perfect optimizer which doesn\u2019t, of course, exist) we", "might say our algorithm is to solve an optimization problem:\nOur objective often has the form\nwhere  is a loss to be minimized during training and  is a regularization term.", "Often, rather than comparing an arbitrary collection of learning algorithms, we\nthink of our learning algorithm as having some parameters that affect the way it", "maps data to a hypothesis. These are not parameters of the hypothesis itself, but\nrather parameters of the algorithm. We call these hyperparameters. A classic example", "would be to use a hyperparameter  to govern the weight of a regularization term\non an objective to be optimized:\nThen we could think of our algorithm as . Picking a good value of  is the", "same as comparing different supervised learning algorithms, which is accomplished\nby validating them and picking the best one!\nC.2 Concrete case: linear regression", "In linear regression the problem formulation is this:\n for values of parameters  and .\nOur learning algorithm has hyperparameter  and can be written as:", "Our learning algorithm has hyperparameter $ $ and can be written as:\nFor a particular training data set and parameter , it \ufb01nds the best hypothesis on", "this data, speci\ufb01ed with parameters , written .A(D)=argmin\nh\u2208HJ(h;D).\nJ(h;D)=E(h,L,D)+R(h),\nL R\nC.1.7 Hyperparameters\n\u03bb\nJ(h;D)=E(h,L,D)+\u03bbR(h).\nA(D;\u03bb) \u03bb\nx=Rd\ny=R\nH={\u03b8\u22a4x+\u03b80} \u03b8\u2208Rd\u03b80\u2208R\nL(g,y)=(g\u2212y)2\n\u03bb", "L(g,y)=(g\u2212y)2\n\u03bb\nA(D;\u03bb)=\u0398\u2217(\u03bb,D)=argmin\n\u03b8,\u03b801\n|D|\u2211\n(x,y)\u2208D(\u03b8\u22a4x+\u03b80\u2212y)2+\u03bb\u2225\u03b8\u22252\nA(D;\u03bb)=\u0398\u2217(\u03bb,D)=argmin\n\u03b8,\u03b801\n|D|\u2211\n(x,y)\u2208D(\u03b8\u22a4x+\u03b80\u2212y)2+\u03bb\u2225\u03b8\u22252.\n\u03bb\n\u0398=(\u03b8,\u03b80) \u0398\u2217(\u03bb,D)evalua tion! We will see this in", "logistic regression. Picking the best value of the hyperparameter is choosing among learning\nalgorithms. We could, most simply, optimize using a single training / validation\nsplit, so  , and", "split, so  , and\nIt would be much better to select the best  using multiple runs or cross-validation;\nthat would just be a different choices of the validation procedure  in the top line.", "Note that we don\u2019t use regularization here because we just want to measure how\ngood the output of the algorithm is at predicting values of new points, and so that\u2019s", "what we measure. We use the regularizer during training when we don\u2019t want to\nfocus only on optimizing predictions on the training data.", "Finally! To make a predictor to ship out into the world, we would use all the data\nwe have, , to train, using the best hyperparameters we know, and return", "Finally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, as", "Here are the same ideas, written out in informal pseudocode:D=Dtrain \u222aDval \n\u03bb\u2217=argmin\n\u03bbV(A\u03bb,L,Dval )\n=argmin\n\u03bbE(\u0398\u2217(\u03bb,Dtrain ), mse, Dval )\n=argmin\n\u03bb1\n|Dval |\u2211\n(x,y)\u2208Dval (\u03b8\u2217(\u03bb,Dtrain )\u22a4x+\u03b8\u2217", "0(\u03bb,Dtrain )\u2212y)2\n\u03bb\nV\nD\n\u0398\u2217=A(D;\u03bb\u2217)\n=\u0398\u2217(\u03bb\u2217,D)\n=argmin\n\u03b8,\u03b801\n|D|\u2211\n(x,y)\u2208D(\u03b8\u22a4x+\u03b80\u2212y)2+\u03bb\u2217\u2225\u03b8\u22252\nEtest =E(\u0398\u2217, mse ,Dtest )\n=1\n|Dtest |\u2211\n(x,y)\u2208Dtot (\u03b8\u2217Tx+\u03b8\u2217\n0\u2212y)2\n# returns theta_best(D, lambda)", "define train(D, lambda):\n    return minimize(mse(theta, D) + lambda * norm(theta)** 2, theta)\n# returns lambda_best using very simple validation", "define simple_tune(D_train, D_val, possible_lambda_vals):\n    scores = [mse(train(D_train, lambda), D_val) for lambda in \npossible_lambda_vals]\n    return possible_lambda_vals[least_index[scores]]", "# returns theta_best overall\ndefine theta_best(D_train, D_val, possible_lambda_vals):\n    return train(D_train + D_val, simple_tune(D_train, D_val, \npossible_lambda_vals))", "# customer evaluation of the theta delivered to them C.3 Concrete case: logistic regression\nIn binary logistic regression the problem formulation is as follows. We are writing", "the class labels as 1 and 0.\n for values of parameters  and .\nProxy loss  Our learning algorithm\nhas hyperparameter  and can be written as:", "For a particular training data set and parameter , it \ufb01nds the best hypothesis on\nthis data, speci\ufb01ed with parameters , written  according to the\nproxy loss .", "proxy loss .\nPicking the best value of the hyperparameter is choosing among learning\nalgorithms based on their actual predictions. We could, most simply, optimize using", "a single training / validation split, so , and we use the real 01 loss:\nIt would be much better to select the best  using multiple runs or cross-validation;", "that would just be a different choices of the validation procedure  in the top line.\nFinally! To make a predictor to ship out into the world, we would use all the data", "we have, , to train, using the best hyperparameters we know, and return\n\u2753  Study Question\nWhat loss function is being optimized inside this algorithm?", "Finally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, asdefine customer_val(theta):\n    return mse(theta, D_test)\nX=Rd\ny={+1,0}", "X=Rd\ny={+1,0}\nH={\u03c3(\u03b8\u22a4x+\u03b80)} \u03b8\u2208Rd\u03b80\u2208R\nL(g,y)=L01( g, h)\nLnll(g,y)=\u2212(ylog(g)+(1\u2212y)log(1\u2212g))\n\u03bb\nA(D;\u03bb)=\u0398\u2217(\u03bb,D)=argmin\n\u03b8,\u03b801\n|D|\u2211\n(x,y)\u2208DLnll(\u03c3(\u03b8\u22a4x+\u03b80),y)+\u03bb\u2225\u03b8\u22252\n\u03bb\n\u0398=(\u03b8,\u03b80) \u0398\u2217(\u03bb,D)\nLnll \nD=Dtrain \u222aDval", "D=Dtrain \u222aDval\n\u03bb\u2217=argmin\n\u03bbV(A\u03bb,L01,Dval )\n=argmin\n\u03bbE(\u0398\u2217(\u03bb,Dtrain ),L01,Dval )\n=argmin\n\u03bb1\n|Dval |\u2211\n(x,y)\u2208Dval L01(\u03c3(\u03b8\u2217(\u03bb,Dtrain )\u22a4x+\u03b8\u2217\n0(\u03bb,Dtrain )),y)\n\u03bb\nV\nD\n\u0398\u2217=A(D;\u03bb\u2217)", "\u03bb\nV\nD\n\u0398\u2217=A(D;\u03bb\u2217)\nEtest=E(\u0398\u2217,L01,Dtest) The customer just wants to buy the right stocks! So we use the real  here for\nvalidation.L01"]