["1 Introduction\nThe main focus of machine learning (ML) is making decisions or predictions based on\ndata. There are a number of other fields with significant overlap in technique, but", "difference in focus: in economics and psychology, the goal is to discover underlying\ncausal processes and in statistics it is to find a model that fits a data set well. In", "those fields, the end product is a model. In machine learning, we often fit models,\nbut as a means to the end of making good predictions or decisions.", "As ML methods have improved in their capability and scope, ML has become\narguably the best way\u2013measured in terms of speed, human engineering time, and", "robustness\u2013to approach many applications. Great examples are face detection,\nspeech recognition, and many kinds of language-processing tasks. Almost any", "application that involves understanding data or signals that come from the real\nworld can be nicely addressed using machine learning.", "One crucial aspect of machine learning approaches to solving problems is that\nhuman engineering plays an important role. A human still has to frame the problem:", "acquire and organize data, design a space of possible solutions, select a learning\nalgorithm and its parameters, apply the algorithm to the data, validate the resulting", "solution to decide whether it\u2019s good enough to use, try to understand the impact on\nthe people who will be affected by its deployment, etc. These steps are of great\nimportance.", "importance.\nThe conceptual basis of learning from data is the problem of induction: Why do we\nthink that previously seen data will help us predict the future? This is a serious long", "standing philosophical problem. We will operationalize it by making assumptions,\nsuch as that all training data are so-called i.i.d.(independent and identically", "distributed), and that queries will be drawn from the same distribution as the\ntraining data, or that the answer comes from a set of possible answers known in\nadvance.", "advance.\n6.390 - Intro to Machine Learning\nCourse Notes\nThis description is paraphrased\nfrom a post on 9/4/12 at\nandrewgelman.com.\nThis aspect is often undervalued.", "This means that the elements in the\nset are related in the sense that\nthey all come from the same\nunderlying probability\ndistribution, but not in other ways.\n\uf4611  Introduction\n\uf52a", "\uf4611  Introduction\n\uf52a\n In general, we need to solve these two problems:\nestimation: When we have data that are noisy reflections of some underlying", "quantity of interest, we have to aggregate the data and make estimates or\npredictions about the quantity. How do we deal with the fact that, for example,", "the same treatment may end up with different results on different trials? How\ncan we predict how well an estimate may compare to future results?", "generalization: How can we predict results of a situation or experiment that\nwe have never encountered before in our data set?", "We can describe problems and their solutions using six characteristics, three of\nwhich characterize the problem and three of which characterize the solution:", "1. Problem class: What is the nature of the training data and what kinds of\nqueries will be made at testing time?\n2. Assumptions: What do we know about the source of the data or the form of", "the solution?\n3. Evaluation criteria: What is the goal of the prediction or estimation system?\nHow will the answers to individual queries be evaluated? How will the overall", "performance of the system be measured?\n4. Model type: Will an intermediate model of the world be made? What aspects\nof the data will be modeled in different variables/parameters? How will the", "model be used to make predictions?\n5. Model class: What particular class of models will be used? What criterion will\nwe use to pick a particular model from the model class?", "6. Algorithm: What computational process will be used to fit the model to the\ndata and/or to make predictions?\nWithout making some assumptions about the nature of the process generating the", "data, we cannot perform generalization. In the following sections, we elaborate on\nthese ideas.\n1.1 Problem class\nThere are many different problem classes in machine learning. They vary according to", "what kind of data is provided and what kind of conclusions are to be drawn from it.\nFive standard problem classes are described below, to establish some notation and\nterminology.", "terminology.\nIn this course, we will focus on classification and regression (two examples of\nsupervised learning), and we will touch on reinforcement learning, sequence\nlearning, and clustering.", "For example, the same treatment\nmay end up with different results\non different trials. How can we\npredict how well an estimate\ncompares to future results?\nDon\u2019t feel you have to memorize", "all these kinds of learning, etc. We\njust want you to have a very high-\n The idea of supervised learning is that the learning system is given inputs and told", "which specific outputs should be associated with them. We divide up supervised\nlearning based on whether the outputs are drawn from a small finite set", "(classification) or a large finite ordered set or continuous set (regression).\nFor a regression problem, the training data \n is in the form of a set of  pairs:\nwhere", "where \n represents an input, most typically a -dimensional vector of real and/or\ndiscrete values, and \n is the output to be predicted, in this case a real-number. The", "values are sometimes called target values.\nThe goal in a regression problem is ultimately, given a new input value \n, to\npredict the value of", ". Regression problems are a kind of supervised learning,\nbecause the desired output \n is specified for each of the training examples \n.", ".\nA classification problem is like regression, except that the values that \n can take\ndo not have an order. The classification problem is binary or two-class if \n (also", "(also\nknown as the class) is drawn from a set of two possible values; otherwise, it is called\nmulti-class.\nUnsupervised learning doesn\u2019t involve learning a function from inputs to outputs", "based on a set of input-output pairs. Instead, one is given a data set and generally\nexpected to find some patterns or structure inherent in it.\nGiven samples", "Given samples \n, the goal is to find a partitioning (or \u201cclustering\u201d)\nof the samples that groups together similar samples. There are many different", "objectives, depending on the definition of the similarity between samples and\nexactly what criterion is to be used (e.g., minimize the average distance between", "elements inside a cluster and maximize the average distance between elements\nacross clusters). Other methods perform a \u201csoft\u201d clustering, in which samples may", "be assigned 0.9 membership in one cluster and 0.1 in another. Clustering is\nsometimes used as a step in the so-called density estimation (described below), and", "sometimes to find useful structure or influential features in data.\n1.1.1 Supervised learning\n1.1.1.1 Regression\nDtrain\nn\nDtrain = {(x(1), y(1)), \u2026 , (x(n), y(n))},\nx(i)\nd\ny(i)\ny\nx(n+1)\ny(n+1)\ny(i)", "x(n+1)\ny(n+1)\ny(i)\nx(i)\n1.1.1.2 Classification\ny(i)\ny(i)\n1.1.2 Unsupervised learning\n1.1.2.1 Clustering\nx(1), \u2026 , x(n) \u2208Rd\nlevel view of (part of) the breadth\nof the field.\nMany textbooks use \n and", "and \ninstead of \n and \n. We find that\nnotation somewhat difficult to\nmanage when \n is itself a vector\nand we need to talk about its\nelements. The notation we are\nusing is standard in some other", "parts of the ML literature.\nxi\nti\nx(i)\ny(i)\nx(i)\n Given samples \n drawn i.i.d. from some distribution \n, the\ngoal is to predict the probability \n of an element drawn from the same", "distribution. Density estimation sometimes plays a role as a \u201csubroutine\u201d in the\noverall learning method for supervised learning, as well.\nGiven samples", "Given samples \n, the problem is to re-represent them as points in\na -dimensional space, where \n. The goal is typically to retain information in", "the data set that will, e.g., allow elements of one class to be distinguished from\nanother.\nDimensionality reduction is a standard technique that is particularly useful for", "visualizing or understanding high-dimensional data. If the goal is ultimately to\nperform regression or classification on the data after the dimensionality is reduced,", "it is usually best to articulate an objective for the overall prediction problem rather\nthan to first do dimensionality reduction without knowing which dimensions will", "be important for the prediction task.\nIn sequence learning, the goal is to learn a mapping from input sequences \nto output sequences \n. The mapping is typically represented as a state", "machine, with one function \n used to compute the next hidden internal state given\nthe input, and another function \n used to compute the output given the current\nhidden state.", "hidden state.\nIt is supervised in the sense that we are told what output sequence to generate for\nwhich input sequence, but the internal functions have to be learned by some", "method other than direct supervision, because we don\u2019t know what the hidden\nstate sequence is.\nIn reinforcement learning, the goal is to learn a mapping from input values", "(typically assumed to be states of an agent or system; for now, think e.g. the velocity\nof a moving car) to output values (typically we want control actions; for now, think", "e.g. if to accelerate or hit the brake). However, we need to learn the mapping\nwithout a direct supervision signal to specify which output values are best for a", "particular input; instead, the learning problem is framed as an agent interacting\nwith an environment, in the following setting:\nThe agent observes the current state \n.\nIt selects an action", "1.1.2.2 Density estimation\nx(1), \u2026 , x(n) \u2208Rd\nPr(X)\nPr(x(n+1))\n1.1.2.3 Dimensionality reduction\nx(1), \u2026 , x(n) \u2208RD\nd\nd < D\n1.1.3 Sequence learning\nx0, \u2026 , xn\ny1, \u2026 , ym\nfs\nfo", "y1, \u2026 , ym\nfs\nfo\n1.1.4 Reinforcement learning\nst\nat.\n It receives a reward, \n, which typically depends on \n and possibly \n.\nThe environment transitions probabilistically to a new state, \n, with a", ", with a\ndistribution that depends only on \n and \n.\nThe agent observes the current state, \n.\nThe goal is to find a policy , mapping  to , (that is, states to actions) such that", "some long-term sum or average of rewards  is maximized.\nThis setting is very different from either supervised learning or unsupervised", "learning, because the agent\u2019s action choices affect both its reward and its ability to\nobserve the environment. It requires careful consideration of the long-term effects of", "actions, as well as all of the other issues that pertain to supervised learning.\nThere are many other problem settings. Here are a few.", "In semi-supervised learning, we have a supervised-learning training set, but there\nmay be an additional set of \n values with no known \n. These values can still be", "used to improve learning performance (if they are drawn from \n that is the\nmarginal of \n that governs the rest of the data set).\nIn active learning, it is assumed to be expensive to acquire a label", "(imagine\nasking a human to read an x-ray image), so the learning algorithm can sequentially\nask for particular inputs \n to be labeled, and must carefully select queries in order", "to learn as effectively as possible while minimizing the cost of labeling.\nIn transfer learning (also called meta-learning), there are multiple tasks, with data", "drawn from different, but related, distributions. The goal is for experience with\nprevious tasks to apply to learning a current task in a way that requires decreased\nexperience with the new task.", "1.2 Assumptions\nThe kinds of assumptions that we can make about the data source or the solution\ninclude:\nThe data are independent and identically distributed (i.i.d.).", "The data are generated by a Markov chain (i.e. outputs only depend only on\nthe current state, with no additional memory).\nThe process generating the data might be adversarial.\nrt\nst\nat\nst+1\nst\nat", "rt\nst\nat\nst+1\nst\nat\nst+1\n\u2026\n\u03c0\ns\na\nr\n1.1.5 Other settings\nx(i)\ny(i)\nPr(X)\nPr(X, Y )\ny(i)\nx(i)\n The \u201ctrue\u201d model that is generating the data can be perfectly described by one", "of some particular set of hypotheses.\nThe effect of an assumption is often to reduce the \u201csize\u201d or \u201cexpressiveness\u201d of the", "space of possible hypotheses and therefore reduce the amount of data required to\nreliably identify an appropriate hypothesis.\n1.3 Evaluation criteria", "Once we have specified a problem class, we need to say what makes an output or\nthe answer to a query good, given the training data. We specify evaluation criteria", "at two levels: how an individual prediction is scored, and how the overall behavior\nof the prediction or estimation system is scored.", "The quality of predictions from a learned model is often expressed in terms of a loss\nfunction. A loss function \n tells you how much you will be penalized for", "making a guess  when the answer is actually . There are many possible loss\nfunctions. Here are some frequently used examples:\n0-1 Loss applies to predictions drawn from finite domains.\nSquared loss", "Squared loss\nAbsolute loss\nAsymmetric loss Consider a situation in which you are trying to predict\nwhether someone is having a heart attack. It might be much worse to predict", "\u201cno\u201d when the answer is really \u201cyes\u201d, than the other way around.\nAny given prediction rule will usually be evaluated based on multiple predictions", "and the loss of each one. At this level, we might be interested in:\nMinimizing expected loss over all the predictions (also known as risk)\nMinimizing maximum loss: the loss of the worst prediction", "Minimizing or bounding regret: how much worse this predictor performs than\nthe best one drawn from some class\nL(g, a)\ng\na\nL(g, a) = {0\nif g = a\n1\notherwise\nL(g, a) = (g \u2212a)2\nL(g, a) = |g \u2212a|", "L(g, a) = |g \u2212a|\nL(g, a) =\n\u23a7\n\u23aa\n\u23a8\n\u23aa\n\u23a9\n1\nif g = 1 and a = 0\n10\nif g = 0 and a = 1\n0\notherwise\n Characterizing asymptotic behavior: how well the predictor will perform in the", "limit of infinite training data\nFinding algorithms that are probably approximately correct: they probably\ngenerate a hypothesis that is right most of the time.", "There is a theory of rational agency that argues that you should always select the\naction that minimizes the expected loss. This strategy will, for example, make you the", "most money in the long run, in a gambling setting. As mentioned above, expected\nloss is also sometimes called risk in ML literature, but that term means other things", "in economics or other parts of decision theory, so be careful...it\u2019s risky to use it. We\nwill, most of the time, concentrate on this criterion.\n1.4 Model type", "1.4 Model type\nRecall that the goal of a ML system is typically to estimate or generalize, based on\ndata provided. Below, we examine the role of model-making in machine learning.", "In some simple cases, in response to queries, we can generate predictions directly\nfrom the training data, without the construction of any intermediate model, or more", "precisely, without the learning of any parameters.\nFor example, in regression or classification, we might generate an answer to a new", "query by averaging answers to recent queries, as in the nearest neighbor method.\nThis two-step process is more typical:", "1. \u201cFit\u201d a model (with some a-prior chosen parameterization) to the training data\n2. Use the model directly to make predictions", "In the parametric models setting of regression or classification, the model will be\nsome hypothesis or prediction rule \n for some functional form . The", "term hypothesis has its roots in statistical learning and the scientific method, where\nmodels or hypotheses about the world are tested against real data, and refined with", "more evidence, observations, or insights. Note that the parameters themselves are\nonly part of the assumptions that we\u2019re making about the world. The model itself is", "a hypothesis that will be refined with more evidence.\nThe idea is that \n is a set of one or more parameter values that will be determined", "by fitting the model to the training data and then be held fixed during testing.\nGiven a new \n, we would then make the prediction \n.\n1.4.1 Non-parametric models\n1.4.2 Parametric models\ny = h(x; \u0398)\nh", "y = h(x; \u0398)\nh\n\u0398\nx(n+1)\nh(x(n+1); \u0398)\n The fitting process is often articulated as an optimization problem: Find a value of\n that minimizes some criterion involving", "and the data. An optimal strategy, if\nwe knew the actual underlying distribution on our data, \n would be to\npredict the value of  that minimizes the expected loss, which is also known as the", "test error. If we don\u2019t have that actual underlying distribution, or even an estimate of\nit, we can take the approach of minimizing the training error: that is, finding the", "prediction rule  that minimizes the average loss on our training data set. So, we\nwould seek \n that minimizes\nwhere the loss function \n measures how bad it would be to make a guess of", "when the actual value is .\nWe will find that minimizing training error alone is often not a good choice: it is\npossible to emphasize fitting the current data too strongly and end up with a", "hypothesis that does not generalize well when presented with new  values.\n1.5 Model class and parameter fitting\nA model class \n is a set of possible models, typically parameterized by a vector of", "parameters \n. What assumptions will we make about the form of the model? When\nsolving a regression problem using a prediction-rule approach, we might try to find\na linear function", "a linear function \n that fits our data well. In this example, the\nparameter vector \n.\nFor problem types such as classification, there are huge numbers of model classes", "that have been considered...we\u2019ll spend much of this course exploring these model\nclasses, especially neural networks models. We will almost completely restrict our", "attention to model classes with a fixed, finite number of parameters. Models that\nrelax this assumption are called \u201cnon-parametric\u201d models.", "How do we select a model class? In some cases, the ML practitioner will have a\ngood idea of what an appropriate model class is, and will specify it directly. In other", "cases, we may consider several model classes and choose the best based on some\nobjective function. In such situations, we are solving a model selection problem:", "model-selection is to pick a model class \n from a (usually finite) set of possible\nmodel classes, whereas model fitting is to pick a particular model in that class,", "specified by (usually continuous) parameters \n.\n1.6 Algorithm\nOnce we have described a class of models and a way of scoring a model given data,", "we have an algorithmic problem: what sequence of computational instructions\nshould we run in order to find a good model from our class? For example,\n\u0398\n\u0398\nPr(X, Y )\ny\nh\n\u0398\nEtrain(h; \u0398) = 1\nn\nn\n\u2211\ni=1", "n\nn\n\u2211\ni=1\nL(h(x(i); \u0398), y(i)) ,\nL(g, a)\ng\na\nx\nM\n\u0398\nh(x; \u03b8, \u03b80) = \u03b8Tx + \u03b80\n\u0398 = (\u03b8, \u03b80)\nM\n\u0398\n determining the parameter vector which minimizes the training error might be", "done using a familiar least-squares minimization algorithm, when the model  is a\nfunction being fit to some data .\nSometimes we can use software that was designed, generically, to perform", "optimization. In many other cases, we use algorithms that are specialized for ML\nproblems, or for particular hypotheses classes. Some algorithms are not easily seen", "as trying to optimize a particular criterion. In fact, a historically important method\nfor finding linear classifiers, the perceptron algorithm, has this character.\nh\nx", "h\nx\n We had legacy PDF notes that used mixed conventions for data matrices: \u201ceach row as a\ndata point\u201d and \u201ceach column as a data point\u201d.\nWe are standardizing to \u201ceach row as a data point.\u201d Thus,", "aligns with \n in the PDF\nnotes if you\u2019ve read those. If you spot inconsistencies or experience any confusion, please\nraise an issue. Thanks!", "Regression is an important machine-learning problem that provides a good starting\npoint for diving deeply into the field.\n2.1 Problem formulation", "A hypothesis  is employed as a model for solving the regression problem, in that it\nmaps inputs  to outputs ,\nwhere \n (i.e., a length  column vector of real numbers), and \n (i.e., a real", "(i.e., a real\nnumber). Real life rarely gives us vectors of real numbers; the  we really want to\ntake as input is usually something like a song, image, or person. In that case, we\u2019ll", "have to define a function \n, whose range is \n, where  represents features of ,\nlike a person\u2019s height or the amount of bass in a song, and then let the", ". In much of the following, we\u2019ll omit explicit mention of  and assume that the \nare in \n, but you should always have in mind that some additional process was", "almost surely required to go from the actual input examples to their feature\nrepresentation, and we\u2019ll talk a lot more about features later in the course.", "Regression is a supervised learning problem, in which we are given a training dataset\nof the form\nwhich gives examples of input values \n and the output values \n that should be", "that should be\nassociated with them. Because  values are real-valued, our hypotheses will have\nthe form\nThis is a good framework when we want to predict a numerical quantity, like", "height, stock value, etc., rather than to divide the inputs into discrete categories.\n2  Regression\nWarning\nX\n~X\nh\nx\ny\nx \u2192\n\u2192y ,\nh\nx \u2208Rd\nd\ny \u2208R\nx\n\u03c6(x)\nRd\n\u03c6\nx\nh : \u03c6(x) \u2192R\n\u03c6\nx(i)\nRd", "\u03c6\nx(i)\nRd\nDtrain = {(x(1), y(1)), \u2026 , (x(n), y(n))} ,\nx(i)\ny(i)\ny\nh : Rd \u2192R .\n\u201cRegression,\u201d in common parlance,\nmeans moving backwards. But this\nis forward progress!", "Real life rarely gives us vectors of\nreal numbers. The  we really want\nto take as input is usually\nsomething like a song, image, or\nperson. In that case, we\u2019ll have to\ndefine a function \n whose", "whose\nrange is \n, where  represents\nfeatures of  (e.g., a person\u2019s height\nor the amount of bass in a song).\nx\n\u03c6(x)\nRd\n\u03c6\nx\n\uf4612  Regression\n\uf52a\n1", "\uf4612  Regression\n\uf52a\n1\n What makes a hypothesis useful? That it works well on new data\u2014that is, it makes\ngood predictions on examples it hasn\u2019t seen.", "However, we don\u2019t know exactly what data this hypothesis might be tested on in\nthe real world. So, we must assume a connection between the training data and", "testing data. Typically, the assumption is that they are drawn independently from\nthe same probability distribution.\nTo make this discussion more concrete, we need a loss function to express how", "unhappy we are when we guess an output  given an input  for which the desired\noutput was .\nGiven a training set \n and a hypothesis  with parameters \n, the training error", "of  can be defined as the average loss on the training data:\nThe training error of  gives us some idea of how well it characterizes the", "relationship between  and  values in our data, but it isn\u2019t the quantity we most\ncare about. What we most care about is test error:\non", "on \n new examples that were not used in the process of finding the hypothesis.\nIt might be worthwhile to stare at the two errors and think about what\u2019s the difference.\nFor example, notice how", "is no longer a variable in the testing error? This is because, in\nevaluating the testing error, the parameters will have been \u201cpicked\u201d or \u201cfixed\u201d already.", "For now, we will try to find a hypothesis with small training error (later, with some\nadded criteria) and try to make some design choices so that it generalizes well to new", "data, meaning that it also has a small test error.\n2.2 Regression as an optimization problem\nGiven data, a loss function, and a hypothesis class, we need a method for finding a", "good hypothesis in the class. One of the most general ways to approach this\nproblem is by framing the machine learning problem as an optimization problem.", "One reason for taking this approach is that there is a rich area of math and\nalgorithms studying and developing efficient methods for solving optimization\ng\nx\na\nDtrain\nh\n\u0398\nh\nEtrain(h; \u0398) = 1\nn\nn\n\u2211", "n\nn\n\u2211\ni=1\nL(h(x(i); \u0398), y(i)) .\n(2.1)\nh\nx\ny\nEtest(h) = 1\nn\u2032\nn+n\u2032\n\u2211\ni=n+1\nL(h(x(i)), y(i)) ,\nn\u2032\nNote\n\u0398\nThis process of converting our data\ninto a numerical form is often", "referred to as data pre-processing.\nThen  maps \n to .\nIn much of the following, we\u2019ll\nomit explicit mention of  and\nassume that the \n are in \n.\nHowever, you should always", "remember that some additional\nprocess was almost surely required\nto go from the actual input\nexamples to their feature\nrepresentation. We will discuss\nfeatures more later in the course.\nh\n\u03c6(x)\nR\n\u03c6", "h\n\u03c6(x)\nR\n\u03c6\nx(i)\nRd\nMy favorite analogy is to problem\nsets. We evaluate a student\u2019s ability\nto generalize by putting questions\non the exam that were not on the\nhomework (training set).", "problems, and lots of very good software implementations of these methods. So, if\nwe can turn our problem into one of these problems, then there will be a lot of work\nalready done for us!", "We begin by writing down an objective function \n, where \n stands for all the\nparameters in our model (i.e., all possible choices over parameters). We often write", "to make clear the dependence on the data \n.\nThe objective function describes how we feel about possible hypotheses \n. We\ngenerally look for parameter values \n that minimize the objective function:", "In the most general case, there is not a guarantee that there exists a unique set of\nparameters which minimize the objective function. However, we will ignore that for", "now. A very common form for a machine-learning objective is:\nThe loss measures how unhappy we are about the prediction \n for the pair", "for the pair\n. Minimizing this loss improves prediction accuracy. The regularizer \nis an additional term that encourages the prediction to remain general, and the", "constant  adjusts the balance between fitting the training examples and\ngeneralizing to unseen examples. We will discuss this balance and the idea of\nregularization further in Section 2.7.", "2.3 Linear regression\nTo make this discussion more concrete, we need to provide a hypothesis class and a\nloss function.\nWe begin by picking a class of hypotheses \n that might provide a good set of", "possible models for the relationship between  and  in our data. We start with a\nvery simple class of linear hypotheses for regression:\nwhere the model parameters are \n. In one dimension (\n), this", "), this\ncorresponds to the familiar slope-intercept form \n of a line. In two\ndimesions (\n), this corresponds to a plane. In higher dimensions, this model", "describes a hyperplane. This hypothesis class is both simple to study and very\npowerful, and will serve as the basis for many other important techniques (even\nneural networks!).\nJ(\u0398)\n\u0398\nJ(\u0398; D)\nD\n\u0398\n\u0398", "\u0398\nJ(\u0398; D)\nD\n\u0398\n\u0398\n\u0398\u2217= arg min\n\u0398 J(\u0398) .\nJ(\u0398) =\n1\nn\nn\n\u2211\ni=1\nL(h(x(i); \u0398), y(i))\nloss\n+\n\u03bb\nnon-negative constant\nR(\u0398).\n\u239b\n\u239c\n\u239d\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n\u239e\n\u239f\n\u23a0\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n(2.2)\nh(x(i); \u0398)\n(x(i), y(i))\nR(\u0398)\n\u03bb\nH\nx\ny", "R(\u0398)\n\u03bb\nH\nx\ny\ny = h(x; \u03b8, \u03b80) = \u03b8Tx + \u03b80 ,\n(2.3)\n\u0398 = (\u03b8, \u03b80)\nd = 1\ny = mx + b\nd = 2\nDon\u2019t be too perturbed by the\nsemicolon where you expected to\nsee a comma! It\u2019s a mathematical", "way of saying that we are mostly\ninterested in this as a function of\nthe arguments before the ; , but we\nshould remember there\u2019s a\ndependence on the stuff after it as\nwell.", "well.\n For now, our objective in linear regression is to find a hypothesis that goes as close\nas possible, on average, to all of our training data. We define a loss function to", "describe how to evaluate the quality of the predictions our hypothesis is making,\nwhen compared to the \u201ctarget\u201d  values in the data set. The choice of loss function", "is part of modeling your domain. In the absence of additional information about a\nregression problem, we typically use squared loss:\nwhere", "where \n is our \u201cguess\u201d from the hypothesis, or the hypothesis\u2019 prediction,\nand  is the \u201cactual\u201d observation (in other words, here  is being used equivalently", "as ). With this choice of squared loss, the average loss as generally defined in\nEquation 2.1 will become the so-called mean squared error (MSE).", "Applying the general optimization framework to the linear regression hypothesis\nclass of Equation 2.3 with squared loss and no regularization, our objective is to find\nvalues for", "values for \n that minimize the MSE:\nresulting in the solution:\nFor one-dimensional data (\n), this corresponds to fitting a line to data. For", ", this hypothesis represents a -dimensional hyperplane embedded in a\n-dimensional space (the input dimension plus the  dimension).", "For example, in the left plot below, we can see data points with labels  and input\ndimensions \n and \n. In the right plot below, we see the result of fitting these", "points with a two-dimensional plane that resides in three dimensions. We interpret\nthe plane as representing a function that provides a  value for any input \n.\ny\nL(g, a) = (g \u2212a)2 .\ng = h(x)\na\na\ny", "g = h(x)\na\na\ny\n\u0398 = (\u03b8, \u03b80)\nJ(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))\n2\n,\n(2.4)\n\u03b8\u2217, \u03b8\u2217\n0 = arg min\n\u03b8,\u03b80 J(\u03b8, \u03b80) .\n(2.5)\nd = 1\nd > 1\nd\n(d + 1)\ny\ny\nx1\nx2\ny\n(x1, x2)\nThe squared loss penalizes guesses", "that are too high the same amount\nas it penalizes guesses that are too\nlow, and has a good mathematical\njustification in the case that your\ndata are generated from an", "underlying linear hypothesis with\nthe so-called Gaussian-\ndistributed noise added to the \nvalues. But there are applications\nin which other losses would be\nbetter, and much of the framework", "we discuss can be applied to\ndifferent loss functions, although\nthis one has a form that also makes\nit particularly computationally\nconvenient.\nWe won\u2019t get into the details of", "Gaussian distribution in our class;\nbut it\u2019s one of the most important\ndistributions and well-worth\nstudying closely at some point.\nOne obvious fact about Gaussian is", "that it\u2019s symmetric; this is in fact\none of the reasons squared loss\ny\n A richer class of hypotheses can be obtained by performing a non-linear feature", "transformation before doing the regression, as we will later see, but it will still end\nup that we have to solve a linear regression problem.\n2.4 A gloriously simple linear regression\nalgorithm", "algorithm\nOkay! Given the objective in Equation 2.4, how can we find good values of  and \n? We\u2019ll study several general-purpose, efficient, interesting algorithms. But before", "we do that, let\u2019s start with the simplest one we can think of: guess a whole bunch ( )\nof different values of  and \n, see which one has the smallest error on the training set,\nand return it.", "and return it.\nAlgorithm 2.1 Random-Regression\nRequire: Data , integer \nfor \n to  do\nRandomly generate hypothesis \nend for\nLet \nreturn", "Let \nreturn \nThis seems kind of silly, but it\u2019s a learning algorithm, and it\u2019s not completely\nuseless.\n\u2753 Study Question", "\u2753 Study Question\nIf your data set has  data points, and the dimension of the  values is , what is\nthe size of an individual \n?\n\u2753 Study Question", "?\n\u2753 Study Question\nHow do you think increasing the number of guesses  will change the training\nerror of the resulting hypothesis?\n2.5 Analytical solution: ordinary least squares", "One very interesting aspect of the problem of finding a linear hypothesis that\nminimizes mean squared error is that we can find a closed-form formula for the", "answer! This general problem is often called the ordinary least squares (ols).\nEverything is easier to deal with if we first ignore the offset \n. So, suppose for now,\nwe have, simply,\n\u03b8\n\u03b80\nk\n\u03b8\n\u03b80\nD\nk", "\u03b8\n\u03b80\nk\n\u03b8\n\u03b80\nD\nk\n1:\ni = 1\nk\n2:\n\u03b8i, \u03b80(i)\n3:\n4:\ni = arg minj J(\u03b8(j), \u03b80(j); D)\n5:\n\u03b8(i), \u03b80(i)\nn\nx\nd\n\u03b8(i)\nk\n\u03b80\ny = \u03b8Tx .\n(2.6)\nworks well under Gaussian\nsettings, as the loss is also\nsymmetric.", "symmetric.\nthis corresponds to a hyperplane\nthat goes through the origin.\n In this case, the objective becomes\nWe approach this just like a minimization problem from calculus homework: take", "the derivative of  with respect to , set it to zero, and solve for . There are\nadditional steps required, to check that the resulting  is a minimum (rather than a", "maximum or an inflection point) but we won\u2019t work through that here. It is possible\nto approach this problem by:\nFinding \n for  in \n,\nConstructing a set of  equations of the form \n, and", ", and\nSolving the system for values of \n.\nThat works just fine. To get practice for applying techniques like this to more", "complex problems, we will work through a more compact (and cool!) matrix view.\nAlong the way, it will be helpful to collect all of the derivatives in one vector. In", "particular, the gradient of  with respect to  is following column vector of length :\n\u2753 Study Question\nWork through the next steps and check your answer against ours below.", "We can think of our training data in terms of matrices \n and \n, where each row of\n is an example, and each row (or rather, element) of \n is the corresponding target\noutput value:\n\u2753 Study Question", "\u2753 Study Question\nWhat are the dimensions of \n and \n?\nJ(\u03b8) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) \u2212y(i))\n2\n.\n(2.7)\nJ\n\u03b8\n\u03b8\n\u03b8\n\u2202J/\u2202\u03b8k\nk\n1, \u2026 , d\nk\n\u2202J/\u2202\u03b8k = 0\n\u03b8k\nJ\n\u03b8\nd\n\u2207\u03b8J =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202J/\u2202\u03b81\n\u22ee\n\u2202J/\u2202\u03b8d\n\u23a4\n\u23a5\n\u23a6\nX\nY\nX\nY\nX =\nY =\n.", "\u23a6\nX\nY\nX\nY\nX =\nY =\n.\n\u23a1\n\u23a2\n\u23a3\nx(1)\n1\n\u2026\nx(1)\nd\n\u22ee\n\u22f1\n\u22ee\nx(n)\n1\n\u2026\nx(n)\nd\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\ny(1)\n\u22ee\ny(n)\n\u23a4\n\u23a5\n\u23a6\nX\nY\n Now we can write\nand using facts about matrix/vector calculus, we get", "Setting this equal to zero and solving for  yields the final closed-form solution:\nand the dimensions work out! So, given our data, we can directly compute the", "linear regression that minimizes mean squared error. That\u2019s pretty awesome!\nNow, how do we deal with the offset? We augment the original feature vector with", "a \u201cfake\u201d feature of value 1, and add a corresponding parameter \n to the  vector.\nThat is, we define columns vectors \n such that,\nwhere the \u201caug\u201d denotes that \n have been augmented.", "Then we can now write the linear hypothesis as if there is no offset,\nWe can do this \u201cappending a fake feature of 1\u201d to all data points to form the\naugmented data matrix", "where  as an -by  vector of all one. Then use the formula in Equation 2.8 to find\nthe \n that minimizes the mean squared error.\nJ(\u03b8) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) \u2212y(i))2 = 1\nn (X\u03b8 \u2212Y )T(X\u03b8 \u2212Y ).\n\u2207\u03b8J(\u03b8) = 1", "\u2207\u03b8J(\u03b8) = 1\nn \u2207\u03b8 [(X\u03b8)TX\u03b8 \u2212Y TX\u03b8 \u2212(X\u03b8)TY + Y TY ]\n= 2\nn (XTX\u03b8 \u2212XTY ).\n\u03b8\n\u03b8\u2217= (XTX)\n\u22121XTY\n(2.8)\n\u03b80\n\u03b8\nxaug, \u03b8aug \u2208Rd+1\nxaug =\n,\n\u03b8aug =\n\u23a1\n\u23a2\n\u23a3\nx1\nx2\n\u22ee\nxd\n1\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\n\u03b81\n\u03b82\n\u22ee\n\u03b8d\n\u03b80\n\u23a4\n\u23a5\n\u23a6\n\u03b8, x", "\u22ee\n\u03b8d\n\u03b80\n\u23a4\n\u23a5\n\u23a6\n\u03b8, x\ny = h(xaug; \u03b8aug) = \u03b8T\naugxaug\n(2.9)\nXaug\nXaug =\n= [\n]\n\u23a1\n\u23a2\n\u23a3\nx(1)\n1\n\u2026\nx(1)\nd\n1\n\u22ee\n\u22f1\n\u22ee\n\u22ee\nx\n(n)\n1\n\u2026\nx\n(n)\nd\n1\n\u23a4\n\u23a5\n\u23a6\nX\n\ud835\udfd9\n\ud835\udfd9\nn\n1\n\u03b8aug\nSee Appendix A if you need some", "help finding this gradient.\nHere are two related alternate\nangles to view this formula, for\nintuition\u2019s sake:\n1. Note that\n is the\npseudo-inverse of \n. Thus, \n\u201cpseudo-solves\u201d", "\u201cpseudo-solves\u201d \n(multiply both sides of this on\nthe left by \n).\n2. Note that\nis the projection matrix onto\nthe column space of \n. Thus,\n solves \n.\n(X TX)\u22121X T = X +\n:\nX\n\u03b8\u2217\nX\u03b8 = Y\nX +", ":\nX\n\u03b8\u2217\nX\u03b8 = Y\nX +\nX(X TX)\u22121X T = projcol(X)\nX\n\u03b8\u2217\nX\u03b8 = projcol(X)Y\nThis is a very special case where\nwe can find the solution in closed\nform. In general, we will need to\nuse iterative optimization", "algorithms to find the best\nparameters. Also, this process of\nsetting the graident/derivatives to\nzero and solving for the\nparameters works out in this\nproblem. But there can be", "exceptions to this rule, and we will\ndiscuss them later in the course.\nBut of course, the constant offset is\nnot really gone, it\u2019s just hidden in\nthe augmentation.\n \u2753 Study Question", "\u2753 Study Question\nStop and prove to yourself that adding that extra feature with value 1 to every\ninput vector and getting rid of the \n parameter, as done in Equation 2.9 is", "equivalent to our original model Equation 2.3.\n2.6 Centering\nIn fact, augmenting a \u201cfake\u201d feature of 1, as described above, is also useful for an", "important idea: namely, why utilizing the so-called centering eliminates the need\nfor fitting an intercept, and thereby offers an alternative way to avoid dealing with\n directly.", "directly.\nBy centering, we mean subtracting the average (mean) of each feature from all data\npoints, and we apply the same operation to the labels. For an example of a dataset", "before and after centering, see here\nThe idea is that, with centered dataset, even if we were to search for an offset term", ", it would naturally fall out to be 0. Intuitively, this makes sense \u2013 if a dataset is\ncentered around the origin, it seems natural that the best fitting plane would go\nthrough the origin.", "through the origin.\nLet\u2019s see how this works out mathematically. First, for a centered dataset, two claims\nimmediately follow (recall that  is an -by-1 vector of all ones):\n1. Each column of", "1. Each column of \n sums up to zero, that is, \n.\n2. Similarly, the mean of the labels is 0, so \n.\nRecall that our ultimate goal is to find an optimal fitting hyperplane, parameterized\nby  and", "by  and \n. In other words, we aim to find \n which at this point, involves\nsimply plugging \n into Equation 2.8.\n\u03b80\n\u03b80\n\u03b80\n\ud835\udfd9\nn\nX\nXT\ud835\udfd9= 0\nY T\ud835\udfd9= \ud835\udfd9TY = 0\n\u03b8\n\u03b80\n\u03b8aug,\nXaug = [\n]\nX\n\ud835\udfd9\n1\n Indeed, the optimal", "naturally falls out to be 0.\n2.7 Regularization\nThe objective function of Equation 2.2 balances (training-data) memorization,", "induced by the loss term, with generalization, induced by the regularization term.\nHere, we address the need for regularization specifically for linear regression, and", "show how this can be realized using one popular regularization technique called\nridge regression.\nIf all we cared about was finding a hypothesis with small loss on the training data,", "we would have no need for regularization, and could simply omit the second term\nin the objective. But remember that our ultimate goal is to perform well on input", "values that we haven\u2019t trained on! It may seem that this is an impossible task, but\nhumans and machine-learning methods do this successfully all the time. What", "allows generalization to new input values is a belief that there is an underlying\nregularity that governs both the training and testing data. One way to describe an", "assumption about such a regularity is by choosing a limited class of possible\nhypotheses. Another way to do this is to provide smoother guidance, saying that,", "within a hypothesis class, we prefer some hypotheses to others. The regularizer\narticulates this preference and the constant  says how much we are willing to trade", "off loss on the training data versus preference over hypotheses.\nFor example, consider what happens when \n and \n is highly correlated with", ", meaning that the data look like a line, as shown in the left panel of the figure\nbelow. Thus, there isn\u2019t a unique best hyperplane. Such correlations happen often in", "real-life data, because of underlying common causes; for example, across a\npopulation, the height of people may depend on both age and amount of food\n\u03b8\u2217\naug = ([\n] [\n])\n\u22121\n[\n]Y\n= [\n]\n\u22121\n[\n]Y\n= [\n]\n\u22121", "]\n\u22121\n[\n]Y\n= [\n]\n\u22121\n[\n]Y\n= [\n]\n\u22121\n[\n]Y\n= [\n]\n= [\n]\n= [\n]\nXT\n\ud835\udfd9T\nX\n\ud835\udfd9\nXT\n\ud835\udfd9T\nXTX\nXT\ud835\udfd9\n\ud835\udfd9TX\n\ud835\udfd9T\ud835\udfd9\nXT\n\ud835\udfd9T\nXTX\nXT\ud835\udfd9\n\ud835\udfd9TX\n\ud835\udfd9T\ud835\udfd9\nXT\n\ud835\udfd9T\nXTX\n0\n0\nn\nXT\n\ud835\udfd9T\n(XTX)\u22121XTY\nn\ud835\udfd9TY\n(XTX)\u22121XTY\n0\n\u03b8\u2217\n\u03b8\u2217\n0\n\u03b80", "0\n\u03b8\u2217\n\u03b8\u2217\n0\n\u03b80\n2.7.1 Regularization and linear regression\n\u03bb\nd = 2,\nx2\nx1\n intake in the same way. This is especially the case when there are many feature", "dimensions used in the regression. Mathematically, this leads to \n close to\nsingularity, such that \n is undefined or has huge values, resulting in", "unstable models (see the middle panel of figure and note the range of the  values\u2014\nthe slope is huge!):\nA common strategy for specifying a regularizer is to use the form", "when we have some idea in advance that \n ought to be near some value \n.\nHere, the notion of distance is quantified by squaring the  norm of the parameter\nvector: for any -dimensional vector", "the  norm of  is defined as,\nIn the absence of such knowledge a default is to regularize toward zero:\nWhen this is done in the example depicted above, the regression model becomes", "stable, producing the result shown in the right-hand panel in the figure. Now the\nslope is much more sensible.\nThere are some kinds of trouble we can get into in regression problems. What if", "is not invertible?\nAnother kind of problem is overfitting: we have formulated an objective that is just\nabout fitting the data as well as possible, but we might also want to regularize to", "keep the hypothesis from getting too attached to the data.\nWe address both the problem of not being able to invert \n and the problem", "and the problem\nof overfitting using a mechanism called ridge regression. We add a regularization\nterm \n to the OLS objective, with a non-negative scalar value  to control the\nXTX\n(XTX)\u22121\ny", "XTX\n(XTX)\u22121\ny\nR(\u0398) = \u2225\u0398 \u2212\u0398prior\u22252\n\u0398\n\u0398prior\nl2\nd\nv \u2208Rd,\nl2\nv\n\u2225v\u2225=\nd\n\u2211\ni=1\n|vi|2 .\n\ue001\n\ue000\n\u23b7\nR(\u0398) = \u2225\u0398\u22252 .\n2.7.2 Ridge regression\n(XTX)\n(XTX)\u22121\n\u2225\u03b8\u22252\n\u03bb", "(XTX)\u22121\n\u2225\u03b8\u22252\n\u03bb\n tradeoff between the training error and the regularization term. Here is the ridge\nregression objective function:\nLarger  values (in magnitude) pressure  values to be near zero.", "Note that, when data isn\u2019t centered, we don\u2019t penalize \n; intuitively, \n is what\n\u201cfloats\u201d the regression surface to the right level for the data you have, and so we", "shouldn\u2019t make it harder to fit a data set where the  values tend to be around one\nmillion than one where they tend to be around one. The other parameters control", "the orientation of the regression surface, and we prefer it to have a not-too-crazy\norientation.\nThere is an analytical expression for the \n values that minimize \n, even when", ", even when\nthe data isn\u2019t centered, but it\u2019s a more complicated to derive than the solution for\nOLS, even though the process is conceptually similar: taking the gradient, setting it", "to zero, and solving for the parameters.\nThe good news is, when the dataset is centered, we again have very clean set up and\nderivation. In particular, the objective can be written as:", "and the solution is:\nOne other great news is that in Equation 2.13, the matrix we are trying to invert can\nalways be inverted! Why is the term \n invertible? Explaining this", "requires some linear algebra. The matrix \n is positive semidefinite, which\nimplies that its eigenvalues \n are greater than or equal to 0. The matrix\n has eigenvalues", "has eigenvalues \n which are guaranteed to be strictly\npositive since \n. Recalling that the determinant of a matrix is simply the\nproduct of its eigenvalues, we get that \n and conclude that", "and conclude that\n is invertible.\n2.8 Evaluating learning algorithms\nJridge(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))\n2\n+ \u03bb\u2225\u03b8\u22252\n(2.10)\n\u03bb\n\u03b8\n\u03b80\n\u03b80\ny\n\u03b8, \u03b80\nJridge\nJridge(\u03b8) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) \u2212y(i))", "i=1\n(\u03b8Tx(i) \u2212y(i))\n2\n+ \u03bb\u2225\u03b8\u22252\n(2.11)\n\u03b8ridge = (XTX + n\u03bbI)\n\u22121XTY\n(2.12)\nDerivation of the Ridge Regression Solution for Centered Data Set\n(XTX + n\u03bbI)\nXTX\n{\u03b3i}i\nXTX + n\u03bbI\n{\u03b3i + n\u03bb}i\n\u03bb > 0", "{\u03b3i + n\u03bb}i\n\u03bb > 0\ndet(XTX + n\u03bbI) > 0\nXTX + n\u03bbI\nCompare Equation 2.10 and\nEquation 2.11. What is the\ndifference between the two? How\nis it possible to drop the offset\nhere?", "here?\n In this section, we will explore how to evaluate supervised machine-learning\nalgorithms. We will study the special case of applying them to regression problems,", "but the basic ideas of validation, hyper-parameter selection, and cross-validation\napply much more broadly.\nWe have seen how linear regression is a well-formed optimization problem, which", "has an analytical solution when ridge regularization is applied. But how can one\nchoose the best amount of regularization, as parameterized by ? Two key ideas", "involve the evaluation of the performance of a hypothesis, and a separate\nevaluation of the algorithm used to produce hypotheses, as described below.", "The performance of a given hypothesis  may be evaluated by measuring test error\non data that was not used to train it. Given a training set \n a regression", "a regression\nhypothesis , and if we choose squared loss, we can define the OLS training error of\n to be the mean square error between its predictions and the expected outputs:", "Test error captures the performance of  on unseen data, and is the mean square\nerror on the test set, with a nearly identical expression as that above, differing only\nin the range of index :\non", "on \n new examples that were not used in the process of constructing .\nIn machine learning in general, not just regression, it is useful to distinguish two\nways in which a hypothesis", "might contribute to test error. Two are:\nStructural error: This is error that arises because there is no hypothesis \n that", "that\nwill perform well on the data, for example because the data was really generated by\na sine wave but we are trying to fit it with a line.", "Estimation error: This is error that arises because we do not have enough data (or\nthe data are in some way unhelpful) to allow us to choose a good \n, or because", ", or because\nwe didn\u2019t solve the optimization problem well enough to find the best  given the\ndata that we had.\nWhen we increase , we tend to increase structural error but decrease estimation", "error, and vice versa.\nNote that this section is relevant to learning algorithms generally\u2014we are just introducing\nthe topic here since we now have an algorithm that can be evaluated!\n\u03bb", "\u03bb\n2.8.1 Evaluating hypotheses\nh\nDn,\nh\nh\nEtrain(h) = 1\nn\nn\n\u2211\ni=1\n[h(x(i)) \u2212y(i)]\n2\n.\nh\ni\nEtest(h) = 1\nn\u2032\nn+n\u2032\n\u2211\ni=n+1\n[h(x(i)) \u2212y(i)]\n2\nn\u2032\nh\nh \u2208H\nh \u2208H\nh \u2208H\nh\n\u03bb\n2.8.2 Evaluating learning algorithms", "A learning algorithm is a procedure that takes a data set \n as input and returns an\nhypothesis  from a hypothesis class \n; it looks like", "; it looks like\nKeep in mind that  has parameters. The learning algorithm itself may have its own\nparameters, and such parameters are often called hyperparameters. The analytical", "solutions presented above for linear regression, e.g., Equation 2.12, may be thought\nof as learning algorithms, where  is a hyperparameter that governs how the", "learning algorithm works and can strongly affect its performance.\nHow should we evaluate the performance of a learning algorithm? This can be", "tricky. There are many potential sources of variability in the possible result of\ncomputing test error on a learned hypothesis :\nWhich particular training examples occurred in", "Which particular testing examples occurred in \nRandomization inside the learning algorithm itself\nGenerally, to evaluate how well a learning algorithm works, given an unlimited data", "source, we would like to execute the following process multiple times:\nTrain on a new training set (subset of our big data source)", "Evaluate resulting  on a validation set that does not overlap the training set\n(but is still a subset of our same big data source)", "Running the algorithm multiple times controls for possible poor choices of training\nset or unfortunate randomization inside the algorithm itself.", "One concern is that we might need a lot of data to do this, and in many applications\ndata is expensive or difficult to acquire. We can re-use data with cross validation (but", "it\u2019s harder to do theoretical analysis).\nAlgorithm 2.1 Cross-Validate\nRequire: Data , integer \nDivide  into  chunks \n (of roughly equal size)\nfor \n to  do\nTrain \n on \n (withholding chunk", "as the validation set)\nCompute \"test\" error \n on withheld data \nend for\nreturn \nIt\u2019s very important to understand that (cross-)validation neither delivers nor", "evaluates a single particular hypothesis . It evaluates the learning algorithm that\nproduces hypotheses.\nDn\nh\nH\nDtrain \u27f6\n\u27f6h\nlearning alg (H)\nh\n\u03bb\nh\nDtrain\nDtest\n2.8.2.1 Validation\nh", "h\n2.8.2.2 Cross validation\nD\nk\n1:\nD\nk\nD1, D2, \u2026 , Dk\n2:\ni = 1\nk\n3:\nhi\nD \u2216Di\nDi\n4:\nEi(hi)\nDi\n5:\n6:\n1\nk \u2211k\ni=1 Ei(hi)\nh\n The hyper-parameters of a learning algorithm affect how the algorithm works but", "they are not part of the resulting hypothesis. So, for example,  in ridge regression\naffects which hypothesis will be returned, but  itself doesn\u2019t show up in the", "hypothesis (the hypothesis is specified using parameters  and \n).\nYou can think about each different setting of a hyper-parameter as specifying a\ndifferent learning algorithm.", "In order to pick a good value of the hyper-parameter, we often end up just trying a\nlot of values and seeing which one works best via validation or cross-validation.\n\u2753 Study Question", "\u2753 Study Question\nHow could you use cross-validation to decide whether to use analytic ridge\nregression or our random-regression algorithm and to pick  for random\nregression or  for ridge regression?", "2.8.2.3 Hyperparameter tuning\n\u03bb\n\u03bb\n\u03b8\n\u03b80\nk\n\u03bb\n This page contains all content from the legacy PDF notes; gradient descent chapter.", "As we phase out the PDF, this page may receive updates not reflected in the static PDF.\nIn the previous chapter, we showed how to describe an interesting objective", "function for machine learning, but we need a way to find the optimal\n, particularly when the objective function is not amenable to\nanalytical optimization. For example, this can be the case when", "involves a\nmore complex loss function, or more general forms of regularization. It can also be\nthe case when there are simply too many parameters to learn for it to be\ncomputationally feasible.", "There is an enormous and fascinating literature on the mathematical and\nalgorithmic foundations of optimization, but for this class, we will consider one of", "the simplest methods, called gradient descent.\nIntuitively, in one or two dimensions, we can easily think of \n as defining a\nsurface over", "surface over \n; that same idea extends to higher dimensions. Now, our objective is\nto find the \n value at the lowest point on that surface. One way to think about", "gradient descent is that you start at some arbitrary point on the surface, look to see\nin which direction the \u201chill\u201d goes down most steeply, take a small step in that", "direction, determine the direction of steepest descent from where you are, take\nanother small step, etc.\nBelow, we explicitly give gradient descent algorithms for one and multidimensional", "objective functions (Section 3.1 and Section 3.2). We then illustrate the application of\ngradient descent to a loss function which is not merely mean squared loss", "(Section 3.3). And we present an important method known as stochastic gradient\ndescent (Section 3.4), which is especially useful when datasets are too large for", "descent in a single batch, and has some important behaviors of its own.\n3.1 Gradient descent in one dimension\nWe start by considering gradient descent in one dimension. Assume \n, and", ", and\nthat we know both \n and its first derivative with respect to \n, \n. Here is\npseudo-code for gradient descent on an arbitrary function . Along with  and its\ngradient", "gradient \n (which, in the case of a scalar \n, is the same as its derivative \n), we\nhave to specify some hyper-parameters. These hyper-parameters include the initial\nvalue for parameter", ", a step-size hyper-parameter , and an accuracy hyper-\nparameter  .\n3  Gradient Descent\nNote\n\u0398\u2217= arg min\u0398 J(\u0398)\nJ(\u0398)\nJ(\u0398)\n\u0398\n\u0398\n\u0398 \u2208R\nJ(\u0398)\n\u0398 J \u2032(\u0398)\nf\nf\n\u2207\u0398f\n\u0398\nf \u2032\n\u0398\n\u03b7\n\u03f5\nYou might want to consider", "studying optimization some day!\nIt\u2019s one of the fundamental tools\nenabling machine learning, and it\u2019s\na beautiful and deep field.\n\uf4613  Gradient Descent\n\uf52a", "\uf52a\n The hyper-parameter  is often called learning rate when gradient descent is applied\nin machine learning. For simplicity,  may be taken as a constant, as is the case in", "the pseudo-code below; and we\u2019ll see adaptive (non-constant) step-sizes soon.\nWhat\u2019s important to notice though, is that even when  is constant, the actual\nmagnitude of the change to", "may not be constant, as that change depends on the\nmagnitude of the gradient itself too.\nprocedure 1D-Gradient-Descent(\n)\nrepeat\nuntil \nreturn \nend procedure", "end procedure\nNote that this algorithm terminates when the derivative of the function  is\nsufficiently small. There are many other reasonable ways to decide to terminate,\nincluding:", "including:\nStop after a fixed number of iterations , i.e., when \n. Practically, this is the\nmost common choice.\nStop when the change in the value of the parameter \n is sufficiently small,\ni.e., when", "i.e., when \n.\n\u2753 Study Question\nConsider all of the potential stopping criteria for 1D-Gradient-Descent , both\nin the algorithm as it appears and listed separately later. Can you think of ways", "that any two of the criteria relate to each other?\nTheorem 3.1 Choose any small distance \n. If we assume that  has a minimum, is", "sufficiently \u201csmooth\u201d and convex, and if the learning rate  is sufficiently small, gradient\ndescent will reach a point within  of a global optimum point \n.", ".\nHowever, we must be careful when choosing the learning rate to prevent slow\nconvergence, non-converging oscillation around the minimum, or divergence.", "The following plot illustrates a convex function \n, starting gradient\ndescent at \n with a step-size of \n. It is very well-behaved!\n\u03b7\n\u03b7\n\u03b7\n\u0398\n1:\n\u0398init, \u03b7, f, f \u2032, \u03f5\n2:\n\u0398(0) \u2190\u0398init\n3:\nt \u21900\n4:\n5:\nt \u2190t + 1", "t \u21900\n4:\n5:\nt \u2190t + 1\n6:\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7 f \u2032(\u0398(t\u22121))\n7:\n|f \u2032(\u0398(t))| < \u03f5\n8:\n\u0398(t)\n9:\nf\nT\nt = T\n\u0398\n\u0398(t) \u2212\u0398(t\u22121) < \u03f5\n\u2223\u2223\n~\u03f5 > 0\nf\n\u03b7\n~\u03f5\n\u0398\nf(x) = (x \u22122)2\nxinit = 4.0\n1/2\n \u22121\n1\n2\n3\n4\n5\n6\n2\n4\nx\nf(x)", "3\n4\n5\n6\n2\n4\nx\nf(x)\nIf  is non-convex, where gradient descent converges to depends on \n. First, let\u2019s\nestablish some definitions. Let  be a real-valued function defined over some\ndomain \n. A point", "domain \n. A point \n is called a global minimum point of  if \n for\nall other \n. A point \n is instead called a local minimum point of a function\n if there exists some constant", "such that for all  within the interval defined\nby \n \n, where  is some distance metric, e.g.,\n A global minimum point is also a local minimum point, but a", "local minimum point does not have to be a global minimum point.\n\u2753 Study Question\nWhat happens in this example with very small ? With very big ?", "If  is non-convex (and sufficiently smooth), one expects that gradient descent (run\nlong enough with small enough learning rate) will get very close to a point at which", "the gradient is zero, though we cannot guarantee that it will converge to a global\nminimum point.\nThere are two notable exceptions to this common sense expectation: First, gradient", "descent can get stagnated while approaching a point  which is not a local\nminimum or maximum, but satisfies \n. For example, for \n, starting\ngradient descent from the initial guess", ", while using learning rate \nwill lead to \n converging to zero as \n. Second, there are functions (even\nconvex ones) with no minimum points, like \n, for which gradient", "descent with a positive learning rate converges to \n.\nThe plot below shows two different \n, and how gradient descent started from\neach point heads toward two different local optimum points.\nf\nxinit\nf", "f\nxinit\nf\nD\nx0 \u2208D\nf\nf(x0) \u2264f(x)\nx \u2208D\nx0 \u2208D\nf\n\u03f5 > 0\nx\nd(x, x0) < \u03f5, f(x0) \u2264f(x)\nd\nd(x, x0) = ||x \u2212x0||.\n\u03b7\n\u03b7\nf\nx\nf \u2032(x) = 0\nf(x) = x3\nxinit = 1\n\u03b7 < 1/3\nx(k)\nk \u2192\u221e\nf(x) = exp(\u2212x)\n+\u221e\nxinit\n \u22122\n\u22121\n1\n2\n3\n4", "\u22122\n\u22121\n1\n2\n3\n4\n4\n6\n8\n10\nx\nf(x)\n3.2 Multiple dimensions\nThe extension to the case of multi-dimensional \n is straightforward. Let\u2019s assume\n, so \n.\nThe gradient of  with respect to \n is", "is\nThe algorithm remains the same, except that the update step in line 5 becomes\nand any termination criteria that depended on the dimensionality of \n would have", "would have\nto change. The easiest thing is to keep the test in line 6 as \n,\nwhich is sensible no matter the dimensionality of \n.\n\u2753 Study Question", ".\n\u2753 Study Question\nWhich termination criteria from the 1D case were defined in a way that assumes\n is one dimensional?\n3.3 Application to regression\n\u0398\n\u0398 \u2208Rm\nf : Rm \u2192R\nf\n\u0398\n\u2207\u0398f =\n\u23a1\n\u23a2\n\u23a3\n\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m", "\u23a2\n\u23a3\n\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m\n\u23a4\n\u23a5\n\u23a6\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7\u2207\u0398f(\u0398(t\u22121))\n\u0398\nf(\u0398(t)) \u2212f(\u0398(t\u22121)) < \u03f5\n\u2223\u2223\n\u0398\n\u0398\n Recall from the previous chapter that choosing a loss function is the first step in", "formulating a machine-learning problem as an optimization problem, and for\nregression we studied the mean square loss, which captures losws as\n. This leads to the ordinary least squares objective", "We use the gradient of the objective with respect to the parameters,\nto obtain an analytical solution to the linear regression problem. Gradient descent", "could also be applied to numerically compute a solution, using the update rule\nNow, let\u2019s add in the regularization term, to get the ridge-regression objective:", "Recall that in ordinary least squares, we finessed handling \n by adding an extra\ndimension of all 1\u2019s. In ridge regression, we really do need to separate the\nparameter vector  from the offset", ", and so, from the perspective of our general-\npurpose gradient descent method, our whole parameter set \n is defined to be\n. We will go ahead and find the gradients separately for each one:", "Note that \n will be of shape \n and \n will be a scalar since we\nhave separated \n from  here.\n\u2753 Study Question\n(guess \u2212actual)2\nJ(\u03b8) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) \u2212y(i))\n2\n.\n\u2207\u03b8J = 2\nn XT\nd\u00d7n\n(X\u03b8 \u2212Y )\nn\u00d71\n,\n\ue152\n\ue154", "(X\u03b8 \u2212Y )\nn\u00d71\n,\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n(3.1)\n\u03b8(t) = \u03b8(t\u22121) \u2212\u03b7 2\nn\nn\n\u2211\ni=1\n([\u03b8(t\u22121)]\nT\nx(i) \u2212y(i))x(i) .\n3.3.1 Ridge regression\nJridge(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))\n2\n+ \u03bb\u2225\u03b8\u22252 .\n\u03b80\n\u03b8\n\u03b80\n\u0398", "+ \u03bb\u2225\u03b8\u22252 .\n\u03b80\n\u03b8\n\u03b80\n\u0398\n\u0398 = (\u03b8, \u03b80)\n\u2207\u03b8Jridge(\u03b8, \u03b80) = 2\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))x(i) + 2\u03bb\u03b8\n\u2202Jridge(\u03b8, \u03b80)\n\u2202\u03b80\n= 2\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i)) .\n\u2207\u03b8Jridge\nd \u00d7 1\n\u2202Jridge/\u2202\u03b80\n\u03b80\n\u03b8", "\u2202Jridge/\u2202\u03b80\n\u03b80\n\u03b8\n Convince yourself that the dimensions of all these quantities are correct, under\nthe assumption that  is \n. How does  relate to \n as discussed for \n in the\nprevious section?", "previous section?\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives\n. What is the shape of \n?\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives\n.", ".\n\u2753 Study Question\nUse these last two results to verify our derivation above.\nPutting everything together, our gradient descent algorithm for ridge regression\nbecomes\nprocedure RR-Gradient-Descent(\n)", ")\nrepeat\nuntil \nreturn \nend procedure\n\u2753 Study Question\nIs it okay that  doesn\u2019t appear in line 8?\n\u2753 Study Question\nIs it okay that the 2\u2019s from the gradient definitions don\u2019t appear in the\nalgorithm?", "algorithm?\n\u03b8\nd \u00d7 1\nd\nm\n\u0398\n\u2207\u03b8||\u03b8||2\n(\u2202||\u03b8||2/\u2202\u03b81, \u2026 , \u2202||\u03b8||2/\u2202\u03b8d)\n\u2207\u03b8||\u03b8||2\n\u2207\u03b8Jridge(\u03b8Tx + \u03b80, y)\n(\u2202Jridge(\u03b8Tx + \u03b80, y)/\u2202\u03b81, \u2026 , \u2202Jridge(\u03b8Tx + \u03b80, y)/\u2202\u03b8d)\n1:\n\u03b8init, \u03b80init, \u03b7, \u03f5\n2:\n\u03b8(0) \u2190\u03b8init\n3:\n\u03b8(0)", "\u03b8(0) \u2190\u03b8init\n3:\n\u03b8(0)\n0\n\u2190\u03b80init\n4:\nt \u21900\n5:\n6:\nt \u2190t + 1\n7:\n\u03b8(t) = \u03b8(t\u22121) \u2212\u03b7 ( 1\nn \u2211n\ni=1 (\u03b8(t\u22121)Tx(i) + \u03b80\n(t\u22121) \u2212y(i))x(i) + \u03bb\u03b8(t\u22121))\n8:\n\u03b8(t)\n0 = \u03b8(t\u22121)\n0\n\u2212\u03b7 ( 1\nn \u2211n\ni=1 (\u03b8(t\u22121)Tx(i) + \u03b80(t\u22121) \u2212y(i)))", "9:\nJridge(\u03b8(t), \u03b8(t)\n0 ) \u2212Jridge(\u03b8(t\u22121), \u03b8(t\u22121)\n0\n) < \u03f5\n\u2223\u2223\n10:\n\u03b8(t), \u03b8(t)\n0\n11:\n\u03bb\nBeware double superscripts! \n is\nthe transpose of the vector .\n[\u03b8]T\n\u03b8\n 3.4 Stochastic gradient descent", "When the form of the gradient is a sum, rather than take one big(ish) step in the\ndirection of the gradient, we can, instead, randomly select one term of the sum, and", "take a very small step in that direction. This seems sort of crazy, but remember that\nall the little steps would average out to the same direction as the big step if you", "were to stay in one place. Of course, you\u2019re not staying in that place, so you move,\nin expectation, in the direction of the gradient.", "Most objective functions in machine learning can end up being written as an\naverage over data points, in which case, stochastic gradient descent (sgd) is", "implemented by picking a data point randomly out of the data set, computing the\ngradient as if there were only that one point in the data set, and taking a small step\nin the negative direction.", "Let\u2019s assume our objective has the form\nwhere  is the number of data points used in the objective (and this may be\ndifferent from the number of points available in the whole data set).", "Here is pseudocode for applying sgd to such an objective ; it assumes we know the\nform of \n for all  in \n:\nprocedure Stochastic-Gradient-Descent(\n)\nfor \n do\nrandomly select \nend for\nend procedure", "end procedure\nNote that now instead of a fixed value of ,  is indexed by the iteration of the\nalgorithm, . Choosing a good stopping criterion can be a little trickier for sgd than", "traditional gradient descent. Here we\u2019ve just chosen to stop after a fixed number of\niterations .\nFor sgd to converge to a local optimum point as  increases, the learning rate has to", "decrease as a function of time. The next result shows one learning rate sequence that\nworks.\nTheorem 3.2 If  is convex, and \n is a sequence satisfying\nf(\u0398) = 1\nn\nn\n\u2211\ni=1\nfi(\u0398) ,\nn\nf\n\u2207\u0398fi\ni\n1 \u2026 n\n1:", "n\nf\n\u2207\u0398fi\ni\n1 \u2026 n\n1:\n\u0398init, \u03b7, f, \u2207\u0398f1, . . . , \u2207\u0398fn, T\n2:\n\u0398(0) \u2190\u0398init\n3:\nt \u21901\n4:\ni \u2208{1, 2, \u2026 , n}\n5:\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7(t) \u2207\u0398fi(\u0398(t\u22121))\n6:\n7:\n\u03b7 \u03b7\nt\nT\nt\nf\n\u03b7(t)\n\u221e\n\u2211\nt=1\n\u03b7(t) = \u221eand\n\u221e\n\u2211\nt=1\n\u03b7(t)2 < \u221e,", "\u221e\n\u2211\nt=1\n\u03b7(t)2 < \u221e,\nSometimes you will see that the\nobjective being written as a sum,\ninstead of an average. In the \u201csum\u201d\nconvention, the \n normalizing\nconstant is getting \u201cabsorbed\u201d into\nindividual", "individual \n.\n1\nn\nfi\nf(\u0398) =\nn\n\u2211\ni=1\nfi(\u0398) .\n then SGD converges with probability one* to the optimal \n.*\nWhy these two conditions? The intuition is that the first condition, on \n, is", ", is\nneeded to allow for the possibility of an unbounded potential range of exploration,\nwhile the second condition, on \n, ensures that the learning rates get smaller\nand smaller as  increases.", "One \u201clegal\u201d way of setting the learning rate is to make \n but people often\nuse rules that decrease more slowly, and so don\u2019t strictly satisfy the criteria for\nconvergence.\n\u2753 Study Question", "\u2753 Study Question\nIf you start a long way from the optimum, would making \n decrease more\nslowly tend to make you move more quickly or more slowly to the optimum?", "There are multiple intuitions for why sgd might be a better choice algorithmically\nthan regular gd (which is sometimes called batch gd (bgd)):", "bgd typically requires computing some quantity over every data point in a data\nset. sgd may perform well after visiting only some of the data. This behavior", "can be useful for very large data sets \u2013 in runtime and memory savings.\nIf your  is actually non-convex, but has many shallow local optimum points", "that might trap bgd, then taking samples from the gradient at some point \nmight \u201cbounce\u201d you around the landscape and away from the local optimum\npoints.", "points.\nSometimes, optimizing  really well is not what we want to do, because it\nmight overfit the training set; so, in fact, although sgd might not get lower", "training error than bgd, it might result in lower test error.\n\u0398\n\u2211\u03b7(t)\n\u2211\u03b7(t)2\nt\n\u03b7(t) = 1/t\n\u03b7(t)\nf\n\u0398\nf\n This page contains all content from the legacy PDF notes; classification chapter.", "As we phase out the PDF, this page may receive updates not reflected in the static PDF.\n4.1 Classification\nClassification is a machine learning problem seeking to map from inputs \n to", "to\noutputs in an unordered set.\nExamples of classification output sets could be \n if we\u2019re\ntrying to figure out what type of fruit we have, or \n if", "if\nwe\u2019re working in an emergency room and trying to give the best medical care to a\nnew patient. We focus on an essential simple case, binary classification, where we aim\nto find a mapping from", "to two outputs. While we should think of the outputs as\nnot having an order, it\u2019s often convenient to encode them as \n. As before, let", ". As before, let\nthe letter  (for hypothesis) represent a classifier, so the classification process looks\nlike:\nLike regression, classification is a supervised learning problem, in which we are given", "a training data set of the form\nWe will assume that each \n is a \n column vector. The intended use of this data\nis that, when given an input \n, the learned hypothesis should generate output \n.", ".\nWhat makes a classifier useful? As in regression, we want it to work well on new\ndata, making good predictions on examples it hasn\u2019t seen. But we don\u2019t know", "exactly what data this classifier might be tested on when we use it in the real world.\nSo, we have to assume a connection between the training data and testing data;", "typically, they are drawn independently from the same probability distribution.\nIn classification, we will often use 0-1 loss for evaluation (as discussed in", "Section 1.3). For that choice, we can write the training error and the testing error. In\nparticular, given a training set \n and a classifier , we define the training error of \nto be\n4  Classification", "4  Classification\nNote\nRd\n{apples, oranges, pears}\n{heart attack, no heart attack}\nRd\n{+1, 0}\nh\nx \u2192\n\u2192y .\nh\nDtrain = {(x(1), y(1)), \u2026 , (x(n), y(n))} .\nx(i)\nd \u00d7 1\nx(i)\ny(i)\nDn\nh\nh", "x(i)\ny(i)\nDn\nh\nh\nThis is in contrast to a continuous\nreal-valued output, as we saw for\nlinear regression.\n\uf4614  Classification\n\uf52a\n3", "\uf52a\n3\n For now, we will try to find a classifier with small training error (later, with some\nadded criteria) and hope it generalizes well to new data, and has a small test error\non", "on \n new examples that were not used in the process of finding the classifier.\nWe begin by introducing the hypothesis class of linear classifiers (Section 4.2) and", "then define an optimization framework to learn linear logistic classifiers (Section 4.3).\n4.2 Linear classifiers", "We start with the hypothesis class of linear classifiers. They are (relatively) easy to\nunderstand, simple in a mathematical sense, powerful on their own, and the basis", "for many other more sophisticated methods. Following their definition, we present\na simple learning algorithm for classifiers.\nA linear classifier in  dimensions is defined by a vector of parameters", "and\nscalar \n. So, the hypothesis class \n of linear classifiers in  dimensions is\nparameterized by the set of all vectors in \n. We\u2019ll assume that  is a \ncolumn vector.", "column vector.\nGiven particular values for  and \n, the classifier is defined by\nRemember that we can think of \n as specifying a -dimensional hyperplane", "(compare the above with Equation 2.3). But this time, rather than being interested in\nthat hyperplane\u2019s values at particular points , we will focus on the separator that it", "induces. The separator is the set of  values such that \n. This is also a\nhyperplane, but in \n dimensions! We can interpret  as a vector that is", "perpendicular to the separator. (We will also say that  is normal to the separator.)\nBelow is an embedded demo illustrating the separator and normal vector. Open\ndemo in full screen.\nEtrain(h) = 1\nn", "Etrain(h) = 1\nn\nn\n\u2211\ni=1\n{\n.\n1\nh(x(i)) \u2260y(i)\n0\notherwise\n(4.1)\nEtest(h) = 1\nn\u2032\nn+n\u2032\n\u2211\ni=n+1\n{1\nh(x(i)) \u2260y(i)\n0\notherwise\nn\u2032\n4.2.1 Linear classifiers: definition\nd\n\u03b8 \u2208Rd\n\u03b80 \u2208R\nH\nd\nRd+1\n\u03b8\nd \u00d7 1\n\u03b8\n\u03b80", "d\nRd+1\n\u03b8\nd \u00d7 1\n\u03b8\n\u03b80\nh(x; \u03b8, \u03b80) = step(\u03b8Tx + \u03b80) = {\n.\n+1\nif \u03b8Tx + \u03b80 > 0\n0\notherwise\n\u03b8, \u03b80\nd\nx\nx\n\u03b8Tx + \u03b80 = 0\nd \u22121\n\u03b8\n\u03b8\nDemo: Linear classifier separator\n \u03b8\u2081:\n0.5\n\u03b8\u2082:\n0.5\n\u03b8\u2080:\n0.0\nToggle z=0\nSurface", "Toggle z=0\nSurface\nBuilt with \u2764\ufe0f by Shen\u00b2 | Report a Bug\nFeatures (x\u2081, x\u2082) & z = \u03b8\u2081x\u2081 + \u03b8\u2082x\u2082 + \u03b8\u2080\n\u22122\n\u22121\n0\n1\n2\n\u22125\n0\n5\nSeparator\nNormal vecto\nPrediction: P\nPrediction: N\nFeature space (x\u2081, x\u2082\nx\u2081\nx\u2082", "x\u2081\nx\u2082\n \nFor example, in two dimensions (\n) the separator has dimension 1, which\nmeans it is a line, and the two components of \n give the orientation of", "the separator, as illustrated in the following example.\nLet  be the linear classifier defined by \n. The diagram below shows the \nvector (in green) and the separator it defines:\nd = 2\n\u03b8 = [\u03b81, \u03b82]T", "d = 2\n\u03b8 = [\u03b81, \u03b82]T\n4.2.2 Linear classifiers: examples\nExample:\nh\n\u03b8 = [\n], \u03b80 = 1\n1\n\u22121\n\u03b8\n \u03b8Tx + \u03b80 = 0\nx1\nx2\n\u03b8\n\u03b82\n\u03b81\nWhat is", "x2\n\u03b8\n\u03b82\n\u03b81\nWhat is \n? We can solve for it by plugging a point on the line into the equation for the\nline. It is often convenient to choose a point on one of the axes, e.g., in this case,\n, for which", ", for which \n, giving \n.\nIn this example, the separator divides \n, the space our \n points live in, into two\nhalf-spaces. The one that is on the same side as the normal vector is the positive half-", "space, and we classify all points in that space as positive. The half-space on the\nother side is negative and all points in it are classified as negative.", "Note that we will call a separator a linear separator of a data set if all of the data with\none label falls on one side of the separator and all of the data with the other label", "falls on the other side of the separator. For instance, the separator in the next\nexample is a linear separator for the illustrated data. If there exists a linear separator", "on a dataset, we call this dataset linearly separable.\nLet  be the linear classifier defined by \n.\nThe diagram below shows several points classified by . In particular, let \n and\n.\n\u03b80\nx = [0, 1]T", ".\n\u03b80\nx = [0, 1]T\n\u03b8T [ ] + \u03b80 = 0\n0\n1\n\u03b80 = 1\nRd\nx(i)\nExample:\nh\n\u03b8 = [\n], \u03b80 = 3\n\u22121\n1.5\nh\nx(1) = [ ]\n3\n2\nx(2) = [\n]\n4\n\u22121\n(\n[ ]\n)\n Thus, \n and", "[ ]\n)\n Thus, \n and \n are given positive (label +1) and negative (label 0) classifications,\nrespectively.\n\u2753 Study Question", "\u2753 Study Question\nWhat is the green vector normal to the separator? Specify it as a column vector.\n\u2753 Study Question\nWhat change would you have to make to \n if you wanted to have the", "separating hyperplane in the same place, but to classify all the points labeled \u2018+\u2019\nin the diagram as negative and all the points labeled \u2018-\u2019 in the diagram as\npositive?", "positive?\n4.3 Linear logistic classifiers\nGiven a data set and the hypothesis class of linear classifiers, our goal will be to find", "the linear classifier that optimizes an objective function relating its predictions to\nthe training data. To make this problem computationally reasonable, we will need", "to take care in how we formulate the optimization problem to achieve this goal.\nFor classification, it is natural to make predictions in \n and use the 0-1 loss\nfunction,", "function, \n, as introduced in Chapter 1:\nh(x(1); \u03b8, \u03b80) = step ([\n] [ ] + 3) = step(3) = +1\nh(x(2); \u03b8, \u03b80) = step ([\n] [\n] + 3) = step(\u22122.5) = 0\n\u22121\n1.5\n3\n2\n\u22121\n1.5\n4\n\u22121\nx(1)\nx(2)\n\u03b8, \u03b80\n{+1, 0}\nL01", "\u03b8, \u03b80\n{+1, 0}\nL01\nL01(g, a) = {\n.\n0\nif g = a\n1\notherwise\n However, even for simple linear classifiers, it is very difficult to find values for \nthat minimize simple 0-1 training error", "This problem is NP-hard, which probably implies that solving the most difficult\ninstances of this problem would require computation time exponential in the number\nof training examples, .", "What makes this a difficult optimization problem is its lack of \u201csmoothness\u201d:\nThere can be two hypotheses, \n and \n, where one is closer in\nparameter space to the optimal parameter values", ", but they make the\nsame number of misclassifications so they have the same  value.\nAll predictions are categorical: the classifier can\u2019t express a degree of certainty", "about whether a particular input  should have an associated value .\nFor these reasons, if we are considering a hypothesis \n that makes five incorrect", "predictions, it is difficult to see how we might change \n so that it will perform\nbetter, which makes it difficult to design an algorithm that searches in a sensible", "way through the space of hypotheses for a good one. For these reasons, we\ninvestigate another hypothesis class: linear logistic classifiers, providing their", "definition, then an approach for learning such classifiers using optimization.\nThe hypotheses in a linear logistic classifier (LLC) are parameterized by a -\ndimensional vector  and a scalar", ", just as is the case for linear classifiers.\nHowever, instead of making predictions in \n, LLC hypotheses generate real-\nvalued outputs in the interval \n. An LLC has the form", "This looks familiar! What\u2019s new?\nThe logistic function, also known as the sigmoid function, is defined as\nand is plotted below, as a function of its input . Its output can be interpreted as a", "probability, because for any value of  the output is in \n.\n\u03b8, \u03b80\nJ(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\nL01(step(\u03b8Tx(i) + \u03b80), y(i)) .\nn\n(\u03b8, \u03b80)\n(\u03b8\u2032, \u03b8\u2032\n0)\n(\u03b8\u2217, \u03b8\u2217\n0)\nJ\nx\ny\n\u03b8, \u03b80\n\u03b8, \u03b80", "J\nx\ny\n\u03b8, \u03b80\n\u03b8, \u03b80\n4.3.1 Linear logistic classifiers: definition\nd\n\u03b8\n\u03b80\n{+1, 0}\n(0, 1)\nh(x; \u03b8, \u03b80) = \u03c3(\u03b8Tx + \u03b80) .\n\u03c3(z) =\n1\n1 + e\u2212z\n,\nz\nz\n(0, 1)\nThe \u201cprobably\u201d here is not because", "we\u2019re too lazy to look it up, but\nactually because of a fundamental\nunsolved problem in computer-\nscience theory, known as \u201cP\nvs. NP.\u201d\n \u22124\n\u22122\n2\n4\n0.5\n1\nz\n\u03c3(z)\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself the output of  is always in the interval \n. Why can\u2019t it\nequal 0 or equal 1? For what value of  does \n?", "?\nWhat does an LLC look like? Let\u2019s consider the simple case where \n, so our\ninput points simply lie along the  axis. Classifiers in this case have dimension ,", "meaning that they are points. The plot below shows LLCs for three different\nparameter settings: \n, \n, and \n\u22124\n\u22122\n2\n4\n0.5\n1\nx\n\u03c3(\u03b8T x + \u03b80)\n\u2753 Study Question", "\u2753 Study Question\nWhich plot is which? What governs the steepness of the curve? What governs\nthe  value where the output is equal to 0.5?", "But wait! Remember that the definition of a classifier is that it\u2019s a mapping from\n or to some other discrete set. So, then, it seems like an LLC is actually\nnot a classifier!", "not a classifier!\nGiven an LLC, with an output value in \n, what should we do if we are forced to\nmake a prediction in \n? A default answer is to predict \n if\n\u03c3\n(0, 1)\nz\n\u03c3(z) = 0.5", "(0, 1)\nz\n\u03c3(z) = 0.5\n4.3.2 Linear logistic classifier: examples\nd = 1\nx\n0\n\u03c3(10x + 1) \u03c3(\u22122x + 1)\n\u03c3(2x \u22123).\nx\nRd \u2192{+1, 0}\n(0, 1)\n{+1, 0}\n+1\n  and  otherwise. The value \n is sometimes called a prediction", "threshold.\nIn fact, for different problem settings, we might prefer to pick a different prediction\nthreshold. The field of decision theory considers how to make this choice. For", "example, if the consequences of predicting \n when the answer should be \n are\nmuch worse than the consequences of predicting \n when the answer should be", ", then we might set the prediction threshold to be greater than \n.\n\u2753 Study Question\nUsing a prediction threshold of 0.5, for what values of  do each of the LLCs\nshown in the figure above predict \n?", "?\nWhen \n, then our inputs  lie in a two-dimensional space with axes \n and \n,\nand the output of the LLC is a surface, as shown below, for \n.\n\u2753 Study Question", ".\n\u2753 Study Question\nConvince yourself that the set of points for which \n, that is, the\n``boundary\u2019\u2019 between positive and negative predictions with prediction\nthreshold \n, is a line in", ", is a line in \n space. What particular line is it for the case in\nthe figure above? How would the plot change for \n, but now with\n? For \n?", "? For \n?\nOptimization is a key approach to solving machine learning problems; this also\napplies to learning linear logistic classifiers (LLCs) by defining an appropriate loss", "function for optimization. A first attempt might be to use the simple 0-1 loss\n\u03c3(\u03b8Tx + \u03b80) > 0.5\n0\n0.5\n+1\n\u22121\n\u22121\n+1\n0.5\nx\n+1\nd = 2\nx\nx1\nx2\n\u03b8 = (1, 1), \u03b80 = 2\n\u03c3(\u03b8Tx + \u03b80) = 0.5\n0.5\n(x1, x2)\n\u03b8 = (1, 1)", "(x1, x2)\n\u03b8 = (1, 1)\n\u03b80 = \u22122\n\u03b8 = (\u22121, \u22121), \u03b80 = 2\n4.3.3 Learning linear logistic classifiers\n function \n that gives a value of 0 for a correct prediction, and a 1 for an incorrect", "prediction. As noted earlier, however, this gives rise to an objective function that is\nvery difficult to optimize, and so we pursue another strategy for defining our\nobjective.", "objective.\nFor learning LLCs, we\u2019d have a class of hypotheses whose outputs are in \n, but\nfor which we have training data with  values in \n. How can we define an", "appropriate loss function? We start by changing our interpretation of the output to\nbe the probability that the input should map to output value 1 (we might also say that", "this is the probability that the input is in class 1 or that the input is \u2018positive.\u2019)\n\u2753 Study Question\nIf \n is the probability that  belongs to class \n, what is the probability that", "belongs to the class \n, assuming there are only these two classes?\nIntuitively, we would like to have low loss if we assign a high probability to the correct", "class. We\u2019ll define a loss function, called negative log-likelihood (NLL), that does just\nthis. In addition, it has the cool property that it extends nicely to the case where we", "would like to classify our inputs into more than two classes.\nIn order to simplify the description, we assume that (or transform our data so that)\nthe labels in the training data are \n.", ".\nWe would like to pick the parameters of our classifier to maximize the probability\nassigned by the LLC to the correct  values, as specified in the training set. Letting\nguess \n, that probability is", "under the assumption that our predictions are independent. This can be cleverly\nrewritten, when \n, as\n\u2753 Study Question\nBe sure you can see why these two expressions are the same.", "The big product above is kind of hard to deal with in practice, though. So what can\nwe do? Because the log function is monotonic, the \n that maximize the quantity\nL01\n(0, 1)\ny\n{+1, 0}\nh(x)\nx\n+1\nx\n\u22121", "h(x)\nx\n+1\nx\n\u22121\ny \u2208{0, 1}\ny\ng(i) = \u03c3(\u03b8Tx(i) + \u03b80)\nn\n\u220f\ni=1\n{\n,\ng(i)\nif y(i) = 1\n1 \u2212g(i)\notherwise\ny(i) \u2208{0, 1}\nn\n\u220f\ni=1\ng(i)y(i)\n(1 \u2212g(i))1\u2212y(i) .\n\u03b8, \u03b80\nRemember to be sure your  values", "have this form if you try to learn an\nLLC using NLL!\ny\nThat crazy huge  represents\ntaking the product over a bunch of\nfactors just as huge  represents\ntaking the sum over a bunch of\nterms.\n\u03a0\n\u03a3", "terms.\n\u03a0\n\u03a3\n above will be the same as the \n that maximize its log, which is the following:\nFinally, we can turn the maximization problem above into a minimization problem", "by taking the negative of the above expression, and writing in terms of minimizing\na loss\nwhere \n is the negative log-likelihood loss function:", "This loss function is also sometimes referred to as the log loss or cross entropy. and it\nwon\u2019t make any real difference. If we ask you for numbers, use log base .", "What is the objective function for linear logistic classification? We can finally put\nall these pieces together and develop an objective function for optimizing", "regularized negative log-likelihood for a linear logistic classifier. In fact, this process\nis usually called \u201clogistic regression,\u201d so we\u2019ll call our objective \n, and define it as\n\u2753 Study Question", "\u2753 Study Question\nConsider the case of linearly separable data. What will the  values that\noptimize this objective be like if \n? What will they be like if  is very big?", "Try to work out an example in one dimension with two data points.\nWhat role does regularization play for classifiers? This objective function has the", "same structure as the one we used for regression, Equation 2.2, where the first term\n(in parentheses) is the average loss, and the second term is for regularization.", "Regularization is needed for building classifiers that can generalize well (just as was\nthe case for regression). The parameter  governs the trade-off between the two", "terms as illustrated in the following example.\nSuppose we wish to obtain a linear logistic classifier for this one-dimensional\ndataset:\n\u03b8, \u03b80\nn\n\u2211\ni=1\n(y(i) log g(i) + (1 \u2212y(i)) log(1 \u2212g(i))) .\nn\n\u2211", "n\n\u2211\ni=1\nLnll(g(i), y(i))\nLnll\nLnll(guess, actual) = \u2212(actual \u22c5log(guess) + (1 \u2212actual) \u22c5log(1 \u2212guess)) .\ne\nJlr\nJlr(\u03b8, \u03b80; D) = ( 1\nn\nn\n\u2211\ni=1\nLnll(\u03c3(\u03b8Tx(i) + \u03b80), y(i))) + \u03bb\u2225\u03b8\u22252 .\n(4.2)\n\u03b8\n\u03bb = 0\n\u03bb\n\u03bb", "(4.2)\n\u03b8\n\u03bb = 0\n\u03bb\n\u03bb\n Clearly, this can be fit very nicely by a hypothesis \n, but what is the best\nvalue for ? Evidently, when there is no regularization (\n), the objective\nfunction", "function \n will approach zero for large values of , as shown in the plot on the\nleft, below. However, would the best hypothesis really have an infinite (or very", "large) value for ? Such a hypothesis would suggest that the data indicate strong\ncertainty that a sharp transition between \n and \n occurs exactly at \n,", ",\ndespite the actual data having a wide gap around \n.\nIn absence of other beliefs about the solution, we might prefer that our linear", "logistic classifier not be overly certain about its predictions, and so we might prefer\na smaller  over a large  By not being overconfident, we might expect a somewhat", "smaller  to perform better on future examples drawn from this same distribution.\nThis preference can be realized using a nonzero value of the regularization trade-off", "parameter, as illustrated in the plot on the right, above, with \n.\nAnother nice way of thinking about regularization is that we would like to prevent", "our hypothesis from being too dependent on the particular training data that we\nwere given: we would like for it to be the case that if the training data were changed", "slightly, the hypothesis would not change by much.\n4.4 Gradient descent for logistic regression\nNow that we have a hypothesis class (LLC) and a loss function (NLL), we need to", "take some data and find parameters! Sadly, there is no lovely analytical solution like\nthe one we obtained for regression, in Section 2.7.2. Good thing we studied gradient\nh(x) = \u03c3(\u03b8x)\n\u03b8\n\u03bb = 0\nJlr(\u03b8)", "\u03b8\n\u03bb = 0\nJlr(\u03b8)\n\u03b8\n\u03b8\ny = 0\ny = 1\nx = 0\nx = 0\n\u03b8\n\u03b8.\n\u03b8\n\u03bb = 0.2\n descent! We can perform gradient descent on the \n objective, as we\u2019ll see next. We", "can also apply stochastic gradient descent to this problem.\nLuckily, \n has enough nice properties that gradient descent and stochastic", "gradient descent should generally \u201cwork\u201d. We\u2019ll soon see some more challenging\noptimization problems though \u2013 in the context of neural networks, in Section 6.7.", "First we need derivatives with respect to both \n (the scalar component) and  (the\nvector component) of \n. Explicitly, they are:\nNote that \n will be of shape \n and \n will be a scalar since we have", "separated \n from  here.\nPutting everything together, our gradient descent algorithm for logistic regression\nbecomes:\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that the dimensions of all these quantities are correct, under\nthe assumption that  is \n.\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives \n.", ".\nWhat is the shape of \n?\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives\n.\n\u2753 Study Question\nUse these last two results to verify our derivation above.", "Algorithm 4.1 LR-Gradient-Descent(\n)\nrepeat\nJlr\nJlr\n\u03b80\n\u03b8\n\u0398\n\u2207\u03b8Jlr(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\n(g(i) \u2212y(i))x(i) + 2\u03bb\u03b8\n\u2202Jlr(\u03b8, \u03b80)\n\u2202\u03b80\n= 1\nn\nn\n\u2211\ni=1\n(g(i) \u2212y(i)) .\n\u2207\u03b8Jlr\nd \u00d7 1\n\u2202Jlr\n\u2202\u03b80\n\u03b80\n\u03b8\n\u03b8\nd \u00d7 1\n\u2207\u03b8\u2225\u03b8\u22252\n(", "\u03b8\n\u03b8\nd \u00d7 1\n\u2207\u03b8\u2225\u03b8\u22252\n(\n\u2202\u2225\u03b8\u22252\n\u2202\u03b81 , \u2026 ,\n\u2202\u2225\u03b8\u22252\n\u2202\u03b8d )\n\u2207\u03b8\u2225\u03b8\u22252\n\u2207\u03b8Lnll(\u03c3(\u03b8Tx + \u03b80), y)\n(\n\u2202Lnll(\u03c3(\u03b8Tx+\u03b80),y)\n\u2202\u03b81\n, \u2026 ,\n\u2202Lnll(\u03c3(\u03b8Tx+\u03b80),y)\n\u2202\u03b8d\n)\n\u03b8init, \u03b80 init, \u03b7, \u03f5\n1: \u03b8(0) \u2190\u03b8init\n2: \u03b8(0)\n0\n\u2190\u03b80 init\n3: t \u21900\n4:", "\u2190\u03b80 init\n3: t \u21900\n4:\n until \nreturn \nLogistic regression, implemented using batch or stochastic gradient descent, is a\nuseful and fundamental machine learning technique. We will also see later that it", "corresponds to a one-layer neural network with a sigmoidal activation function,\nand so is an important step toward understanding neural networks.", "Much like the squared-error loss function that we saw for linear regression, the NLL\nloss function for linear logistic regression is a convex function of the parameters \nand", "and \n (below is a proof if you\u2019re interested). This means that running gradient\ndescent with a reasonable set of hyperparameters will behave nicely.\n4.5 Handling multiple classes", "So far, we have focused on the binary classification case, with only two possible\nclasses. But what can we do if we have multiple possible classes (e.g., we want to", "predict the genre of a movie)? There are two basic strategies:\nTrain multiple binary classifiers using different subsets of our data and\ncombine their outputs to make a class prediction.", "Directly train a multi-class classifier using a hypothesis class that is a\ngeneralization of logistic regression, using a one-hot output encoding and NLL\nloss.", "loss.\nThe method based on NLL is in wider use, especially in the context of neural\nnetworks, and is explored here. In the following, we will assume that we have a\ndata set \n in which the inputs", "but the outputs \n are drawn from a set of\n classes \n. Next, we extend the idea of NLL directly to multi-class\nclassification with \n classes, where the training label is represented with what is", "called a one-hot vector \n, where \n if the example is of class \nand \n otherwise. Now, we have a problem of mapping an input \n that is in\n into a", "into a \n-dimensional output. Furthermore, we would like this output to be\ninterpretable as a discrete probability distribution over the possible classes, which\n5:\nt \u2190t + 1\n6:\n\u03b8(t) \u2190\u03b8(t\u22121) \u2212\u03b7( 1\nn \u2211n", "n \u2211n\ni=1(\u03c3(\u03b8(t\u22121)Tx(i) + \u03b8(t\u22121)\n0\n) \u2212y(i))x(i) + 2\u03bb \u03b8(t\u22121))\n7:\n\u03b8(t)\n0 \u2190\u03b8(t\u22121)\n0\n\u2212\u03b7( 1\nn \u2211n\ni=1(\u03c3(\u03b8(t\u22121)Tx(i) + \u03b8(t\u22121)\n0\n) \u2212y(i)))\n8:\nJlr(\u03b8(t), \u03b8(t)\n0 ) \u2212Jlr(\u03b8(t\u22121), \u03b8(t\u22121)\n0\n) < \u03f5\n\u2223\u2223\n9:\n\u03b8(t), \u03b8(t)\n0", "\u2223\u2223\n9:\n\u03b8(t), \u03b8(t)\n0\n4.4.1 Convexity of the NLL Loss Function\n\u03b8\n\u03b80\nProof of convexity of the NLL loss function\nD\nx(i) \u2208Rd\ny(i)\nK\n{c1, \u2026 , cK}\nK\ny = [\n]T\ny1, \u2026 , yK\nyk = 1\nk\nyk = 0\nx(i)\nRd\nK", "k\nyk = 0\nx(i)\nRd\nK\n means the elements of the output vector have to be non-negative (greater than or\nequal to 0) and sum to 1.\nWe will do this in two steps. First, we will map our input", "into a vector value\n by letting  be a whole \n matrix of parameters, and \n be a \nvector, so that\nNext, we have to extend our use of the sigmoid function to the multi-dimensional", "softmax function, that takes a whole vector \n and generates\nwhich can be interpreted as a probability distribution over \n items. To make the\nfinal prediction of the class label, we can then look at", "find the most likely\nprobability over these \n entries in \n (i.e. find the largest entry in \n) and return the\ncorresponding index as the \u201cone-hot\u201d element of  in our prediction.\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that the vector of  values will be non-negative and sum to 1.\nPutting these steps together, our hypotheses will be", "Now, we retain the goal of maximizing the probability that our hypothesis assigns\nto the correct output \n for each input . We can write this probability, letting \nstand for our \u201cguess\u201d,", ", for a single example \n as \n.\n\u2753 Study Question\nHow many elements that are not equal to 1 will there be in this product?", "The negative log of the probability that we are making a correct guess is, then, for\none-hot vector  and probability distribution vector ,", "We\u2019ll call this nllm for negative log likelihood multiclass. It is also worth noting that the\nNLLM loss function is also convex; however, we will omit the proof.\nx(i)\nz(i) \u2208RK\n\u03b8\nd \u00d7 K\n\u03b80\nK \u00d7 1", "\u03b8\nd \u00d7 K\n\u03b80\nK \u00d7 1\nz = \u03b8Tx + \u03b80 .\nz \u2208RK\ng = softmax(z) =\n.\n\u23a1\n\u23a2\n\u23a3\nexp(z1)/ \u2211i exp(zi)\n\u22ee\nexp(zK)/ \u2211i exp(zi)\n\u23a4\n\u23a5\n\u23a6\nK\ng,\nK\ng,\ng,\n1\ng\nh(x; \u03b8, \u03b80) = softmax(\u03b8Tx + \u03b80) .\nyk\nx\ng\nh(x)\n(x, y)\n\u220fK\nk=1 gyk\nk\ny\ng", "\u220fK\nk=1 gyk\nk\ny\ng\nLnllm(g, y) = \u2212\nK\n\u2211\nk=1\nyk \u22c5log(gk) .\nLet\u2019s check dimensions! \n is\n and  is \n, and \n is\n, so  is \n and we\u2019re\ngood!\n\u03b8T\nK \u00d7 d\nx\nd \u00d7 1\n\u03b80\nK \u00d7 1\nz\nK \u00d7 1\n \u2753 Study Question", "\u2753 Study Question\nBe sure you see that is \n is minimized when the guess assigns high\nprobability to the true class.\n\u2753 Study Question\nShow that \n for \n is the same as \n.", "is the same as \n.\n4.6 Prediction accuracy and validation\nIn order to formulate classification with a smooth objective function that we can", "optimize robustly using gradient descent, we changed the output from discrete\nclasses to probability values and the loss function from 0-1 loss to NLL. However,", "when time comes to actually make a prediction we usually have to make a hard\nchoice: buy stock in Acme or not? And, we get rewarded if we guessed right,", "independent of how sure or not we were when we made the guess.\nThe performance of a classifier is often characterized by its accuracy, which is the", "percentage of a data set that it predicts correctly in the case of 0-1 loss. We can see\nthat accuracy of hypothesis  on data \n is the fraction of the data set that does not\nincur any loss:\nwhere", "where \n is the final guess for one class or the other that we make from \n,\ne.g., after thresholding. It\u2019s noteworthy here that we use a different loss function for", "optimization than for evaluation. This is a compromise we make for computational\nease and efficiency.\nLnllm\nLnllm\nK = 2\nLnll\nh\nD\nA(h; D) = 1 \u22121\nn\nn\n\u2211\ni=1\nL01(g(i), y(i)) ,\ng(i)\nh(x(i))", "g(i)\nh(x(i))\n This page contains all content from the legacy PDF notes; features chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Linear regression and classification are powerful tools, but in the real world, data\noften exhibit non-linear behavior that cannot immediately be captured by the linear", "models which we have built so far. For example, suppose the true behavior of a\nsystem (with \n) looks like this wavelet:", "Such behavior is actually ubiquitous in physical systems, e.g., in the vibrations of\nthe surface of a drum, or scattering of light through an aperture. However, no single", "hyperplane would be a very good fit to such peaked responses!\nA richer class of hypotheses can be obtained by performing a non-linear feature\ntransformation \n before doing the regression. That is,", "is a linear\nfunction of , but \n is a non-linear function of \n if  is a non-linear\nfunction of .\nThere are many different ways to construct . Some are relatively systematic and", "domain independent. Others are directly related to the semantics (meaning) of the\noriginal features, and we construct them deliberately with our application (goal) in\nmind.", "mind.\n5.1 Gaining intuition about feature\ntransformations\nIn this section, we explore the effects of non-linear feature transformations on\nsimple classification problems, to gain intuition.", "Let\u2019s look at an example data set that starts in 1-D:\n5  Feature Representation\nNote\nd = 2\n\u03d5(x)\n\u03b8Tx + \u03b80\nx\n\u03b8T\u03d5(x) + \u03b80\nx,\n\u03d5\nx\n\u03d5\n\uf4615  Feature Representation\n\uf52a\n x\n0", "\uf52a\n x\n0\nThese points are not linearly separable, but consider the transformation\n. Plotting this transformed data (in two-dimensional space, since", "there are now two features), we see that it is now separable. There are lots of\npossible separators; we have just shown one of them here.\nx\nx2\nseparator", "x\nx2\nseparator\nA linear separator in  space is a nonlinear separator in the original space! Let\u2019s see\nhow this plays out in our simple example. Consider the separator \n(which corresponds to \n and", "and \n in our transformed space), which\nlabels the half-plane \n as positive. What separator does it correspond to in\nthe original 1-D space? We have to ask the question: which  values have the", "property that \n. The answer is \n and \n, so those two points constitute\nour separator, back in the original space. Similarly, by evaluating where \nand where", "and where \n, we can find the regions of 1D space that are labeled positive\nand negative (respectively) by this separator.\nExample\n\u03d5(x) = [x, x2]T\nExample\n\u03d5\nx2 \u22121 = 0\n\u03b8 = [0, 1]T\n\u03b80 = \u22121\nx2 \u22121 > 0\nx", "\u03b80 = \u22121\nx2 \u22121 > 0\nx\nx2 \u22121 = 0\n+1\n\u22121\nx2 \u22121 > 0\nx2 \u22121 < 0\nExample\n x\n0\n1\n-1\n5.2 Systematic feature construction\nHere are two different ways to systematically construct features in a problem", "independent way.\nIf the features in your problem are already naturally numerical, one systematic\nstrategy for constructing a new feature space is to use a polynomial basis. The idea is", "that, if you are using the th-order basis (where  is a positive integer), you include\na feature for every possible product of  different dimensions in your original input.", "Here is a table illustrating the th order polynomial basis for different values of ,\ncalling out the cases when \n and \n:\nOrder\nin general (\n)\n0\n1\n2\n3\n\u22ee\n\u22ee\n\u22ee", ")\n0\n1\n2\n3\n\u22ee\n\u22ee\n\u22ee\nThis transformation can be used in combination with linear regression or logistic\nregression (or any other regression or classification model). When we\u2019re using a", "linear regression or classification model, the key insight is that a linear regressor or\nseparator in the transformed space is a non-linear regressor or separator in the\noriginal space.", "original space.\nTo give a regression example, the wavelet pictured at the start of this chapter can be\nfit much better using a polynomial feature representation up to order \n,", ",\ncompared to just using a simple hyperplane in the original (single-dimensional)\nfeature space:\n5.2.1 Polynomial basis\nk\nk\nk\nk\nk\nd = 1\nd > 1\nd = 1\nd > 1\n[1]\n[1]\n[1, x]T\n[1, x1, \u2026 , xd]T\n[1, x, x2]T", "[1, x, x2]T\n[1, x1, \u2026 , xd, x2\n1, x1x2, \u2026]T\n[1, x, x2, x3]T\n[1, x1, \u2026 , xd, x2\n1, x1x2, \u2026 , x3\n1, x1x2\n2, x1x2x3, \u2026]T\nk = 8\n The raw data (with \n random samples) is plotted on the left, and the", "regression result (curved surface) is on the right.\nNow let\u2019s look at a classification example and see how polynomial feature\ntransformation may help us.", "One well-known example is the \u201cexclusive or\u201d (xor) data set, the drosophila of\nmachine-learning data sets:\nClearly, this data set is not linearly separable. So, what if we try to solve the xor", "classification problem using a polynomial basis as the feature transformation? We\ncan just take our two-dimensional data and transform it into a higher-dimensional", "data set, by applying some feature transformation . Now, we have a classification\nproblem as usual.\nLet\u2019s try it for \n on our xor problem. The feature transformation is\n\u2753 Study Question", "\u2753 Study Question\nIf we train a classifier after performing this feature transformation, would we\nlose any expressive power if we let \n (i.e., trained without offset instead of\nwith offset)?", "with offset)?\nWe might run a classification learning algorithm and find a separator with\ncoefficients \n and \n. This corresponds to\nn = 1000\nExample\n\u03d5\nk = 2\n\u03d5([x1, x2]T) = [1, x1, x2, x2\n1, x1x2, x2", "1, x1x2, x2\n2]T .\n\u03b80 = 0\n\u03b8 = [0, 0, 0, 0, 4, 0]T\n\u03b80 = 0\n2\n2\nD. Melanogaster is a species of\nfruit fly, used as a simple system in\nwhich to study genetics, since 1910.", "and is plotted below, with the gray shaded region classified as negative and the\nwhite region classified as positive:\n\u2753 Study Question", "\u2753 Study Question\nBe sure you understand why this high-dimensional hyperplane is a separator,\nand how it corresponds to the figure.", "For fun, we show some more plots below. Here is another result for a linear\nclassifier on xor generated with logistic regression and gradient descent, using a", "random initial starting point and second-order polynomial basis:\nHere is a harder data set. Logistic regression with gradient descent failed to", "separate it with a second, third, or fourth-order basis feature representation, but\n0 + 0x1 + 0x2 + 0x2\n1 + 4x1x2 + 0x2\n2 + 0 = 0\nExample\nExample", "Example\nExample\n succeeded with a fifth-order basis. Shown below are some results after \ngradient descent iterations (from random starting points) for bases of order 2", "(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).\n\u2753 Study Question\nPercy Eptron has a domain with four numeric input features, \n. He\ndecides to use a representation of the form", "where \n means the vector  concatenated with the vector .\nWhat is the dimension of Percy\u2019s representation? Under what assumptions\nabout the original features is this a reasonable choice?", "Another cool idea is to use the training data itself to construct a feature space. The\nidea works as follows. For any particular point  in the input space \n, we can\n\u223c1000\nExample\n(x1, \u2026 , x4)", "(x1, \u2026 , x4)\n\u03d5(x) = PolyBasis((x1, x2), 3)\u2322PolyBasis((x3, x4), 3)\na\u2322b\na\nb\n5.2.2 (Optional) Radial basis functions\np\nX\n construct a feature \n which takes any element \n and returns a scalar value", "that is related to how far  is from the  we started with.\nLet\u2019s start with the basic case, in which \n. Then we can define\nThis function is maximized when \n and decreases exponentially as  becomes", "more distant from .\nThe parameter  governs how quickly the feature value decays as we move away\nfrom the center point . For large values of , the \n values are nearly 0 almost", "everywhere except right near ; for small values of , the features have a high value\nover a larger part of the space.\nNow, given a dataset \n containing  points, we can make a feature transformation", "that maps points in our original space, \n, into points in a new space, \n. It is\ndefined as follows:\nSo, we represent a new datapoint  in terms of how far it is from each of the", "datapoints in our training set.\nThis idea can be generalized in several ways and is the fundamental concept\nunderlying kernel methods, that are not directly covered in this class but we", "recommend you read about some time. This idea of describing objects in terms of\ntheir similarity to a set of reference objects is very powerful and can be applied to\ncases where", "cases where \n is not a simple vector space, but where the inputs are graphs or\nstrings or other types of objects, as long as there is a distance metric defined on the\ninput space.", "input space.\n5.3 (Optional) Hand-constructing features for real\ndomains\nIn many machine-learning applications, we are given descriptions of the inputs", "with many different types of attributes, including numbers, words, and discrete\nfeatures. An important factor in the success of an ML application is the way that the", "features are chosen to be encoded by the human who is framing the learning\nproblem.\nGetting a good encoding of discrete features is particularly important. You want to", "create \u201copportunities\u201d for the ML system to find the underlying patterns. Although\nthere are machine-learning methods that have special mechanisms for handling", "discrete inputs, most of the methods we consider in this class will assume the input\nfp\nx \u2208X\nx\np\nX = Rd\nfp(x) = e\u2212\u03b2\u2225p\u2212x\u22252 .\np = x\nx\np\n\u03b2\np\n\u03b2\nfp\np\n\u03b2\nD\nn\n\u03d5\nRd\nRn", "fp\np\n\u03b2\nD\nn\n\u03d5\nRd\nRn\n\u03d5(x) = [fx(1)(x), fx(2)(x), \u2026 , fx(n)(x)]T .\nx\nX\n5.3.1 Discrete features\n vectors  are in \n. So, we have to figure out some reasonable strategies for turning", "discrete values into (vectors of) real numbers.\nWe\u2019ll start by listing some encoding strategies, and then work through some", "examples. Let\u2019s assume we have some feature in our raw data that can take on one\nof  discrete values.\nNumeric: Assign each of these values a number, say \n. We", ". We\nmight want to then do some further processing, as described in Section 1.3.3.\nThis is a sensible strategy only when the discrete values really do signify some", "sort of numeric quantity, so that these numerical values are meaningful.\nThermometer code: If your discrete values have a natural ordering, from", ", but not a natural mapping into real numbers, a good strategy is to use\na vector of length  binary variables, where we convert discrete input value\n into a vector in which the first  values are", "and the rest are \n.\nThis does not necessarily imply anything about the spacing or numerical\nquantities of the inputs, but does convey something about ordering.", "Factored code: If your discrete values can sensibly be decomposed into two\nparts (say the \u201cmaker\u201d and \u201cmodel\u201d of a car), then it\u2019s best to treat those as two", "separate features, and choose an appropriate encoding of each one from this\nlist.\nOne-hot code: If there is no obvious numeric, ordering, or factorial structure,", "then the best strategy is to use a vector of length , where we convert discrete\ninput value \n into a vector in which all values are \n, except for the \nth, which is \n.", "th, which is \n.\nBinary code: It might be tempting for the computer scientists among us to use\nsome binary code, which would let us represent  values using a vector of\nlength", "length \n. This is a bad idea! Decoding a binary code takes a lot of work, and\nby encoding your inputs this way, you\u2019d be forcing your system to learn the\ndecoding algorithm.", "decoding algorithm.\nAs an example, imagine that we want to encode blood types, that are drawn from\nthe set \n. There is no obvious linear", "numeric scaling or even ordering to this set. But there is a reasonable factoring, into\ntwo features: \n and \n. And, in fact, we can further reasonably\nfactor the first group into \n,", ", \n. So, here are two plausible\nencodings of the whole set:\nUse a 6-D vector, with two components of the vector each encoding the\ncorresponding factor using a one-hot encoding.", "Use a 3-D vector, with one dimension for each factor, encoding its presence as\n and absence as \n (this is sometimes better than \n). In this case, \nwould be \n and \n would be \n.\nx\nRd\nk", "would be \n.\nx\nRd\nk\n1.0/k, 2.0/k, \u2026 , 1.0\n1, \u2026 , k\nk\n0 < j \u2264k\nj\n1.0\n0.0\nk\n0 < j \u2264k\n0.0\nj\n1.0\nk\nlog k\n{A+, A\u2212, B+, B\u2212, AB+, AB\u2212, O+, O\u2212}\n{A, B, AB, O}\n{+, \u2212}\n{A, notA} {B, notB}\n1.0\n\u22121.0\n0.0\nAB+", "1.0\n\u22121.0\n0.0\nAB+\n[1.0, 1.0, 1.0]T\nO\u2212\n[\u22121.0, \u22121.0, \u22121.0]T\n \u2753 Study Question\nHow would you encode \n in both of these approaches?", "The problem of taking a text (such as a tweet or a product review, or even this\ndocument!) and encoding it as an input for a machine-learning algorithm is", "interesting and complicated. Much later in the class, we\u2019ll study sequential input\nmodels, where, rather than having to encode a text as a fixed-length feature vector,", "we feed it into a hypothesis word by word (or even character by character!).\nThere are some simple encodings that work well for basic applications. One of them", "is the bag of words (bow) model, which can be used to encode documents. The idea is\nto let  be the number of words in our vocabulary (either computed from the", "training set or some other body of text or dictionary). We will then make a binary\nvector (with values \n and \n) of length , where element  has value \n if word \noccurs in the document, and", "otherwise.\nIf some feature is already encoded as a numeric value (heart rate, stock price,\ndistance, etc.) then we should generally keep it as a numeric value. An exception", "might be a situation in which we know there are natural \u201cbreakpoints\u201d in the\nsemantics: for example, encoding someone\u2019s age in the US, we might make an", "explicit distinction between under and over 18 (or 21), depending on what kind of\nthing we are trying to predict. It might make sense to divide into discrete bins", "(possibly spacing them closer together for the very young) and to use a one-hot\nencoding for some sorts of medical situations in which we don\u2019t expect a linear (or", "even monotonic) relationship between age and some physiological features.\n\u2753 Study Question\nConsider using a polynomial basis of order  as a feature transformation  on", "our data. Would increasing  tend to increase or decrease structural error? What\nabout estimation error?\nA+\n5.3.2 Text\nd\n1.0\n0.0\nd\nj\n1.0\nj\n0.0\n5.3.3 Numeric values\nk\n\u03d5\nk", "k\n\u03d5\nk\n This page contains all content from the legacy PDF notes; neural networks chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "You\u2019ve probably been hearing a lot about \u201cneural networks.\u201d Now that we have\nseveral useful machine-learning concepts (hypothesis classes, classification,", "regression, gradient descent, regularization, etc.), we are well equipped to\nunderstand neural networks in detail.", "This is, in some sense, the \u201cthird wave\u201d of neural nets. The basic idea is founded on\nthe 1943 model of neurons of McCulloch and Pitts and the learning ideas of Hebb.", "There was a great deal of excitement, but not a lot of practical success: there were\ngood training methods (e.g., perceptron) for linear functions, and interesting", "examples of non-linear functions, but no good way to train non-linear functions\nfrom data. Interest died out for a while, but was re-kindled in the 1980s when", "several people came up with a way to train neural networks with \u201cback-\npropagation,\u201d which is a particular style of implementing gradient descent, that we\nwill study here.", "will study here.\nAs with many good ideas in science, the basic idea for how to train non-linear\nneural networks with gradient descent was independently developed by more than\none researcher.", "one researcher.\nBy the mid-90s, the enthusiasm waned again, because although we could train non-\nlinear networks, the training tended to be slow and was plagued by a problem of", "getting stuck in local optima. Support vector machines (SVMs) that use\nregularization of high-dimensional hypotheses by seeking to maximize the margin,", "alongside kernel methods that provide an efficient and beautiful way of using\nfeature transformations to non-linearly transform data into a higher-dimensional", "space, provided reliable learning methods with guaranteed convergence and no\nlocal optima.\nHowever, during the SVM enthusiasm, several groups kept working on neural", "networks, and their work, in combination with an increase in available data and\ncomputation, has made neural networks rise again. They have become much more", "reliable and capable, and are now the method of choice in many applications. There\nare many, many variations of neural networks, which we can\u2019t even begin to survey.", "We will study the core \u201cfeed-forward\u201d networks with \u201cback-propagation\u201d training,\nand then, in later chapters, address some of the major advances beyond this core.", "We can view neural networks from several different perspectives:\n6  Neural Networks\nNote\nThe number of neural network\nvariants increases daily, as may be\nseen on arxiv.org .\n\uf229\n6  Neural Networks", "6  Neural Networks\n\uf4616  Neural Networks\n\uf52a\n View 1: An application of stochastic gradient descent for classification and\nregression with a potentially very rich hypothesis class.", "View 2: A brain-inspired network of neuron-like computing elements that learn\ndistributed representations.\nView 3: A method for building applications that make predictions based on huge", "amounts of data in very complex domains.\nWe will mostly take view 1, with the understanding that the techniques we develop", "will enable the applications in view 3. View 2 was a major motivation for the early\ndevelopment of neural networks, but the techniques we will study do not seem to", "actually account for the biological learning processes in brains.\n6.1 Basic element\nThe basic element of a neural network is a \u201cneuron,\u201d pictured schematically below.", "We will also sometimes refer to a neuron as a \u201cunit\u201d or \u201cnode.\u201d\n\ue050\nx1\n.. .\nxm\nf(\u00b7)\na\nw1\nwm\nw0\nz\ninput\npre-activation\noutput\nactivation function", "activation function\nIt is a (generally non-linear) function of an input vector \n to a single output\nvalue \n.\nIt is parameterized by a vector of weights \n and an offset or\nthreshold \n.", "threshold \n.\nWe also specify an activation function \n. In general, this is chosen to be a\nnon-linear function, which means the neuron is non-linear. In the case that the", "activation function is the identity (\n) or another linear function, then the\nneuron is a linear function of ). The activation can theoretically be any function,", "though we will only be able to work with it if it is differentiable.\nThe function represented by the neuron is expressed as:\nx \u2208Rm\na \u2208R\n(w1, \u2026 , wm) \u2208Rm\nw0 \u2208R\nf : R \u2192R\nf(x) = x\nx\na = f(z) = f ((\nm\n\u2211", "a = f(z) = f ((\nm\n\u2211\nj=1\nxjwj) + w0) = f(wTx + w0) .\nSome prominent researchers are, in\nfact, working hard to find\nanalogues of these methods in the\nbrain.\nSorry for changing our notation", "here. We were using  as the\ndimension of the input, but we are\ntrying to be consistent here with\nmany other accounts of neural\nnetworks. It is impossible to be\nconsistent with all of them though", "\u2014there are many different ways of\ntelling this story.\nd\nThis should remind you of our \nand \n for linear models.\n\u03b8\n\u03b80\n\uf229\n6  Neural Networks", "6  Neural Networks\n Before thinking about a whole network, we can consider how to train a single unit.\nGiven a loss function \n and a dataset \n,", "and a dataset \n,\nwe can do (stochastic) gradient descent, adjusting the weights \n to minimize\nwhere \n is the output of our single-unit neural net for a given input.", "We have already studied two special cases of the neuron: linear logistic classifiers\n(LLCs) with NLL loss and regressors with quadratic loss! The activation function for\nthe LLC is", "the LLC is \n and for linear regression it is simply \n.\n\u2753 Study Question\nJust for a single neuron, imagine for some reason, that we decide to use\nactivation function \n and loss function", "and loss function\n. Derive a gradient descent update for \nand \n.\n6.2 Networks\nNow, we\u2019ll put multiple neurons together into a network. A neural network in\ngeneral takes in an input", "and generates an output \n. It is constructed\nout of multiple neurons; the inputs of each neuron might be elements of  and/or", "outputs of other neurons. The outputs of the neural network are generated by \noutput units.\nIn this chapter, we will only consider feed-forward networks. In a feed-forward", "network, you can think of the network as defining a function-call graph that is\nacyclic: that is, the input to a neuron can never depend on that neuron\u2019s output.", "Data flows one way, from the inputs to the outputs, and the function computed by\nthe network is just a composition of the functions computed by the individual\nneurons.", "neurons.\nAlthough the graph structure of a feed-forward neural network can really be\nanything (as long as it satisfies the feed-forward constraint), for simplicity in", "software and analysis, we usually organize them into layers. A layer is a group of\nneurons that are essentially \u201cin parallel\u201d: their inputs are the outputs of neurons in", "the previous layer, and their outputs are the inputs to the neurons in the next layer.\nWe\u2019ll start by describing a single layer, and then go on to the case of multiple layers.\nL(guess, actual)", "L(guess, actual)\n{(x(1), y(1)), \u2026 , (x(n), y(n))}\nw, w0\nJ(w, w0) = \u2211\ni\nL (NN(x(i); w, w0), y(i)) ,\nNN\nf(x) = \u03c3(x)\nf(x) = x\nf(z) = ez\nL(guess, actual) = (guess \u2212actual)2\nw\nw0\nx \u2208Rm\na \u2208Rn\nx\nn", "w0\nx \u2208Rm\na \u2208Rn\nx\nn\n6.2.1 Single layer\n\uf229\n6  Neural Networks\n A layer is a set of units that, as we have just described, are not connected to each", "other. The layer is called fully connected if, as in the diagram below, all of the inputs\n(i.e., \n in this case) are connected to every unit in the layer. A layer has\ninput", "input \n and output (also known as activation) \n.\n\ue050\n\ue050\n\ue050\n.. .\n\ue050\nx1\nx2\n.. .\nxm\nf\nf\nf\n.. .\nf\na1\na2\na3\n.. .\nan\nW, W0\nSince each unit has a vector of weights and a single offset, we can think of the", "weights of the whole layer as a matrix, \n, and the collection of all the offsets as a\nvector \n. If we have \n inputs,  units, and  outputs, then\n is an \n matrix,\n is an \n column vector,", "column vector,\n, the input, is an \n column vector,\n, the pre-activation, is an \n column vector,\n, the activation, is an \n column vector,\nand the output vector is", "The activation function  is applied element-wise to the pre-activation values .\nA single neural network generally combines multiple layers, most typically by", "feeding the outputs of one layer into the inputs of another layer.\nx1, x2, \u2026 xm\nx \u2208Rm\na \u2208Rn\nW\nW0\nm\nn\nn\nW\nm \u00d7 n\nW0\nn \u00d7 1\nX\nm \u00d7 1\nZ = W TX + W0\nn \u00d7 1\nA\nn \u00d7 1\nA = f(Z) = f(W TX + W0) .\nf\nZ", "f\nZ\n6.2.2 Many layers\n\uf229\n6  Neural Networks\n We have to start by establishing some nomenclature. We will use  to name a layer,\nand let \n be the number of inputs to the layer and", "be the number of outputs\nfrom the layer. Then, \n and \n are of shape \n and \n, respectively.\nNote that the input to layer  is the output from layer \n, so we have \n,\nand as a result \n is of shape", "is of shape \n, or equivalently \n. Let \n be the\nactivation function of layer . Then, the pre-activation outputs are the \n vector\nand the activation outputs are simply the \n vector", "vector\nHere\u2019s a diagram of a many-layered network, with two blocks for each layer, one\nrepresenting the linear part of the operation and one representing the non-linear", "activation function. We will use this structural decomposition to organize our\nalgorithmic thinking and implementation.\nW 1\nW 1\n0\nf 1\nW 2\nW 2\n0\nf 2\n\u00b7 \u00b7 \u00b7\nW L\nW L\n0\nf L\nX = A0\nZ1\nA1\nZ2\nA2\nAL\u22121\nZL\nAL", "A1\nZ2\nA2\nAL\u22121\nZL\nAL\nlay er 1\nlay er 2\nlay er L\n6.3 Choices of activation function\nThere are many possible choices for the activation function. We will start by", "thinking about whether it\u2019s really necessary to have an  at all.\nWhat happens if we let  be the identity? Then, in a network with  layers (we\u2019ll\nleave out", "leave out \n for simplicity, but keeping it wouldn\u2019t change the form of this\nargument),\nSo, multiplying out the weight matrices, we find that\nwhich is a linear function of", "! Having all those layers did not change the\nrepresentational capacity of the network: the non-linearity of the activation function\nis crucial.\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that any function representable by any number of linear\nlayers (where  is the identity function) can be represented by a single layer.\nl\nml\nnl\nW l\nW l\n0\nml \u00d7 nl", "W l\nW l\n0\nml \u00d7 nl\nnl \u00d7 1\nl\nl \u22121\nml = nl\u22121\nAl\u22121\nml \u00d7 1\nnl\u22121 \u00d7 1\nf l\nl\nnl \u00d7 1\nZ l = W lTAl\u22121 + W l\n0\nnl \u00d7 1\nAl = f l(Z l) .\nf\nf\nL\nW0\nAL = W LTAL\u22121 = W LTW L\u22121T \u22efW 1TX .\nAL = W totalX ,\nX\nf", "AL = W totalX ,\nX\nf\nIt is technically possible to have\ndifferent activation functions\nwithin the same layer, but, again,\nfor convenience in specification\nand implementation, we generally", "have the same activation function\nwithin a layer.\n\uf229\n6  Neural Networks\n Now that we are convinced we need a non-linear activation, let\u2019s examine a few", "common choices. These are shown mathematically below, followed by plots of these\nfunctions.\nStep function:\nRectified linear unit (ReLU):", "Sigmoid function: Also known as a logistic function. This can sometimes be\ninterpreted as probability, because for any value of  the output is in \n:\nHyperbolic tangent: Always in the range \n:", ":\nSoftmax function: Takes a whole vector \n and generates as output a vector\n with the property that \n, which means we can interpret it as\na probability distribution over  items:\n\u22122\n\u22121\n1\n2\n\u22120.5\n0.5\n1", "\u22121\n1\n2\n\u22120.5\n0.5\n1\n1.5\nz\nstep(z)\n\u22122\n\u22121\n1\n2\n\u22120.5\n0.5\n1\n1.5\nz\nReLU(z)\n\u22124\n\u22122\n2\n4\n\u22121\n\u22120.5\n0.5\n1\nz\n\u03c3(z)\n\u22124\n\u22122\n2\n4\n\u22121\n\u22120.5\n0.5\n1\nz\ntanh(z)\nstep(z) = {0\nif z < 0\n1\notherwise\nReLU(z) = {\n= max(0, z)\n0", "= max(0, z)\n0\nif z < 0\nz\notherwise\nz\n(0, 1)\n\u03c3(z) =\n1\n1 + e\u2212z\n(\u22121, 1)\ntanh(z) = ez \u2212e\u2212z\nez + e\u2212z\nZ \u2208Rn\nA \u2208(0, 1)n\n\u2211n\ni=1 Ai = 1\nn\nsoftmax(z) =\n\u23a1\n\u23a2\n\u23a3\nexp(z1)/ \u2211i exp(zi)\n\u22ee\nexp(zn)/ \u2211i exp(zi)\n\u23a4\n\u23a5\n\u23a6\n\uf229", "\u23a4\n\u23a5\n\u23a6\n\uf229\n6  Neural Networks\n The original idea for neural networks involved using the step function as an\nactivation, but because the derivative of the step function is zero everywhere except", "at the discontinuity (and there it is undefined), gradient-descent methods won\u2019t be\nuseful in finding a good setting of the weights, and so we won\u2019t consider the step", "function further. Step functions have been replaced, in a sense, by the sigmoid,\nReLU, and tanh activation functions.\n\u2753 Study Question", "\u2753 Study Question\nConsider sigmoid, ReLU, and tanh activations. Which one is most like a step\nfunction? Is there an additional parameter you could add to a sigmoid that", "would make it be more like a step function?\n\u2753 Study Question\nWhat is the derivative of the ReLU function? Are there some values of the input\nfor which the derivative vanishes?", "ReLUs are especially common in internal (\u201chidden\u201d) layers, sigmoid activations are\ncommon for the output for binary classification, and softmax activations are", "common for the output for multi-class classification (see Section 4.3.3 for an\nexplanation).\n6.4 Loss functions and activation functions\nAt layer", "At layer \n which is the output layer, we need to specify a loss function, and\npossibly an activation function as well. Different loss functions make different", "assumptions about the range of values they will get as input and, as we have seen,\ndifferent activation functions will produce output values in different ranges. When", "you are designing a neural network, it\u2019s important to make these things fit together\nwell. In particular, we will think about matching loss functions with the activation\nfunction in the last layer,", ". Here is a table of loss functions and activations that\nmake sense for them:\nLoss\ntask\nsquared\nlinear\nregression\nnll\nsigmoid\nbinary classification\nnllm\nsoftmax\nmulti-class classification", "We explored squared loss in Chapter 2 and (nll and nllm) in Chapter 4.\nL,\nf L\nf L\n\uf229\n6  Neural Networks\n 6.5 Error back-propagation", "We will train neural networks using gradient descent methods. It\u2019s possible to use\nbatch gradient descent, in which we sum up the gradient over all the points (as in", "Section 3.2 of Chapter 3) or stochastic gradient descent (SGD), in which we take a\nsmall step with respect to the gradient considering a single point at a time (as in\nSection 3.4 of Chapter 3).", "Our notation is going to get pretty hairy pretty quickly. To keep it as simple as we\ncan, we\u2019ll focus on computing the contribution of one data point \n to the gradient", "to the gradient\nof the loss with respect to the weights, for SGD; you can simply sum up these\ngradients over all the data points if you wish to do batch descent.", "So, to do SGD for a training example \n, we need to compute\n, where \n represents all weights \n in all the layers\n. This seems terrifying, but is actually quite easy to do using the chain\nrule.", "rule.\nRemember that we are always computing the gradient of the loss function with\nrespect to the weights for a particular value of \n. That tells us how much we want", "to change the weights, in order to reduce the loss incurred on this particular\ntraining example.\nTo get some intuition for how these derivations work, we\u2019ll first suppose everything", "in our neural network is one-dimensional. In particular, we\u2019ll assume there are\n inputs and \n outputs at every layer. So layer  looks like:\nIn the equation above, we\u2019re using the lowercase letters", "to\nemphasize that all of these quantities are scalars just for the moment. We\u2019ll look at\nthe more general matrix case below.\nTo use SGD, then, we want to compute \n and", "and\n for each layer  and each data point \n. Below we\u2019ll write\n\u201closs\u201d as an abbreviation for \n. Then our first quantity of interest is\n. The chain rule gives us the following.", "First, let\u2019s look at the case \n:\nx(i)\n(x, y)\n\u2207WL(NN(x; W), y)\nW\nW l, W l\n0\nl = (1, \u2026 , L)\n(x, y)\n6.5.1 First, suppose everything is one-dimensional\nml = 1\nnl = 1\nl\nal = f l(zl),\nzl = wlal\u22121 + wl\n0.", "zl = wlal\u22121 + wl\n0.\nal, zl, wl, al\u22121, wl\n0\n\u2202L(NN(x; W), y)/\u2202wl\n\u2202L(NN(x; W), y)/\u2202wl\n0\nl\n(x, y)\nL(NN(x; W), y)\n\u2202loss/\u2202wl\nl = L\n\u2202loss\n\u2202wL = \u2202loss\n\u2202aL \u22c5\u2202aL\n\u2202zL \u22c5\u2202zL\n\u2202wL\n= \u2202loss\n\u2202aL \u22c5(f L)\u2032(zL) \u22c5aL\u22121.", "Remember the chain rule! If\n and \n, so that\n, then\na = f(b)\nb = g(c)\na = f(g(c))\nda\ndc = da\ndb \u22c5db\ndc\n= f \u2032(b)g\u2032(c)\n= f \u2032(g(c))g\u2032(c)\nCheck your understanding: why\ndo we need exactly these quantities", "for SGD?\n\uf229\n6  Neural Networks\n Now we can look at the case of general :\nNote that every multiplication above is scalar multiplication because every term in", "every product above is a scalar. And though we solved for all the other terms in the\nproduct, we haven\u2019t solved for \n because the derivative will depend on", "which loss function you choose. Once you choose a loss function though, you\nshould be able to compute this derivative.\n\u2753 Study Question\nSuppose you choose squared loss. What is \n?\n\u2753 Study Question", "?\n\u2753 Study Question\nCheck the derivations above yourself. You should use the chain rule and also\nsolve for the individual derivatives that arise in the chain rule.\n\u2753 Study Question", "\u2753 Study Question\nCheck that the final layer (\n) case is a special case of the general layer  case\nabove.\n\u2753 Study Question\nDerive \n for yourself, for both the final layer (\n) and\ngeneral .", ") and\ngeneral .\n\u2753 Study Question\nDoes the \n case remind you of anything from earlier in this course?\n\u2753 Study Question\nWrite out the full SGD algorithm for this neural network.\nl\n\u2202loss\n\u2202wl\n= \u2202loss", "l\n\u2202loss\n\u2202wl\n= \u2202loss\n\u2202aL \u22c5\u2202aL\n\u2202zL \u22c5\n\u2202zL\n\u2202aL\u22121 \u22c5\u2202aL\u22121\n\u2202zL\u22121 \u22ef\u2202zl+1\n\u2202al\n\u22c5\u2202al\n\u2202zl \u22c5\u2202zl\n\u2202wl\n= \u2202loss\n\u2202aL \u22c5(f L)\u2032(zL) \u22c5wL \u22c5(f L\u22121)\u2032(zL\u22121) \u22ef\u22c5wl+1 \u22c5(f l)\u2032(zl) \u22c5al\u22121\n= \u2202loss\n\u2202zl\n\u22c5al\u22121.\n\u2202loss/\u2202aL\n\u2202loss/\u2202aL", "\u2202loss/\u2202aL\n\u2202loss/\u2202aL\nl = L\nl\n\u2202L(NN(x; W), y)/\u2202wl\n0\nl = L\nl\nL = 1\n\uf229\n6  Neural Networks\n It\u2019s pretty typical to run the chain rule from left to right like we did above. But, for", "where we\u2019re going next, it will be useful to notice that it\u2019s completely equivalent to\nwrite it in the other direction. So we can rewrite our result from above as follows:", "Next we\u2019re going to do everything that we did above, but this time we\u2019ll allow any\nnumber of inputs \n and outputs \n at every layer. First, we\u2019ll tell you the results", "that correspond to our derivations above. Then we\u2019ll talk about why they make\nsense. And finally we\u2019ll derive them carefully.", "OK, let\u2019s start with the results! Again, below we\u2019ll be using \u201closs\u201d as an\nabbreviation for \n. Then,\nwhere\nor equivalently,", "or equivalently,\nFirst, compare each equation to its one-dimensional counterpart, and make sure\nyou see the similarities. That is, compare the general weight derivatives in", "Equation 6.4 to the one-dimensional case in Equation 6.1. Compare the intermediate\nderivative of loss with respect to the pre-activations \n in Equation 6.5 to the one-", "dimensional case in Equation 6.2. And finally compare the version where we\u2019ve\nsubstituted in some of the derivatives in Equation 6.6 to Equation 6.3. Hopefully\n\u2202loss\n\u2202wl\n= al\u22121 \u22c5\u2202loss\n\u2202zl\n(6.1)\n\u2202loss", "\u2202zl\n(6.1)\n\u2202loss\n\u2202zl\n= \u2202al\n\u2202zl \u22c5\u2202zl+1\n\u2202al\n\u22ef\u2202aL\u22121\n\u2202zL\u22121 \u22c5\n\u2202zL\n\u2202aL\u22121 \u22c5\u2202aL\n\u2202zL \u22c5\u2202loss\n\u2202aL\n(6.2)\n= \u2202al\n\u2202zl \u22c5wl+1 \u22ef\u2202aL\u22121\n\u2202zL\u22121 \u22c5wL \u22c5\u2202aL\n\u2202zL \u22c5\u2202loss\n\u2202aL .\n(6.3)\n6.5.2 The general case\nml\nnl\nL(NN(x; W), y)", "nl\nL(NN(x; W), y)\n\u2202loss\n\u2202W l\nml\u00d7nl\n= Al\u22121\nml\u00d71\n( \u2202loss\n\u2202Z l )\nT\n1\u00d7nl\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n(6.4)\n\u2202loss\n\u2202Z l = \u2202Al\n\u2202Z l \u22c5\u2202Z l+1\n\u2202Al\n\u22ef\u22c5\u2202AL\u22121\n\u2202Z L\u22121 \u22c5\n\u2202Z L\n\u2202AL\u22121 \u22c5\u2202AL\n\u2202Z L \u22c5\u2202loss\n\u2202AL\n(6.5)", "\u2202AL\n(6.5)\n\u2202loss\n\u2202Z l = \u2202Al\n\u2202Z l \u22c5W l+1 \u22ef\u22c5\u2202AL\u22121\n\u2202Z L\u22121 \u22c5W L \u22c5\u2202AL\n\u2202Z L \u22c5\u2202loss\n\u2202AL .\n(6.6)\nZ l\nEven though we have reordered\nthe gradients for notational\nconvenience, when actually", "computing the product in\nEquation 6.3, it is computationally\nmuch cheaper to run the\nmultiplications from right-to-left\nthan from left-to-right. Convince\nyourself of this, by reasoning", "through the cost of the matrix\nmultiplications in each case.\nThere are lots of weights in a\nneural network, which means we\nneed to compute a lot of gradients.\nLuckily, as we can see, the", "gradients associated with weights\nin earlier layers depend on the\nsame terms as the gradients\nassociated with weights in later\nlayers. This means we can reuse\nterms and save ourselves some", "computation!\n\uf229\n6  Neural Networks\n you see how the forms are very analogous. But in the matrix case, we now have to\nbe careful about the matrix dimensions. We\u2019ll check these matrix dimensions below.", "Let\u2019s start by talking through each of the terms in the matrix version of these\nequations. Recall that loss is a scalar, and \n is a matrix of size \n. You can", ". You can\nread about the conventions in the course for derivatives starting in this chapter in\nAppendix A. By these conventions (not the only possible conventions!), we have\nthat", "that \n will be a matrix of size \n whose \n entry is the scalar\n. In some sense, we\u2019re just doing a bunch of traditional scalar", "derivatives, and the matrix notation lets us write them all simultaneously and\nsuccinctly. In particular, for SGD, we need to find the derivative of the loss with", "respect to every scalar component of the weights because these are our model\u2019s\nparameters and therefore are the things we want to update in SGD.\nThe next quantity we see in Equation 6.4 is", ", which we recall has size \n (or\nequivalently \n since it represents the outputs of the \n layer). Finally, we\nsee \n. Again, loss is a scalar, and \n is a \n vector. So by the", "vector. So by the\nconventions in Appendix A, we have that \n has size \n. The transpose\nthen has size \n. Now you should be able to check that the dimensions all make", "sense in Equation 6.4; in particular, you can check that inner dimensions agree in\nthe matrix multiplication and that, after the multiplication, we should be left with", "something that has the dimensions on the lefthand side.\nNow let\u2019s look at Equation 6.6. We\u2019re computing \n so that we can use it in", "Equation 6.4. The weights are familiar. The one part that remains is terms of the\nform \n. Checking out Appendix A, we see that this term should be a matrix\nof size \n since \n and \n both have size", "both have size \n. The \n entry of this matrix\nis \n. This scalar derivative is something that you can compute when you", "know your activation function. If you\u2019re not using a softmax activation function, \ntypically is a function only of \n, which means that \n should equal 0\nwhenever \n, and that \n.\n\u2753 Study Question", ".\n\u2753 Study Question\nCompute the dimensions of every term in Equation 6.5 and Equation 6.6 using\nAppendix A. After you\u2019ve done that, check that all the matrix multiplications", "work; that is, check that the inner dimensions agree and that the lefthand side\nand righthand side of these equations have the same dimensions.\n\u2753 Study Question", "\u2753 Study Question\nIf I use the identity activation function, what is \n for any ? What is the\nfull matrix \n?\nW l\nml \u00d7 nl\n\u2202loss/\u2202W l\nml \u00d7 nl\n(i, j)\n\u2202loss/\u2202W l\ni,j\nAl\u22121\nml \u00d7 1\nnl\u22121 \u00d7 1\nl \u22121\n\u2202loss/\u2202Z l", "l \u22121\n\u2202loss/\u2202Z l\nZ l\nnl \u00d7 1\n\u2202loss/\u2202Z l\nnl \u00d7 1\n1 \u00d7 nl\n\u2202loss/\u2202Z l\n\u2202Al/\u2202Z l\nnl \u00d7 nl\nAl\nZ l\nnl \u00d7 1\n(i, j)\n\u2202Al\nj/\u2202Z l\ni\nAl\nj\nZ l\nj\n\u2202Al\nj/\u2202Z l\ni\ni \u2260j\n\u2202Al\nj/\u2202Z l\nj = (f l)\u2032(Z l\nj)\n\u2202Al\nj/\u2202Z l\nj\nj\n\u2202Al/\u2202Z l\n\uf229", "j\nj\n\u2202Al/\u2202Z l\n\uf229\n6  Neural Networks\n You can use everything above without deriving it yourself. But if you want to find\nthe gradients of loss with respect to", "(which we need for SGD!), then you\u2019ll want\nto know how to actually do these derivations. So next we\u2019ll work out the\nderivations.", "derivations.\nThe key trick is to just break every equation down into its scalar meaning. For\ninstance, the \n element of \n is \n. If you think about it for a", "moment (and it might help to go back to the one-dimensional case), the loss is a\nfunction of the elements of \n, and the elements of \n are a function of the \n.\nThere are \n elements of", "elements of \n, so we can use the chain rule to write\nTo figure this out, let\u2019s remember that \n. We can write one\nelement of the \n vector, then, as \n. It follows that\n will be zero except when", "(check you agree!). So we can rewrite\nEquation 6.7 as\nFinally, then, we match entries of the matrices on both sides of the equation above\nto recover Equation 6.4.\n\u2753 Study Question", "\u2753 Study Question\nCheck that Equation 6.8 and Equation 6.4 say the same thing.\n\u2753 Study Question\nConvince yourself that \n by comparing the entries of the\nmatrices on both sides on the equality sign.", "\u2753 Study Question\nConvince yourself that Equation 6.5 is true.\n\u2753 Study Question\nApply the same reasoning to find the gradients of \n with respect to \n.\n6.5.3 Derivations for the general case\nW l\n0", "W l\n0\n(i, j)\n\u2202loss/\u2202W l\n\u2202loss/\u2202W l\ni,j\nZ l\nZ l\nW l\ni,j\nnl\nZ l\n\u2202loss\n\u2202W l\ni,j\n=\nnl\n\u2211\nk=1\n\u2202loss\n\u2202Z l\nk\n\u2202Z l\nk\n\u2202W l\ni,j\n.\n(6.7)\nZ l = (W l)\u22a4Al\u22121 + W l\n0\nZ l\nZ l\nb = \u2211ml\na=1 W l\na,bAl\u22121\na\n+ (W l\n0)b\n\u2202Z l", "a\n+ (W l\n0)b\n\u2202Z l\nk/\u2202W l\ni,j\nk = j\n\u2202loss\n\u2202W l\ni,j\n= \u2202loss\n\u2202Z l\nj\n\u2202Z l\nj\n\u2202W l\ni,j\n= \u2202loss\n\u2202Z l\nj\nAl\u22121\ni\n.\n(6.8)\n\u2202Z l/\u2202Al\u22121 = W l\nloss\nW l\n0\n\uf229\n6  Neural Networks", "6  Neural Networks\n This general process of computing the gradients of the loss with respect to the\nweights is called error back-propagation.", "The idea is that we first do a forward pass to compute all the  and  values at all the\nlayers, and finally the actual loss. Then, we can work backward and compute the", "gradient of the loss with respect to the weights in each layer, starting at layer  and\ngoing back to layer 1.\nW 1\nW 1\n0\nf 1\nW 2\nW 2\n0\nf 2\n\u00b7 \u00b7 \u00b7\nW L\nW L\n0\nf L\nLoss\nX = A0\nZ1\nA1\nZ2\nA2\nAL\u22121\nZL\nAL\ny", "Z2\nA2\nAL\u22121\nZL\nAL\ny\n\u2202loss\n\u2202AL\n\u2202loss\n\u2202ZL\n\u2202loss\n\u2202AL\u22121\n\u2202loss\n\u2202A2\n\u2202loss\n\u2202Z2\n\u2202loss\n\u2202A1\n\u2202loss\n\u2202Z1\nIf we view our neural network as a sequential composition of modules (in our work", "so far, it has been an alternation between a linear transformation with a weight\nmatrix, and a component-wise application of a non-linear activation function), then", "we can define a simple API for a module that will let us compute the forward and\nbackward passes, as well as do the necessary weight updates for gradient descent.", "Each module has to provide the following \u201cmethods.\u201d We are already using letters\n with particular meanings, so here we will use  as the vector input to the\nmodule and  as the vector output:\nforward:", "forward: \nbackward: \nweight grad: \n only needed for modules that have weights\nIn homework we will ask you to implement these modules for neural network", "components, and then use them to construct a network and train it as described in\nthe next section.\n6.6 Training\nHere we go! Here\u2019s how to do stochastic gradient descent training on a feed-", "forward neural network. After this pseudo-code, we motivate the choice of\ninitialization in lines 2 and 3. The actual computation of the gradient values (e.g.,", ") is not directly defined in this code, because we want to make the\nstructure of the computation clear.\n\u2753 Study Question\n6.5.4 Reflecting on backpropagation\na\nz\nL\na, x, y, z\nu\nv\nu \u2192v", "a, x, y, z\nu\nv\nu \u2192v\nu, v, \u2202L/\u2202v \u2192\u2202L/\u2202u\nu, \u2202L/\u2202v \u2192\u2202L/\u2202W\nW\n\u2202loss/\u2202AL\nNotice that the backward pass does\nnot output \n, even though the\nforward pass maps from  to . In\nthe backward pass, we are always", "directly computing and ``passing\naround\u2019\u2019 gradients of the loss.\n\u2202v/\u2202u\nu\nv\n\uf229\n6  Neural Networks\n What is \n?\n\u2753 Study Question\nWhich terms in the code below depend on \n?\nprocedure SGD-NEURAL-NET(\n)", ")\nfor \n to  do\nend for\nfor \n to  do\n//forward pass to compute \nfor \n to  do\nend for\nfor \n down to  do//error back-propagation\n//SGD update\nend for\nend for\nend procedure\nInitializing", "Initializing \n is important; if you do it badly there is a good chance the neural\nnetwork training won\u2019t work well. First, it is important to initialize the weights to", "random values. We want different parts of the network to tend to \u201caddress\u201d\ndifferent aspects of the problem; if they all start at the same weights, the symmetry", "will often keep the values from moving in useful directions. Second, many of our\nactivation functions have (near) zero slope when the pre-activation  values have", "large magnitude, so we generally want to keep the initial weights small so we will\nbe in a situation where the gradients are non-zero, so that gradient descent will", "have some useful signal about which way to go.\nOne good general-purpose strategy is to choose each weight at random from a\nGaussian (normal) distribution with mean 0 and standard deviation \n where", "where\n is the number of inputs to the unit.\n\u2753 Study Question\n\u2202Z l/\u2202W l\nf L\n1:\nDn, T, L, (m1, \u2026 , mL), (f 1, \u2026 , f L), Loss\n2:\nl \u21901\nL\n3:\nW l\nij \u223cGaussian(0, 1/ml)\n4:\nW l\n0j \u223cGaussian(0, 1)\n5:\n6:\nt \u21901", "5:\n6:\nt \u21901\nT\n7:\ni \u2190random sample from {1, \u2026 , n}\n8:\nA0 \u2190x(i)\nAL\n9:\nl \u21901\nL\n10:\nZ l \u2190W lTAl\u22121 + W l\n0\n11:\nAl \u2190f l(Z l)\n12:\n13:\nloss \u2190Loss(AL,  y(i))\n14:\nl \u2190L\n1\n15:\n\u2202loss\n\u2202Al\n\u2190{\n\u2202Z l+1\n\u2202Al\n\u22c5\n\u2202loss", "\u2202Z l+1\n\u2202Al\n\u22c5\n\u2202loss\n\u2202Z l+1\nif l < L,\n\u2202loss\n\u2202AL\notherwise\n16:\n\u2202loss\n\u2202Z l \u2190\u2202Al\n\u2202Z l \u22c5\u2202loss\n\u2202Al\n17:\n\u2202loss\n\u2202W l \u2190Al\u22121 ( \u2202loss\n\u2202Z l )\n\u22a4\n18:\n\u2202loss\n\u2202W l\n0\n\u2190\u2202loss\n\u2202Z l\n19:\nW l \u2190W l \u2212\u03b7(t) \u2202loss\n\u2202W l\n20:\nW l", "\u2202W l\n20:\nW l\n0 \u2190W l\n0 \u2212\u03b7(t) \u2202loss\n\u2202W l\n0\n21:\n22:\n23:\nW\nz\n(1/m)\nm\n\uf229\n6  Neural Networks\n If the input  to this unit is a vector of 1\u2019s, what would the expected pre-", "activation  value be with these initial weights?\nWe write this choice (where \n means \u201cis drawn randomly from the distribution\u201d) as", "It will often turn out (especially for fancier activations and loss functions) that\ncomputing \n is easier than computing \n and \n So, we may instead ask for", "an implementation of a loss function to provide a backward method that computes\n directly.\n6.7 Optimizing neural network parameters", "Because neural networks are just parametric functions, we can optimize loss with\nrespect to the parameters using standard gradient-descent software, but we can take", "advantage of the structure of the loss function and the hypothesis class to improve\noptimization. As we have seen, the modular function-composition structure of a", "neural network hypothesis makes it easy to organize the computation of the\ngradient. As we have also seen earlier, the structure of the loss function as a sum", "over terms, one per training data point, allows us to consider stochastic gradient\nmethods. In this section we\u2019ll consider some alternative strategies for organizing", "training, and also for making it easier to handle the step-size parameter.\nAssume that we have an objective of the form\nwhere  is the function computed by a neural network, and \n stands for all the", "stands for all the\nweight matrices and vectors in the network.\nRecall that, when we perform batch (or the vanilla) gradient descent, we use the\nupdate rule\nwhich is equivalent to", "So, we sum up the gradient of loss at each training point, with respect to \n, and\nthen take a step in the negative direction of the gradient.\nx\nz\n\u223c\nW l\nij \u223cGaussian (0,\n1\nml ).\n\u2202loss\n\u2202Z L\n\u2202loss\n\u2202AL", "\u2202Z L\n\u2202loss\n\u2202AL\n\u2202AL\n\u2202Z L .\n\u2202loss/\u2202Z L\n6.7.1 Batches\nJ(W) = 1\nn\nn\n\u2211\ni=1\nL(h(x(i); W), y(i)) ,\nh\nW\nWt = Wt\u22121 \u2212\u03b7\u2207WJ(Wt\u22121) ,\nWt = Wt\u22121 \u2212\u03b7\nn\n\u2211\ni=1\n\u2207WL(h(x(i); Wt\u22121), y(i)) .\nW\n\uf229\n6  Neural Networks", "6  Neural Networks\n In stochastic gradient descent, we repeatedly pick a point \n at random from\nthe data set, and execute a weight update on that point alone:", "As long as we pick points uniformly at random from the data set, and decrease  at\nan appropriate rate, we are guaranteed, with high probability, to converge to at least\na local optimum.", "a local optimum.\nThese two methods have offsetting virtues. The batch method takes steps in the\nexact gradient direction but requires a lot of computation before even a single step", "can be taken, especially if the data set is large. The stochastic method begins moving\nright away, and can sometimes make very good progress before looking at even a", "substantial fraction of the whole data set, but if there is a lot of variability in the\ndata, it might require a very small  to effectively average over the individual steps", "moving in \u201ccompeting\u201d directions.\nAn effective strategy is to \u201caverage\u201d between batch and stochastic gradient descent\nby using mini-batches. For a mini-batch of size \n, we select", ", we select \n distinct data points\nuniformly at random from the data set and do the update based just on their\ncontributions to the gradient", "Most neural network software packages are set up to do mini-batches.\n\u2753 Study Question\nFor what value of \n is mini-batch gradient descent equivalent to stochastic", "gradient descent? To batch gradient descent?\nPicking \n unique data points at random from a large data-set is potentially", "computationally difficult. An alternative strategy, if you have an efficient procedure\nfor randomly shuffling the data set (or randomly shuffling a list of indices into the", "data set) is to operate in a loop, roughly as follows:\nprocedure Mini-Batch-SGD(NN, data, K)\nwhile not done do\nRandom-Shuffle(data)\nfor \n to \n do\nBatch-Gradient-Update(NN, data[(i-1)K : iK])\nend for", "end for\nend while\nend procedure\n(x(i), y(i))\nWt = Wt\u22121 \u2212\u03b7\u2207WL(h(x(i); Wt\u22121), y(i)) .\n\u03b7\n\u03b7\nK\nK\nWt = Wt\u22121 \u2212\u03b7\nK\nK\n\u2211\ni=1\n\u2207WL(h(x(i); Wt\u22121), y(i)) .\nK\nK\n1:\n2:\nn \u2190length(data)\n3:\n4:\n5:\ni \u21901\n\u2308n\nK \u2309\n6:\n7:\n8:", "\u2308n\nK \u2309\n6:\n7:\n8:\n9:\nIn line 4 of the algorithm above, \nis known as the ceiling function; it\nreturns the smallest integer greater\nthan or equal to its input. E.g.,\n and \n.\n\u2308\u22c5\u2309\n\u23082.5\u2309= 3\n\u23083\u2309= 3\n\uf229", "\u23082.5\u2309= 3\n\u23083\u2309= 3\n\uf229\n6  Neural Networks\n Picking a value for  is difficult and time-consuming. If it\u2019s too small, then\nconvergence is slow and if it\u2019s too large, then we risk divergence or slow", "convergence due to oscillation. This problem is even more pronounced in stochastic\nor mini-batch mode, because we know we need to decrease the step size for the\nformal guarantees to hold.", "It\u2019s also true that, within a single neural network, we may well want to have\ndifferent step sizes. As our networks become deep (with increasing numbers of", "layers) we can find that magnitude of the gradient of the loss with respect the\nweights in the last layer, \n, may be substantially different from the", "gradient of the loss with respect to the weights in the first layer \n. If you\nlook carefully at Equation 6.6, you can see that the output gradient is multiplied by", "all the weight matrices of the network and is \u201cfed back\u201d through all the derivatives\nof all the activation functions. This can lead to a problem of exploding or vanishing", "gradients, in which the back-propagated gradient is much too big or small to be\nused in an update rule with the same step size.", "So, we can consider having an independent step-size parameter for each weight, and\nupdating it based on a local view of how the gradient updates have been going.", "Some common strategies for this include momentum (\u201caveraging\u201d recent gradient\nupdates), Adadelta (take larger steps in parts of the space where \n is nearly flat),", "is nearly flat),\nand Adam (which combines these two previous ideas). Details of these approaches\nare described in Section B.1.\n6.8 Regularization", "6.8 Regularization\nSo far, we have only considered optimizing loss on the training data as our objective\nfor neural network training. But, as we have discussed before, there is a risk of", "overfitting if we do this. The pragmatic fact is that, in current deep neural networks,\nwhich tend to be very large and to be trained with a large amount of data,", "overfitting is not a huge problem. This runs counter to our current theoretical\nunderstanding and the study of this question is a hot area of research. Nonetheless,", "there are several strategies for regularizing a neural network, and they can\nsometimes be important.\nOne group of strategies can, interestingly, be shown to have similar effects to each", "other: early stopping, weight decay, and adding noise to the training data.\nEarly stopping is the easiest to implement and is in fairly common use. The idea is", "to train on your training set, but at every epoch (a pass through the whole training\n6.7.2 Adaptive step-size\n\u03b7\n\u2202loss/\u2202WL\n\u2202loss/\u2202W1\nJ(W)\n6.8.1 Methods related to ridge regression", "This section is very strongly\ninfluenced by Sebastian Ruder\u2019s\nexcellent blog posts on the topic:\n{ruder.io/optimizing-gradient-\ndescent}\nResult is due to Bishop, described\nin his textbook and here.", "Warning: If you use your\nvalidation set in this way \u2013 i.e., to\n\uf229\n6  Neural Networks\n set, or possibly more frequently), evaluate the loss of the current \n on a validation", "on a validation\nset. It will generally be the case that the loss on the training set goes down fairly\nconsistently with each iteration, the loss on the validation set will initially decrease,", "but then begin to increase again. Once you see that the validation loss is\nsystematically increasing, you can stop training and return the weights that had the\nlowest validation error.", "Another common strategy is to simply penalize the norm of all the weights, as we\ndid in ridge regression. This method is known as weight decay, because when we\ntake the gradient of the objective", "we end up with an update of the form\nThis rule has the form of first \u201cdecaying\u201d \n by a factor of \n and then\ntaking a gradient step.\nFinally, the same effect can be achieved by perturbing the", "values of the training\ndata by adding a small amount of zero-mean normally distributed noise before each\ngradient computation. It makes intuitive sense that it would be more difficult for", "the network to overfit to particular training data if they are changed slightly on\neach training step.\nDropout is a regularization method that was designed to work with deep neural", "networks. The idea behind it is, rather than perturbing the data every time we train,\nwe\u2019ll perturb the network! We\u2019ll do this by randomly, on each training step,", "selecting a set of units in each layer and prohibiting them from participating. Thus,\nall of the units will have to take a kind of \u201ccollective\u201d responsibility for getting the", "answer right, and will not be able to rely on any small subset of the weights to do\nall the necessary computation. This tends also to make the network more robust to\ndata perturbations.", "data perturbations.\nDuring the training phase, for each training example, for each unit, randomly with\nprobability  temporarily set \n. There will be no contribution to the output and", "no gradient update for the associated unit.\nWhen we are done training and want to use the network to make predictions, we\nmultiply all weights by  to achieve the same average activation levels.\nW", "W\nJ(W) =\nn\n\u2211\ni=1\nL(NN(x(i)), y(i); W) + \u03bb\u2225W\u22252\nWt = Wt\u22121 \u2212\u03b7 ((\u2207WL(NN(x(i)), y(i); Wt\u22121)) + 2\u03bbWt\u22121)\n= Wt\u22121(1 \u22122\u03bb\u03b7) \u2212\u03b7 (\u2207WL(NN(x(i)), y(i); Wt\u22121)) .\nWt\u22121\n(1 \u22122\u03bb\u03b7)\nx(i)\n6.8.2 Dropout\np\na\u2113\nj = 0\np", "p\na\u2113\nj = 0\np\nset the number of epochs (or any\nother hyperparameter associated\nwith your learning algorithm) \u2013\nthen error on the validation set no\nlonger provides a \u201cpure\u201d estimate", "of error on the test set (i.e.,\ngeneralization error). This is\nbecause information about the\nvalidation set has \u201cleaked\u201d into the\ndesign of your algorithm. See also\nthe discussion on Validation and", "Cross-Validation in Chapter 2.\n\uf229\n6  Neural Networks\n Implementing dropout is easy! In the forward pass during training, we let\nwhere  denotes component-wise product and", "is a vector of \u2019s and \u2019s drawn\nrandomly with probability . The backwards pass depends on \n, so we do not need\nto make any further changes to the algorithm.\nIt is common to set  to", ", but this is something one might experiment with to get\ngood results on your problem and data.\nAnother strategy that seems to help with regularization and robustness in training", "is batch normalization.\nIt was originally developed to address a problem of covariate shift: that is, if you\nconsider the second layer of a two-layer neural network, the distribution of its input", "values is changing over time as the first layer\u2019s weights change. Learning when the\ninput distribution is changing is extra difficult: you have to change your weights to", "improve your predictions, but also just to compensate for a change in your inputs\n(imagine, for instance, that the magnitude of the inputs to your layer is increasing", "over time\u2014then your weights will have to decrease, just to keep your predictions\nthe same).\nSo, when training with mini-batches, the idea is to standardize the input values for", "each mini-batch, just in the way that we did it in Section 5.3.3 of Chapter 5,\nsubtracting off the mean and dividing by the standard deviation of each input", "dimension. This means that the scale of the inputs to each layer remains the same,\nno matter how the weights in previous layers change. However, this somewhat", "complicates matters, because the computation of the weight updates will need to\ntake into account that we are performing this transformation. In the modular view,", "batch normalization can be seen as a module that is applied to \n, interposed after\nthe product with \n and before input to \n.", ".\nAlthough batch-norm was originally justified based on the problem of covariate\nshift, it\u2019s not clear that that is actually why it seems to improve performance. Batch", "normalization can also end up having a regularizing effect for similar reasons that\nadding noise and dropout do: each mini-batch of data ends up being mildly", "perturbed, which prevents the network from exploiting very particular values of\nthe data points. For those interested, the equations for batch normalization,", "including a derivation of the forward pass and backward pass, are described in\nSection B.2.\na\u2113= f(z\u2113) \u2217d\u2113\n\u2217\nd\u2113\n0\n1\np\na\u2113\np\n0.5\n6.8.3 Batch normalization\nzl\nW l\nf l\nFor more details see", "arxiv.org/abs/1502.03167.\nWe follow here the suggestion from\nthe original paper of applying\nbatch normalization before the\nactivation function. Since then it\nhas been shown that, in some", "cases, applying it after works a bit\nbetter. But there aren\u2019t any definite\nfindings on which works better\nand when.\n\uf229\n6  Neural Networks\n \uf229\n6  Neural Networks", "6  Neural Networks\n This page contains all content from the legacy PDF notes; convolutional neural networks\nchapter.", "chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.\nSo far, we have studied what are called fully connected neural networks, in which all", "of the units at one layer are connected to all of the units in the next layer. This is a\ngood arrangement when we don\u2019t know anything about what kind of mapping", "from inputs to outputs we will be asking the network to learn to approximate. But if\nwe do know something about our problem, it is better to build it into the structure", "of our neural network. Doing so can save computation time and significantly\ndiminish the amount of training data required to arrive at a solution that\ngeneralizes robustly.", "One very important application domain of neural networks, where the methods\nhave achieved an enormous amount of success in recent years, is signal processing.", "Signals might be spatial (in two-dimensional camera images or three-dimensional\ndepth or CAT scans) or temporal (speech or music). If we know that we are", "addressing a signal-processing problem, we can take advantage of invariant\nproperties of that problem. In this chapter, we will focus on two-dimensional spatial", "problems (images) but use one-dimensional ones as a simple example. In a later\nchapter, we will address temporal problems.", "Imagine that you are given the problem of designing and training a neural network\nthat takes an image as input, and outputs a classification, which is positive if the", "image contains a cat and negative if it does not. An image is described as a two-\ndimensional array of pixels, each of which may be represented by three integer", "values, encoding intensity levels in red, green, and blue color channels.\nThere are two important pieces of prior structural knowledge we can bring to bear\non this problem:", "on this problem:\nSpatial locality: The set of pixels we will have to take into consideration to find\na cat will be near one another in the image.", "Translation invariance: The pattern of pixels that characterizes a cat is the\nsame no matter where in the image the cat occurs.", "We will design neural network structures that take advantage of these properties.\n7.1 Filters\n7  Convolutional Neural Networks\nNote\nA pixel is a \u201cpicture element.\u201d\nSo, for example, we won\u2019t have to", "consider some combination of\npixels in the four corners of the\nimage, in order to see if they\nencode cat-ness.\nCats don\u2019t look different if they\u2019re\non the left or the right side of the\nimage.", "image.\n\uf4617  Convolutional Neural Networks\n\uf52a\n We begin by discussing image filters.\nAn image filter is a function that takes in a local spatial neighborhood of pixel", "values and detects the presence of some pattern in that data.\nLet\u2019s consider a very simple case to start, in which we have a 1-dimensional binary", "\u201cimage\u201d and a filter  of size two. The filter is a vector of two numbers, which we\nwill move along the image, taking the dot product between the filter values and the", "image values at each step, and aggregating the outputs to produce a new image.\nLet \n be the original image, of size ; then pixel  of the the output image is\nspecified by", "specified by\nTo ensure that the output image is also of dimension , we will generally \u201cpad\u201d the\ninput image with 0 values if we need to access pixels that are beyond the bounds of", "the input image. This process of applying the filter to the image to create a new\nimage is called \u201cconvolution.\u201d\nIf you are already familiar with what a convolution is, you might notice that this", "definition corresponds to what is often called a correlation and not to a convolution.\nIndeed, correlation and convolution refer to different operations in signal", "processing. However, in the neural networks literature, most libraries implement\nthe correlation (as described in this chapter) but call it convolution. The distinction", "is not significant; in principle, if convolution is required to solve the problem, the\nnetwork could learn the necessary weights. For a discussion of the difference", "between convolution and correlation and the conventions used in the literature you\ncan read Section 9.1 in this excellent book: Deep Learning.\nHere is a concrete example. Let the filter", ". Then given the image in\nthe first line below, we can convolve it with filter \n to obtain the second image.\nYou can think of this filter as a detector for \u201cleft edges\u201d in the original image\u2014to see", "this, look at the places where there is a  in the output image, and see what pattern\nexists at that position in the input image. Another interesting filter is", ". The third image (the last line below) shows the result of\nconvolving the first image with \n, where we see that the output pixel \ncorresponds to when the center of \n is aligned at input pixel .", "\u2753 Study Question\nConvince yourself that filter \n can be understood as a detector for isolated\npositive pixels in the binary image.\nF\nX\nd\ni\nYi = F \u22c5(Xi\u22121, Xi) .\nd\nF1 = (\u22121, +1)\nF1\n1\nF2 = (\u22121, +1, \u22121)", "1\nF2 = (\u22121, +1, \u22121)\nF2\ni\nF2\ni\nF2\nUnfortunately in\nAI/ML/CS/Math, the word\n``filter\u2019\u2019 gets used in many ways: in\naddition to the one we describe\nhere, it can describe a temporal", "process (in fact, our moving\naverages are a kind of filter) and\neven a somewhat esoteric algebraic\nstructure.\nAnd filters are also sometimes\ncalled convolutional kernels.\n 0\n0\n1\n1\n1\n0\n1\n0\n0\n0\nImage:", "1\n0\n1\n0\n0\n0\nImage:\nF1:\n-1\n+1\n0\n0\n1\n0\n0\n-1\n1\n-1\n0\n0\nAfter con v olution (with F1):\n0\n-1\n0\n-1\n0\n-2\n1\n-1\n0\n0\nAfter con v olution (with F2):\nF2\n-1\n+1\n-1", "F2\n-1\n+1\n-1\nTwo-dimensional versions of filters like these are thought to be found in the visual\ncortex of all mammalian brains. Similar patterns arise from statistical analysis of", "natural images. Computer vision people used to spend a lot of time hand-designing\nfilter banks. A filter bank is a set of sets of filters, arranged as shown in the diagram\nbelow.\nImage", "below.\nImage\nAll of the filters in the first group are applied to the original image; if there are \nsuch filters, then the result is  new images, which are called channels. Now imagine", "stacking all these new images up so that we have a cube of data, indexed by the\noriginal row and column indices of the image, as well as by the channel. The next", "set of filters in the filter bank will generally be three-dimensional: each one will be\napplied to a sub-range of the row and column indices of the image and to all of the\nchannels.", "channels.\nThese 3D chunks of data are called tensors. The algebra of tensors is fun, and a lot\nlike matrix algebra, but we won\u2019t go into it in any detail.", "Here is a more complex example of two-dimensional filtering. We have two \nfilters in the first layer, \n and \n. You can think of each one as \u201clooking\u201d for three\npixels in a row, \n vertically and", "vertically and \n horizontally. Assuming our input image is\n, then the result of filtering with these two filters is an \n tensor. Now\nk\nk\n3 \u00d7 3\nf1\nf2\nf1\nf2\nn \u00d7 n\nn \u00d7 n \u00d7 2", "f2\nn \u00d7 n\nn \u00d7 n \u00d7 2\nThere are now many useful neural-\nnetwork software packages, such\nas TensorFlow and PyTorch that\nmake operations on tensors easy.", "we apply a tensor filter (hard to draw!) that \u201clooks for\u201d a combination of two\nhorizontal and two vertical bars (now represented by individual pixels in the two", "channels), resulting in a single final \n image.\nWhen we have a color image as input, we treat it as having three channels, and\nhence as an \n tensor.\nf2\nf1\ntensor\n\ufb01lter", "f2\nf1\ntensor\n\ufb01lter\nWe are going to design neural networks that have this structure. Each \u201cbank\u201d of the\nfilter bank will correspond to a neural-network layer. The numbers in the individual", "filters will be the \u201cweights\u201d (plus a single additive bias or offset value for each\nfilter) of the network, that we will train using gradient descent. What makes this", "interesting and powerful (and somewhat confusing at first) is that the same weights\nare used many many times in the computation of each layer. This weight sharing", "means that we can express a transformation on a large image with relatively few\nparameters; it also means we\u2019ll have to take care in figuring out exactly how to train\nit!", "it!\nWe will define a filter layer  formally with:\nnumber of filters \n;\nsize of one filter is \n plus  bias value (for this one filter);\nstride", "stride \n is the spacing at which we apply the filter to the image; in all of our\nexamples so far, we have used a stride of 1, but if we were to \u201cskip\u201d and apply", "the filter only at odd-numbered indices of the image, then it would have a\nstride of two (and produce a resulting image of half the size);\ninput tensor size \npadding:", "padding: \n is how many extra pixels \u2013 typically with value 0 \u2013 we add around\nthe edges of the input. For an input of size \n, our new\neffective input size with padding becomes\n.\nn \u00d7 n\nn \u00d7 n \u00d7 3\nl\nml", "n \u00d7 n \u00d7 3\nl\nml\nkl \u00d7 kl \u00d7 ml\u22121\n1\nsl\nnl\u22121 \u00d7 nl\u22121 \u00d7 ml\u22121\npl\nnl\u22121 \u00d7 nl\u22121 \u00d7 ml\u22121\n(nl\u22121 + 2 \u22c5pl) \u00d7 (nl\u22121 + 2 \u22c5pl) \u00d7 ml\u22121\nFor simplicity, we are assuming\nthat all images and filters are", "square (having the same number of\nrows and columns). That is in no\nway necessary, but is usually fine\nand definitely simplifies our\nnotation.\n This layer will produce an output tensor of size", ", where\n. The weights are the values defining the filter:\nthere will be \n different \n tensors of weight values; plus each filter", "may have a bias term, which means there is one more weight value per filter. A filter\nwith a bias operates just like the filter examples above, except we add the bias to the", "output. For instance, if we incorporated a bias term of 0.5 into the filter \n above,\nthe output would be \n instead of\n.", "instead of\n.\nThis may seem complicated, but we get a rich class of mappings that exploit image\nstructure and have many fewer weights than a fully connected layer would.\n\u2753 Study Question", "\u2753 Study Question\nHow many weights are in a convolutional layer specified as above?\n\u2753 Study Question\nIf we used a fully-connected layer with the same size inputs and outputs, how", "many weights would it have?\n7.2 Max pooling\nIt is typical (both in engineering and in natrure) to structure filter banks into a", "pyramid, in which the image sizes get smaller in successive layers of processing. The\nidea is that we find local patterns, like bits of edges in the early layers, and then", "look for patterns in those patterns, etc. This means that, effectively, we are looking\nfor patterns in larger pieces of the image as we apply successive filters. Having a", "stride greater than one makes the images smaller, but does not necessarily\naggregate information over that spatial range.", "Another common layer type, which accomplishes this aggregation, is max pooling. A\nmax pooling layer operates like a filter, but has no weights. You can think of it as", "purely functional, like a ReLU in a fully connected network. It has a filter size, as in a\nfilter layer, but simply returns the maximum value in its field.", "Usually, we apply max pooling with the following traits:\n, so that the resulting image is smaller than the input image; and\n, so that the whole image is covered.", "As a result of applying a max pooling layer, we don\u2019t keep track of the precise\nlocation of a pattern. This helps our filters to learn to recognize patterns\nnl \u00d7 nl \u00d7 ml", "nl \u00d7 nl \u00d7 ml\nnl = \u2308(nl\u22121 + 2 \u22c5pl \u2212(kl \u22121))/sl\u2309\nml\nkl \u00d7 kl \u00d7 ml\u22121\nF2\n(\u22120.5, 0.5, \u22120.5, 0.5, \u22121.5, 1.5, \u22120.5, 0.5)\n(\u22121, 0, \u22121, 0, \u22122, 1, \u22121, 0)\nstride > 1\nk \u2265stride\nRecall that \n is the function; it", "returns the smallest integer greater\nthan or equal to its input. E.g.,\n and \n.\n\u2308\u22c5\u2309\n\u23082.5\u2309= 3\n\u23083\u2309= 3\nWe sometimes use the term\nreceptive field or just field to mean\nthe area of an input image that a", "filter is being applied to.\n independent of their location.\nConsider a max pooling layer where both the strides and  are set to be 2. This\nwould map a \n image to a \n image. Note that max pooling", "layers do not have additional bias or offset values.\n\u2753 Study Question\nMaximilian Poole thinks it would be a good idea to add two max pooling layers", "of size , one right after the other, to their network. What single layer would be\nequivalent?\nOne potential concern about max-pooling layers is that they actually don\u2019t", "completely preserve translation invariance. If you do max-pooling with a stride\nother than 1 (or just pool over the whole image size), then shifting the pattern you", "are hoping to detect within the image by a small amount can change the output of\nthe max-pooling layer substantially, just because there are discontinuities induced", "by the way the max-pooling window matches up with its input image. Here is an\ninteresting paper that illustrates this phenomenon clearly and suggests that one", "should first do max-pooling with a stride of 1, then do \u201cdownsampling\u201d by\naveraging over a window of outputs.\n7.3 Typical architecture\nHere is the form of a typical convolutional network:", "At the end of each filter layer, we typically apply a ReLU activation function. There\nmay be multiple filter plus ReLU layers. Then we have a max pooling layer. Then", "we have some more filter + ReLU layers. Then we have max pooling again. Once\nthe output is down to a relatively small size, there is typically a last fully-connected", "layer, leading into an activation function such as softmax that produces the final\noutput. The exact design of these structures is an art\u2014there is not currently any", "clear theoretical (or even systematic empirical) understanding of how these various\ndesign choices affect overall performance of the network.", "The critical point for us is that this is all just a big neural network, which takes an\ninput and computes an output. The mapping is a differentiable function of the", "weights, which means we can adjust the weights to decrease the loss by performing\nk\n64 \u00d7 64 \u00d7 3\n32 \u00d7 32 \u00d7 3\nk\nThe \u201cdepth\u201d dimension in the\nlayers shown as cuboids\ncorresponds to the number of", "channels in the output tensor.\n(Figure source: Mathworks)\nWell, technically the derivative\ndoes not exist at every point, both\nbecause of the ReLU and the max", "gradient descent, and we can compute the relevant gradients using back-\npropagation!\n7.4 Backpropagation in a simple CNN", "Let\u2019s work through a very simple example of how back-propagation can work on a\nconvolutional network. The architecture is shown below. Assume we have a one-\ndimensional single-channel image", "of size \n, and a single filter \n of size\n (where we omit the filter bias) for the first convolutional operation\ndenoted \u201cconv\u201d in the figure below. Then we pass the intermediate result", "through a ReLU layer to obtain the activation \n, and finally through a fully-\nconnected layer with weights \n, denoted \u201cfc\u201d below, with no additional\nactivation function, resulting in the output \n.", ".\nX = A0\n0\n0\npad with 0\u2019s \n(to get output \nof same shap e)\nW 1\nZ1\nA1\nZ2 = A2\nW 2\nconv\nReLU\nfc\nFor simplicity assume  is odd, let the input image \n, and assume we are", ", and assume we are\nusing squared loss. Then we can describe the forward pass as follows:\n\u2753 Study Question\nAssuming a stride of \n for a filter of size , how much padding do we need to", "add to the top and bottom of the image? We see one zero at the top and bottom\nin the figure just above; what filter size is implicitly being shown in the figure?", "(Recall the padding is for the sake of getting an output the same size as the\ninput.)\nX\nn \u00d7 1 \u00d7 1\nW 1\nk \u00d7 1 \u00d7 1\nZ 1\nA1\nW 2\nA2\nk\nX = A0\nZ 1\ni = W 1TA0\n[i\u2212\u230ak/2\u230b:i+\u230ak/2\u230b]\nA1 = ReLU(Z 1)", "A1 = ReLU(Z 1)\nA2 = Z 2 = W 2TA1\nLsquare(A2, y) = (A2 \u2212y)2\n1,\nk\n7.4.1 Weight update\npooling operations, but we ignore\nthat fact.\n How do we update the weights in filter \n?\n is the \n matrix such that", "matrix such that \n. So, for\nexample, if \n, which corresponds to column 10 in this matrix, which\nillustrates the dependence of pixel 10 of the output image on the weights, and\nif", "if \n, then the elements in column 10 will be \n.\n is the \n diagonal matrix such that\n, an \n vector\nMultiplying these components yields the desired gradient, of shape \n.", ".\nOne last point is how to handle back-propagation through a max-pooling operation.\nLet\u2019s study this via a simple example. Imagine\nwhere \n and", "where \n and \n are each computed by some network. Consider doing back-\npropagation through the maximum. First consider the case where \n. Then the", ". Then the\nerror value at  is propagated back entirely to the network computing the value \n.\nThe weights in the network computing \n will ultimately be adjusted, and the\nnetwork computing", "network computing \n will be untouched.\n\u2753 Study Question\nWhat is \n ?\nW 1\n\u2202loss\n\u2202W 1 = \u2202Z 1\n\u2202W 1\n\u2202A1\n\u2202Z 1\n\u2202loss\n\u2202A1\n\u2202Z 1/\u2202W 1\nk \u00d7 n\n\u2202Z 1\ni /\u2202W 1\nj = Xi\u2212\u230ak/2\u230b+j\u22121\ni = 10\nk = 5\nX8, X9, X10, X11, X12", "\u2202A1/\u2202Z 1\nn \u00d7 n\n\u2202A1\ni /\u2202Z 1\ni = {1\nif Z 1\ni > 0\n0\notherwise\n\u2202loss/\u2202A1 = (\u2202loss/\u2202A2)(\u2202A2/\u2202A1) = 2(A2 \u2212y)W 2\nn \u00d7 1\nk \u00d7 1\n7.4.2 Max pooling\ny = max(a1, a2) ,\na1\na2\na1 > a2\ny\na1\na1\na2\n\u2207(x,y) max(x, y)", "a2\n\u2207(x,y) max(x, y)\n This page contains all content from the legacy PDF notes; autoencoders chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "In previous chapters, we have largely focused on classification and regression\nproblems, where we use supervised learning with training samples that have both", "features/inputs and corresponding outputs or labels, to learn hypotheses or models\nthat can then be used to predict labels for new data.", "In contrast to supervised learning paradigm, we can also have an unsupervised\nlearning setting, where we only have features but no corresponding outputs or", "labels for our dataset. On natural question aries then: if there are no labels, what are\nwe learning?\nOne canonical example of unsupervised learning is clustering, which is discussed in", "Section 12.3. In clustering, the goal is to develop algorithms that can reason about\n\u201csimilarity\u201d among data points\u2019s features, and group the data points into clusters.", "Autoencoders are another family of unsupervised learning algorithms, in this case\nseeking to obtain insights about our data by learning compressed versions of the", "original data, or, in other words, by finding a good lower-dimensional feature\nrepresentations of the same data set. Such insights might help us to discover and", "characterize underlying factors of variation in data, which can aid in scientific\ndiscovery; to compress data for efficient storage or communication; or to pre-", "process our data prior to supervised learning, perhaps to reduce the amount of data\nthat is needed to learn a good classifier or regressor.\n8.1 Autoencoder structure\nAssume that we have input data", ", where \n. We seek to\nlearn an autoencoder that will output a new dataset \n, where\n with \n. We can think about \n as the new representation of data point", ". For example, in Figure 8.1 we show the learned representations of a dataset of\nMNIST digits with \n. We see, after inspecting the individual data points, that", "unsupervised learning has found a compressed (or latent) representation where\nimages of the same digit are close to each other, potentially greatly aiding", "subsequent clustering or classification tasks.\n8  Representation Learning (Autoencoders)\nNote\nD = {x(1), \u2026 , x(n)}\nx(i) \u2208Rd\nDout = {a(1), \u2026 , a(n)}\na(i) \u2208Rk\nk < d\na(i)\nx(i)\nk = 2", "a(i)\nx(i)\nk = 2\n\uf4618  Representation Learning (Autoencoders)\n\uf52a\n Formally, an autoencoder consists of two functions, a vector-valued encoder", "that deterministically maps the data to the representation space \n, and a decoder \n that maps the representation space back into the\noriginal data space.", "In general, the encoder and decoder functions might be any functions appropriate\nto the domain. Here, we are particularly interested in neural network embodiments", "of encoders and decoders. The basic architecture of one such autoencoder,\nconsisting of only a single layer neural network in each of the encoder and decoder,", "is shown in Figure 8.2; note that bias terms \n and \n into the summation nodes\nexist, but are omitted for clarity in the figure. In this example, the original -\ndimensional input is compressed into", "dimensions via the encoder\n with \n and \n, and where the\nnon-linearity \n is applied to each dimension of the vector. To recover (an\napproximation to) the original instance, we then apply the decoder", ", where \n denotes a different non-linearity\n(activation function). In general, both the decoder and the encoder could involve", "multiple layers, as opposed to the single layer shown here. Learning seeks\nparameters \n and \n such that the reconstructed instances,\n, are close to the original input \n.\n8.2 Autoencoder Learning", "g : Rd \u2192Rk\na \u2208Rk\nh : Rk \u2192Rd\nW 1\n0\nW 2\n0\nd\nk = 3\ng(x; W 1, W 1\n0 ) = f1(W 1Tx + W 1\n0 )\nW 1 \u2208Rd\u00d7k\nW 1\n0 \u2208Rk\nf1\nh(a; W 2, W 2\n0 ) = f2(W 2Ta + W 2\n0 )\nf2\nW 1, W 1\n0\nW 2, W 2\n0\nh(g(x(i); W 1, W 1", "h(g(x(i); W 1, W 1\n0 ); W 2, W 2\n0 )\nx(i)\nFigure 8.1: Compression of digits\ndataset into two dimensions. The\ninput \n, an image of a\nhandwritten digit, is shown at the\nnew low-dimensional", "new low-dimensional\nrepresentation \n.\nx(i)\n(a1, a2)\nFigure 8.2: Autoencoder structure,\nshowing the encoder (left half,\nlight green), and the decoder (right\nhalf, light blue), encoding inputs", "to the representation , and\ndecoding the representation to\nproduce , the reconstruction. In\nthis specific example, the\nrepresentation (\n, \n, \n) only has\nthree dimensions.\nx\na\n~x\na1 a2 a3", "x\na\n~x\na1 a2 a3\n We learn the weights in an autoencoder using the same tools that we previously\nused for supervised learning, namely (stochastic) gradient descent of a multi-layer", "neural network to minimize a loss function. All that remains is to specify the loss\nfunction \n, which tells us how to measure the discrepancy between the\nreconstruction \n and the original input . For", "example, for continuous-valued  it might make sense to use squared loss, i.e.,\n.\nLearning then seeks to optimize the parameters of  and  so as to minimize the", "reconstruction error, measured according to this loss function:\n8.3 Evaluating an autoencoder\nWhat makes a good learned representation in an autoencoder? Notice that, without", "further constraints, it is always possible to perfectly reconstruct the input. For\nexample, we could let \n and  and  be the identity functions. In this case, we", "would not obtain any compression of the data.\nTo learn something useful, we must create a bottleneck by making  to be smaller\n(often much smaller) than . This forces the learning algorithm to seek", "transformations that describe the original data using as simple a description as\npossible. Thinking back to the digits dataset, for example, an example of a", "compressed representation might be the digit label (i.e., 0\u20139), rotation, and stroke\nthickness. Of course, there is no guarantee that the learning algorithm will discover", "precisely this representation. After learning, we can inspect the learned\nrepresentations, such as by artificially increasing or decreasing one of the\ndimensions (e.g.,", "dimensions (e.g., \n) and seeing how it affects the output \n, to try to better\nunderstand what it has learned.\nAs with clustering, autoencoders can be a preliminary step toward building other", "models, such as a regressor or classifier. For example, once a good encoder has been\nlearned, the decoder might be replaced with another neural network that is then", "trained with supervised learning (perhaps using a smaller dataset that does include\nlabels).\n8.4 Linear encoders and decoders\nWe close by mentioning that even linear encoders and decoders can be very", "powerful. In this case, rather than minimizing the above objective with gradient\ndescent, a technique called principal components analysis (PCA) can be used to obtain\nL(~x, x)\n~x = h(g(x; W 1, W 1", "0 ); W 2, W 2\n0 )\nx\nx\nLSE(~x, x) = \u2211d\nj=1(xj \u2212~xj)2\nh\ng\nmin\nW 1,W 1\n0 ,W 2,W 2\n0\nn\n\u2211\ni=1\nLSE (h(g(x(i); W 1, W 1\n0 ); W 2, W 2\n0 ), x(i))\nk = d\nh\ng\nk\nd\na1\nh(a)\nAlternatively, you could think of", "this as multi-task learning, where\nthe goal is to predict each\ndimension of . One can mix-and-\nmatch loss functions as appropriate\nfor each dimension\u2019s data type.\nx", "x\n a closed-form solution to the optimization problem using a singular value\ndecomposition (SVD). Just as a multilayer neural network with nonlinear", "activations for regression (learned by gradient descent) can be thought of as a\nnonlinear generalization of a linear regressor (fit by matrix algebraic operations),", "the neural network based autoencoders discussed above (and learned with gradient\ndescent) can be thought of as a generalization of linear PCA (as solved with matrix\nalgebra by SVD).", "algebra by SVD).\n8.5 Advanced encoders and decoders\nAdvanced neural networks built on encoder-decoder architectures have become", "increasingly powerful. One prominent example is generative networks, designed to\ncreate new outputs that resemble\u2014but differ from\u2014existing training examples. A", "notable type, variational autoencoders, learns a compressed representation\ncapturing statistical properties (such as mean and variance) of training data. These", "latent representations can then be sampled to generate novel outputs using the\ndecoder.\nAnother influential encoder-decoder architecture is the Transformer, covered in", "Chapter 9. Transformers consist of multiple encoder and decoder layers combined\nwith self-attention mechanisms, which excel at predicting sequential data, such as", "words and sentences in natural language processing (NLP).\nCentral to autoencoders and Transformers is the idea of learning representations.", "Autoencoders compress data into efficient, informative representations, while NLP\nmodels encode language\u2014words, phrases, sentences\u2014into numerical forms. This", "numerical encoding leads us to the concept of vector embeddings.\n8.6 Embeddings\nIn NLP, words are represented as vectors, commonly known as word embeddings. A", "key property of good embeddings is that their numerical closeness mirrors semantic\nsimilarity. For instance, semantically related words such as \u201cdog\u201d and \u201ccat\u201d should", "have vectors close together, while unrelated words like \u201ccat\u201d and \u201ctable\u201d should be\nfarther apart.\nSimilarity between embeddings is frequently measured using the inner product:", "The inner product indicates how aligned two vectors are: highly positive values\nimply strong similarity, negative values indicate opposition, and values near zero", "suggest no similarity (up to a scaling factor related to the magnitude).\naTb = a \u22c5b\n A groundbreaking embedding method, word2vec (2012), significantly advanced NLP", "by producing embeddings where vector arithmetic corresponded to real-world\nsemantic relationships. For instance:\nSuch embeddings revealed meaningful semantic relationships like analogies across", "diverse vocabulary (e.g., uncle \u2013 man + woman \u2248 aunt).\nImportantly, embeddings don\u2019t need exact coordinates\u2014it\u2019s their relative", "positioning within the vector space that matters. Embeddings are considered\neffective if they facilitate downstream NLP tasks, such as predicting missing words,", "classifying texts, or language translation.\nFor example, effective embeddings allow models to accurately predict a missing\nword in a sentence:\nAfter the rain, the grass was ____.", "Or a model could be built that tries to correctly predict words in the middle of\nsentences:\nThe child fell __ __ during the long car ride", "This task exemplifies self-supervision, a training approach where models generate\nlabels directly from the data itself, eliminating the need for manual labeling.", "Training neural networks through self-supervision involves optimizing their ability\nto predict words accurately from large text corpora (e.g., Wikipedia). Through such", "optimization, embeddings capture subtle semantic and syntactic nuances, greatly\nenhancing NLP capabilities.\nThe idea of good embeddings will play a central role when we discuss attention", "mechanisms in Chapter 9, where embeddings dynamically adjust based on context\n(via the so-called attention mechanism), enabling a more nuanced understanding of\nlanguage.", "language.\nembeddingparis \u2212embeddingfrance + embeddingitaly \u2248embeddingrome\n We are actively overhauling the Transformers chapter from the legacy PDF notes to", "enhance clarity and presentation. Please feel free to raise issues or request more\nexplanation on specific topics.\nTransformers are a very recent family of architectures that were originally", "introduced in the field of natural language processing (NLP) in 2017, as an\napproach to process and understand human language. Since then, they have", "revolutionized not only NLP but also other domains such as image processing and\nmulti-modal generative AI. Their scalability and parallelizability have made them", "the backbone of large-scale foundation models, such as GPT, BERT, and Vision\nTransformers (ViT), powering many state-of-the-art applications.", "Like CNNs, transformers factorize signal processing into stages, each involving\nindependently and identically processed chunks. Transformers have many intricate", "components; however, we\u2019ll focus on their most crucial innovation: a new type of\nlayer called the attention layer. Attention layers enable transformers to effectively", "mix information across chunks, allowing the entire transformer pipeline to model\nlong-range dependencies among these chunks. To help make Transformers more", "digestible, in this chapter, we will first succinctly motivate and describe them in an\noverview Section 9.1. Then, we will dive into the details following the flow of data \u2013", "first describing how to represent inputs Section 9.2, and then describe the attention\nmechanism Section 9.3, and finally we then assemble all these ideas together to", "arrive at the full transformer architecture in Section 9.5.\n9.1 Transformers Overview\nTransformers are powerful neural architectures designed primarily for sequential", "data, such as text. At their core, transformers are typically auto-regressive, meaning\nthey generate sequences by predicting each token sequentially, conditioned on", "previously generated tokens. This auto-regressive property ensures that the\ntransformer model inherently captures temporal dependencies, making them", "especially suited for language modeling tasks like text generation and completion.\nSuppose our training data contains this sentence: \u201cTo date, the cleverest thinker was", "Issac.\u201d The transformer model will learn to predict the next token in the sequence,\ngiven the previous tokens. For example, when predicting the token \u201ccleverest,\u201d the", "model will condition its prediction on the tokens \u201cTo,\u201d \u201cdate,\u201d and \u201cthe.\u201d This\n9  Transformers\nCaution\nHuman language is inherently\nsequential in nature (e.g.,\ncharacters form words, words form", "sentences, and sentences form\nparagraphs and documents). Prior\nto the advent of the transformers\narchitecture, recurrent neural\nnetworks (RNNs) briefly\ndominated the field for their ability", "to process sequential information.\nHowever, RNNs, like many other\narchitectures, processed sequential\ninformation in an\niterative/sequential fashion,\nwhereby each item of a sequence", "was individually processed one\nafter another. Transformers offer\nmany advantages over RNNs,\nincluding their ability to process all\nitems in a sequence in a parallel\nfashion (as do CNNs).", "\uf4619  Transformers\n\uf52a\n process continues until the entire sequence is generated.\nThe animation above illustrates the auto-regressive nature of transformers.", "Below is another example. Suppose the sentence is the 2nd law of robotics: \u201cA robot\nmust obey the orders given it by human beings\u2026\u201d The training objective of a", "transformer would be to make each token\u2019s prediction, conditioning on previously\ngenerated tokens, forming a step-by-step probability distribution over the\nvocabulary.", "vocabulary.\nThe transformer architecture processes inputs by applying multiple identical\nbuilding blocks stacked in layers. Each block performs a transformation that", "progressively refines the internal representation of the data.\nSpecifically, each block consists of two primary sub-layers: an attention layer", "Section 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6.\nAttention layers mix information across different positions (or \"chunks\") in the", "sequence, allowing the model to effectively capture dependencies regardless of\ndistance. Meanwhile, the feed-forward network significantly enhances the", "expressiveness of these representations by applying non-linear transformations\nindependently to each position.\nA notable strength of transformers is their capacity for parallel processing.", "Transformers process entire sequences simultaneously rather than sequentially\ntoken-by-token. This parallelization significantly boosts computational efficiency", "and makes it feasible to train larger and deeper models.\nIn this overview, we emphasize the auto-regressive nature of transformers, their", "layered approach to transforming representations, the parallel processing\n advantage, and the critical role of the feed-forward layers in enhancing their\nexpressive power.", "expressive power.\nThere are additional essential components and enhancements\u2014such as causal\nattention mechanisms and positional encoding\u2014that further empower", "transformers. We\u2019ll explore these \"bells and whistles\" in greater depth in subsequent\ndiscussions.\n9.2 Embedding and Representations", "We start by describing how language is commonly represented, then we provide a\nbrief explanation of why it can be useful to predict subsequent items (e.g.,\nwords/tokens) in a sequence.", "As a reminder, two key components of any ML system are: (1) the representation of\nthe data; and (2) the actual modelling to perform some desired task. Computers, by", "default, have no natural way of representing human language. Modern computers\nare based on the Von Neumann architecture and are essentially very powerful", "calculators, with no natural understanding of what any particular string of\ncharacters means to us humans. Considering the rich complexities of language (e.g.,", "humor, sarcasm, social and cultural references and implications, slang, homonyms,\netc), you can imagine the innate difficulties of appropriately representing", "languages, along with the challenges for computers to then model and\n\u201cunderstand\u201d language.\nThe field of NLP aims to represent words with vectors of floating-point numbers", "(aka word embeddings) such that they capture semantic meaning. More precisely,\nthe degree to which any two words are related in the \u2018real-world\u2019 to us humans", "should be reflected by their corresponding vectors (in terms of their numeric\nvalues). So, words such as \u2018dog\u2019 and \u2018cat\u2019 should be represented by vectors that are", "more similar to one another than, say, \u2018cat\u2019 and \u2018table\u2019 are.\nTo measure how similar any two word embeddings are (in terms of their numeric", "values) it is common to use some similarity as the metric, e.g. the dot-product\nsimilarity we saw in Chapter 8.\nThus, one can imagine plotting every word embedding in -dimensional space and", "observing natural clusters to form, whereby similar words (e.g., synonyms) are\nlocated near each other. The problem of determining how to parse (aka tokenize)", "individual words is known as tokenization. This is an entire topic of its own, so we\nwill not dive into the full details here. However, the high-level idea of tokenization", "is straightforward: the individual inputs of data that are represented and processed\nby a model are referred to as tokens. And, instead of processing each word as a", "whole, words are typically split into smaller, meaningful pieces (akin to syllables).\nFor example, the word \u201cevaluation\u201d may be input into a model as 3 individual\nd\nHow can we define an optimal", "vocabulary of such tokens? How\nmany distinct tokens should we\nhave in our vocabulary? How\nshould we handle digits or other\npunctuation? How does this work\nfor non-English languages, in", "particular, script-based languages\nwhere word boundaries are less\nobvious (e.g., Chinese or\nJapanese)? All of these are open", "tokens (eval + ua + tion). Thus, when we refer to tokens, know that we\u2019re referring\nto these sub-word units. For any given application/model, all of the language data", "must be predefined by a finite vocabulary of valid tokens (typically on the order of\n40,000 distinct tokens).\n9.3 Query, Key, Value, and Attention Output", "Attention mechanisms efficiently process global information by selectively focusing\non the most relevant parts of the input. Given an input sentence, each token is", "processed sequentially to predict subsequent tokens. As more context (previous\ntokens) accumulates, this context ideally becomes increasingly beneficial\u2014provided", "the model can appropriately utilize it. Transformers employ a mechanism known as\nattention, which enables models to identify and prioritize contextually relevant\ntokens.", "tokens.\nFor example, consider the partial sentence: \u201cAnqi forgot ___\u201c. At this point, the\nmodel has processed tokens\u201dAnqi\u201d and \u201cforgot,\u201d and aims to predict the next", "token. Numerous valid completions exist, such as articles (\u201cthe,\u201d \u201can\u201d),\nprepositions (\u201cto,\u201d \u201cabout\u201d), or possessive pronouns (\u201cher,\u201d \u201chis,\u201d \u201ctheir\u201d). A well-", "trained model should assign higher probabilities to contextually relevant tokens,\nsuch as \u201cher,\u201d based on the feminine-associated name \u201cAnqi.\u201d Attention", "mechanisms guide the model to selectively focus on these relevant contextual cues\nusing query, key, and value vectors.", "Our goal is for each input token to learn how much attention it should give to every\nother token in the sequence. To achieve this, each token is assigned a unique query", "vector used to \u201cprobe\u201d or assess other tokens\u2014including itself\u2014to determine\nrelevance.\nA token\u2019s query vector \n is computed by multiplying the input token \n (a -", "(a -\ndimensional vector) by a learnable query weight matrix \n (of dimension \n,\n is a hyperparameter typically chosen such that \n):", "):\nThus, for a sequence of  tokens, we generate  distinct query vectors.\nTo complement query vectors, we introduce key vectors, which tokens use to", "\u201canswer\u201d queries about their relevance. Specifically, when evaluating token \n, its\nquery vector \n is compared to each token\u2019s key vector \n to determine the attention\nweight. Each key vector", "is computed similarly using a learnable key weight\nmatrix \n:\n9.3.1 Query Vectors\nqi\nxi\nd\nWq\nd \u00d7 dk\ndk\ndk < d\nqi = W T\nq xi\nn\nn\n9.3.2 Key Vectors\nx3\nq3\nkj\nki\nWk\nT\nNLP research problems receiving", "increased attention lately.\n The attention mechanism calculates similarity using the dot product, which\nefficiently measures vector similarity:\nThe vector", "The vector \n (softmax\u2019d attention scores) quantifies how much attention token \nshould pay to each token in the sequence, normalized so that elements sum to 1.\nNormalizing by", "Normalizing by \n prevents large dot-product magnitudes, stabilizing training.\nTo incorporate meaningful contributions from attended tokens, we use value\nvectors (", "vectors (\n), providing distinct representations for contribution to attention outputs.\nEach token\u2019s value vector is computed with another learnable matrix \n:", ":\nFinally, attention outputs are computed as weighted sums of value vectors, using\nthe softmax\u2019d attention scores:\nThis vector \n represents token \n\u2019s enriched embedding, incorporating context", "from across the sequence, weighted by learned attention.\n9.4 Self-attention Layer\nSelf-attention is an attention mechanism where the keys, values, and queries are all\ngenerated from the same input.", "At a very high level, typical transformer with self-attention layers maps\n. In particular, the transformer takes in  tokens, each having feature\ndimension", "dimension \n and through many layers of transformation (most important of which\nare self-attention layers); the transformer finally outputs a sequence of  tokens,\neach of which -dimensional still.", "With a self-attention layer, there can be multiple attention head. We start with\nunderstanding a single head.\nA single self-attention head is largely the same as our discussion in Section 9.3. The", "main additional info introduced in this part is a compact matrix form. The layer\nki = W T\nk xi\nai = softmax( [qT\ni k1, qT\ni k2, \u2026 , qT\ni kn]\n\u221adk\n)\nT\n\u2208R1\u00d7n\nai\nqi\n\u221adk\n9.3.3 Value Vectors\nvi\nWv\nvi = W T", "vi\nWv\nvi = W T\nv xi\n9.3.4 Attention Output\nzi =\nn\n\u2211\nj=1\naijvj \u2208Rdk\nzi\nxi\nRn\u00d7d \u27f6Rn\u00d7d\nn\nd,\nn\nd\n9.4.1 A Single Self-attention Head", "takes in  tokens, each having feature dimension . Thus, all tokens can be\ncollectively written as \n, where the -th row of \n stores the -th token,\ndenoted as \n. For each token", ". For each token \n, self-attention computes (via learned\nprojection matrices, discussed in Section 9.3), a query \n, key \n, and\nvalue \n, and overall, we will have  queries,  keys, and  values; all of", "these vectors live in the same dimension in practice, and we often denote all three\nembedding dimension via a unified \n.\nThe self-attention output is calculated as a weighted sum:\nwhere", "where \n is the th element in \n.\nSo far, we\u2019ve discussed self-attention focusing on a single token input-output.\nActually, we can calculate all outputs \n (\n) at the same time using a", "matrix form. For clearness, we first introduce the \n query matrix,\n key matrix, and \n value matrix:\nIt should be straightforward to understand that the \n, \n, \n matrices simply stack\n, \n, and", ", \n, and \n in a row-wise manner, respectively. Now, the the full attention matrix\n is:\nwhich often time is shorten as:\nNote that the Softmax operation is applied in a row-wise manner. The th row  of", "this matrix corresponds to the softmax\u2019d attention scores computed for query \nover all keys (i.e., \n). The full output of the self-attention layer can then be written\ncompactly as:\nn\nd\nX \u2208Rn\u00d7d\ni\nX\ni", "n\nd\nX \u2208Rn\u00d7d\ni\nX\ni\nxi \u2208R1\u00d7d\nxi\nqi \u2208Rdq\nki \u2208Rdk\nvi \u2208Rdv\nn\nn\nn\ndk\nzi =\nn\n\u2211\nj=1\naijvj \u2208Rdk\naij\nj\nai\nzi i = 1, 2, \u2026 , n\nQ \u2208Rn\u00d7dk\nK \u2208Rn\u00d7dk\nV \u2208Rn\u00d7dk\nQ =\n\u2208Rn\u00d7d,\nK =\n\u2208Rn\u00d7d,\nV =\n\u2208Rn\u00d7dv\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1\nq\u22a4\n2\n\u22ee\nq\u22a4\nn\n\u23a4", "q\u22a4\n1\nq\u22a4\n2\n\u22ee\nq\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\nk\u22a4\n1\nk\u22a4\n2\n\u22ee\nk\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\nv\u22a4\n1\nv\u22a4\n2\n\u22ee\nv\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\nQ K V\nqi ki\nvi\nA \u2208Rn\u00d7n\nA =\n\u23a1\n\u23a2\n\u23a3\nsoftmax ([\n]/\u221adk)\nsoftmax ([\n]/\u221adk)\n\u22ee\nsoftmax ([\n]/\u221adk)\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4", "1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n(9.1)\n= softmax ( QK \u22a4\n\u221adk\n)\nA = softmax\n1\n\u221adk\n\u239b\n\u239c\n\u239d\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\n\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1", "\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n\u239e\n\u239f\n\u23a0\ni\nA\nqi\n\u03b1i\n\u22a4\n where \n is the matrix of value vectors stacked row-wise, and \n is", "is\nthe output, whose th row corresponds to the attention output for the th query (i.e.,\n).\nYou will also see this compact notation Attention  in the literature, which is an", "operation of three arguments \n, \n, and \n (and we add an emphasis that the\nsoftmax is performed on each row):\nHuman language can be very nuanced. There are many properties of language that", "collectively contribute to a human\u2019s understanding of any given sentence. For\nexample, words have different tenses (past, present, future, etc), genders,", "abbreviations, slang references, implied words or meanings, cultural references,\nsituational relevance, etc. While the attention mechanism allows us to appropriately", "focus on tokens in the input sentence, it\u2019s unreasonable to expect a single set of\n matrices to fully represent \u2013 and for a model to capture \u2013 the meaning of\na sentence with all of its complexities.", "To address this limitation, the idea of multi-head attention is introduced. Instead of\nrelying on just one attention head (i.e., a single set of \n matrices), the", "matrices), the\nmodel uses multiple attention heads, each with its own independently learned set\nof \n matrices. This allows each head to attend to different parts of the", "input tokens and to model different types of semantic relationships. For instance,\none head might focus on syntactic structure and another on verb tense or sentiment.", "These different \u201cperspectives\u201d are then concatenated and projected to produce a\nricher, more expressive representation of the input.", "Now, we introduce the formal math notations. Let us denote the number of head as\n. For the th head, the input \n is linearly projected into query, key, and", "value matrices using the projection matrices \n, \n, and\n (recall that usually \n):\nThe output of the -th head is \n: \n. After\ncomputing all  heads, we concatenate their outputs and apply a final linear", "Z =\n= AV \u2208Rn\u00d7dk\n\u23a1\n\u23a2\n\u23a3\nz\u22a4\n1\nz\u22a4\n2\n\u22ee\nz\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\nV \u2208Rn\u00d7dk\nZ \u2208Rn\u00d7dk\ni\ni\nzi\nQ K\nV\nAttention(Q, K, V ) = softmaxrow ( QK \u22a4\n\u221adk\n)V\n9.4.2 Multi-head Self-attention\n{Q, K, V }\n{Q, K, V }\n{Q, K, V }\nH\nh\nX \u2208Rn\u00d7d", "H\nh\nX \u2208Rn\u00d7d\nW h\nq \u2208Rd\u00d7dq W h\nk \u2208Rd\u00d7dk\nW h\nv \u2208Rd\u00d7dv\ndq = dk = dv\nQh = XW h\nq\nK h = XW h\nk\nV h = XW h\nv\ni\nZ h Z h = Attention(Qh, K h, V h) \u2208Rn\u00d7dk\nh\n projection:", "h\n projection:\nwhere the concatenation operation concatenates \n horizontally, yielding a matrix\nof size \n, and \n is a final linear projection matrix.\n9.5 Transformers Architecture Details", "An extremely observant reader might have been suspicious of a small but very\nimportant detail that we have not yet discussed: the attention mechanism, as", "introduced so far, does not encode the order of the input tokens. For instance, when\ncomputing softmax\u2019d attention scores and building token representations, the", "model is fundamentally permutation-equivariant \u2014 the same set of tokens, even if\nscrambled into a different order, would result in identical outputs permuted in the", "same order \u2014 Formally, when we fix \n and switch the input \n with \n, then the output \n and \n will be switched. However, natural language is not a bag\nof words: meaning is tied closely to word order.", "To address this, transformers incorporate positional embeddings \u2014 additional\ninformation that encodes the position of each token in the sequence. These", "embeddings are added to the input token embeddings before any attention layers\nare applied, effectively injecting ordering information into the model.", "There are two main strategies for positional embeddings: (i) learned positional\nembeddings, where a trainable vector \n is assigned to each position (i.e.,\ntoken index)", "token index) \n. These vectors are learned alongside all other model\nparameters and allow the model to discover how best to encode position for a given", "task, (ii) fixed positional embeddings, such as sinusoidal positional embedding\nproposed in the original Transformer paper:\nwhere \n is the token index, while \n is the dimension index.", "Namely, this sinusoidal positional embedding uses sine for the even dimension and\ncosine for the odd dimension. Regardless of learnable or fixed positional", "embedding, it will enter the computation of attention at the input place:\n\u200b where \n is the th original input token, and \n is its positional\nembedding. The", "embedding. The \n will now be what we really feed into the attention layer, so that\nthe input to the attention mechanism now carries information about both what the", "token is and where it appears in the sequence.\nMultiHead(X) = Concat(Z 1, \u2026 , Z H)(W O)T\nZ h\nn \u00d7 Hdk\nW O \u2208Rd\u00d7Hdk\n9.5.1 Positional Embeddings\n{Wq, Wk, Wv}\nxi\nxj\nzi\nzj\npi \u2208Rd\ni = 0, 1, 2, . . . , n", "p(i,2k) = sin (\ni\n100002k/d )\np(i,2k+1) = cos (\ni\n100002k/d )\ni = 1, 2, . . , n\nk = 1, 2, . . . , d\nx\u2217\ni = xi + pi ,\nxi\ni\npi\nx\u2217\ni", "xi\ni\npi\nx\u2217\ni\n This simple additive design enables attention layers to leverage both semantic\ncontent and ordering structure when deciding where to focus. In practice, this", "addition occurs at the very first layer of the transformer stack, and all subsequent\nlayers operate on position-aware representations. This is a key design choice that", "allows transformers to work effectively with sequences of text, audio, or even image\npatches (as in Vision Transformers).\nMore generally, a mask may be applied to limit which tokens are used in the", "attention computation. For example, one common mask limits the attention\ncomputation to tokens that occur previously in time to the one being used for the", "query. This prevents the attention mechanism from \u201clooking ahead\u201d in scenarios\nwhere the transformer is being used to generate one token at a time. This causal", "masking is done by introducing a mask matrix \n that restricts attention to\nonly current and previous positions. A typical causal mask is a lower-triangular\nmatrix:", "matrix:\nand we now have the masked attention matrix:\nThe softmax is performed to each row independently. The attention output is still\n. Essentially, the lower-triangular property of", "ensures that the self-\nattention operation for the -th query only considers tokens \n. Note that we\nshould apply the masking before performing softmax, so that the attention matrix", "can be properly normalized (i.e., each row sum to 1).\nEach self-attention stage is trained to have key, value, and query embeddings that", "lead it to pay specific attention to some particular feature of the input. We generally\nwant to pay attention to many different kinds of features in the input; for example,", "in translation one feature might be be the verbs, and another might be objects or\nsubjects. A transformer utilizes multiple instances of self-attention, each known as", "an \u201cattention head,\u201d to allow combinations of attention paid to many different\nfeatures.\n9.5.2 Causal Self-attention\nM \u2208Rn\u00d7n\nM =\n\u23a1\n\u23a2\n\u23a3\n0\n\u2212\u221e\n\u2212\u221e\n\u22ef\n\u2212\u221e\n0\n0\n\u2212\u221e\n\u22ef\n\u2212\u221e\n0\n0\n0\n\u22ef\n\u2212\u221e\n\u22ee\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n0\n0\n0\n\u22ef\n0\n\u23a4\n\u23a5\n\u23a6", "\u22f1\n\u22ee\n0\n0\n0\n\u22ef\n0\n\u23a4\n\u23a5\n\u23a6\nA = softmax\n1\n\u221adk\n+ M\n\u239b\n\u239c\n\u239d\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\n\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n\u239e\n\u239f\n\u23a0\nY = AV\nM\nj\n0, 1, . . . , j", "M\nj\n0, 1, . . . , j\n  This page contains all content from the legacy PDF notes; markov decision processes\nchapter.", "chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.\nConsider a robot learning to navigate through a maze, a game-playing AI", "developing strategies through self-play, or a self-driving car making driving\ndecisions in real-time. These problems share a common challenge: the agent must", "make a sequence of decisions where each choice affects future possibilities and\nrewards. Unlike static prediction tasks where we learn a one-time mapping from", "inputs to outputs, these problems require reasoning about the consequences of\nactions over time.\nThis sequential and dynamical nature demands mathematical tools beyond the", "more static supervised or unsupervised learning approaches. The most general\nframework for such problems is reinforcement learning (RL), where an agent learns to", "take actions in an unknown environment to maximize cumulative rewards over\ntime.\nIn this chapter, we\u2019ll first study Markov decision processes (MDPs), which provide the", "mathematical foundation for understanding and solving sequential decision\nmaking problems like RL. MDPs formalize the interaction between an agent and its", "environment, capturing the key elements of states, actions, rewards, and transitions.\n10.1 Definition and value functions\nFormally, a Markov decision process is \n where  is the state space,", "is the action space, and:\n is a transition model, where\nspecifying a conditional probability distribution;\n is a reward function, where \n specifies an immediate", "reward for taking action  when in state ; and\n is a discount factor, which we\u2019ll discuss in Section 10.1.2.\nIn this class, we assume the rewards are deterministic functions. Further, in this", "MDP chapter, we assume the state space and action space are discrete and finite.\n10  Markov Decision Processes\nNote\n\u27e8S, A, T, R, \u03b3\u27e9\nS\nA\nT : S \u00d7 A \u00d7 S \u2192R\nT(s, a, s\u2032) = Pr(St = s\u2032|St\u22121 = s, At\u22121 = a) ,", "R : S \u00d7 A \u2192R\nR(s, a)\na\ns\n\u03b3 \u2208[0, 1]\nThe notation \n uses a capital\nletter  to stand for a random\nvariable, and small letter  to stand\nfor a concrete value. So \n here is a", "here is a\nrandom variable that can take on\nelements of  as values.\nSt = s\u2032\nS\ns\nSt\nS\n\uf46110  Markov Decision Processes\n\uf52a", "\uf52a\n The following description of a simple machine as Markov decision process provides a\nconcrete example of an MDP.", "The machine has three possible operations (actions): wash , paint , and eject  (each with\na corresponding button). Objects are put into the machine, and each time you push a", "button, something is done to the object. However, it\u2019s an old machine, so it\u2019s not very\nreliable. The machine has a camera inside that can clearly detect what is going on with the", "object and will output the state of the object: dirty , clean , painted , or ejected .\nFor each action, this is what is done to the object:\nWash", "Wash\nIf you perform the wash  operation on any object\u2014whether it\u2019s dirty, clean, or\npainted\u2014it will end up clean  with probability 0.9 and dirty  otherwise.\nPaint", "Paint\nIf you perform the paint  operation on a clean object, it will become nicely painted\nwith probability 0.8. With probability 0.1, the paint misses but the object stays clean,", "and with probability 0.1, the machine dumps rusty dust all over the object, making it\ndirty .\nIf you perform the paint  operation on a painted  object, it stays painted  with\nprobability 1.0.", "probability 1.0.\nIf you perform the paint  operation on a dirty  object, it stays dirty  with\nprobability 1.0.\nEject\nIf you perform an eject  operation on any object, the object comes out of the", "machine and the process is terminated. The object remains ejected  regardless of\nany further actions.\nThese descriptions specify the transition model , and the transition function for each", "action can be depicted as a state machine diagram. For example, here is the diagram for\nwash :\nExample\nT\n dirty\nclean\npainted\nejected\n0.1\n0.9\n0.9\n0.1\n0.1\n0.9\n1.0", "0.9\n0.1\n0.1\n0.9\n1.0\nYou get reward +10 for ejecting a painted object, reward 0 for ejecting a non-painted\nobject, reward 0 for any action on an \u201cejected\u201d object, and reward -3 otherwise. The MDP", "description would be completed by also specifying a discount factor.\nA policy is a function  that specifies what action to take in each state. The policy is", "what we will want to learn; it is akin to the strategy that a player employs to win a\ngiven game. Below, we take just the initial steps towards this eventual goal. We", "describe how to evaluate how good a policy is, first in the finite horizon case\nSection 10.1.1 when the total number of transition steps is finite. In the finite", "horizon case, we typically denote the policy as \n, where  is a non-negative\ninteger denoting the number of steps remaining and \n is the current state. Then", "we consider the infinite horizon case Section 10.1.2, when you don\u2019t know when the\ngame will be over.\nThe goal of a policy is to maximize the expected total reward, averaged over the", "stochastic transitions that the domain makes. Let\u2019s first consider the case where\nthere is a finite horizon , indicating the total number of steps of interaction that the", "agent will have with the MDP.\nWe seek to measure the goodness of a policy. We do so by defining for a given\nhorizon  and MDP policy \n, the \u201chorizon  value\u201d of a state, \n. We do this by", ". We do this by\ninduction on the horizon, which is the number of steps left to go.\nThe base case is when there are no steps remaining, in which case, no matter what\nstate we\u2019re in, the value is 0, so", "Then, the value of a policy in state  at horizon \n is equal to the reward it will\nget in state  plus the next state\u2019s expected horizon  value, discounted by a factor \n\u03c0\n\u03c0h(s)\nh\ns \u2208S", "\u03c0\n\u03c0h(s)\nh\ns \u2208S\n10.1.1 Finite-horizon value functions\nh\nh\n\u03c0h\nh\nV\u03c0\nh(s)\nV\u03c0\n0(s) = 0 .\n(10.1)\ns\nh + 1\ns\nh\n\u03b3\n . So, starting with horizons 1 and 2, and then moving to the general case, we have:", "The sum over  is an expectation: it considers all possible next states , and\ncomputes an average of their \n-horizon values, weighted by the probability", "that the transition function from state  with the action chosen by the policy \nassigns to arriving in state , and discounted by .\n\u2753 Study Question\nWhat is the value of", "for any given state\u2013action pair \n?\n\u2753 Study Question\nConvince yourself that the definitions in Equation 10.1 and Equation 10.3 are\nspecial cases of the more general formulation in Equation 10.4.", "Then we can say that a policy  is better than policy  for horizon  if and only if\nfor all \n, \n and there exists at least one \n such that\n.", "such that\n.\nMore typically, the actual finite horizon is not known, i.e., when you don\u2019t know\nwhen the game will be over! This is called the infinite horizon version of the problem.", "How does one evaluate the goodness of a policy in the infinite horizon case?\nIf we tried to simply take our definitions above and use them for an infinite", "horizon, we could get in trouble. Imagine we get a reward of 1 at each step under\none policy and a reward of 2 at each step under a different policy. Then the reward", "as the number of steps grows in each case keeps growing to become infinite in the\nlimit of more and more steps. Even though it seems intuitive that the second policy", "should be better, we can\u2019t justify that by saying \n.\nV\u03c0\n1(s) = R(s, \u03c01(s)) + 0\n(10.2)\nV\u03c0\n2(s) = R(s, \u03c02(s)) + \u03b3 \u2211\ns\u2032\nT(s, \u03c02(s), s\u2032)V\u03c0\n1(s\u2032)\n(10.3)\n\u22ee\nV\u03c0\nh(s) = R(s, \u03c0h(s)) + \u03b3 \u2211\ns\u2032\nT(s, \u03c0h(s), s\u2032)V\u03c0", "T(s, \u03c0h(s), s\u2032)V\u03c0\nh\u22121(s\u2032)\n(10.4)\ns\u2032\ns\u2032\n(h \u22121)\ns\n\u03c0h(s)\ns\u2032\n\u03b3\n\u2211\ns\u2032\nT(s, a, s\u2032)\n(s, a)\n\u03c0\n\u00af\u03c0\nh\ns \u2208S V\u03c0\nh(s) \u2265V\u00af\u03c0\nh(s)\ns \u2208S\nV\u03c0\nh(s) > V\u00af\u03c0\nh(s)\n10.1.2 Infinite-horizon value functions\n\u221e< \u221e", "\u221e< \u221e\n One standard approach to deal with this problem is to consider the discounted\ninfinite horizon. We will generalize from the finite-horizon case by adding a\ndiscount factor.", "discount factor.\nIn the finite-horizon case, we valued a policy based on an expected finite-horizon\nvalue:\nwhere \n is the reward received at time .\nWhat is", "What is \n? This mathematical notation indicates an expectation, i.e., an average taken\nover all the random possibilities which may occur for the argument. Here, the expectation", "is taken over the conditional probability \n, where \n is the random variable\nfor the reward, subject to the policy being  and the state being \n. Since  is a function,", "this notation is shorthand for conditioning on all of the random variables implied by\npolicy  and the stochastic transitions of the MDP.\nA very important point is that", "is always deterministic (in this class) for any given\n and . Here \n represents the set of all possible \n at time step ; this \n is a", "is a\nrandom variable because the state we\u2019re in at step  is itself a random variable, due to\nprior stochastic state transitions up to but not including at step  and prior (deterministic)", "actions dictated by policy \nNow, for the infinite-horizon case, we select a discount factor \n, and\nevaluate a policy based on its expected infinite horizon value:", "Note that the  indices here are not the number of steps to go, but actually the\nnumber of steps forward from the starting state (there is no sensible notion of \u201csteps", "to go\u201d in the infinite horizon case).\nEquation 10.5 and Equation 10.6 are a conceptual stepping stone. Our main objective is to", "get to Equation 10.8, which can also be viewed as including  in Equation 10.4, with the\nappropriate definition of the infinite-horizon value.", "There are two good intuitive motivations for discounting. One is related to\neconomic theory and the present value of money: you\u2019d generally rather have some", "money today than that same amount of money next week (because you could use it\nnow or invest it). The other is to think of the whole process terminating, with\nprobability", "probability \n on each step of the interaction. (At every step, your expected\nfuture lifetime, given that you have survived until now, is \n.) This value is", ".) This value is\nthe expected amount of reward the agent would gain under this terminating model.\nE [\nh\u22121\n\u2211\nt=0\n\u03b3 tRt \u2223\u03c0, s0] ,\n(10.5)\nRt\nt\nNote\nE [\u22c5]\nPr(Rt = r \u2223\u03c0, s0)\nRt\n\u03c0\ns0\n\u03c0\n\u03c0\nR(s, a)\ns\na\nRt", "\u03c0\n\u03c0\nR(s, a)\ns\na\nRt\nR(st, a)\nt\nRt\nt\nt\n\u03c0.\n0 \u2264\u03b3 \u22641\nE [\n\u221e\n\u2211\nt=0\n\u03b3 tRt \u2223\u03c0, s0] = E [R0 + \u03b3R1 + \u03b3 2R2 + \u2026 \u2223\u03c0, s0] .\n(10.6)\nt\nNote\n\u03b3\n1 \u2212\u03b3\n1/(1 \u2212\u03b3)\n \u2753 Study Question", "\u2753 Study Question\nVerify this fact: if, on every day you wake up, there is a probability of \n that\ntoday will be your last day, then your expected lifetime is \n days.", "days.\nLet us now evaluate a policy in terms of the expected discounted infinite-horizon\nvalue that the agent will get in the MDP if it executes that policy. We define the", "infinite-horizon value of a state  under policy  as\nBecause the expectation of a linear combination of random variables is the linear\ncombination of the expectations, we have", "The equation defined in Equation 10.8 is known as the Bellman Equation, which\nbreaks down the value function into the immediate reward and the (discounted)", "future value function. You could write down one of these equations for each of the\n states. There are  unknowns \n. These are linear equations, and", "standard software (e.g., using Gaussian elimination or other linear algebraic\nmethods) will, in most cases, enable us to find the value of each state under this\npolicy.\n10.2 Finding policies for MDPs", "Given an MDP, our goal is typically to find a policy that is optimal in the sense that\nit gets as much total reward as possible, in expectation over the stochastic", "transitions that the domain makes. We build on what we have learned about\nevaluating the goodness of a policy (Section 10.1.2), and find optimal policies for the", "finite horizon case (Section 10.2.1), then the infinite horizon case (Section 10.2.2).\nHow can we go about finding an optimal policy for an MDP? We could imagine", "enumerating all possible policies and calculating their value functions as in the\nprevious section and picking the best one \u2013 but that\u2019s too much work!", "The first observation to make is that, in a finite-horizon problem, the best action to\ntake depends on the current state, but also on the horizon: imagine that you are in a", "situation where you could reach a state with reward 5 in one step or a state with\nreward 100 in two steps. If you have at least two steps to go, then you\u2019d move\n1 \u2212\u03b3\n1/(1 \u2212\u03b3)\ns\n\u03c0\nV\u03c0", "1/(1 \u2212\u03b3)\ns\n\u03c0\nV\u03c0\n\u221e(s) = E[R0 + \u03b3R1 + \u03b3 2R2 + \u22ef\u2223\u03c0, S0 = s]\n= E[R0 + \u03b3(R1 + \u03b3(R2 + \u03b3 \u2026))) \u2223\u03c0, S0 = s] .\n(10.7)\nV\u03c0\n\u221e(s) = E[R0 \u2223\u03c0, S0 = s] + \u03b3E[R1 + \u03b3(R2 + \u03b3 \u2026))) \u2223\u03c0, S0 = s]\n= R(s, \u03c0(s)) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, \u03c0(s), s\u2032)V\u03c0\n\u221e(s\u2032) .\n(10.8)\nn = |S|\nn\nV\u03c0(s)\n10.2.1 Finding optimal finite-horizon policies\n toward the reward 100 state, but if you only have one step left to go, you should go", "in the direction that will allow you to gain 5!\nFor the finite-horizon case, we define \n to be the expected value of\nstarting in state ,\nexecuting action , and\ncontinuing for", "continuing for \n more steps executing an optimal policy for the\nappropriate horizon on each step.\nSimilar to our definition of \n for evaluating a policy, we define the \n function", "function\nrecursively according to the horizon. The only difference is that, on each step with\nhorizon , rather than selecting an action specified by a given policy, we select the", "value of  that will maximize the expected \n value of the next state.\nwhere \n denotes the next time-step state/action pair. We can solve for the\nvalues of", "values of \n with a simple recursive algorithm called finite-horizon value iteration\nthat just computes \n starting from horizon 0 and working backward to the desired\nhorizon . Given \n, an optimal", ", an optimal \n can be found as follows:\nwhich gives the immediate best action(s) to take when there are  steps left; then\n gives the best action(s) when there are \n steps left, and so on. In the", "case where there are multiple best actions, we typically can break ties randomly.\nAdditionally, it is worth noting that in order for such an optimal policy to be", "computed, we assume that the reward function \n is bounded on the set of all\npossible (state, action) pairs. Furthermore, we will assume that the set of all possible\nactions is finite.", "actions is finite.\n\u2753 Study Question\nThe optimal value function is unique, but the optimal policy is not. Think of a\nsituation in which there is more than one optimal policy.\nQ\u2217\nh(s, a)\ns\na\nh \u22121\nV\u2217\nh", "s\na\nh \u22121\nV\u2217\nh\nQ\u2217\nh\nh\na\nQ\u2217\nh\nQ\u2217\n0(s, a) = 0\nQ\u2217\n1(s, a) = R(s, a) + 0\nQ\u2217\n2(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\n1(s\u2032, a\u2032)\n\u22ee\nQ\u2217\nh(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\nh\u22121(s\u2032, a\u2032)", "a\u2032\nQ\u2217\nh\u22121(s\u2032, a\u2032)\n(s\u2032, a\u2032)\nQ\u2217\nh\nQ\u2217\nh\nh\nQ\u2217\nh\n\u03c0\u2217\nh\n\u03c0\u2217\nh(s) = arg max\na\nQ\u2217\nh(s, a) .\nh\n\u03c0\u2217\nh\u22121(s)\n(h \u22121)\nR(s, a)\n10.2.2 Finding optimal infinite-horizon policies\nWe can also define the action-value", "function for a fixed policy ,\ndenoted by \n. This quantity\nrepresents the expected sum of\ndiscounted rewards obtained by\ntaking action  in state  and\nthereafter following the policy", "over the remaining horizon of\n steps.\nSimilar to \n, \n satisfies\nthe Bellman recursion/equations\nintroduced earlier. In fact, for a\ndeterministic policy :\nHowever, since our primary goal in", "dealing with action values is\ntypically to identify an optimal\npolicy, we will not dwell\nextensively on (\n). Instead,\nwe will place more emphasis on\nthe optimal action-value functions\n.\n\u03c0\nQ\u03c0\nh(s, a)", ".\n\u03c0\nQ\u03c0\nh(s, a)\na\ns\n\u03c0\nh \u22121\nV\u03c0\nh(s) Q\u03c0\nh(s, a)\n\u03c0\nQ\u03c0\nh(s, \u03c0(s)) = V\u03c0\nh(s).\nQ\u03c0\nh(s, a)\nQ\u2217\nh(s, a)\n In contrast to the finite-horizon case, the best way of behaving in an infinite-horizon", "discounted MDP is not time-dependent. That is, the decisions you make at time\n looking forward to infinity, will be the same decisions that you make at time", "for any positive , also looking forward to infinity.\nAn important theorem about MDPs is: in the infinite-horizon case, there exists a\nstationary optimal policy", "(there may be more than one) such that for all \nand all other policies , we have\nThere are many methods for finding an optimal policy for an MDP. We have already", "seen the finite-horizon value iteration case. Here we will study a very popular and\nuseful method for the infinite-horizon case, infinite-horizon value iteration. It is also", "important to us, because it is the basis of many reinforcement-learning methods.\nWe will again assume that the reward function \n is bounded on the set of all", "possible (state, action) pairs and additionally that the number of actions in the\naction space is finite. Define \n to be the expected infinite-horizon value of", "being in state , executing action , and executing an optimal policy \n thereafter.\nUsing similar reasoning to the recursive definition of \n we can express this value\nrecursively as", "recursively as\nThis is also a set of equations, one for each \n pair. This time, though, they are\nnot linear (due to the \n operation), and so they are not easy to solve. But there is", "a theorem that says they have a unique solution!\nOnce we know the optimal action-value function \n, then we can extract an\noptimal policy \n as\nWe can iteratively solve for the", "values with the infinite-horizon value iteration\nalgorithm, shown below:\nAlgorithm 10.1 Infinite-Horizon-Value-Iteration\nRequire: , \n, , , , \nInitialization:\nfor each \n and \n do\nend for", "and \n do\nend for\nwhile not converged do\nfor each \n and \n do\nend for\nif \n then\nreturn \nend if\nt = 0\nt = T\nT\n\u03c0\u2217\ns \u2208S\n\u03c0\nV\u03c0(s) \u2264V\u03c0\u2217(s) .\nR(s, a)\nQ\u2217\n\u221e(s, a)\ns\na\n\u03c0\u2217\nV\u03c0,\nQ\u2217\n\u221e(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\n\u221e(s\u2032, a\u2032) .\n(s, a)\nmax\nQ\u2217\n\u221e(s, a)\n\u03c0\u2217\n\u03c0\u2217(s) = arg max\na\nQ\u2217\n\u221e(s, a)\nQ\u2217\nS A T R \u03b3 \u03f5\n1:\n2:\ns \u2208S\na \u2208A\n3:\nQold(s, a) \u21900\n4:\n5:\n6:\ns \u2208S\na \u2208A\n7:\nQnew(s, a) \u2190R(s, a) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQold(s\u2032, a\u2032)\n8:\n9:\nmax\ns,a |Qold(s, a) \u2212Qnew(s, a)| < \u03f5\n10:\nQnew\n11:\n end while\nThere are a lot of nice theoretical results about infinite-horizon value iteration. For", "some given (not necessarily optimal) \n function, define \n.\nAfter executing infinite-horizon value iteration with convergence hyper-\nparameter ,\nThere is a value of  such that", "As the algorithm executes, \n decreases monotonically on each\niteration.\nThe algorithm can be executed asynchronously, in parallel: as long as all", "pairs are updated infinitely often in an infinite run, it still converges to the\noptimal value.\n12:\nQold \u2190Qnew\n13:\nTheory\nQ\n\u03c0Q(s) = arg maxa Q(s, a)\n\u03f5\n\u2225V\u03c0Qnew \u2212V\u03c0\u2217\u2225max < \u03f5 .\n\u03f5", "\u03f5\n\u2225Qold \u2212Qnew\u2225max < \u03f5 \u27f9\u03c0Qnew = \u03c0\u2217\n\u2225V\u03c0Qnew \u2212V\u03c0\u2217\u2225max\n(s, a)\n This page contains all content from the legacy PDF notes; reinforcement learning chapter.", "As we phase out the PDF, this page may receive updates not reflected in the static PDF.\nReinforcement learning (RL) is a type of machine learning where an agent learns to", "make decisions by interacting with an environment. Unlike other learning\nparadigms, RL has several distinctive characteristics:", "The agent interacts directly with an environment, receiving feedback in the\nform of rewards or penalties\nThe agent can choose actions that influences what information it gains from the\nenvironment", "environment\nThe agent updates its decision-making strategy incrementally as it gains more\nexperience\nIn a reinforcement learning problem, the interaction between the agent and", "environment follows a specific pattern:\nLearner\nEnvironmen t\nreward\nstate\naction\nThe interaction cycle proceeds as follows:\n1. Agent observes the current state", "2. Agent selects and executes an action \n3. Agent receives a reward \n from the environment\n4. Agent observes the new state \n5. Agent selects and executes a new action \n6. Agent receives a new reward", "7. This cycle continues\u2026\nSimilar to MDP Chapter 10, in an RL problem, the agent\u2019s goal is to learn a policy - a\nmapping from states to actions - that maximizes its expected cumulative reward", "over time. This policy guides the agent\u2019s decision-making process, helping it choose\nactions that lead to the most favorable outcomes.\n11  Reinforcement Learning\nNote\ns(i)\na(i)\nr(i)\ns(i+1)\na(i+1)", "r(i)\ns(i+1)\na(i+1)\nr(i+1)\n\uf46111  Reinforcement Learning\n\uf52a\n 11.1 Reinforcement learning algorithms overview\nApproaches to reinforcement learning differ significantly according to what kind of", "hypothesis or model is being learned. Roughly speaking, RL methods can be\ncategorized into model-free methods and model-based methods. The main", "distinction is that model-based methods explicitly learn the transition and reward\nmodels to assist the end-goal of learning a policy; model-free methods do not. We", "will start our discussion with the model-free methods, and introduce two of the\narguably most popular types of algorithms, Q-learning Section 11.1.2 and policy", "gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,\nwe briefly consider \u201cbandit\u201d problems Section 11.5, which differ from our MDP", "learning context by having probabilistic rewards.\nModel-free methods are methods that do not explicitly learn transition and reward", "models. Depending on what is explicitly being learned, model-free methods are\nsometimes further categorized into value-based methods (where the goal is to", "learn/estimate a value function) and policy-based methods (where the goal is to\ndirectly learn an optimal policy). It\u2019s important to note that such categorization is", "approximate and the boundaries are blurry. In fact, current RL research tends to\ncombine the learning of value functions, policies, and transition and reward models", "all into a complex learning algorithm, in an attempt to combine the strengths of\neach approach.\nQ-learning is a frequently used class of RL algorithms that concentrates on learning", "(estimating) the state-action value function, i.e., the \n function. Specifically, recall\nthe MDP value-iteration update:", "The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,\nwhere we do not know the transition function  or reward function \n, and instead", ", and instead\nrely on samples to perform the updates.\nprocedure Q-Learning(\n)\nfor all \n do\nend for\nwhile \n do\n11.1.1 Model-free methods\n11.1.2 Q-learning\nQ\nQ(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQ(s\u2032, a\u2032)\nT\nR\n1:\nS, A, \u03b3, \u03b1, s0, max_iter\n2:\ni \u21900\n3:\ns \u2208S, a \u2208A\n4:\nQold(s, a) \u21900\n5:\n6:\ns \u2190s0\n7:\ni < max_iter\nThe thing that most students seem\nto get confused about is when we", "do value iteration and when we do\nQ-learning. Value iteration\nassumes you know  and \n and\njust need to compute \n. In Q-\nlearning, we don\u2019t know or even\ndirectly estimate  and \n: we\nestimate", ": we\nestimate \n directly from\nexperience!\nT\nR\nQ\nT\nR\nQ\n end while\nreturn \nend procedure\nWith the pseudo\u2011code provided for Q\u2011Learning, there are a few key things to note.", "First, we must determine which state to initialize the learning from. In the context of\na game, this initial state may be well defined. In the context of a robot navigating an", "environment, one may consider sampling the initial state at random. In any case,\nthe initial state is necessary to determine the trajectory the agent will experience as\nit navigates the environment.", "Second, different contexts will influence how we want to choose when to stop\niterating through the while loop. Again, in some games there may be a clear", "terminating state based on the rules of how it is played. On the other hand, a robot\nmay be allowed to explore an environment ad infinitum. In such a case, one may", "consider either setting a fixed number of transitions (as done explicitly in the\npseudo\u2011code) to take; or we may want to stop iterating in the example once the", "values in the Q\u2011table are not changing, after the algorithm has been running for a\nwhile.\nFinally, a single trajectory through the environment may not be sufficient to", "adequately explore all state\u2011action pairs. In these instances, it becomes necessary to\nrun through a number of iterations of the Q\u2011Learning algorithm, potentially with", "different choices of initial state \n.\nOf course, we would then want to modify Q\u2011Learning such that the Q table is not\nreset with each call.\nNow, let\u2019s dig into what is happening in Q\u2011Learning. Here,", "represents the\nlearning rate, which needs to decay for convergence purposes, but in practice is often\nset to a constant. It\u2019s also worth mentioning that Q-learning assumes a discrete state", "and action space where states and actions take on discrete values like \n etc.\nIn contrast, a continuous state space would allow the state to take values from, say,", "a continuous range of numbers; for example, the state could be any real number in\nthe interval \n. Similarly, a continuous action space would allow the action to be", "drawn from, e.g., a continuous range of numbers. There are now many extensions\ndeveloped based on Q-learning that can handle continuous state and action spaces", "(we\u2019ll look at one soon), and therefore the algorithm above is also sometimes\nreferred to more specifically as tabular Q-learning.\nIn the Q-learning update rule\n8:\na \u2190select_action(s, Qold(s, a))\n9:", "9:\n(r, s\u2032) \u2190execute(a)\n10:\nQnew(s, a) \u2190(1 \u2212\u03b1) Qold(s, a) + \u03b1(r + \u03b3 maxa\u2032 Qold(s\u2032, a\u2032))\n11:\ns \u2190s\u2032\n12:\ni \u2190i + 1\n13:\nQold \u2190Qnew\n14:\n15:\nQnew\n16:\ns0\n\u03b1 \u2208(0, 1]\n1, 2, 3, \u2026\n[1, 3]", "1, 2, 3, \u2026\n[1, 3]\nQ[s, a] \u2190(1 \u2212\u03b1)Q[s, a] + \u03b1(r + \u03b3 max\na\u2032\nQ[s\u2032, a\u2032])\n(11.1)\nThis notion of running a number of\ninstances of Q\u2011Learning is often\nreferred to as experiencing\nmultiple episodes.", "multiple episodes.\n the term \n is often referred to as the one-step look-ahead target.\nThe update can be viewed as a combination of two different iterative processes that", "we have already seen: the combination of an old estimate with the target using a\nrunning average with a learning rate \nEquation 11.1 can also be equivalently rewritten as", "which allows us to interpret Q-learning in yet another way: we make an update (or\ncorrection) based on the temporal difference between the target and the current\nestimated value", "estimated value \nThe Q-learning algorithm above includes a procedure called select_action , that,\ngiven the current state  and current \n function, has to decide which action to take.\nIf the", "If the \n value is estimated very accurately and the agent is deployed to \u201cbehave\u201d in\nthe world (as opposed to \u201clearn\u201d in the world), then generally we would want to", "choose the apparently optimal action \n.\nBut, during learning, the \n value estimates won\u2019t be very good and exploration is", "important. However, exploring completely at random is also usually not the best\nstrategy while learning, because it is good to focus your attention on the parts of the", "state space that are likely to be visited when executing a good policy (not a bad or\nrandom one).\nA typical action-selection strategy that attempts to address this exploration versus", "exploitation dilemma is the so-called -greedy strategy:\nwith probability \n, choose \n;\nwith probability , choose the action \n uniformly at random.", "where the  probability of choosing a random action helps the agent to explore and\ntry out actions that might not seem so desirable at the moment.", "Q-learning has the surprising property that it is guaranteed to converge to the actual\noptimal \n function! The conditions specified in the theorem are: visit every state-", "action pair infinitely often, and the learning rate  satisfies a scheduling condition.\nThis implies that for exploration strategy specifically, any strategy is okay as long as", "it tries every state-action infinitely often on an infinite run (so that it doesn\u2019t\nconverge prematurely to a bad action choice).", "Q-learning can be very inefficient. Imagine a robot that has a choice between\nmoving to the left and getting a reward of 1, then returning to its initial state, or", "moving to the right and walking down a 10-step hallway in order to get a reward of\n1000, then returning to its initial state.\n(r + \u03b3 maxa\u2032 Q[s\u2032, a\u2032])\n\u03b1.\nQ[s, a] \u2190Q[s, a] + \u03b1((r + \u03b3 max\na\u2032", "a\u2032\nQ[s\u2032, a\u2032]) \u2212Q[s, a]),\n(11.2)\nQ[s, a].\ns\nQ\nQ\narg maxa\u2208A Q(s, a)\nQ\n\u03f5\n1 \u2212\u03f5\narg maxa\u2208A Q(s, a)\n\u03f5\na \u2208A\n\u03f5\nQ\n\u03b1\n robot\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n+1000\n+1\n-1", "8\n9\n10\n+1000\n+1\n-1\nThe first time the robot moves to the right and goes down the hallway, it will\nupdate the \n value just for state 9 on the hallway and action ``right\u2019\u2019 to have a high", "value, but it won\u2019t yet understand that moving to the right in the earlier steps was a\ngood choice. The next time it moves down the hallway it updates the value of the", "state before the last one, and so on. After 10 trips down the hallway, it now can see\nthat it is better to move to the right than to the left.\nMore concretely, consider the vector of Q values", ", representing\nthe Q values for moving right at each of the positions \n. Position index 0\nis the starting position of the robot as pictured above.\nThen, for \n and \n, Equation 11.2 becomes", "Starting with Q values of 0,\nSince the only nonzero reward from moving right is \n, after our\nrobot makes it down the hallway once, our new Q vector is\nAfter making its way down the hallway again,", "updates:\nSimilarly,\nQ\nQ(i = 0, \u2026 , 9; right)\ni = 0, \u2026 , 9\n\u03b1 = 1\n\u03b3 = 0.9\nQ(i, right) = R(i, right) + 0.9 max\na\nQ(i + 1, a) .\nQ(0)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nR(9, right) = 1000", "R(9, right) = 1000\nQ(1)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\nQ(8, right) = 0 + 0.9 Q(9, right) = 900\nQ(2)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n900\n1000", "0\n0\n0\n0\n0\n900\n1000\nQ(3)(i = 0, \u2026 , 9; right) = [\n] ,\n0\n0\n0\n0\n0\n0\n0\n810\n900\n1000\nQ(4)(i = 0, \u2026 , 9; right) = [\n] ,\n0\n0\n0\n0\n0\n0\n729\n810\n900\n1000\nWe are violating our usual", "notational conventions here, and\nwriting \n to mean the Q value\nfunction that results after the robot\nruns all the way to the end of the\nhallway, when executing the policy", "that always moves to the right.\nQi\n and the robot finally sees the value of moving right from position 0.\n\u2753 Study Question\nDetermine the Q value functions that result from always executing the \u201cmove", "left\u201d policy.\n11.2 Function approximation: Deep Q learning\nIn our Q-learning algorithm above, we essentially keep track of each \n value in a\ntable, indexed by  and . What do we do if  and/or", "are large (or continuous)?\nWe can use a function approximator like a neural network to store Q values. For\nexample, we could design a neural network that takes inputs  and , and outputs", ". We can treat this as a regression problem, optimizing this loss:\nwhere \n is now the output of the neural network.\nThere are several different architectural choices for using a neural network to", "approximate \n values:\nOne network for each action , that takes  as input and produces \n as\noutput;\nOne single network that takes  as input and produces a vector \n,\nconsisting of the", "consisting of the \n values for each action; or\nOne single network that takes \n concatenated into a vector (if  is discrete,", "we would probably use a one-hot encoding, unless it had some useful internal\nstructure) and produces \n as output.", "as output.\nThe first two choices are only suitable for discrete (and not too big) action sets. The\nlast choice can be applied for continuous actions, but then it is difficult to find\n.", ".\nThere are not many theoretical guarantees about Q-learning with function\napproximation and, indeed, it can sometimes be fairly unstable (learning to perform", "well for a while, and then suddenly getting worse, for example). But neural\nnetwork Q-learning has also had some significant successes.\n\u2026\nQ(10)(i = 0, \u2026 , 9; right) = [\n] ,\n387.4\n420.5\n478.3\n531.4", "420.5\n478.3\n531.4\n590.5\n656.1\n729\n810\n900\n1000\nQ\ns\na\nS\nA\ns\na\nQ(s, a)\n(Q(s, a) \u2212(r + \u03b3 max\na\u2032\nQ(s\u2032, a\u2032)))\n2\nQ(s, a)\nQ\na\ns\nQ(s, a)\ns\nQ(s, \u22c5)\nQ\ns, a\na\nQ(s, a)\narg maxa\u2208A Q(s, a)\nHere, we can see the", "exploration/exploitation dilemma\nin action: from the perspective of\n, it will seem that getting the\nimmediate reward of  is a better\nstrategy without exploring the long\nhallway.\ns0 = 0\n1", "hallway.\ns0 = 0\n1\nThis is the so-called squared\nBellman error; as the name\nsuggests, it\u2019s closely related to the\nBellman equation we saw in MDPs\nin Chapter Chapter 10. Roughly", "speaking, this error measures how\nmuch the Bellman equality is\nviolated.\nFor continuous action spaces, it is\npopular to use a class of methods\ncalled actor-critic methods, which", "combine policy and value-function\nlearning. We won\u2019t get into them in\ndetail here, though.\n One form of instability that we do know how to guard against is catastrophic", "forgetting. In standard supervised learning, we expect that the training  values\nwere drawn independently from some distribution.", "But when a learning agent, such as a robot, is moving through an environment, the\nsequence of states it encounters will be temporally correlated. For example, the", "robot might spend 12 hours in a dark environment and then 12 in a light one. This\ncan mean that while it is in the dark, the neural-network weight-updates will make\nthe", "the \n function \"forget\" the value function for when it\u2019s light.\nOne way to handle this is to use experience replay, where we save our", "experiences in a replay buffer. Whenever we take a step in the world, we add the\n to the replay buffer and use it to do a Q-learning update. Then we also", "randomly select some number of tuples from the replay buffer, and do Q-learning\nupdates based on them as well. In general, it may help to keep a sliding window of", "just the 1000 most recent experiences in the replay buffer. (A larger buffer will be\nnecessary for situations when the optimal policy might visit a large part of the state", "space, but we like to keep the buffer size small for memory reasons and also so that\nwe don\u2019t focus on parts of the state space that are irrelevant for the optimal policy.)", "The idea is that it will help us propagate reward values through our state space\nmore efficiently if we do these updates. We can see it as doing something like value", "iteration, but using samples of experience rather than a known model.\nAn alternative strategy for learning the \n function that is somewhat more robust\nthan the standard", "than the standard \n-learning algorithm is a method called fitted Q.\nprocedure Fitted-Q-Learning(\n)\n//e.g., \n can be drawn randomly from \ninitialize neural-network representation of \nwhile True do", "while True do\n experience from executing -greedy policy based on  for \n steps\n represented as tuples \nfor each tuple \n do\nend for\nre-initialize neural-network representation of \nend while", "end while\nend procedure\nHere, we alternate between using the policy induced by the current \n function to\ngather a batch of data \n, adding it to our overall data set \n, and then using", ", and then using\nsupervised neural-network training to learn a representation of the \n value\nfunction on the whole data set. This method does not mix the dynamic-\nx\nQ\n(s, a, s\u2032, r)\n(s, a, s\u2032, r)", "(s, a, s\u2032, r)\n11.2.1 Fitted Q-learning\nQ\nQ\n1:\nA, s0, \u03b3, \u03b1, \u03f5, m\n2:\ns \u2190s0\ns0\nS\n3:\nD \u2190\u2205\n4:\nQ\n5:\n6:\nDnew \u2190\n\u03f5\nQ\nm\n7:\nD \u2190D \u222aDnew\n(s, a, s\u2032, r)\n8:\nDsupervised \u2190\u2205\n9:\n(s, a, s\u2032, r) \u2208D\n10:\nx \u2190(s, a)\n11:", "10:\nx \u2190(s, a)\n11:\ny \u2190r + \u03b3 maxa\u2032\u2208A Q(s\u2032, a\u2032)\n12:\nDsupervised \u2190Dsupervised \u222a{(x, y)}\n13:\n14:\nQ\n15:\nQ \u2190supervised-NN-regression(Dsupervised)\n16:\n17:\nQ\nDnew\nD\nQ\nAnd, in fact, we routinely shuffle", "their order in the data file, anyway.\n programming phase (computing new \n values based on old ones) with the function\napproximation phase (supervised training of the neural network) and avoids", "catastrophic forgetting. The regression training in line 10 typically uses squared\nerror as a loss function and would be trained until the fit is good (possibly\nmeasured on held-out data).", "11.3 Policy gradient\nA different model-free strategy is to search directly for a good policy. The strategy\nhere is to define a functional form \n for the policy, where  represents the", "parameters we learn from experience. We choose  to be differentiable, and often\ndefine\n, a conditional probability distribution over our possible actions.", "Now, we can train the policy parameters using gradient descent:\nWhen  has relatively low dimension, we can compute a numeric estimate of", "the gradient by running the policy multiple times for different values of , and\ncomputing the resulting rewards.\nWhen  has higher dimensions (e.g., it represents the set of parameters in a", "complicated neural network), there are more clever algorithms, e.g., one called\nREINFORCE, but they can often be difficult to get to work reliably.", "Policy search is a good choice when the policy has a simple known form, but the\nMDP would be much more complicated to estimate.\n11.4 Model-based RL", "11.4 Model-based RL\nThe conceptually simplest approach to RL is to model \n and  from the data we\nhave gotten so far, and then use those models, together with an algorithm for", "solving MDPs (such as value iteration) to find a policy that is near-optimal given\nthe current models.\nAssume that we have had some set of interactions with the environment, which can", "be characterized as a set of tuples of the form \n.\nBecause the transition function \n specifies probabilities, multiple\nobservations of \n may be needed to model the transition function. One", "approach to building a model \n for the true \n is to estimate it using\na simple counting strategy:\nQ\nf(s; \u03b8) = a\n\u03b8\nf\nf(s, a; \u03b8) = Pr(a|s)\n\u03b8\n\u03b8\n\u03b8\nR\nT\n(s(t), a(t), s(t+1), r(t))\nT(s, a, s\u2032)\n(s, a, s\u2032)", "(s, a, s\u2032)\n^T(s, a, s\u2032)\nT(s, a, s\u2032)\n^T(s, a, s\u2032) = #(s, a, s\u2032) + 1\n#(s, a) + |S| .\nThis means the chance of choosing\nan action depends on which state\nthe agent is in. Suppose, e.g., a", "robot is trying to get to a goal and\ncan go left or right. An\nunconditional policy can say: I go\nleft 99% of the time; a conditional\npolicy can consider the robot\u2019s", "state, and say: if I\u2019m to the right of\nthe goal, I go left 99% of the time.\n Here, \n represents the number of times in our data set we have the\nsituation where \n, \n, \n, and \n represents the number of", "times in our data set we have the situation where \n, \n.\nAdding 1 and \n to the numerator and denominator, respectively, is a form of", "smoothing called the Laplace correction. It ensures that we never estimate that a\nprobability is 0, and keeps us from dividing by 0. As the amount of data we gather", "increases, the influence of this correction fades away.\nIn contrast, the reward function \n is a deterministic function, such that\nknowing the reward  for a given", "is sufficient to fully determine the function\nat that point. Our model \n can simply be a record of observed rewards, such that\n.\nGiven empirical models  and", "for the transition and reward functions, we can\nnow solve the MDP \n to find an optimal policy using value iteration, or\nuse a search algorithm to find an action to take for a particular state.", "This approach is effective for problems with small state and action spaces, where it\nis not too hard to get enough experience to model  and \n well; but it is difficult to", "generalize this method to handle continuous (or very large discrete) state spaces,\nand is a topic of current research.\n11.5 Bandit problems", "Bandit problems are a subset of reinforcement learning problems. A basic bandit\nproblem is given by:\nA set of actions \n;\nA set of reward values \n; and\nA probabilistic reward function \n, i.e.,", ", i.e., \n is a function that\ntakes an action and a reward and returns the probability of getting that reward\nconditioned on that action being taken,\n. Each time the agent takes an action, a", "new value is drawn from this distribution.\nThe most typical bandit problem has \n and \n. This is called a -\narmed bandit problem, where the decision is which \u201carm\u201d (action ) to select, and the", "reward is either getting a payoff ( ) or not ( ).\nThe important question is usually one of exploration versus exploitation. Imagine you\nhave tried each action 10 times, and now you have estimates", "for the\nprobabilities \n. Which arm should you pick next? You could:\nexploit your knowledge, choosing the arm with the highest value of expected\nreward; or\n#(s, a, s\u2032)\ns(t) = s a(t) = a s(t+1) = s\u2032", "#(s, a)\ns(t) = s a(t) = a\n|S|\nR(s, a)\nr\n(s, a)\n^R\n^R(s, a) = r = R(s, a)\n^T\n^R\n(S, A, ^T, ^R)\nT\nR\nA\nR\nRp : A \u00d7 R \u2192R\nRp\nRp(a, r) = Pr(reward = r \u2223action = a)\nR = {0, 1}\n|A| = k\nk\na\n1\n0\n^Rp(a, r)", "k\na\n1\n0\n^Rp(a, r)\nRp(a, r)\nConceptually, this is similar to\nhaving \u201cinitialized\u201d our estimate\nfor the transition function with\nuniform random probabilities\nbefore making any observations.", "Notice that this probablistic\nrewards set up in bandits differs\nfrom the \u201crewards are\ndeterministic\u201d assumptions we\nmade so far.\nWhy \u201cbandit\u201d? In English slang,\n\u201cone-armed bandit\u201d refers to a slot", "machine because it has one arm\nand takes your money! Here, we\nhave a similar machine but with \narms.\nk\n explore further, trying some or all actions more times to get better estimates of\nthe \n values.", "the \n values.\nThe theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1\nour discount factor), the more time we should spend exploring, so that we don\u2019t", "converge prematurely on a bad choice of action.\nBandit problems are reinforcement learning problems (and very different from\nbatch supervised learning) in that:", "The agent gets to influence what data it obtains (selecting  gives it another\nsample from \n), and\nThe agent is penalized for mistakes it makes while it is learning (trying to", "maximize the expected reward it gets while behaving).\nIn a contextual bandit problem, you have multiple possible states from some set ,\nand a separate bandit problem associated with each one.", "Bandit problems are an essential subset of reinforcement learning. It\u2019s important to\nbe aware of the issues, but we will not study solutions to them in this class.\nRp(a, r)\nh\na\nR(a, r)\nS", "h\na\nR(a, r)\nS\n This page contains all content from the legacy PDF notes; non-parametric models chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Neural networks have adaptable complexity, in the sense that we can try different\nstructural models and use cross validation to find one that works well on our data.", "Beyond neural networks, we may further broaden the class of models that we can\nfit to our data, for example as illustrated by the techniques introduced in this\nchapter.", "chapter.\nHere, we turn to models that automatically adapt their complexity to the training\ndata. The name non-parametric methods is misleading: it is really a class of methods", "that does not have a fixed parameterization in advance. Rather, the complexity of\nthe parameterization can grow as we acquire more data.", "Some non-parametric models, such as nearest-neighbor, rely directly on the data to\nmake predictions and do not compute a model that summarizes the data. Other", "non-parametric methods, such as decision trees, can be seen as dynamically\nconstructing something that ends up looking like a more traditional parametric", "model, but where the actual training data affects exactly what the form of the model\nwill be.\nThe non-parametric methods we consider here tend to have the form of a\ncomposition of simple models:", "Nearest neighbor models: Section 12.1 where we don\u2019t process data at training\ntime, but do all the work when making predictions, by looking for the closest", "training example(s) to a given new data point.\nTree models: Section 12.2 where we partition the input space and use different", "simple predictions on different regions of the space; the hypothesis space can\nbecome arbitrarily large allowing finer and finer partitions of the input space.", "Ensemble models: Section 12.2.3 in which we train several different classifiers on\nthe whole space and average the answers; this decreases the estimation error. In", "particular, we will look at bootstrap aggregation, or bagging of trees.\nBoosting is a way to construct a model composed of a sequence of component", "models (e.g., a model consisting of a sequence of trees, each subsequent tree\nseeking to correct errors in the previous trees) that decreases both estimation", "and structural error. We won\u2019t consider this in detail in this class.\n12  Non-parametric methods\nNote\n\uf46112  Non-parametric methods\n\uf52a", "\uf52a\n * -means clustering methods, Section 12.3 where we partition data into groups\nbased on similarity without predefined labels, adapting complexity by\nadjusting the number of clusters.", "Why are we studying these methods, in the heyday of complicated models such as\nneural networks ?\nThey are fast to implement and have few or no hyperparameters to tune.", "They often work as well as or better than more complicated methods.\nPredictions from some of these models can be easier to explain to a human", "user: decision trees are fairly directly human-interpretable, and nearest\nneighbor methods can justify their decisions to some extent by showing a few", "training examples that the predictions were based on.\n12.1 Nearest Neighbor\nIn nearest-neighbor models, we don\u2019t do any processing of the data at training time", "\u2013 we just remember it! All the work is done at prediction time.\nInput values  can be from any domain \n (\n, documents, tree-structured objects,\netc.). We just need a distance metric,", ", which satisfies the following,\nfor all \n:\nGiven a data-set \n, our predictor for a new \n is\nthat is, the predicted output associated with the training point that is closest to the", "query point . Tie breaking is typically done at random.\nThis same algorithm works for regression and classification!\nThe nearest neighbor prediction function can be described by dividing the space up", "into regions whose closest point is each individual training point as shown below :\nk\nx\nX Rd\nd : X \u00d7 X \u2192R+\nx, x\u2032, x\u2032\u2032 \u2208X\nd(x, x) = 0\nd(x, x\u2032) = d(x\u2032, x)\nd(x, x\u2032\u2032) \u2264d(x, x\u2032) + d(x\u2032, x\u2032\u2032)", "D = {(x(i), y(i))}n\ni=1\nx \u2208X\nh(x) = y(i)\nwhere\ni = arg min\ni\nd(x, x(i)) ,\nx\n In each region, we predict the associated  value.", "There are several useful variations on this method. In -nearest-neighbors, we find\nthe  training points nearest to the query point  and output the majority  value", "for classification or the average for regression. We can also do locally weighted\nregression in which we fit locally linear regression models to the  nearest points,", "possibly giving less weight to those that are farther away. In large data-sets, it is\nimportant to use good data structures (e.g., ball trees) to perform the nearest-", "neighbor look-ups efficiently (without looking at all the data points each time).\n12.2 Tree Models\nThe idea here is that we would like to find a partition of the input space and then fit", "very simple models to predict the output in each piece. The partition is described\nusing a (typically binary) \u201ctree\u201d that recursively splits the space.\nTree methods differ by:", "The class of possible ways to split the space at each node; these are typically\nlinear splits, either aligned with the axes of the space, or sometimes using more\ngeneral classifiers.", "The class of predictors within the partitions; these are often simply constants,\nbut may be more general classification or regression models.", "The way in which we control the complexity of the hypothesis: it would be\nwithin the capacity of these methods to have a separate partition element for\neach individual training example.\ny\nk\nk\nx\ny\nk", "y\nk\nk\nx\ny\nk\n The algorithm for making the partitions and fitting the models.\nOne advantage of tree models is that they are easily interpretable by humans. This", "is important in application domains, such as medicine, where there are human\nexperts who often ultimately make critical decisions and who need to feel confident", "in their understanding of recommendations made by an algorithm. Below is an\nexample decision tree, illustrating how one might be able to understand the\ndecisions made by the tree.", "#Example Here is a sample tree (reproduced from Breiman, Friedman, Olshen, Stone\n(1984)):\nThese methods are most appropriate for domains where the input space is not very", "high-dimensional and where the individual input features have some substantially\nuseful information individually or in small groups. Trees would not be good for", "image input, but might be good in cases with, for example, a set of meaningful\nmeasurements of the condition of a patient in the hospital, as in the example above.", "We\u2019ll concentrate on the CART/ID3 (\u201cclassification and regression trees\u201d and\n\u201citerative dichotomizer 3\u201d, respectively) family of algorithms, which were invented", "independently in the statistics and the artificial intelligence communities. They\nwork by greedily constructing a partition, where the splits are axis aligned and by", "fitting a constant model in the leaves. The interesting questions are how to select the\nsplits and how to control complexity. The regression and classification versions are\nvery similar.", "very similar.\nAs a concrete example, consider the following images:\nNote\n  \nThe left image depicts a set of labeled data points in a two-dimensional feature", "space. The right shows a partition into regions by a decision tree, in this case having\nno classification errors in the final partitions.\nThe predictor is made up of", "a partition function, , mapping elements of the input space into exactly one of\n regions, \n, and\na collection of \n output values, \n, one for each region.", "If we already knew a division of the space into regions, we would set \n, the\nconstant output for region \n, to be the average of the training output values in\nthat region. For a training data set", ", we let  be an\nindicator set of all of the elements within \n, so that \n for our whole\ndata set. We can define \n as the subset of data set samples that are in region \n, so\nthat \n. Then", ", so\nthat \n. Then\nWe can define the error in a region as \n. For example, \n as the sum of squared\nerror would be expressed as\nIdeally, we should select the partition to minimize", "for some regularization constant . It is enough to search over all partitions of the\ntraining data (not all partitions of the input space!) to optimize this, but the problem\nis NP-complete.", "is NP-complete.\n12.2.1 Regression\n\u03c0\nM\nR1, \u2026 , RM\nM\nOm\nOm\nRm\nD = {(x(i), y(i))}, i = 1, \u2026 n\nI\nD\nI = {1, \u2026 , n}\nIm\nRm\nIm = {i \u2223x(i) \u2208Rm}\nOm = averagei\u2208Im y(i) .\nEm\nEm\nEm = \u2211\ni\u2208Im\n(y(i) \u2212Om)2 .\n\u03bbM +\nM\n\u2211", "\u03bbM +\nM\n\u2211\nm=1\nEm ,\n\u03bb\n12.2.1.1 Building a tree\n So, we\u2019ll be greedy. We establish a criterion, given a set of data, for finding the best", "single split of that data, and then apply it recursively to partition the space. For the\ndiscussion below, we will select the partition of the data that minimizes the sum of the", "sum of squared errors of each partition element. Then later, we will consider other\nsplitting criteria.\nGiven a data set \n, we now consider  to be an\nindicator of the subset of elements within", "that we wish to build a tree (or subtree)\nfor. That is,  may already indicate a subset of data set \n, based on prior splits in\nconstructing our overall tree. We define terms as follows:", "indicates the set of examples (subset of ) whose feature value in dimension\n is greater than or equal to split point ;\n indicates the set of examples (subset of ) whose feature value in dimension", "is less than ;\n is the average  value of the data points indicated by set \n; and\n is the average  value of the data points indicated by set \n.", ".\nHere is the pseudocode. In what follows,  is the largest leaf size that we will allow\nin the tree, and is a hyperparameter of the algorithm.\nprocedure BuildTree(\n)\nif \n then\nreturn \nelse", "then\nreturn \nelse\nfor all split dimension , split value  do\nend for\nreturn \nend if\nend procedure\nIn practice, we typically start by calling BuildTree  with the first input equal to our", "whole data set (that is, with \n). But then that call of BuildTree  can\nrecursively lead to many other calls of BuildTree .\nLet\u2019s think about how long each call of BuildTree  takes to run. We have to", "consider all possible splits. So we consider a split in each of the  dimensions. In\neach dimension, we only need to consider splits between two data points (any other\nD = {(x(i), y(i))}, i = 1, \u2026 n\nI", "I\nD\nI\nD\nI +\nj,s\nI\nj\ns\nI \u2212\nj,s\nI\nj\ns\n^y+\nj,s\ny\nI +\nj,s\n^y\u2212\nj,s\ny\nI \u2212\nj,s\nk\n1:\nI, k\n2:\n|I| \u2264k\n3:\n^y \u2190\n1\n|I| \u2211i\u2208I y(i)\n4:\nLeaf(value = ^y)\n5:\n6:\nj\ns\n7:\nI +\nj,s \u2190{i \u2208I \u2223x(i)\nj\n\u2265s}\n8:\nI \u2212\nj,s \u2190{i \u2208I \u2223x(i)", "j,s \u2190{i \u2208I \u2223x(i)\nj\n< s}\n9:\n^y+\nj,s \u2190\n1\n|I +\nj,s| \u2211i\u2208I +\nj,s y(i)\n10:\n^y\u2212\nj,s \u2190\n1\n|I \u2212\nj,s| \u2211i\u2208I \u2212\nj,s y(i)\n11:\nEj,s \u2190\u2211i\u2208I +\nj,s(y(i) \u2212^y+\nj,s)2 + \u2211i\u2208I \u2212\nj,s(y(i) \u2212^y\u2212\nj,s)2\n12:\n13:", "j,s)2\n12:\n13:\n(j\u2217, s\u2217) \u2190arg minj,s Ej,s\n14:\n15:\nNode(j\u2217, s\u2217, BuildTree(I \u2212\nj\u2217,s\u2217, k), BuildTree(I +\nj\u2217,s\u2217, k))\n16:\n17:\nI = {1, \u2026 , n}\nd", "I = {1, \u2026 , n}\nd\n split will give the same error on the training data). So, in total, we consider \nsplits in each call to BuildTree .", "It might be tempting to regularize by using a somewhat large value of , or by\nstopping when splitting a node does not significantly decrease the error. One", "problem with short-sighted stopping criteria is that they might not see the value of\na split that will require one more split before it seems useful. So, we will tend to", "build a tree that is too large, and then prune it back.\nWe define cost complexity of a tree , where \n ranges over its leaves, as\nand", "and \n is the number of leaves. For a fixed , we can find a  that (approximately)\nminimizes \n by \u201cweakest-link\u201d pruning:\nCreate a sequence of trees by successively removing the bottom-level split that", "minimizes the increase in overall error, until the root is reached.\nReturn the  in the sequence that minimizes the cost complexity.\nWe can choose an appropriate  using cross validation.", "The strategy for building and pruning classification trees is very similar to the\nstrategy for regression trees.\nGiven a region \n corresponding to a leaf of the tree, we would pick the output", "class  to be the value that exists most frequently (the majority value) in the data\npoints whose  values are in that region, i.e., data points indicated by \n:", ":\nLet\u2019s now define the error in a region as the number of data points that do not have\nthe value \n:\nWe define the empirical probability of an item from class  occurring in region \n as:\nwhere", "as:\nwhere \n is the number of training points in region \n; that is, \n For later\nuse, we\u2019ll also define the empirical probabilities of split values, \n, as the", ", as the\nfraction of points with dimension  in split  occurring in region \n (one branch of\nO(dn)\n12.2.1.2 Pruning\nk\nT\nm\nC\u03b1(T) =\n|T|\n\u2211\nm=1\nEm(T) + \u03b1|T| ,\n|T|\n\u03b1\nT\nC\u03b1(T)\nT\n\u03b1\n12.2.2 Classification\nRm\ny\nx", "Rm\ny\nx\nIm\nOm = majorityi\u2208Im y(i) .\nOm\nEm = {i \u2223i \u2208Im and y(i) \u2260Om}\n.\n\u2223\u2223\nk\nm\n^Pm,k = ^P(Im, k) =\n{i \u2223i \u2208Im and y(i) = k}\nNm\n,\n\u2223\u2223\nNm\nm\nNm = |Im|.\n^Pm,j,s\nj\ns\nm\n the tree), and", "m\n the tree), and \n as the complement (the fraction of points in the other\nbranch).\nIn our greedy algorithm, we need a way to decide which split to make next. There", "are many criteria that express some measure of the \u201cimpurity\u201d in child nodes. Some\nmeasures include:\nMisclassification error:\nGini index:\nEntropy:\nSo that the entropy \n is well-defined when", ", we will stipulate that\n.\nThese splitting criteria are very similar, and it\u2019s not entirely obvious which one is\nbetter. We will focus on entropy, just to be concrete.", "Analogous to how for regression we choose the dimension  and split  that\nminimizes the sum of squared error \n, for classification, we choose the dimension", "and split  that minimizes the weighted average entropy over the \u201cchild\u201d data\npoints in each of the two corresponding splits, \n and \n. We calculate the entropy", "in each split based on the empirical probabilities of class memberships in the split,\nand then calculate the weighted average entropy \n as", "as\nChoosing the split that minimizes the entropy of the children is equivalent to\nmaximizing the information gain of the test \n, defined by", ", defined by\nIn the two-class case (with labels 0 and 1), all of the splitting criteria mentioned\nabove have the values\n1 \u2212^Pm,j,s\nSplitting criteria\nQm(T) = Em\nNm\n= 1 \u2212^Pm,Om\nQm(T) = \u2211\nk", "Qm(T) = \u2211\nk\n^Pm,k(1 \u2212^Pm,k)\nQm(T) = H(Im) = \u2212\u2211\nk\n^Pm,k log2 ^Pm,k\nH\n^P = 0\n0 log2 0 = 0\nj\ns\nEj,s\nj\ns\nI +\nj,s\nI \u2212\nj,s\n^H\n^H = (fraction of points in left data set) \u22c5H(I \u2212\nj,s)", "j,s)\n+(fraction of points in right data set) \u22c5H(I +\nj,s)\n= (1 \u2212^Pm,j,s)H(I \u2212\nj,s) + ^Pm,j,sH(I +\nj,s)\n=\n|I \u2212\nj,s|\nNm\n\u22c5H(I \u2212\nj,s) +\n|I +\nj,s|\nNm\n\u22c5H(I +\nj,s) .\nxj = s\ninfoGain(xj = s, Im) =\nH(Im) \u2212(", "H(Im) \u2212(\n|I \u2212\nj,s|\nNm\n\u22c5H(I \u2212\nj,s) +\n|I +\nj,s|\nNm\n\u22c5H(I +\nj,s))\n{\n The respective impurity curves are shown below, where \n; the vertical axis\nplots \n for each of the three criteria.", "There used to be endless haggling about which impurity function one should use. It\nseems to be traditional to use entropy to select which node to split while growing the", "tree, and misclassification error in the pruning criterion.\nOne important limitation or drawback in conventional trees is that they can have", "high estimation error: small changes in the data can result in very big changes in the\nresulting tree.\nBootstrap aggregation is a technique for reducing the estimation error of a non-linear", "predictor, or one that is adaptive to the data. The key idea applied to trees, is to\nbuild multiple trees with different subsets of the data, and then create an ensemble", "model that combines the results from multiple trees to make a prediction.\nConstruct \n new data sets of size . Each data set is constructed by sampling \ndata points with replacement from", ". A single data set is called bootstrap sample\nof \n.\nTrain a predictor \n on each bootstrap sample.\nRegression case: bagged predictor is\nClassification case: Let", "be the number of classes. We find a majority bagged\npredictor as follows. We let \n be a \u201cone-hot\u201d vector with a single 1 and\n{\n.\n0.0\nwhen  ^Pm,0 = 0.0\n0.0\nwhen  ^Pm,0 = 1.0\np = ^Pm,0\nQm(T)", "p = ^Pm,0\nQm(T)\n12.2.3 Bagging\nB\nn\nn\nD\nD\n^f b(x)\n^fbag(x) = 1\nB\nB\n\u2211\nb=1\n^f b(x) .\nK\n^f b(x)\n  zeros, and define the predicted output  for predictor \n as\n. Then", "as\n. Then\nwhich is a vector containing the proportion of classifiers that predicted each\nclass  for input . Then the overall predicted output is", "There are theoretical arguments showing that bagging does, in fact, reduce\nestimation error. However, when we bag a model, any simple intrepetability is lost.", "Random forests are collections of trees that are constructed to be de-correlated, so\nthat using them to vote gives maximal advantage. In competitions, they often have", "excellent classification performance among large collections of much fancier\nmethods.\nIn what follows, \n, \n, and  are hyperparameters of the algorithm.\nprocedure RandomForest(\n)\nfor \n to  do", ")\nfor \n to  do\nDraw a bootstrap sample \n of size  from \nGrow tree \n on \n:\nwhile there are splittable nodes do\nSelect \n variables at random from the  total variables", "Pick the best variable and split point among those \nSplit the current node\nend while\nend for\nreturn \nend procedure\nGiven the ensemble of trees, vote to make a prediction on a new .", "There are many variations on the tree theme. One is to employ different regression\nor classification methods in each leaf. For example, a linear regression might be", "used to model the examples in each leaf, rather than using a constant value.\nIn the relatively simple trees that we\u2019ve considered, splits have been based on only", "a single feature at a time, and with the resulting splits being axis-parallel. Other\nmethods for splitting are possible, including consideration of multiple features and", "linear classifiers based on those, potentially resulting in non-axis-parallel splits.\nComplexity is a concern in such cases, as many possible combinations of features\nK \u22121\n^y\nf b", "K \u22121\n^y\nf b\n^yb(x) = arg maxk ^f b(x)k\n^fbag(x) = 1\nB\nB\n\u2211\nb=1\n^f b(x),\nk\nx\n^ybag(x) = arg max\nk\n^fbag(x)k .\n12.2.4 Random Forests\nB m\nn\n1:\nB, m, n\n2:\nb = 1\nB\n3:\nDb\nn\nD\n4:\nTb\nDb\n5:\n6:\nm\nd\n7:\nm\n8:\n9:", "6:\nm\nd\n7:\nm\n8:\n9:\n10:\n11:\n12:\n{Tb}B\nb=1\n13:\nx\n12.2.5 Tree variants and tradeoffs\n may need to be considered, to select the best variable combination (rather than a\nsingle split variable).", "Another generalization is a hierarchical mixture of experts, where we make a \u201csoft\u201d\nversion of trees, in which the splits are probabilistic (so every point has some degree", "of membership in every leaf). Such trees can be trained using a form of gradient\ndescent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,", "gradient boosted trees) and implementations are readily available (e.g., XGBoost).\nTrees have a number of strengths, and remain a valuable tool in the machine", "learning toolkit. Some benefits include being relatively easy to interpret, fast to\ntrain, and ability to handle multi-class classification in a natural way. Trees can", "easily handle different loss functions; one just needs to change the predictor and\nloss being applied in the leaves. Methods also exist to identify which features are", "particularly important or influential in forming the tree, which can aid in human\nunderstanding of the data set. Finally, in many situations, trees perform", "surprisingly well, often comparable to more complicated regression or classification\nmodels. Indeed, in some settings it is considered good practice to start with trees", "(especially random forest or boosted trees) as a \u201cbaseline\u201d machine learning model,\nagainst which one can evaluate performance of more sophisticated models.", "While tree-based methods excel at supervised learning tasks, we now turn to\nanother important class of non-parametric methods that focus on discovering", "structure in unlabeled data. These clustering methods share some conceptual\nsimilarities with tree-based approaches - both aim to partition the input space into", "meaningful regions - but clustering methods operate without supervision, making\nthem particularly valuable for exploratory data analysis and pattern discovery.\n12.3 -means Clustering", "Clustering is an unsupervised learning method where we aim to discover\nmeaningful groupings or categories in a dataset based on patterns or similarities", "within the data itself, without relying on pre-assigned labels. It is widely used for\nexploratory data analysis, pattern recognition, and segmentation tasks, allowing us", "to interpret and manage complex datasets by uncovering hidden structures and\nrelationships.\nOftentimes a dataset can be partitioned into different categories. A doctor may", "notice that their patients come in cohorts and different cohorts respond to different\ntreatments. A biologist may gain insight by identifying that bats and whales,", "despite outward appearances, have some underlying similarity, and both should be\nconsidered members of the same category, i.e., \u201cmammal\u201d. The problem of", "automatically identifying meaningful groupings in datasets is called clustering.\nOnce these groupings are found, they can be leveraged toward interpreting the data", "and making optimal decisions for each group.\nk\n Mathematically, clustering looks a bit like classification: we wish to find a mapping", "from datapoints, , to categories, . However, rather than the categories being\npredefined labels, the categories in clustering are automatically discovered partitions\nof an unlabeled dataset.", "Because clustering does not learn from labeled examples, it is an example of an\nunsupervised learning algorithm. Instead of mimicking the mapping implicit in\nsupervised training pairs", ", clustering assigns datapoints to categories\nbased on how the unlabeled data \n is distributed in data space.\nIntuitively, a \u201ccluster\u201d is a group of datapoints that are all nearby to each other and", "far away from other clusters. Let\u2019s consider the following scatter plot. How many\nclusters do you think there are?\nThere seem to be about five clumps of datapoints and those clumps are what we", "would like to call clusters. If we assign all datapoints in each clump to a cluster\ncorresponding to that clump, then we might desire that nearby datapoints are", "assigned to the same cluster, while far apart datapoints are assigned to different\nclusters.\nIn designing clustering algorithms, three critical things we need to decide are:", "How do we measure distance between datapoints? What counts as \u201cnearby\u201d\nand \u201cfar apart\u201d?\nHow many clusters should we look for?\nHow do we evaluate how good a clustering is?", "We will see how to begin making these decisions as we work through a concrete\nclustering algorithm in the next section.\nOne of the simplest and most commonly used clustering algorithms is called k-", "means. The goal of the k-means algorithm is to assign datapoints to  clusters in\nsuch a way that the variance within clusters is as small as possible. Notice that this\n12.3.1 Clustering formalisms\nx", "x\ny\n{x(i), y(i)}n\ni=1\n{x(i)}n\ni=1\n12.3.2 The k-means formulation\nk\nFigure 12.1: A dataset we would\nlike to cluster. How many clusters\ndo you think there are?", "matches our intuitive idea that a cluster should be a tightly packed set of\ndatapoints.\nSimilar to the way we showed that supervised learning could be formalized", "mathematically as the minimization of an objective function (loss function +\nregularization), we will show how unsupervised learning can also be formalized as", "minimizing an objective function. Let us denote the cluster assignment for a\ndatapoint \n as \n, i.e., \n means we are assigning datapoint", "to cluster number 1. Then the k-means objective can be quantified with the\nfollowing objective function (which we also call the \u201ck-means loss\u201d):\nwhere \n and \n, so that \n is the", ", so that \n is the\nmean of all datapoints in cluster , and using \n to denote the indicator function\n(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum", "(over data points) of the loss is the variance of datapoints within cluster . We sum\nup the variance of all  clusters to get our overall loss.", "The k-means algorithm minimizes this loss by alternating between two steps: given\nsome initial cluster assignments: 1) compute the mean of all data in each cluster and", "assign this as the \u201ccluster mean\u201d, and 2) reassign each datapoint to the cluster with\nnearest cluster mean. Figure 12.2 shows what happens when we repeat these steps\non the dataset from above.", "Each time we reassign the data to the nearest cluster mean, the k-means loss\ndecreases (the datapoints end up closer to their assigned cluster mean), or stays the", "same. And each time we recompute the cluster means the loss also decreases (the\nmeans end up closer to their assigned datapoints) or stays the same. Overall then,", "the clustering gets better and better, according to our objective \u2013 until it stops\nimproving.\nAfter four iterations of cluster assignment + update means in our example, the k-", "means algorithm stops improving. We say it has converged, and its final solution is\nshown in Figure 12.3.\nx(i)\ny(i) \u2208{1, 2, \u2026 , k}\ny(i) = 1\nx(i)\nk\n\u2211\nj=1\nn\n\u2211\ni=1\n\ud835\udfd9(y(i) = j) x(i) \u2212\u03bc(j)\n2 ,\n\u2225\u2225\n(12.1)", "2 ,\n\u2225\u2225\n(12.1)\n\u03bc(j) =\n1\nNj \u2211n\ni=1 \ud835\udfd9(y(i) = j)x(i)\nNj = \u2211n\ni=1 \ud835\udfd9(y(i) = j)\n\u03bc(j)\nj\n\ud835\udfd9(\u22c5)\nj\nk\n12.3.2.0.0.1 K-means algorithm\nFigure 12.2: The first three steps of\nrunning the k-means algorithm on", "this data. Datapoints are colored\naccording to the cluster to which\nthey are assigned. Cluster means\nare the larger X\u2019s with black\noutlines.", "outlines.\n It seems to converge to something reasonable! Now let\u2019s write out the algorithm in\ncomplete detail:\nprocedure KMeans(\n)\nInitialize centroids \n and assignments \n randomly\nfor \n to  do\nfor", "for \n to  do\nfor \n to  do\nend for\nfor \n to  do\nend for\nif \n then\nbreak//convergence\nend if\nend for\nreturn \nend procedure", "end procedure\nThe for-loop over the  datapoints assigns each datapoint to the nearest cluster\ncenter. The for-loop over the k clusters updates the cluster center to be the mean of", "all datapoints currently assigned to that cluster. As suggested above, it can be\nshown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it", "converges to a local minimum of the loss.\nIt\u2019s like classification except it picked what the classes are rather than being given\nexamples of what the classes are.", "We can also use gradient descent to optimize the k-means objective. To show how to\napply gradient descent, we first rewrite the objective as a differentiable function\nonly of :", "only of :\n is the value of the k-means loss given that we pick the optimal assignments of\nthe datapoints to cluster means (that\u2019s what the \n does). Now we can use the\ngradient", "gradient \n to find the values for  that achieve minimum loss when cluster\n1:\nk, \u03c4, {x(i)}n\ni=1\n2:\n\u03bc(1), \u2026 , \u03bc(k)\ny(1), \u2026 , y(n)\n3:\nt = 1\n\u03c4\n4:\nyold \u2190y\n5:\ni = 1\nn\n6:\ny(i) \u2190arg minj\u2208{1,\u2026,k} x(i) \u2212\u03bc(j)\n2", "2\n\u2225\u2225\n7:\n8:\nj = 1\nk\n9:\nNj \u2190\u2211n\ni=1 \ud835\udfd9(y(i) = j)\n10:\n\u03bc(j) \u2190\n1\nNj \u2211n\ni=1 \ud835\udfd9(y(i) = j) x(i)\n11:\n12:\ny = yold\n13:\n14:\n15:\n16:\n17:\n\u03bc, y\n18:\nn\n12.3.2.0.0.2 Using gradient descent to minimize k-means objective", "\u03bc\nL(\u03bc) =\nn\n\u2211\ni=1\nmin\nj\nx(i) \u2212\u03bc(j)\n2 .\n\u2225\u2225\nL(\u03bc)\nminj\n\u2202L(\u03bc)\n\u2202\u03bc\n\u03bc\nFigure 12.3: Converged result.\n assignments are optimal. Finally, we read off the optimal cluster assignments, given", "the optimized , just by assigning datapoints to their nearest cluster mean:\nThis procedure yields a local minimum of Equation 12.1, as does the standard k-", "means algorithm we presented (though they might arrive at different solutions). It\nmight not be the global optimum since the objective is not convex (due to \n, as", ", as\nthe minimum of multiple convex functions is not necessarily convex).\nThe standard k-means algorithm, as well as the variant that uses gradient descent,", "both are only guaranteed to converge to a local minimum, not necessarily the global\nminimum of the loss. Thus the answer we get out depends on how we initialize the", "cluster means. Figure 12.4 is an example of a different initialization on our toy data,\nwhich results in a worse converged clustering:", "A variety of methods have been developed to pick good initializations (see, for\nexample, the k-means++ algorithm). One simple option is to run the standard k-", "means algorithm multiple times, with different random initial conditions, and then\npick from these the clustering that achieves the lowest k-means loss.", "A very important parameter in cluster algorithms is the number of clusters we are\nlooking for. Some advanced algorithms can automatically infer a suitable number of", "clusters, but most of the time, like with k-means, we will have to pick  \u2013 it\u2019s a\nhyperparameter of the algorithm.\nFigure 12.5 shows an example of the effect. Which result looks more correct? It can", "be hard to say! Using higher k we get more clusters, and with more clusters we can\nachieve lower within-cluster variance \u2013 the k-means objective will never increase,\n\u03bc\ny(i) = arg min\nj\nx(i) \u2212\u03bc(j)\n2 .", "j\nx(i) \u2212\u03bc(j)\n2 .\n\u2225\u2225\nminj\n12.3.2.0.0.3 Importance of initialization\n12.3.2.0.0.4 Importance of k\nk\nFigure 12.4: With the initialization\nof the means to the left, the yellow", "and red means end up splitting\nwhat perhaps should be one cluster\nin half.\nFigure 12.5: Example of k-means\nrun on our toy data, with two\ndifferent values of k. Setting k=4,", "on the left, results in one cluster\nbeing merged, compared to setting\nk=5, on the right. Which clustering\ndo you think is better? How could\nyou decide?", "you decide?\n and will typically strictly decrease as we increase k. Eventually, we can increase k to\nequal the total number of datapoints, so that each datapoint is assigned to its own", "cluster. Then the k-means objective is zero, but the clustering reveals nothing.\nClearly, then, we cannot use the k-means objective itself to choose the best value for", "k. In Section 1.3, we will discuss some ways of evaluating the success of clustering\nbeyond its ability to minimize the k-means objective, and it\u2019s with these sorts of", "methods that we might decide on a proper value of k.\nAlternatively, you may be wondering: why bother picking a single k? Wouldn\u2019t it be", "nice to reveal a hierarchy of clusterings of our data, showing both coarse and fine\ngroupings? Indeed hierarchical clustering is another important class of clustering", "algorithms, beyond k-means. These methods can be useful for discovering tree-like\nstructure in data, and they work a bit like this: initially a coarse split/clustering of", "the data is applied at the root of the tree, and then as we descend the tree we split\nand cluster the data in ever more fine-grained ways. A prototypical example of", "hierarchical clustering is to discover a taxonomy of life, where creatures may be\ngrouped at multiple granularities, from species to families to kingdoms. You may", "find a suite of clustering algorithms in SKLEARN\u2019s cluster module.\nClustering algorithms group data based on a notion of similarity, and thus we need", "to define a distance metric between datapoints. This notion will also be useful in\nother machine learning approaches, such as nearest-neighbor methods that we see", "in Chapter 12. In k-means and other methods, our choice of distance metric can\nhave a big impact on the results we will find.\nOur k-means algorithm uses the Euclidean distance, i.e., \n, with a loss", ", with a loss\nfunction that is the square of this distance. We can modify k-means to use different\ndistance metrics, but a more common trick is to stick with Euclidean distance but", "measured in a feature space. Just like we did for regression and classification\nproblems, we can define a feature map from the data to a nicer feature\nrepresentation,", "representation, \n, and then apply k-means to cluster the data in the feature\nspace.\nAs a simple example, suppose we have two-dimensional data that is very stretched", "out in the first dimension and has less dynamic range in the second dimension.\nThen we may want to scale the dimensions so that each has similar dynamic range,", "prior to clustering. We could use standardization, like we did in Chapter 5.\nIf we want to cluster more complex data, like images, music, chemical compounds,", "etc., then we will usually need more sophisticated feature representations. One\ncommon practice these days is to use feature representations learned with a neural", "network. For example, we can use an autoencoder to compress images into feature\nvectors, then cluster those feature vectors.\n12.3.2.0.0.5 k-means in feature space\nx(i) \u2212\u03bc(j)\n\u2225\u2225\n\u03d5(x)", "x(i) \u2212\u03bc(j)\n\u2225\u2225\n\u03d5(x)\n12.3.3 How to evaluate clustering algorithms\n One of the hardest aspects of clustering is knowing how to evaluate it. This is", "actually a big issue for all unsupervised learning methods, since we are just looking\nfor patterns in the data, rather than explicitly trying to predict target values (which", "was the case with supervised learning).\nRemember, evaluation metrics are not the same as loss functions, so we can\u2019t just", "measure success by looking at the k-means loss. In prediction problems, it is critical\nthat the evaluation is on a held-out test set, while the loss is computed over training", "data. If we evaluate on training data we cannot detect overfitting. Something\nsimilar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too", "large can precisely \u201cfit\u201d the data (minimize the loss), but yields no general insight.\nOne way to evaluate our clusters is to look at the consistency with which they are", "found when we run on different subsamples of our training data, or with different\nhyperparameters of our clustering algorithm (e.g., initializations). For example, if", "running on several bootstrapped samples (random subsets of our data) results in\nvery different clusters, it should call into question the validity of any of the\nindividual results.", "individual results.\nIf we have some notion of what ground truth clusters should be, e.g., a few data\npoints that we know should be in the same cluster, then we can measure whether or", "not our discovered clusters group these examples correctly.\nClustering is often used for visualization and interpretability, to make it easier for", "humans to understand the data. Here, human judgment may guide the choice of\nclustering algorithm. More quantitatively, discovered clusters may be used as input", "to downstream tasks. For example, as we saw in the lab, we may fit a different\nregression function on the data within each cluster. Figure 12.6 gives an example", "where this might be useful. In cases like this, the success of a clustering algorithm\ncan be indirectly measured based on the success of the downstream application", "(e.g., does it make the downstream predictions more accurate).\nFigure 12.6: Averaged across the\nwhole population, risk of heart\ndisease positively correlates with\nhours of exercise. However, if we", "cluster the data, we can observe\nthat there are four subgroups of the\npopulation which correspond to\ndifferent age groups, and within\neach subgroup the correlation is\nnegative. We can make better", "predictions, and better capture the\npresumed true effect, if we cluster\nthis data and then model the trend\nin each cluster separately.", "What are some conventions for derivatives of matrices and vectors? It will always\nwork to explicitly write all indices and treat everything as scalars, but we", "introduce here some shortcuts that are often faster to use and helpful for\nunderstanding.\nThere are at least two consistent but different systems for describing shapes and", "rules for doing matrix derivatives. In the end, they all are correct, but it is\nimportant to be consistent.\nWe will use what is often called the \u2018Hessian\u2019 or denominator layout, in which we", "say that for\n of size \n and  of size \n, \n is a matrix of size \n with the \nentry \n. This denominator layout convention has been adopted by the field", "of machine learning to ensure that the shape of the gradient is the same as the\nshape of the respective derivative. This is somewhat controversial at large, but alas,", "we shall continue with denominator layout.\nThe discussion below closely follows the Wikipedia on matrix derivatives.\nA.1 The shapes of things\nHere are important special cases of the rule above:", "Scalar-by-scalar: For  of size \n and  of size \n, \n is the (scalar)\npartial derivative of  with respect to .\nScalar-by-vector: For  of size \n and  of size \n, \n (also written", ", \n (also written\n, the gradient of  with respect to ) is a column vector of size \n with\nthe \n entry \n:\nVector-by-scalar: For  of size \n and  of size \n, \n is a row\nvector of size \n with the \n entry", "with the \n entry \n:\nVector-by-vector: For  of size \n and  of size \n, \n is a matrix of\nsize \n with the \n entry \n:\nAppendix A \u2014 Matrix derivative common\ncases\nx\nn \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\nn \u00d7 m\n(i, j)", "n \u00d7 m\n(i, j)\n\u2202yj/\u2202xi\nx\n1 \u00d7 1\ny\n1 \u00d7 1 \u2202y/\u2202x\ny\nx\nx\nn \u00d7 1\ny\n1 \u00d7 1 \u2202y/\u2202x\n\u2207xy\ny\nx\nn \u00d7 1\nith\n\u2202y/\u2202xi\n\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y/\u2202x1\n\u2202y/\u2202x2\n\u22ee\n\u2202y/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nx\n1 \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\n1 \u00d7 m\njth\n\u2202yj/\u2202x\n\u2202y/\u2202x = [\n].\n\u2202y1/\u2202x", "\u2202y/\u2202x = [\n].\n\u2202y1/\u2202x\n\u2202y2/\u2202x\n\u22ef\n\u2202ym/\u2202x\nx\nn \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\nn \u00d7 m\n(i, j)\n\u2202yj/\u2202xi\n\uf461Appendices > A  Matrix derivative common cases\n\uf52a\n1\n Scalar-by-matrix: For \n of size \n and  of size \n, \n (also written", ", \n (also written\n, the gradient of  with respect to \n) is a matrix of size \n with the\n entry \n:\nYou may notice that in this list, we have not included matrix-by-matrix, matrix-by-", "vector, or vector-by-matrix derivatives. This is because, generally, they cannot be\nexpressed nicely in matrix form and require higher order objects (e.g., tensors) to", "represent their derivatives. These cases are beyond the scope of this course.\nAdditionally, notice that for all cases, you can explicitly compute each element of", "the derivative object using (scalar) partial derivatives. You may find it useful to\nwork through some of these by hand as you are reviewing matrix derivatives.\nA.2 Some vector-by-vector identities", "Here are some examples of \n. In each case, assume  is \n,  is \n,  is\na scalar constant,  is a vector that does not depend on  and \n is a matrix that", "is a matrix that\ndoes not depend on ,  and  are scalars that do depend on , and  and  are\nvectors that do depend on . We also have vector-valued functions  and .", "First, we will cover a couple of fundamental cases: suppose that  is an \nvector which is not a function of , an \n vector. Then,\nis an", "is an \n matrix of 0s. This is similar to the scalar case of differentiating a\nconstant. Next, we can consider the case of differentiating a vector with respect to\nitself:\nThis is the", "This is the \n identity matrix, with 1\u2019s along the diagonal and 0\u2019s elsewhere. It\nmakes sense, because \n is 1 for \n and 0 otherwise. This identity is also\nsimilar to the scalar case.\n\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3", "\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y1/\u2202x1\n\u2202y2/\u2202x1\n\u22ef\n\u2202ym/\u2202x1\n\u2202y1/\u2202x2\n\u2202y2/\u2202x2\n\u22ef\n\u2202ym/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202y1/\u2202xn\n\u2202y2/\u2202xn\n\u22ef\n\u2202ym/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nX\nn \u00d7 m\ny\n1 \u00d7 1 \u2202y/\u2202X\n\u2207Xy\ny\nX\nn \u00d7 m\n(i, j)\n\u2202y/\u2202Xi,j\n\u2202y/\u2202X =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y/\u2202X1,1\n\u22ef\n\u2202y/\u2202X1,m\n\u22ee\n\u22f1", "\u22ef\n\u2202y/\u2202X1,m\n\u22ee\n\u22f1\n\u22ee\n\u2202y/\u2202Xn,1\n\u22ef\n\u2202y/\u2202Xn,m\n\u23a4\n\u23a5\n\u23a6\n\u2202y/\u2202x\nx\nn \u00d7 1 y\nm \u00d7 1 a\na\nx\nA\nx u\nv\nx\nu\nv\nx\nf\ng\nA.2.1 Some fundamental cases\na\nm \u00d7 1\nx\nn \u00d7 1\n\u2202a\n\u2202x = 0,\n(A.1)\nn \u00d7 m\n\u2202x\n\u2202x = I\nn \u00d7 n\n\u2202xj/xi\ni = j", "n \u00d7 n\n\u2202xj/xi\ni = j\n Let the dimensions of \n be \n. Then the object \n is an \n vector. We can\nthen compute the derivative of \n with respect to  as:\nNote that any element of the column vector", "can be written as, for \n:\nThus, computing the \n entry of \n requires computing the partial derivative\nTherefore, the \n entry of \n is the \n entry of \n:\nSimilarly, for objects", "of the same shape, one can obtain,\nSuppose that \n are both vectors of size \n. Then,\nSuppose that  is a scalar constant and  is an \n vector that is a function of .\nThen,", "Then,\nOne can extend the previous identity to vector- and matrix-valued constants.\nSuppose that  is a vector with shape \n and  is a scalar which depends on .\nThen,\nFirst, checking dimensions, \n is", "is \n and  is \n so \n is \n and our\nanswer is \n as it should be. Now, checking a value, element \n of the\nA.2.2 Derivatives involving a constant matrix\nA\nm \u00d7 n\nAx\nm \u00d7 1\nAx\nx\n\u2202Ax\n\u2202x\n=\n\u23a1\n\u23a2\n\u23a3\n\u2202(Ax)1/\u2202x1", "=\n\u23a1\n\u23a2\n\u23a3\n\u2202(Ax)1/\u2202x1\n\u2202(Ax)2/\u2202x1\n\u22ef\n\u2202(Ax)m/\u2202x1\n\u2202(Ax)1/\u2202x2\n\u2202(Ax)2/\u2202x2\n\u22ef\n\u2202(Ax)m/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202(Ax)1/\u2202xn\n\u2202(Ax)2/\u2202xn\n\u22ef\n\u2202(Ax)m/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nAx\nj = 1, \u2026 , m\n(Ax)j =\nn\n\u2211\nk=1\nAj,kxk.\n(i, j)\n\u2202Ax\n\u2202x\n\u2202(Ax)j/\u2202xi :", "\u2202Ax\n\u2202x\n\u2202(Ax)j/\u2202xi :\n\u2202(Ax)j/\u2202xi = \u2202(\nn\n\u2211\nk=1\nAj,kxk)/\u2202xi = Aj,i\n(i, j)\n\u2202Ax\n\u2202x\n(j, i)\nA\n\u2202Ax\n\u2202x\n= AT\n(A.2)\nx, A\n\u2202xTA\n\u2202x\n= A\n(A.3)\nA.2.3 Linearity of derivatives\nu, v\nm \u00d7 1\n\u2202(u + v)\n\u2202x\n= \u2202u\n\u2202x + \u2202v\n\u2202x", "\u2202x\n= \u2202u\n\u2202x + \u2202v\n\u2202x\n(A.4)\na\nu\nm \u00d7 1\nx\n\u2202au\n\u2202x = a \u2202u\n\u2202x\na\nm \u00d7 1\nv\nx\n\u2202va\n\u2202x = \u2202v\n\u2202x aT\n\u2202v/\u2202x\nn \u00d7 1\na\nm \u00d7 1\naT\n1 \u00d7 m\nn \u00d7 m\n(i, j)\n answer is \n = \n which corresponds to element \n of\n.", "of\n.\nSimilarly, suppose that \n is a matrix which does not depend on  and  is a\ncolumn vector which does depend on . Then,", "Suppose that  is a scalar which depends on , while  is a column vector of shape\n and  is a column vector of shape \n. Then,\nOne can see this relationship by expanding the derivative as follows:", "Then, one can use the product rule for scalar-valued functions,\nto obtain the desired result.\nSuppose that  is a vector-valued function with output vector of shape \n, and", ", and\nthe argument to  is a column vector  of shape \n which depends on . Then,\none can obtain the chain rule as,\nFollowing \u201cthe shapes of things,\u201d \n is \n and \n is \n, where\nelement \n is", "element \n is \n. The same chain rule applies for further compositions\nof functions:\nA.3 Some other identities\nYou can get many scalar-by-vector and vector-by-scalar cases as special cases of the", "rules above, making one of the relevant vectors just be 1 x 1. Here are some other\nones that are handy. For more, see the Wikipedia article on Matrix derivatives (for", "consistency, only use the ones in denominator layout).\n\u2202vaj/\u2202xi\n(\u2202v/\u2202xi)aj\n(i, j)\n(\u2202v/\u2202x)aT\nA\nx\nu\nx\n\u2202Au\n\u2202x\n= \u2202u\n\u2202x AT\nA.2.4 Product rule (vector-valued numerator)\nv\nx\nu\nm \u00d7 1\nx\nn \u00d7 1\n\u2202vu\n\u2202x = v \u2202u", "n \u00d7 1\n\u2202vu\n\u2202x = v \u2202u\n\u2202x + \u2202v\n\u2202x uT\n\u2202vu\n\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202(vu1)/\u2202x1\n\u2202(vu2)/\u2202x1\n\u22ef\n\u2202(vum)/\u2202x1\n\u2202(vu1)/\u2202x2\n\u2202(vu2)/\u2202x2\n\u22ef\n\u2202(vum)/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202(vu1)/\u2202xn\n\u2202(vu2)/\u2202xn\n\u22ef\n\u2202(vum)/\u2202xn\n\u23a4\n\u23a5\n\u23a6", "\u22ef\n\u2202(vum)/\u2202xn\n\u23a4\n\u23a5\n\u23a6\n\u2202(vuj)/\u2202xi = v(\u2202uj/\u2202xi) + (\u2202v/\u2202xi)uj,\nA.2.5 Chain rule\ng\nm \u00d7 1\ng\nu\nd \u00d7 1\nx\n\u2202g(u)\n\u2202x\n= \u2202u\n\u2202x\n\u2202g(u)\n\u2202u\n\u2202u/\u2202x\nn \u00d7 d\n\u2202g(u)/\u2202u\nd \u00d7 m\n(i, j)\n\u2202g(u)j/\u2202ui\n\u2202f(g(u))\n\u2202x\n= \u2202u\n\u2202x\n\u2202g(u)\n\u2202u\n\u2202f(g)", "\u2202x\n\u2202g(u)\n\u2202u\n\u2202f(g)\n\u2202g\nT\n A.4 Derivation of gradient for linear regression\nRecall here that \n is a matrix of of size \n and \n is an \n vector.", "is an \n vector.\nApplying identities Equation A.3, Equation A.5,Equation A.4, Equation A.2,\nEquation A.1\nA.5 Matrix derivatives using Einstein summation", "You do not have to read or learn this! But you might find it interesting or helpful.\nConsider the objective function for linear regression, written out as products of\nmatrices:\nwhere \n is \n, \n is", "where \n is \n, \n is \n, and  is \n. How does one show, with no\nshortcuts, that\nOne neat way, which is very explicit, is to simply write all the matrices as variables\nwith row and column indices, e.g.,", "is the row , column  entry of the matrix\n. Furthermore, let us use the convention that in any product, all indices which\nappear more than once get summed over; this is a popular convention in", "theoretical physics, and lets us suppress all the summation symbols which would\notherwise clutter the following expresssions. For example, \n would be the", "would be the\nimplicit summation notation giving the element at the \n row of the matrix-vector\nproduct \n.\nUsing implicit summation notation with explicit indices, we can rewrite \n as\n\u2202uTv\n\u2202x\n= \u2202u", "as\n\u2202uTv\n\u2202x\n= \u2202u\n\u2202x v + \u2202v\n\u2202x u\n(A.5)\n\u2202uT\n\u2202x\n= ( \u2202u\n\u2202x )\nT\n(A.6)\nX\nn \u00d7 d\nY\nn \u00d7 1\n\u2202(X\u03b8 \u2212Y)T(X\u03b8 \u2212Y)/n\n\u2202\u03b8\n= 2\nn\n\u2202(X\u03b8 \u2212Y)\n\u2202\u03b8\n(X\u03b8 \u2212Y)\n= 2\nn ( \u2202X\u03b8\n\u2202\u03b8\n\u2212\u2202Y\n\u2202\u03b8 )(X\u03b8 \u2212Y)\n= 2\nn (XT \u22120)(X\u03b8 \u2212Y)\n= 2\nn XT(X\u03b8 \u2212Y)", "= 2\nn XT(X\u03b8 \u2212Y)\nJ(\u03b8) = 1\nn (X\u03b8 \u2212Y )T(X\u03b8 \u2212Y ) ,\nX\nn \u00d7 d Y\nn \u00d7 1\n\u03b8\nd \u00d7 1\n\u2207\u03b8J = 2\nn XT(X\u03b8 \u2212Y ) ?\nXab\na\nb\nX\nXab\u03b8b\nath\nX\u03b8\nJ(\u03b8)\nJ(\u03b8) = 1\nn (Xab\u03b8b \u2212Ya) (Xac\u03b8c \u2212Ya) .", "Note that we no longer need the transpose on the first term, because all that\ntranspose accomplished was to take a dot product between the vector given by the", "left term, and the vector given by the right term. With implicit summation, this is\naccomplished by the two terms sharing the repeated index .\nTaking the derivative of  with respect to the", "element of  thus gives, using the\nchain rule for (ordinary scalar) multiplication:\nwhere the second line follows from the first, with the definition that \n only\nwhen \n (and similarly for", "). And the third line follows from the second by\nrecognizing that the two terms in the second line are identical. Now note that in\nthis implicit summation notation, the", "element of the matrix product of  and\n is \n. That is, ordinary matrix multiplication sums over indices\nwhich are adjacent to each other, because a row of  times a column of \n becomes", "becomes\na scalar number. So the term in the above equation with \n is not a matrix\nproduct of \n with \n. However, taking the transpose \n switches row and column\nindices, so \n. And", "indices, so \n. And \n is a matrix product of \n with \n! Thus, we\nhave that\nwhich is the desired result.\na\nJ\ndth\n\u03b8\ndJ\nd\u03b8d\n=\n1\nn [Xab\u03b4bd (Xac\u03b8c \u2212Ya) + (Xab\u03b8b \u2212Ya)Xac\u03b4cd]\n=\n1", "=\n1\nn [Xad (Xac\u03b8c \u2212Ya) + (Xab\u03b8b \u2212Ya)Xad]\n=\n2\nn Xad (Xab\u03b8b \u2212Ya) ,\n\u03b4bd = 1\nb = d\n\u03b4cd\na, b\nA\nB\n(AB)ac = AabBbc\nA\nB\nXadXab\nX\nX\nXT\nXad = X T\nda\nX T\ndaXab\nXT\nX\ndJ\nd\u03b8d\n= 2\nn X T\nda (Xab\u03b8b \u2212Ya)\n= 2", "da (Xab\u03b8b \u2212Ya)\n= 2\nn [XT (X\u03b8 \u2212Y )]d ,\n B.1 Strategies towards adaptive step-size\nWe\u2019ll start by looking at the notion of a running average. It\u2019s a computational", "strategy for estimating a possibly weighted average of a sequence of data. Let our\ndata sequence be \n; then we define a sequence of running average values,\n using the equations\nwhere \n. If", "where \n. If \n is a constant, then this is a moving average, in which\nSo, you can see that inputs \n closer to the end of the sequence have more effect on\n than early inputs.\nIf, instead, we set", ", then we get the actual average.\n\u2753 Study Question\nProve to yourself that the previous assertion holds.\nNow, we can use methods that are a bit like running averages to describe strategies", "for computing . The simplest method is momentum, in which we try to \u201caverage\u201d\nrecent gradient updates, so that if they have been bouncing back and forth in some", "direction, we take out that component of the motion. For momentum, we have\nAppendix B \u2014 Optimizing Neural\nNetworks\nB.1.1 Running averages\nc1, c2, \u2026\nC0, C1, C2, \u2026\nC0 = 0,\nCt = \u03b3t Ct\u22121 + (1 \u2212\u03b3t) ct,", "\u03b3t \u2208(0, 1)\n\u03b3t\nCT = \u03b3 CT\u22121 + (1 \u2212\u03b3) cT\n= \u03b3(\u03b3 CT\u22122 + (1 \u2212\u03b3) cT\u22121) + (1 \u2212\u03b3) cT\n=\nT\n\u2211\nt=1\n\u03b3 T\u2212t(1 \u2212\u03b3) ct.\nct\nCT\n\u03b3t = t\u22121\nt\nB.1.2 Momentum\n\u03b7\nV0 = 0,\nVt = \u03b3 Vt\u22121 + \u03b7 \u2207WJ(Wt\u22121),\nWt = Wt\u22121 \u2212Vt.", "Wt = Wt\u22121 \u2212Vt.\n\uf461Appendices > B  Optimizing Neural Networks\n\uf52a\n This doesn\u2019t quite look like an adaptive step size. But what we can see is that, if we\nlet", "let \n, then the rule looks exactly like doing an update with step size \non a moving average of the gradients with parameter :\n\u2753 Study Question", "\u2753 Study Question\nProve to yourself that these formulations are equivalent.\nWe will find that \n will be bigger in dimensions that consistently have the same\nsign for", "sign for \n and smaller for those that don\u2019t. Of course we now have two\nparameters to set (  and ), but the hope is that the algorithm will perform better", "overall, so it will be worth trying to find good values for them. Often  is set to be\nsomething like \n.\nThe red arrows show the update after each successive step of mini-batch gradient", "descent with momentum. The blue points show the direction of the gradient with\nrespect to the mini-batch at each step. Momentum smooths the path taken towards", "the local minimum and leads to faster convergence.\n\u2753 Study Question\nIf you set \n, would momentum have more of an effect or less of an effect\nthan if you set it to \n?", "?\nAnother useful idea is this: we would like to take larger steps in parts of the space\nwhere \n is nearly flat (because there\u2019s no risk of taking too big a step due to the", "gradient being large) and smaller steps when it is steep. We\u2019ll apply this idea to\neach weight independently, and end up with a method called adadelta, which is a\n\u03b7 = \u03b7\u2032(1 \u2212\u03b3)\n\u03b7\u2032\n\u03b3\nM0 = 0,", "\u03b7\u2032\n\u03b3\nM0 = 0,\nMt = \u03b3 Mt\u22121 + (1 \u2212\u03b3) \u2207WJ(Wt\u22121),\nWt = Wt\u22121 \u2212\u03b7\u2032 Mt.\nVt\n\u2207W\n\u03b7\n\u03b3\n\u03b3\n0.9\n\u03b3 = 0.1\n0.9\nB.1.3 Adadelta\nJ(W)\nMomentum", "J(W)\nMomentum\n variant on adagrad (for adaptive gradient). Even though our weights are indexed by\nlayer, input unit, and output unit, for simplicity here, just let \n be any weight in", "be any weight in\nthe network (we will do the same thing for all of them).\nThe sequence \n is a moving average of the square of the th component of the", "gradient. We square it in order to be insensitive to the sign\u2014we want to know\nwhether the magnitude is big or small. Then, we perform a gradient update to\nweight , but divide the step size by", ", which is larger when the surface is\nsteeper in direction  at point \n in weight space; this means that the step size\nwill be smaller when it\u2019s steep and larger when it\u2019s flat.", "Adam has become the default method of managing step sizes in neural networks.\nIt combines the ideas of momentum and adadelta. We start by writing moving", "averages of the gradient and squared gradient, which reflect estimates of the mean\nand variance of the gradient for weight :\nA problem with these estimates is that, if we initialize \n, they will", ", they will\nalways be biased (slightly too small). So we will correct for that bias by defining\nNote that \n is \n raised to the power , and likewise for \n. To justify these", ". To justify these\ncorrections, note that if we were to expand \n in terms of \n and\n, the coefficients would sum to 1. However, the coefficient behind\n is \n and since", "is \n and since \n, the sum of coefficients of nonzero terms is \n;\nhence the correction. The same justification holds for \n.\n\u2753 Study Question\nWj\ngt,j = \u2207WJ(Wt\u22121)j,\nGt,j = \u03b3 Gt\u22121,j + (1 \u2212\u03b3) g2\nt,j,", "t,j,\nWt,j = Wt\u22121,j \u2212\n\u03b7\n\u221aGt,j + \u03f5\ngt,j.\nGt,j\nj\nj\n\u221aGt,j + \u03f5\nj\nWt\u22121\nB.1.4 Adam\nj\ngt,j = \u2207WJ(Wt\u22121)j,\nmt,j = B1 mt\u22121,j + (1 \u2212B1) gt,j,\nvt,j = B2 vt\u22121,j + (1 \u2212B2) g2\nt,j.\nm0 = v0 = 0\n^mt,j =\nmt,j\n1 \u2212Bt\n1\n,", "mt,j\n1 \u2212Bt\n1\n,\n^vt,j =\nvt,j\n1 \u2212Bt\n2\n,\nWt,j = Wt\u22121,j \u2212\n\u03b7\n\u221a^vt,j + \u03f5\n^mt,j.\nBt\n1\nB1\nt\nBt\n2\nmt,j\nm0,j\ng0,j, g1,j, \u2026 , gt,j\nm0,j\nBt\n1\nm0,j = 0\n1 \u2212Bt\n1\nvt,j\nAlthough, interestingly, it may", "actually violate the convergence\nconditions of SGD:\narxiv.org/abs/1705.08292\n Define \n directly as a moving average of \n. What is the decay (\nparameter)?", "parameter)?\nEven though we now have a step size for each weight, and we have to update\nvarious quantities on each iteration of gradient descent, it\u2019s relatively easy to", "implement by maintaining a matrix for each quantity (\n, \n, \n, \n) in each layer\nof the network.\nB.2 Batch Normalization Details\nLet\u2019s think of the batch-normalization layer as taking", "as input and producing an\noutput \n. But now, instead of thinking of \n as an \n vector, we have to\nexplicitly think about handling a mini-batch of data of size \n all at once, so \n will\nbe an", "will\nbe an \n matrix, and so will the output \n.\nOur first step will be to compute the batchwise mean and standard deviation. Let \nbe the \n vector where\nand let \n be the \n vector where", "vector where\nThe basic normalized version of our data would be a matrix, element \n of which\nis\nwhere  is a very small constant to guard against division by zero.\nHowever, if we let these be our", "values, we really are forcing something too\nstrong on our data\u2014our goal was to normalize across the data batch, but not", "necessarily force the output values to have exactly mean 0 and standard deviation 1.\nSo, we will give the layer the opportunity to shift and scale the outputs by adding", "new weights to the layer. These weights are \n and \n, each of which is an \nvector. Using the weights, we define the final output to be\nThat\u2019s the forward pass. Whew!", "Now, for the backward pass, we have to do two things: given \n,\n^mt,j\ngt,j\n\u03b3\nm\u2113\nt v\u2113\nt g\u2113\nt g2\nt\n\u2113\nZ l\n\u02c6Z l\nZ l\nnl \u00d7 1\nK\nZ l\nnl \u00d7 K\n\u02c6Z l\n\u03bcl\nnl \u00d7 1\n\u03bcl\ni = 1\nK\nK\n\u2211\nj=1\nZ l\nij,\n\u03c3l\nnl \u00d7 1\n\u03c3l\ni =\n1\nK\nK\n\u2211", "\u03c3l\ni =\n1\nK\nK\n\u2211\nj=1\n(Z l\nij \u2212\u03bcl\ni)\n2\n.\n\ue001\n\ue000\n\u23b7\n(i, j)\nZ\nl\nij =\nZ l\nij \u2212\u03bcl\ni\n\u03c3l\ni + \u03f5\n,\n\u2013\n\u03f5\n\u02c6Z l\nGl\nBl\nnl \u00d7 1\n\u02c6Z l\nij = Gl\ni Z\nl\nij + Bl\ni.\n\u2013\n\u2202L\n\u2202\u02c6Z l\n Compute \n for back-propagation, and\nCompute \n and", "Compute \n and \n for gradient updates of the weights in this layer.\nSchematically, we have\nIt\u2019s hard to think about these derivatives in matrix terms, so we\u2019ll see how it works\nfor the components.", "contributes to \n for all data points  in the batch. So,\nSimilarly, \n contributes to \n for all data points  in the batch. Thus,\nNow, let\u2019s figure out how to do backprop. We can start schematically:", "And because dependencies only exist across the batch, but not across the unit\noutputs,\nThe next step is to note that\nAnd now that\nwhere \n if \n and 0 otherwise. We need two more pieces:", "Putting the whole thing together, we get\n\u2202L\n\u2202Z l\n\u2202L\n\u2202Gl\n\u2202L\n\u2202Bl\n\u2202L\n\u2202B = \u2202L\n\u2202\u02c6Z\n\u2202\u02c6Z\n\u2202B .\nBi\n\u02c6Zij\nj\n\u2202L\n\u2202Bi\n= \u2211\nj\n\u2202L\n\u2202\u02c6Zij\n\u2202\u02c6Zij\n\u2202Bi\n= \u2211\nj\n\u2202L\n\u2202\u02c6Zij\n.\nGi\n\u02c6Zij\nj\n\u2202L\n\u2202Gi\n= \u2211\nj\n\u2202L\n\u2202\u02c6Zij\n\u2202\u02c6Zij\n\u2202Gi\n= \u2211\nj\n\u2202L", "\u2202\u02c6Zij\n\u2202Gi\n= \u2211\nj\n\u2202L\n\u2202\u02c6Zij\nZij.\n\u2013\n\u2202L\n\u2202Z = \u2202L\n\u2202\u02c6Z\n\u2202\u02c6Z\n\u2202Z .\n\u2202L\n\u2202Zij\n=\nK\n\u2211\nk=1\n\u2202L\n\u2202\u02c6Zik\n\u2202\u02c6Zik\n\u2202Zij\n.\n\u2202\u02c6Zik\n\u2202Zij\n= \u2202\u02c6Zik\n\u2202Zik\n\u2202Zik\n\u2202Zij\n= Gi\n\u2202Zik\n\u2202Zij\n.\n\u2013\n\u2013\n\u2013\n\u2202Zik\n\u2202Zij\n= (\u03b4jk \u2212\u2202\u03bci\n\u2202Zij\n) 1\n\u03c3i\n\u2212Zik \u2212\u03bci\n\u03c32", ") 1\n\u03c3i\n\u2212Zik \u2212\u03bci\n\u03c32\ni\n\u2202\u03c3i\n\u2202Zij\n,\n\u2013\n\u03b4jk = 1\nj = k\n\u2202\u03bci\n\u2202Zij\n= 1\nK ,\n\u2202\u03c3i\n\u2202Zij\n= Zij \u2212\u03bci\nK \u03c3i\n.\n\u2202L\n\u2202Zij\n=\nK\n\u2211\nk=1\n\u2202L\n\u2202\u02c6Zik\nGi\n1\nK \u03c3i\n(K \u03b4jk \u22121 \u2212(Zik \u2212\u03bci)(Zij \u2212\u03bci)\n\u03c32\ni\n).", "\u03c32\ni\n).\n  In which we try to describe the outlines of the \u201clifecycle\u201d of supervised learning,\nincluding hyperparameter tuning and evaluation of the final product.\nC.1 General case", "C.1 General case\nWe start with a very generic setting.\nGiven: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()", "such that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning\nalgorithm () takes as input a data set of the form\nwhere \n and \n and returns an \n.", "and returns an \n.\nGiven a problem specification and a set of data \n, we evaluate hypothesis \naccording to average loss, or error,", "If the data used for evaluation were not used during learning of the hypothesis then this\nis a reasonable estimate of how well the hypothesis will make additional", "predictions on new data from the same source.\nA validation strategy  takes an algorithm \n, a loss function , and a data source \nand produces a real number which measures how well", "performs on data from\nthat distribution.\nIn the simplest case, we can divide \n into two sets, \n and \n, train on the\nfirst, and then evaluate the resulting hypothesis on the second. In that case,", "Appendix C \u2014 Supervised learning in a\nnutshell\nC.1.1 Minimal problem specification \ue9cb\nD = {(x(1), y(1)), \u2026 , (x(n), y(n))}\nx(i) \u2208X\ny(i) \u2208y\nh \u2208H\nC.1.2 Evaluating a hypothesis\nD\nh\nE(h, L, D) =\n1\n|D|\nD\n\u2211", "1\n|D|\nD\n\u2211\ni=1\nL (h (x(i)), y(i))\nC.1.3 Evaluating a supervised learning algorithm\nV\nA\nL\nD\nA\nC.1.3.1 Using a validation set\nD\nDtrain \nDval \nV(A, L, D) = E (A (Dtrain ), L, Dval )", "\uf461Appendices > C  Supervised learning in a nutshell\n\uf52a\n We can\u2019t reliably evaluate an algorithm based on a single application to a single", "training and test set, because there are many aspects of the training and testing\ndata, as well as, sometimes, randomness in the algorithm itself, that cause variance", "in the performance of the algorithm. To get a good idea of how well an algorithm\nperforms, we need to, multiple times, train it and evaluate the resulting hypothesis,\nand report the average over", "executions of the algorithm of the error of the\nhypothesis it produced each time.\nWe divide the data into 2 K random non-overlapping subsets:\n.\nThen,", ".\nThen,\nIn cross validation, we do a similar computation, but allow data to be re-used in the\n different iterations of training and testing the algorithm (but never share training", "and testing data for a single iteration!). See Section 2.8.2.2 for details.\nNow, if we have two different algorithms \n and \n, we might be interested in", "knowing which one will produce hypotheses that generalize the best, using data\nfrom a particular source. We could compute \n and \n, and", "and \n, and\nprefer the algorithm with lower validation error. More generally, given algorithms\n, we would prefer\nNow what? We have to deliver a hypothesis to our customer. We now know how to", "find the algorithm, \n, that works best for our type of data. We can apply it to all of\nour data to get the best hypothesis we know how to create, which would be", "and deliver this resulting hypothesis as our best product.\nA majority of learning algorithms have the form of optimizing some objective\ninvolving the training data and a loss function.", "C.1.3.2 Using multiple training/evaluation runs\nK\nDtrain \n1\n, Dval \n1 , \u2026 , Dtrain \nK\n, Dval \nK\nV(A, L, D) = 1\nK\nK\n\u2211\nk=1\nE(A(Dtrain\nk\n), L, Dval\nk ) .\nC.1.3.3 Cross validation\nK", "K\nC.1.4 Comparing supervised learning algorithms\nA1\nA2\nV (A1, L, D)\nV (A\u2208, L, D)\nA1, \u2026 , AM\nA\u2217= arg min\nm V (AM, L, D)\nC.1.5 Fielding a hypothesis\nA\u2217\nh\u2217= A\u2217(D)\nC.1.6 Learning algorithms as optimizers", "Interestingly, this loss function is\nnot always the same as the loss\nfunction that is used for\n So for example, (assuming a perfect optimizer which doesn\u2019t, of course, exist) we", "might say our algorithm is to solve an optimization problem:\nOur objective often has the form\nwhere  is a loss to be minimized during training and \n is a regularization term.", "Often, rather than comparing an arbitrary collection of learning algorithms, we\nthink of our learning algorithm as having some parameters that affect the way it", "maps data to a hypothesis. These are not parameters of the hypothesis itself, but\nrather parameters of the algorithm. We call these hyperparameters. A classic example", "would be to use a hyperparameter  to govern the weight of a regularization term\non an objective to be optimized:\nThen we could think of our algorithm as \n. Picking a good value of  is the", "same as comparing different supervised learning algorithms, which is accomplished\nby validating them and picking the best one!\nC.2 Concrete case: linear regression", "In linear regression the problem formulation is this:\n for values of parameters \n and \n.\nOur learning algorithm has hyperparameter  and can be written as:", "Our learning algorithm has hyperparameter $ $ and can be written as:\nFor a particular training data set and parameter , it finds the best hypothesis on\nthis data, specified with parameters", ", written \n.\nA(D) = arg min\nh\u2208H J (h; D).\nJ (h; D) = E(h, L, D) + R(h),\nL\nR\nC.1.7 Hyperparameters\n\u03bb\nJ (h; D) = E(h, L, D) + \u03bbR(h).\nA(D; \u03bb)\n\u03bb\nx = Rd\ny = R\nH = {\u03b8\u22a4x + \u03b80}\n\u03b8 \u2208Rd\n\u03b80 \u2208R\nL(g, y) = (g \u2212y)2", "L(g, y) = (g \u2212y)2\n\u03bb\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2225\u03b8\u22252\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2225\u03b8\u22252.\n\u03bb\n\u0398 = (\u03b8, \u03b80)\n\u0398\u2217(\u03bb, D)", "\u0398\u2217(\u03bb, D)\nevaluation! We will see this in\nlogistic regression.\n Picking the best value of the hyperparameter is choosing among learning", "algorithms. We could, most simply, optimize using a single training / validation\nsplit, so \n \n, and\nIt would be much better to select the best  using multiple runs or cross-validation;", "that would just be a different choices of the validation procedure  in the top line.\nNote that we don\u2019t use regularization here because we just want to measure how", "good the output of the algorithm is at predicting values of new points, and so that\u2019s\nwhat we measure. We use the regularizer during training when we don\u2019t want to", "focus only on optimizing predictions on the training data.\nFinally! To make a predictor to ship out into the world, we would use all the data\nwe have,", "we have, \n, to train, using the best hyperparameters we know, and return\nFinally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, as", "Here are the same ideas, written out in informal pseudocode:\nD = Dtrain \u222aDval \n\u03bb\u2217= arg min\n\u03bb V (A\u03bb, L, Dval )\n= arg min\n\u03bb E (\u0398\u2217(\u03bb, Dtrain ),  mse, Dval )\n= arg min\n\u03bb\n1\n|Dval |\n\u2211\n(x,y)\u2208Dval", "\u2211\n(x,y)\u2208Dval \n(\u03b8\u2217(\u03bb, Dtrain )\n\u22a4x + \u03b8\u2217\n0 (\u03bb, Dtrain ) \u2212y)\n2\n\u03bb\nV\nD\n\u0398\u2217= A (D; \u03bb\u2217)\n= \u0398\u2217(\u03bb\u2217, D)\n= arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2217\u2225\u03b8\u22252\nE test  = E (\u0398\u2217,  mse , Dtest )\n=\n1\n|Dtest |\n\u2211", "=\n1\n|Dtest |\n\u2211\n(x,y)\u2208Dtot \n(\u03b8\u2217Tx + \u03b8\u2217\n0 \u2212y)\n2\n# returns theta_best(D, lambda)\ndefine train(D, lambda):\n    return minimize(mse(theta, D) + lambda * norm(theta)**2, theta)", "# returns lambda_best using very simple validation\ndefine simple_tune(D_train, D_val, possible_lambda_vals):\n    scores = [mse(train(D_train, lambda), D_val) for lambda in \npossible_lambda_vals]", "return possible_lambda_vals[least_index[scores]]\n# returns theta_best overall\ndefine theta_best(D_train, D_val, possible_lambda_vals):", "return train(D_train + D_val, simple_tune(D_train, D_val, \npossible_lambda_vals))\n# customer evaluation of the theta delivered to them\n C.3 Concrete case: logistic regression", "In binary logistic regression the problem formulation is as follows. We are writing\nthe class labels as 1 and 0.\n for values of parameters \n and \n.\nProxy loss \n Our learning algorithm", "has hyperparameter  and can be written as:\nFor a particular training data set and parameter , it finds the best hypothesis on\nthis data, specified with parameters \n, written \n according to the", "according to the\nproxy loss \n.\nPicking the best value of the hyperparameter is choosing among learning\nalgorithms based on their actual predictions. We could, most simply, optimize using", "a single training / validation split, so \n, and we use the real 01 loss:\nIt would be much better to select the best  using multiple runs or cross-validation;", "that would just be a different choices of the validation procedure  in the top line.\nFinally! To make a predictor to ship out into the world, we would use all the data\nwe have,", "we have, \n, to train, using the best hyperparameters we know, and return\n\u2753 Study Question\nWhat loss function is being optimized inside this algorithm?", "Finally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, as\ndefine customer_val(theta):\n    return mse(theta, D_test)\nX = Rd", "X = Rd\ny = {+1, 0}\nH = {\u03c3 (\u03b8\u22a4x + \u03b80)}\n\u03b8 \u2208Rd\n\u03b80 \u2208R\nL(g, y) = L01( g,  h)\nLnll(g, y) = \u2212(y log(g) + (1 \u2212y) log(1 \u2212g))\n\u03bb\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\nLnll (\u03c3 (\u03b8\u22a4x + \u03b80), y) + \u03bb\u2225\u03b8\u22252", "\u03bb\n\u0398 = (\u03b8, \u03b80)\n\u0398\u2217(\u03bb, D)\nLnll \nD = Dtrain  \u222aDval\n\u03bb\u2217= arg min\n\u03bb V (A\u03bb, L01, Dval )\n= arg min\n\u03bb E (\u0398\u2217(\u03bb, Dtrain ), L01, Dval )\n= arg min\n\u03bb\n1\n|Dval |\n\u2211\n(x,y)\u2208Dval \nL01 (\u03c3 (\u03b8\u2217(\u03bb, Dtrain )\n\u22a4x + \u03b8\u2217", "\u22a4x + \u03b8\u2217\n0 (\u03bb, Dtrain )), y)\n\u03bb\nV\nD\n\u0398\u2217= A(D; \u03bb\u2217)\nE test = E(\u0398\u2217, L01, Dtest)\n The customer just wants to buy the right stocks! So we use the real \n here for\nvalidation.\nL01"]