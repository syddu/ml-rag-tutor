["This page contains all content from the legacy PDF notes; neural networks chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "You\u2019ve probably been hearing a lot about \u201cneural networks.\u201d Now that we have\nseveral useful machine-learning concepts (hypothesis classes, classification,", "regression, gradient descent, regularization, etc.), we are well equipped to\nunderstand neural networks in detail.", "This is, in some sense, the \u201cthird wave\u201d of neural nets. The basic idea is founded on\nthe 1943 model of neurons of McCulloch and Pitts and the learning ideas of Hebb.", "There was a great deal of excitement, but not a lot of practical success: there were\ngood training methods (e.g., perceptron) for linear functions, and interesting", "examples of non-linear functions, but no good way to train non-linear functions\nfrom data. Interest died out for a while, but was re-kindled in the 1980s when", "several people came up with a way to train neural networks with \u201cback-\npropagation,\u201d which is a particular style of implementing gradient descent, that we\nwill study here.", "will study here.\nAs with many good ideas in science, the basic idea for how to train non-linear\nneural networks with gradient descent was independently developed by more than\none researcher.", "one researcher.\nBy the mid-90s, the enthusiasm waned again, because although we could train non-\nlinear networks, the training tended to be slow and was plagued by a problem of", "getting stuck in local optima. Support vector machines (SVMs) that use\nregularization of high-dimensional hypotheses by seeking to maximize the margin,", "alongside kernel methods that provide an efficient and beautiful way of using\nfeature transformations to non-linearly transform data into a higher-dimensional", "space, provided reliable learning methods with guaranteed convergence and no\nlocal optima.\nHowever, during the SVM enthusiasm, several groups kept working on neural", "networks, and their work, in combination with an increase in available data and\ncomputation, has made neural networks rise again. They have become much more", "reliable and capable, and are now the method of choice in many applications. There\nare many, many variations of neural networks, which we can\u2019t even begin to survey.", "We will study the core \u201cfeed-forward\u201d networks with \u201cback-propagation\u201d training,\nand then, in later chapters, address some of the major advances beyond this core.", "We can view neural networks from several different perspectives:\n6  Neural Networks\nNote\nThe number of neural network\nvariants increases daily, as may be\nseen on arxiv.org .\n\uf229\n6  Neural Networks", "6  Neural Networks\n\uf4616  Neural Networks\n\uf52a\n View 1: An application of stochastic gradient descent for classification and\nregression with a potentially very rich hypothesis class.", "View 2: A brain-inspired network of neuron-like computing elements that learn\ndistributed representations.\nView 3: A method for building applications that make predictions based on huge", "amounts of data in very complex domains.\nWe will mostly take view 1, with the understanding that the techniques we develop", "will enable the applications in view 3. View 2 was a major motivation for the early\ndevelopment of neural networks, but the techniques we will study do not seem to", "actually account for the biological learning processes in brains.\n6.1 Basic element\nThe basic element of a neural network is a \u201cneuron,\u201d pictured schematically below.", "We will also sometimes refer to a neuron as a \u201cunit\u201d or \u201cnode.\u201d\n\ue050\nx1\n.. .\nxm\nf(\u00b7)\na\nw1\nwm\nw0\nz\ninput\npre-activation\noutput\nactivation function", "activation function\nIt is a (generally non-linear) function of an input vector \n to a single output\nvalue \n.\nIt is parameterized by a vector of weights \n and an offset or\nthreshold \n.", "threshold \n.\nWe also specify an activation function \n. In general, this is chosen to be a\nnon-linear function, which means the neuron is non-linear. In the case that the", "activation function is the identity (\n) or another linear function, then the\nneuron is a linear function of ). The activation can theoretically be any function,", "though we will only be able to work with it if it is differentiable.\nThe function represented by the neuron is expressed as:\nx \u2208Rm\na \u2208R\n(w1, \u2026 , wm) \u2208Rm\nw0 \u2208R\nf : R \u2192R\nf(x) = x\nx\na = f(z) = f ((\nm\n\u2211", "a = f(z) = f ((\nm\n\u2211\nj=1\nxjwj) + w0) = f(wTx + w0) .\nSome prominent researchers are, in\nfact, working hard to find\nanalogues of these methods in the\nbrain.\nSorry for changing our notation", "here. We were using  as the\ndimension of the input, but we are\ntrying to be consistent here with\nmany other accounts of neural\nnetworks. It is impossible to be\nconsistent with all of them though", "\u2014there are many different ways of\ntelling this story.\nd\nThis should remind you of our \nand \n for linear models.\n\u03b8\n\u03b80\n\uf229\n6  Neural Networks", "6  Neural Networks\n Before thinking about a whole network, we can consider how to train a single unit.\nGiven a loss function \n and a dataset \n,", "and a dataset \n,\nwe can do (stochastic) gradient descent, adjusting the weights \n to minimize\nwhere \n is the output of our single-unit neural net for a given input.", "We have already studied two special cases of the neuron: linear logistic classifiers\n(LLCs) with NLL loss and regressors with quadratic loss! The activation function for\nthe LLC is", "the LLC is \n and for linear regression it is simply \n.\n\u2753 Study Question\nJust for a single neuron, imagine for some reason, that we decide to use\nactivation function \n and loss function", "and loss function\n. Derive a gradient descent update for \nand \n.\n6.2 Networks\nNow, we\u2019ll put multiple neurons together into a network. A neural network in\ngeneral takes in an input", "and generates an output \n. It is constructed\nout of multiple neurons; the inputs of each neuron might be elements of  and/or", "outputs of other neurons. The outputs of the neural network are generated by \noutput units.\nIn this chapter, we will only consider feed-forward networks. In a feed-forward", "network, you can think of the network as defining a function-call graph that is\nacyclic: that is, the input to a neuron can never depend on that neuron\u2019s output.", "Data flows one way, from the inputs to the outputs, and the function computed by\nthe network is just a composition of the functions computed by the individual\nneurons.", "neurons.\nAlthough the graph structure of a feed-forward neural network can really be\nanything (as long as it satisfies the feed-forward constraint), for simplicity in", "software and analysis, we usually organize them into layers. A layer is a group of\nneurons that are essentially \u201cin parallel\u201d: their inputs are the outputs of neurons in", "the previous layer, and their outputs are the inputs to the neurons in the next layer.\nWe\u2019ll start by describing a single layer, and then go on to the case of multiple layers.\nL(guess, actual)", "L(guess, actual)\n{(x(1), y(1)), \u2026 , (x(n), y(n))}\nw, w0\nJ(w, w0) = \u2211\ni\nL (NN(x(i); w, w0), y(i)) ,\nNN\nf(x) = \u03c3(x)\nf(x) = x\nf(z) = ez\nL(guess, actual) = (guess \u2212actual)2\nw\nw0\nx \u2208Rm\na \u2208Rn\nx\nn", "w0\nx \u2208Rm\na \u2208Rn\nx\nn\n6.2.1 Single layer\n\uf229\n6  Neural Networks\n A layer is a set of units that, as we have just described, are not connected to each", "other. The layer is called fully connected if, as in the diagram below, all of the inputs\n(i.e., \n in this case) are connected to every unit in the layer. A layer has\ninput", "input \n and output (also known as activation) \n.\n\ue050\n\ue050\n\ue050\n.. .\n\ue050\nx1\nx2\n.. .\nxm\nf\nf\nf\n.. .\nf\na1\na2\na3\n.. .\nan\nW, W0\nSince each unit has a vector of weights and a single offset, we can think of the", "weights of the whole layer as a matrix, \n, and the collection of all the offsets as a\nvector \n. If we have \n inputs,  units, and  outputs, then\n is an \n matrix,\n is an \n column vector,", "column vector,\n, the input, is an \n column vector,\n, the pre-activation, is an \n column vector,\n, the activation, is an \n column vector,\nand the output vector is", "The activation function  is applied element-wise to the pre-activation values .\nA single neural network generally combines multiple layers, most typically by", "feeding the outputs of one layer into the inputs of another layer.\nx1, x2, \u2026 xm\nx \u2208Rm\na \u2208Rn\nW\nW0\nm\nn\nn\nW\nm \u00d7 n\nW0\nn \u00d7 1\nX\nm \u00d7 1\nZ = W TX + W0\nn \u00d7 1\nA\nn \u00d7 1\nA = f(Z) = f(W TX + W0) .\nf\nZ", "f\nZ\n6.2.2 Many layers\n\uf229\n6  Neural Networks\n We have to start by establishing some nomenclature. We will use  to name a layer,\nand let \n be the number of inputs to the layer and", "be the number of outputs\nfrom the layer. Then, \n and \n are of shape \n and \n, respectively.\nNote that the input to layer  is the output from layer \n, so we have \n,\nand as a result \n is of shape", "is of shape \n, or equivalently \n. Let \n be the\nactivation function of layer . Then, the pre-activation outputs are the \n vector\nand the activation outputs are simply the \n vector", "vector\nHere\u2019s a diagram of a many-layered network, with two blocks for each layer, one\nrepresenting the linear part of the operation and one representing the non-linear", "activation function. We will use this structural decomposition to organize our\nalgorithmic thinking and implementation.\nW 1\nW 1\n0\nf 1\nW 2\nW 2\n0\nf 2\n\u00b7 \u00b7 \u00b7\nW L\nW L\n0\nf L\nX = A0\nZ1\nA1\nZ2\nA2\nAL\u22121\nZL\nAL", "A1\nZ2\nA2\nAL\u22121\nZL\nAL\nlay er 1\nlay er 2\nlay er L\n6.3 Choices of activation function\nThere are many possible choices for the activation function. We will start by", "thinking about whether it\u2019s really necessary to have an  at all.\nWhat happens if we let  be the identity? Then, in a network with  layers (we\u2019ll\nleave out", "leave out \n for simplicity, but keeping it wouldn\u2019t change the form of this\nargument),\nSo, multiplying out the weight matrices, we find that\nwhich is a linear function of", "! Having all those layers did not change the\nrepresentational capacity of the network: the non-linearity of the activation function\nis crucial.\n\u2753 Study Question", "\u2753 Study Question\nConvince yourself that any function representable by any number of linear\nlayers (where  is the identity function) can be represented by a single layer.\nl\nml\nnl\nW l\nW l\n0\nml \u00d7 nl", "W l\nW l\n0\nml \u00d7 nl\nnl \u00d7 1\nl\nl \u22121\nml = nl\u22121\nAl\u22121\nml \u00d7 1\nnl\u22121 \u00d7 1\nf l\nl\nnl \u00d7 1\nZ l = W lTAl\u22121 + W l\n0\nnl \u00d7 1\nAl = f l(Z l) .\nf\nf\nL\nW0\nAL = W LTAL\u22121 = W LTW L\u22121T \u22efW 1TX .\nAL = W totalX ,\nX\nf", "AL = W totalX ,\nX\nf\nIt is technically possible to have\ndifferent activation functions\nwithin the same layer, but, again,\nfor convenience in specification\nand implementation, we generally", "have the same activation function\nwithin a layer.\n\uf229\n6  Neural Networks\n Now that we are convinced we need a non-linear activation, let\u2019s examine a few", "common choices. These are shown mathematically below, followed by plots of these\nfunctions.\nStep function:\nRectified linear unit (ReLU):", "Sigmoid function: Also known as a logistic function. This can sometimes be\ninterpreted as probability, because for any value of  the output is in \n:\nHyperbolic tangent: Always in the range \n:", ":\nSoftmax function: Takes a whole vector \n and generates as output a vector\n with the property that \n, which means we can interpret it as\na probability distribution over  items:\n\u22122\n\u22121\n1\n2\n\u22120.5\n0.5\n1", "\u22121\n1\n2\n\u22120.5\n0.5\n1\n1.5\nz\nstep(z)\n\u22122\n\u22121\n1\n2\n\u22120.5\n0.5\n1\n1.5\nz\nReLU(z)\n\u22124\n\u22122\n2\n4\n\u22121\n\u22120.5\n0.5\n1\nz\n\u03c3(z)\n\u22124\n\u22122\n2\n4\n\u22121\n\u22120.5\n0.5\n1\nz\ntanh(z)\nstep(z) = {0\nif z < 0\n1\notherwise\nReLU(z) = {\n= max(0, z)\n0", "= max(0, z)\n0\nif z < 0\nz\notherwise\nz\n(0, 1)\n\u03c3(z) =\n1\n1 + e\u2212z\n(\u22121, 1)\ntanh(z) = ez \u2212e\u2212z\nez + e\u2212z\nZ \u2208Rn\nA \u2208(0, 1)n\n\u2211n\ni=1 Ai = 1\nn\nsoftmax(z) =\n\u23a1\n\u23a2\n\u23a3\nexp(z1)/ \u2211i exp(zi)\n\u22ee\nexp(zn)/ \u2211i exp(zi)\n\u23a4\n\u23a5\n\u23a6\n\uf229", "\u23a4\n\u23a5\n\u23a6\n\uf229\n6  Neural Networks\n The original idea for neural networks involved using the step function as an\nactivation, but because the derivative of the step function is zero everywhere except", "at the discontinuity (and there it is undefined), gradient-descent methods won\u2019t be\nuseful in finding a good setting of the weights, and so we won\u2019t consider the step", "function further. Step functions have been replaced, in a sense, by the sigmoid,\nReLU, and tanh activation functions.\n\u2753 Study Question", "\u2753 Study Question\nConsider sigmoid, ReLU, and tanh activations. Which one is most like a step\nfunction? Is there an additional parameter you could add to a sigmoid that", "would make it be more like a step function?\n\u2753 Study Question\nWhat is the derivative of the ReLU function? Are there some values of the input\nfor which the derivative vanishes?", "ReLUs are especially common in internal (\u201chidden\u201d) layers, sigmoid activations are\ncommon for the output for binary classification, and softmax activations are", "common for the output for multi-class classification (see Section 4.3.3 for an\nexplanation).\n6.4 Loss functions and activation functions\nAt layer", "At layer \n which is the output layer, we need to specify a loss function, and\npossibly an activation function as well. Different loss functions make different", "assumptions about the range of values they will get as input and, as we have seen,\ndifferent activation functions will produce output values in different ranges. When", "you are designing a neural network, it\u2019s important to make these things fit together\nwell. In particular, we will think about matching loss functions with the activation\nfunction in the last layer,", ". Here is a table of loss functions and activations that\nmake sense for them:\nLoss\ntask\nsquared\nlinear\nregression\nnll\nsigmoid\nbinary classification\nnllm\nsoftmax\nmulti-class classification", "We explored squared loss in Chapter 2 and (nll and nllm) in Chapter 4.\nL,\nf L\nf L\n\uf229\n6  Neural Networks\n 6.5 Error back-propagation", "We will train neural networks using gradient descent methods. It\u2019s possible to use\nbatch gradient descent, in which we sum up the gradient over all the points (as in", "Section 3.2 of Chapter 3) or stochastic gradient descent (SGD), in which we take a\nsmall step with respect to the gradient considering a single point at a time (as in\nSection 3.4 of Chapter 3).", "Our notation is going to get pretty hairy pretty quickly. To keep it as simple as we\ncan, we\u2019ll focus on computing the contribution of one data point \n to the gradient", "to the gradient\nof the loss with respect to the weights, for SGD; you can simply sum up these\ngradients over all the data points if you wish to do batch descent.", "So, to do SGD for a training example \n, we need to compute\n, where \n represents all weights \n in all the layers\n. This seems terrifying, but is actually quite easy to do using the chain\nrule.", "rule.\nRemember that we are always computing the gradient of the loss function with\nrespect to the weights for a particular value of \n. That tells us how much we want", "to change the weights, in order to reduce the loss incurred on this particular\ntraining example.\nTo get some intuition for how these derivations work, we\u2019ll first suppose everything", "in our neural network is one-dimensional. In particular, we\u2019ll assume there are\n inputs and \n outputs at every layer. So layer  looks like:\nIn the equation above, we\u2019re using the lowercase letters", "to\nemphasize that all of these quantities are scalars just for the moment. We\u2019ll look at\nthe more general matrix case below.\nTo use SGD, then, we want to compute \n and", "and\n for each layer  and each data point \n. Below we\u2019ll write\n\u201closs\u201d as an abbreviation for \n. Then our first quantity of interest is\n. The chain rule gives us the following.", "First, let\u2019s look at the case \n:\nx(i)\n(x, y)\n\u2207WL(NN(x; W), y)\nW\nW l, W l\n0\nl = (1, \u2026 , L)\n(x, y)\n6.5.1 First, suppose everything is one-dimensional\nml = 1\nnl = 1\nl\nal = f l(zl),\nzl = wlal\u22121 + wl\n0.", "zl = wlal\u22121 + wl\n0.\nal, zl, wl, al\u22121, wl\n0\n\u2202L(NN(x; W), y)/\u2202wl\n\u2202L(NN(x; W), y)/\u2202wl\n0\nl\n(x, y)\nL(NN(x; W), y)\n\u2202loss/\u2202wl\nl = L\n\u2202loss\n\u2202wL = \u2202loss\n\u2202aL \u22c5\u2202aL\n\u2202zL \u22c5\u2202zL\n\u2202wL\n= \u2202loss\n\u2202aL \u22c5(f L)\u2032(zL) \u22c5aL\u22121.", "Remember the chain rule! If\n and \n, so that\n, then\na = f(b)\nb = g(c)\na = f(g(c))\nda\ndc = da\ndb \u22c5db\ndc\n= f \u2032(b)g\u2032(c)\n= f \u2032(g(c))g\u2032(c)\nCheck your understanding: why\ndo we need exactly these quantities", "for SGD?\n\uf229\n6  Neural Networks\n Now we can look at the case of general :\nNote that every multiplication above is scalar multiplication because every term in", "every product above is a scalar. And though we solved for all the other terms in the\nproduct, we haven\u2019t solved for \n because the derivative will depend on", "which loss function you choose. Once you choose a loss function though, you\nshould be able to compute this derivative.\n\u2753 Study Question\nSuppose you choose squared loss. What is \n?\n\u2753 Study Question", "?\n\u2753 Study Question\nCheck the derivations above yourself. You should use the chain rule and also\nsolve for the individual derivatives that arise in the chain rule.\n\u2753 Study Question", "\u2753 Study Question\nCheck that the final layer (\n) case is a special case of the general layer  case\nabove.\n\u2753 Study Question\nDerive \n for yourself, for both the final layer (\n) and\ngeneral .", ") and\ngeneral .\n\u2753 Study Question\nDoes the \n case remind you of anything from earlier in this course?\n\u2753 Study Question\nWrite out the full SGD algorithm for this neural network.\nl\n\u2202loss\n\u2202wl\n= \u2202loss", "l\n\u2202loss\n\u2202wl\n= \u2202loss\n\u2202aL \u22c5\u2202aL\n\u2202zL \u22c5\n\u2202zL\n\u2202aL\u22121 \u22c5\u2202aL\u22121\n\u2202zL\u22121 \u22ef\u2202zl+1\n\u2202al\n\u22c5\u2202al\n\u2202zl \u22c5\u2202zl\n\u2202wl\n= \u2202loss\n\u2202aL \u22c5(f L)\u2032(zL) \u22c5wL \u22c5(f L\u22121)\u2032(zL\u22121) \u22ef\u22c5wl+1 \u22c5(f l)\u2032(zl) \u22c5al\u22121\n= \u2202loss\n\u2202zl\n\u22c5al\u22121.\n\u2202loss/\u2202aL\n\u2202loss/\u2202aL", "\u2202loss/\u2202aL\n\u2202loss/\u2202aL\nl = L\nl\n\u2202L(NN(x; W), y)/\u2202wl\n0\nl = L\nl\nL = 1\n\uf229\n6  Neural Networks\n It\u2019s pretty typical to run the chain rule from left to right like we did above. But, for", "where we\u2019re going next, it will be useful to notice that it\u2019s completely equivalent to\nwrite it in the other direction. So we can rewrite our result from above as follows:", "Next we\u2019re going to do everything that we did above, but this time we\u2019ll allow any\nnumber of inputs \n and outputs \n at every layer. First, we\u2019ll tell you the results", "that correspond to our derivations above. Then we\u2019ll talk about why they make\nsense. And finally we\u2019ll derive them carefully.", "OK, let\u2019s start with the results! Again, below we\u2019ll be using \u201closs\u201d as an\nabbreviation for \n. Then,\nwhere\nor equivalently,", "or equivalently,\nFirst, compare each equation to its one-dimensional counterpart, and make sure\nyou see the similarities. That is, compare the general weight derivatives in", "Equation 6.4 to the one-dimensional case in Equation 6.1. Compare the intermediate\nderivative of loss with respect to the pre-activations \n in Equation 6.5 to the one-", "dimensional case in Equation 6.2. And finally compare the version where we\u2019ve\nsubstituted in some of the derivatives in Equation 6.6 to Equation 6.3. Hopefully\n\u2202loss\n\u2202wl\n= al\u22121 \u22c5\u2202loss\n\u2202zl\n(6.1)\n\u2202loss", "\u2202zl\n(6.1)\n\u2202loss\n\u2202zl\n= \u2202al\n\u2202zl \u22c5\u2202zl+1\n\u2202al\n\u22ef\u2202aL\u22121\n\u2202zL\u22121 \u22c5\n\u2202zL\n\u2202aL\u22121 \u22c5\u2202aL\n\u2202zL \u22c5\u2202loss\n\u2202aL\n(6.2)\n= \u2202al\n\u2202zl \u22c5wl+1 \u22ef\u2202aL\u22121\n\u2202zL\u22121 \u22c5wL \u22c5\u2202aL\n\u2202zL \u22c5\u2202loss\n\u2202aL .\n(6.3)\n6.5.2 The general case\nml\nnl\nL(NN(x; W), y)", "nl\nL(NN(x; W), y)\n\u2202loss\n\u2202W l\nml\u00d7nl\n= Al\u22121\nml\u00d71\n( \u2202loss\n\u2202Z l )\nT\n1\u00d7nl\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n(6.4)\n\u2202loss\n\u2202Z l = \u2202Al\n\u2202Z l \u22c5\u2202Z l+1\n\u2202Al\n\u22ef\u22c5\u2202AL\u22121\n\u2202Z L\u22121 \u22c5\n\u2202Z L\n\u2202AL\u22121 \u22c5\u2202AL\n\u2202Z L \u22c5\u2202loss\n\u2202AL\n(6.5)", "\u2202AL\n(6.5)\n\u2202loss\n\u2202Z l = \u2202Al\n\u2202Z l \u22c5W l+1 \u22ef\u22c5\u2202AL\u22121\n\u2202Z L\u22121 \u22c5W L \u22c5\u2202AL\n\u2202Z L \u22c5\u2202loss\n\u2202AL .\n(6.6)\nZ l\nEven though we have reordered\nthe gradients for notational\nconvenience, when actually", "computing the product in\nEquation 6.3, it is computationally\nmuch cheaper to run the\nmultiplications from right-to-left\nthan from left-to-right. Convince\nyourself of this, by reasoning", "through the cost of the matrix\nmultiplications in each case.\nThere are lots of weights in a\nneural network, which means we\nneed to compute a lot of gradients.\nLuckily, as we can see, the", "gradients associated with weights\nin earlier layers depend on the\nsame terms as the gradients\nassociated with weights in later\nlayers. This means we can reuse\nterms and save ourselves some", "computation!\n\uf229\n6  Neural Networks\n you see how the forms are very analogous. But in the matrix case, we now have to\nbe careful about the matrix dimensions. We\u2019ll check these matrix dimensions below.", "Let\u2019s start by talking through each of the terms in the matrix version of these\nequations. Recall that loss is a scalar, and \n is a matrix of size \n. You can", ". You can\nread about the conventions in the course for derivatives starting in this chapter in\nAppendix A. By these conventions (not the only possible conventions!), we have\nthat", "that \n will be a matrix of size \n whose \n entry is the scalar\n. In some sense, we\u2019re just doing a bunch of traditional scalar", "derivatives, and the matrix notation lets us write them all simultaneously and\nsuccinctly. In particular, for SGD, we need to find the derivative of the loss with", "respect to every scalar component of the weights because these are our model\u2019s\nparameters and therefore are the things we want to update in SGD.\nThe next quantity we see in Equation 6.4 is", ", which we recall has size \n (or\nequivalently \n since it represents the outputs of the \n layer). Finally, we\nsee \n. Again, loss is a scalar, and \n is a \n vector. So by the", "vector. So by the\nconventions in Appendix A, we have that \n has size \n. The transpose\nthen has size \n. Now you should be able to check that the dimensions all make", "sense in Equation 6.4; in particular, you can check that inner dimensions agree in\nthe matrix multiplication and that, after the multiplication, we should be left with", "something that has the dimensions on the lefthand side.\nNow let\u2019s look at Equation 6.6. We\u2019re computing \n so that we can use it in", "Equation 6.4. The weights are familiar. The one part that remains is terms of the\nform \n. Checking out Appendix A, we see that this term should be a matrix\nof size \n since \n and \n both have size", "both have size \n. The \n entry of this matrix\nis \n. This scalar derivative is something that you can compute when you", "know your activation function. If you\u2019re not using a softmax activation function, \ntypically is a function only of \n, which means that \n should equal 0\nwhenever \n, and that \n.\n\u2753 Study Question", ".\n\u2753 Study Question\nCompute the dimensions of every term in Equation 6.5 and Equation 6.6 using\nAppendix A. After you\u2019ve done that, check that all the matrix multiplications", "work; that is, check that the inner dimensions agree and that the lefthand side\nand righthand side of these equations have the same dimensions.\n\u2753 Study Question", "\u2753 Study Question\nIf I use the identity activation function, what is \n for any ? What is the\nfull matrix \n?\nW l\nml \u00d7 nl\n\u2202loss/\u2202W l\nml \u00d7 nl\n(i, j)\n\u2202loss/\u2202W l\ni,j\nAl\u22121\nml \u00d7 1\nnl\u22121 \u00d7 1\nl \u22121\n\u2202loss/\u2202Z l", "l \u22121\n\u2202loss/\u2202Z l\nZ l\nnl \u00d7 1\n\u2202loss/\u2202Z l\nnl \u00d7 1\n1 \u00d7 nl\n\u2202loss/\u2202Z l\n\u2202Al/\u2202Z l\nnl \u00d7 nl\nAl\nZ l\nnl \u00d7 1\n(i, j)\n\u2202Al\nj/\u2202Z l\ni\nAl\nj\nZ l\nj\n\u2202Al\nj/\u2202Z l\ni\ni \u2260j\n\u2202Al\nj/\u2202Z l\nj = (f l)\u2032(Z l\nj)\n\u2202Al\nj/\u2202Z l\nj\nj\n\u2202Al/\u2202Z l\n\uf229", "j\nj\n\u2202Al/\u2202Z l\n\uf229\n6  Neural Networks\n You can use everything above without deriving it yourself. But if you want to find\nthe gradients of loss with respect to", "(which we need for SGD!), then you\u2019ll want\nto know how to actually do these derivations. So next we\u2019ll work out the\nderivations.", "derivations.\nThe key trick is to just break every equation down into its scalar meaning. For\ninstance, the \n element of \n is \n. If you think about it for a", "moment (and it might help to go back to the one-dimensional case), the loss is a\nfunction of the elements of \n, and the elements of \n are a function of the \n.\nThere are \n elements of", "elements of \n, so we can use the chain rule to write\nTo figure this out, let\u2019s remember that \n. We can write one\nelement of the \n vector, then, as \n. It follows that\n will be zero except when", "(check you agree!). So we can rewrite\nEquation 6.7 as\nFinally, then, we match entries of the matrices on both sides of the equation above\nto recover Equation 6.4.\n\u2753 Study Question", "\u2753 Study Question\nCheck that Equation 6.8 and Equation 6.4 say the same thing.\n\u2753 Study Question\nConvince yourself that \n by comparing the entries of the\nmatrices on both sides on the equality sign.", "\u2753 Study Question\nConvince yourself that Equation 6.5 is true.\n\u2753 Study Question\nApply the same reasoning to find the gradients of \n with respect to \n.\n6.5.3 Derivations for the general case\nW l\n0", "W l\n0\n(i, j)\n\u2202loss/\u2202W l\n\u2202loss/\u2202W l\ni,j\nZ l\nZ l\nW l\ni,j\nnl\nZ l\n\u2202loss\n\u2202W l\ni,j\n=\nnl\n\u2211\nk=1\n\u2202loss\n\u2202Z l\nk\n\u2202Z l\nk\n\u2202W l\ni,j\n.\n(6.7)\nZ l = (W l)\u22a4Al\u22121 + W l\n0\nZ l\nZ l\nb = \u2211ml\na=1 W l\na,bAl\u22121\na\n+ (W l\n0)b\n\u2202Z l", "a\n+ (W l\n0)b\n\u2202Z l\nk/\u2202W l\ni,j\nk = j\n\u2202loss\n\u2202W l\ni,j\n= \u2202loss\n\u2202Z l\nj\n\u2202Z l\nj\n\u2202W l\ni,j\n= \u2202loss\n\u2202Z l\nj\nAl\u22121\ni\n.\n(6.8)\n\u2202Z l/\u2202Al\u22121 = W l\nloss\nW l\n0\n\uf229\n6  Neural Networks", "6  Neural Networks\n This general process of computing the gradients of the loss with respect to the\nweights is called error back-propagation.", "The idea is that we first do a forward pass to compute all the  and  values at all the\nlayers, and finally the actual loss. Then, we can work backward and compute the", "gradient of the loss with respect to the weights in each layer, starting at layer  and\ngoing back to layer 1.\nW 1\nW 1\n0\nf 1\nW 2\nW 2\n0\nf 2\n\u00b7 \u00b7 \u00b7\nW L\nW L\n0\nf L\nLoss\nX = A0\nZ1\nA1\nZ2\nA2\nAL\u22121\nZL\nAL\ny", "Z2\nA2\nAL\u22121\nZL\nAL\ny\n\u2202loss\n\u2202AL\n\u2202loss\n\u2202ZL\n\u2202loss\n\u2202AL\u22121\n\u2202loss\n\u2202A2\n\u2202loss\n\u2202Z2\n\u2202loss\n\u2202A1\n\u2202loss\n\u2202Z1\nIf we view our neural network as a sequential composition of modules (in our work", "so far, it has been an alternation between a linear transformation with a weight\nmatrix, and a component-wise application of a non-linear activation function), then", "we can define a simple API for a module that will let us compute the forward and\nbackward passes, as well as do the necessary weight updates for gradient descent.", "Each module has to provide the following \u201cmethods.\u201d We are already using letters\n with particular meanings, so here we will use  as the vector input to the\nmodule and  as the vector output:\nforward:", "forward: \nbackward: \nweight grad: \n only needed for modules that have weights\nIn homework we will ask you to implement these modules for neural network", "components, and then use them to construct a network and train it as described in\nthe next section.\n6.6 Training\nHere we go! Here\u2019s how to do stochastic gradient descent training on a feed-", "forward neural network. After this pseudo-code, we motivate the choice of\ninitialization in lines 2 and 3. The actual computation of the gradient values (e.g.,", ") is not directly defined in this code, because we want to make the\nstructure of the computation clear.\n\u2753 Study Question\n6.5.4 Reflecting on backpropagation\na\nz\nL\na, x, y, z\nu\nv\nu \u2192v", "a, x, y, z\nu\nv\nu \u2192v\nu, v, \u2202L/\u2202v \u2192\u2202L/\u2202u\nu, \u2202L/\u2202v \u2192\u2202L/\u2202W\nW\n\u2202loss/\u2202AL\nNotice that the backward pass does\nnot output \n, even though the\nforward pass maps from  to . In\nthe backward pass, we are always", "directly computing and ``passing\naround\u2019\u2019 gradients of the loss.\n\u2202v/\u2202u\nu\nv\n\uf229\n6  Neural Networks\n What is \n?\n\u2753 Study Question\nWhich terms in the code below depend on \n?\nprocedure SGD-NEURAL-NET(\n)", ")\nfor \n to  do\nend for\nfor \n to  do\n//forward pass to compute \nfor \n to  do\nend for\nfor \n down to  do//error back-propagation\n//SGD update\nend for\nend for\nend procedure\nInitializing", "Initializing \n is important; if you do it badly there is a good chance the neural\nnetwork training won\u2019t work well. First, it is important to initialize the weights to", "random values. We want different parts of the network to tend to \u201caddress\u201d\ndifferent aspects of the problem; if they all start at the same weights, the symmetry", "will often keep the values from moving in useful directions. Second, many of our\nactivation functions have (near) zero slope when the pre-activation  values have", "large magnitude, so we generally want to keep the initial weights small so we will\nbe in a situation where the gradients are non-zero, so that gradient descent will", "have some useful signal about which way to go.\nOne good general-purpose strategy is to choose each weight at random from a\nGaussian (normal) distribution with mean 0 and standard deviation \n where", "where\n is the number of inputs to the unit.\n\u2753 Study Question\n\u2202Z l/\u2202W l\nf L\n1:\nDn, T, L, (m1, \u2026 , mL), (f 1, \u2026 , f L), Loss\n2:\nl \u21901\nL\n3:\nW l\nij \u223cGaussian(0, 1/ml)\n4:\nW l\n0j \u223cGaussian(0, 1)\n5:\n6:\nt \u21901", "5:\n6:\nt \u21901\nT\n7:\ni \u2190random sample from {1, \u2026 , n}\n8:\nA0 \u2190x(i)\nAL\n9:\nl \u21901\nL\n10:\nZ l \u2190W lTAl\u22121 + W l\n0\n11:\nAl \u2190f l(Z l)\n12:\n13:\nloss \u2190Loss(AL,  y(i))\n14:\nl \u2190L\n1\n15:\n\u2202loss\n\u2202Al\n\u2190{\n\u2202Z l+1\n\u2202Al\n\u22c5\n\u2202loss", "\u2202Z l+1\n\u2202Al\n\u22c5\n\u2202loss\n\u2202Z l+1\nif l < L,\n\u2202loss\n\u2202AL\notherwise\n16:\n\u2202loss\n\u2202Z l \u2190\u2202Al\n\u2202Z l \u22c5\u2202loss\n\u2202Al\n17:\n\u2202loss\n\u2202W l \u2190Al\u22121 ( \u2202loss\n\u2202Z l )\n\u22a4\n18:\n\u2202loss\n\u2202W l\n0\n\u2190\u2202loss\n\u2202Z l\n19:\nW l \u2190W l \u2212\u03b7(t) \u2202loss\n\u2202W l\n20:\nW l", "\u2202W l\n20:\nW l\n0 \u2190W l\n0 \u2212\u03b7(t) \u2202loss\n\u2202W l\n0\n21:\n22:\n23:\nW\nz\n(1/m)\nm\n\uf229\n6  Neural Networks\n If the input  to this unit is a vector of 1\u2019s, what would the expected pre-", "activation  value be with these initial weights?\nWe write this choice (where \n means \u201cis drawn randomly from the distribution\u201d) as", "It will often turn out (especially for fancier activations and loss functions) that\ncomputing \n is easier than computing \n and \n So, we may instead ask for", "an implementation of a loss function to provide a backward method that computes\n directly.\n6.7 Optimizing neural network parameters", "Because neural networks are just parametric functions, we can optimize loss with\nrespect to the parameters using standard gradient-descent software, but we can take", "advantage of the structure of the loss function and the hypothesis class to improve\noptimization. As we have seen, the modular function-composition structure of a", "neural network hypothesis makes it easy to organize the computation of the\ngradient. As we have also seen earlier, the structure of the loss function as a sum", "over terms, one per training data point, allows us to consider stochastic gradient\nmethods. In this section we\u2019ll consider some alternative strategies for organizing", "training, and also for making it easier to handle the step-size parameter.\nAssume that we have an objective of the form\nwhere  is the function computed by a neural network, and \n stands for all the", "stands for all the\nweight matrices and vectors in the network.\nRecall that, when we perform batch (or the vanilla) gradient descent, we use the\nupdate rule\nwhich is equivalent to", "So, we sum up the gradient of loss at each training point, with respect to \n, and\nthen take a step in the negative direction of the gradient.\nx\nz\n\u223c\nW l\nij \u223cGaussian (0,\n1\nml ).\n\u2202loss\n\u2202Z L\n\u2202loss\n\u2202AL", "\u2202Z L\n\u2202loss\n\u2202AL\n\u2202AL\n\u2202Z L .\n\u2202loss/\u2202Z L\n6.7.1 Batches\nJ(W) = 1\nn\nn\n\u2211\ni=1\nL(h(x(i); W), y(i)) ,\nh\nW\nWt = Wt\u22121 \u2212\u03b7\u2207WJ(Wt\u22121) ,\nWt = Wt\u22121 \u2212\u03b7\nn\n\u2211\ni=1\n\u2207WL(h(x(i); Wt\u22121), y(i)) .\nW\n\uf229\n6  Neural Networks", "6  Neural Networks\n In stochastic gradient descent, we repeatedly pick a point \n at random from\nthe data set, and execute a weight update on that point alone:", "As long as we pick points uniformly at random from the data set, and decrease  at\nan appropriate rate, we are guaranteed, with high probability, to converge to at least\na local optimum.", "a local optimum.\nThese two methods have offsetting virtues. The batch method takes steps in the\nexact gradient direction but requires a lot of computation before even a single step", "can be taken, especially if the data set is large. The stochastic method begins moving\nright away, and can sometimes make very good progress before looking at even a", "substantial fraction of the whole data set, but if there is a lot of variability in the\ndata, it might require a very small  to effectively average over the individual steps", "moving in \u201ccompeting\u201d directions.\nAn effective strategy is to \u201caverage\u201d between batch and stochastic gradient descent\nby using mini-batches. For a mini-batch of size \n, we select", ", we select \n distinct data points\nuniformly at random from the data set and do the update based just on their\ncontributions to the gradient", "Most neural network software packages are set up to do mini-batches.\n\u2753 Study Question\nFor what value of \n is mini-batch gradient descent equivalent to stochastic", "gradient descent? To batch gradient descent?\nPicking \n unique data points at random from a large data-set is potentially", "computationally difficult. An alternative strategy, if you have an efficient procedure\nfor randomly shuffling the data set (or randomly shuffling a list of indices into the", "data set) is to operate in a loop, roughly as follows:\nprocedure Mini-Batch-SGD(NN, data, K)\nwhile not done do\nRandom-Shuffle(data)\nfor \n to \n do\nBatch-Gradient-Update(NN, data[(i-1)K : iK])\nend for", "end for\nend while\nend procedure\n(x(i), y(i))\nWt = Wt\u22121 \u2212\u03b7\u2207WL(h(x(i); Wt\u22121), y(i)) .\n\u03b7\n\u03b7\nK\nK\nWt = Wt\u22121 \u2212\u03b7\nK\nK\n\u2211\ni=1\n\u2207WL(h(x(i); Wt\u22121), y(i)) .\nK\nK\n1:\n2:\nn \u2190length(data)\n3:\n4:\n5:\ni \u21901\n\u2308n\nK \u2309\n6:\n7:\n8:", "\u2308n\nK \u2309\n6:\n7:\n8:\n9:\nIn line 4 of the algorithm above, \nis known as the ceiling function; it\nreturns the smallest integer greater\nthan or equal to its input. E.g.,\n and \n.\n\u2308\u22c5\u2309\n\u23082.5\u2309= 3\n\u23083\u2309= 3\n\uf229", "\u23082.5\u2309= 3\n\u23083\u2309= 3\n\uf229\n6  Neural Networks\n Picking a value for  is difficult and time-consuming. If it\u2019s too small, then\nconvergence is slow and if it\u2019s too large, then we risk divergence or slow", "convergence due to oscillation. This problem is even more pronounced in stochastic\nor mini-batch mode, because we know we need to decrease the step size for the\nformal guarantees to hold.", "It\u2019s also true that, within a single neural network, we may well want to have\ndifferent step sizes. As our networks become deep (with increasing numbers of", "layers) we can find that magnitude of the gradient of the loss with respect the\nweights in the last layer, \n, may be substantially different from the", "gradient of the loss with respect to the weights in the first layer \n. If you\nlook carefully at Equation 6.6, you can see that the output gradient is multiplied by", "all the weight matrices of the network and is \u201cfed back\u201d through all the derivatives\nof all the activation functions. This can lead to a problem of exploding or vanishing", "gradients, in which the back-propagated gradient is much too big or small to be\nused in an update rule with the same step size.", "So, we can consider having an independent step-size parameter for each weight, and\nupdating it based on a local view of how the gradient updates have been going.", "Some common strategies for this include momentum (\u201caveraging\u201d recent gradient\nupdates), Adadelta (take larger steps in parts of the space where \n is nearly flat),", "is nearly flat),\nand Adam (which combines these two previous ideas). Details of these approaches\nare described in Section B.1.\n6.8 Regularization", "6.8 Regularization\nSo far, we have only considered optimizing loss on the training data as our objective\nfor neural network training. But, as we have discussed before, there is a risk of", "overfitting if we do this. The pragmatic fact is that, in current deep neural networks,\nwhich tend to be very large and to be trained with a large amount of data,", "overfitting is not a huge problem. This runs counter to our current theoretical\nunderstanding and the study of this question is a hot area of research. Nonetheless,", "there are several strategies for regularizing a neural network, and they can\nsometimes be important.\nOne group of strategies can, interestingly, be shown to have similar effects to each", "other: early stopping, weight decay, and adding noise to the training data.\nEarly stopping is the easiest to implement and is in fairly common use. The idea is", "to train on your training set, but at every epoch (a pass through the whole training\n6.7.2 Adaptive step-size\n\u03b7\n\u2202loss/\u2202WL\n\u2202loss/\u2202W1\nJ(W)\n6.8.1 Methods related to ridge regression", "This section is very strongly\ninfluenced by Sebastian Ruder\u2019s\nexcellent blog posts on the topic:\n{ruder.io/optimizing-gradient-\ndescent}\nResult is due to Bishop, described\nin his textbook and here.", "Warning: If you use your\nvalidation set in this way \u2013 i.e., to\n\uf229\n6  Neural Networks\n set, or possibly more frequently), evaluate the loss of the current \n on a validation", "on a validation\nset. It will generally be the case that the loss on the training set goes down fairly\nconsistently with each iteration, the loss on the validation set will initially decrease,", "but then begin to increase again. Once you see that the validation loss is\nsystematically increasing, you can stop training and return the weights that had the\nlowest validation error.", "Another common strategy is to simply penalize the norm of all the weights, as we\ndid in ridge regression. This method is known as weight decay, because when we\ntake the gradient of the objective", "we end up with an update of the form\nThis rule has the form of first \u201cdecaying\u201d \n by a factor of \n and then\ntaking a gradient step.\nFinally, the same effect can be achieved by perturbing the", "values of the training\ndata by adding a small amount of zero-mean normally distributed noise before each\ngradient computation. It makes intuitive sense that it would be more difficult for", "the network to overfit to particular training data if they are changed slightly on\neach training step.\nDropout is a regularization method that was designed to work with deep neural", "networks. The idea behind it is, rather than perturbing the data every time we train,\nwe\u2019ll perturb the network! We\u2019ll do this by randomly, on each training step,", "selecting a set of units in each layer and prohibiting them from participating. Thus,\nall of the units will have to take a kind of \u201ccollective\u201d responsibility for getting the", "answer right, and will not be able to rely on any small subset of the weights to do\nall the necessary computation. This tends also to make the network more robust to\ndata perturbations.", "data perturbations.\nDuring the training phase, for each training example, for each unit, randomly with\nprobability  temporarily set \n. There will be no contribution to the output and", "no gradient update for the associated unit.\nWhen we are done training and want to use the network to make predictions, we\nmultiply all weights by  to achieve the same average activation levels.\nW", "W\nJ(W) =\nn\n\u2211\ni=1\nL(NN(x(i)), y(i); W) + \u03bb\u2225W\u22252\nWt = Wt\u22121 \u2212\u03b7 ((\u2207WL(NN(x(i)), y(i); Wt\u22121)) + 2\u03bbWt\u22121)\n= Wt\u22121(1 \u22122\u03bb\u03b7) \u2212\u03b7 (\u2207WL(NN(x(i)), y(i); Wt\u22121)) .\nWt\u22121\n(1 \u22122\u03bb\u03b7)\nx(i)\n6.8.2 Dropout\np\na\u2113\nj = 0\np", "p\na\u2113\nj = 0\np\nset the number of epochs (or any\nother hyperparameter associated\nwith your learning algorithm) \u2013\nthen error on the validation set no\nlonger provides a \u201cpure\u201d estimate", "of error on the test set (i.e.,\ngeneralization error). This is\nbecause information about the\nvalidation set has \u201cleaked\u201d into the\ndesign of your algorithm. See also\nthe discussion on Validation and", "Cross-Validation in Chapter 2.\n\uf229\n6  Neural Networks\n Implementing dropout is easy! In the forward pass during training, we let\nwhere  denotes component-wise product and", "is a vector of \u2019s and \u2019s drawn\nrandomly with probability . The backwards pass depends on \n, so we do not need\nto make any further changes to the algorithm.\nIt is common to set  to", ", but this is something one might experiment with to get\ngood results on your problem and data.\nAnother strategy that seems to help with regularization and robustness in training", "is batch normalization.\nIt was originally developed to address a problem of covariate shift: that is, if you\nconsider the second layer of a two-layer neural network, the distribution of its input", "values is changing over time as the first layer\u2019s weights change. Learning when the\ninput distribution is changing is extra difficult: you have to change your weights to", "improve your predictions, but also just to compensate for a change in your inputs\n(imagine, for instance, that the magnitude of the inputs to your layer is increasing", "over time\u2014then your weights will have to decrease, just to keep your predictions\nthe same).\nSo, when training with mini-batches, the idea is to standardize the input values for", "each mini-batch, just in the way that we did it in Section 5.3.3 of Chapter 5,\nsubtracting off the mean and dividing by the standard deviation of each input", "dimension. This means that the scale of the inputs to each layer remains the same,\nno matter how the weights in previous layers change. However, this somewhat", "complicates matters, because the computation of the weight updates will need to\ntake into account that we are performing this transformation. In the modular view,", "batch normalization can be seen as a module that is applied to \n, interposed after\nthe product with \n and before input to \n.", ".\nAlthough batch-norm was originally justified based on the problem of covariate\nshift, it\u2019s not clear that that is actually why it seems to improve performance. Batch", "normalization can also end up having a regularizing effect for similar reasons that\nadding noise and dropout do: each mini-batch of data ends up being mildly", "perturbed, which prevents the network from exploiting very particular values of\nthe data points. For those interested, the equations for batch normalization,", "including a derivation of the forward pass and backward pass, are described in\nSection B.2.\na\u2113= f(z\u2113) \u2217d\u2113\n\u2217\nd\u2113\n0\n1\np\na\u2113\np\n0.5\n6.8.3 Batch normalization\nzl\nW l\nf l\nFor more details see", "arxiv.org/abs/1502.03167.\nWe follow here the suggestion from\nthe original paper of applying\nbatch normalization before the\nactivation function. Since then it\nhas been shown that, in some", "cases, applying it after works a bit\nbetter. But there aren\u2019t any definite\nfindings on which works better\nand when.\n\uf229\n6  Neural Networks\n \uf229\n6  Neural Networks"]