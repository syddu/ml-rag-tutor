["We are actively overhauling the Transformers chapter from the legacy PDF notes to\nenhance clarity and presentation. Please feel free to raise issues or request more\nexplanation on specific topics.", "Transformers are a very recent family of architectures that were originally\nintroduced in the field of natural language processing (NLP) in 2017, as an", "approach to process and understand human language. Since then, they have\nrevolutionized not only NLP but also other domains such as image processing and", "multi-modal generative AI. Their scalability and parallelizability have made them\nthe backbone of large-scale foundation models, such as GPT, BERT, and Vision", "Transformers (ViT), powering many state-of-the-art applications.\nLike CNNs, transformers factorize signal processing into stages, each involving", "independently and identically processed chunks. Transformers have many intricate\ncomponents; however, we\u2019ll focus on their most crucial innovation: a new type of", "layer called the attention layer. Attention layers enable transformers to effectively\nmix information across chunks, allowing the entire transformer pipeline to model", "long-range dependencies among these chunks. To help make Transformers more\ndigestible, in this chapter, we will first succinctly motivate and describe them in an", "overview Section 9.1. Then, we will dive into the details following the flow of data \u2013\nfirst describing how to represent inputs Section 9.2, and then describe the attention", "mechanism Section 9.3, and finally we then assemble all these ideas together to\narrive at the full transformer architecture in Section 9.5.\n9.1 Transformers Overview", "Transformers are powerful neural architectures designed primarily for sequential\ndata, such as text. At their core, transformers are typically auto-regressive, meaning", "they generate sequences by predicting each token sequentially, conditioned on\npreviously generated tokens. This auto-regressive property ensures that the", "transformer model inherently captures temporal dependencies, making them\nespecially suited for language modeling tasks like text generation and completion.", "Suppose our training data contains this sentence: \u201cTo date, the cleverest thinker was\nIssac.\u201d The transformer model will learn to predict the next token in the sequence,", "given the previous tokens. For example, when predicting the token \u201ccleverest,\u201d the\nmodel will condition its prediction on the tokens \u201cTo,\u201d \u201cdate,\u201d and \u201cthe.\u201d This\n9  Transformers\nCaution", "Caution\nHuman language is inherently\nsequential in nature (e.g.,\ncharacters form words, words form\nsentences, and sentences form\nparagraphs and documents). Prior\nto the advent of the transformers", "architecture, recurrent neural\nnetworks (RNNs) briefly\ndominated the field for their ability\nto process sequential information.\nHowever, RNNs, like many other\narchitectures, processed sequential", "information in an\niterative/sequential fashion,\nwhereby each item of a sequence\nwas individually processed one\nafter another. Transformers offer\nmany advantages over RNNs,", "including their ability to process all\nitems in a sequence in a parallel\nfashion (as do CNNs).\n\uf4619  Transformers\n\uf52a\n process continues until the entire sequence is generated.", "The animation above illustrates the auto-regressive nature of transformers.\nBelow is another example. Suppose the sentence is the 2nd law of robotics: \u201cA robot", "must obey the orders given it by human beings\u2026\u201d The training objective of a\ntransformer would be to make each token\u2019s prediction, conditioning on previously", "generated tokens, forming a step-by-step probability distribution over the\nvocabulary.\nThe transformer architecture processes inputs by applying multiple identical", "building blocks stacked in layers. Each block performs a transformation that\nprogressively refines the internal representation of the data.", "Specifically, each block consists of two primary sub-layers: an attention layer\nSection 9.4 and a feed-forward network (or multi-layer perceptron) Chapter 6.", "Attention layers mix information across different positions (or \"chunks\") in the\nsequence, allowing the model to effectively capture dependencies regardless of", "distance. Meanwhile, the feed-forward network significantly enhances the\nexpressiveness of these representations by applying non-linear transformations\nindependently to each position.", "A notable strength of transformers is their capacity for parallel processing.\nTransformers process entire sequences simultaneously rather than sequentially", "token-by-token. This parallelization significantly boosts computational efficiency\nand makes it feasible to train larger and deeper models.", "In this overview, we emphasize the auto-regressive nature of transformers, their\nlayered approach to transforming representations, the parallel processing", "advantage, and the critical role of the feed-forward layers in enhancing their\nexpressive power.\nThere are additional essential components and enhancements\u2014such as causal", "attention mechanisms and positional encoding\u2014that further empower\ntransformers. We\u2019ll explore these \"bells and whistles\" in greater depth in subsequent\ndiscussions.\n9.2 Embedding and Representations", "We start by describing how language is commonly represented, then we provide a\nbrief explanation of why it can be useful to predict subsequent items (e.g.,\nwords/tokens) in a sequence.", "As a reminder, two key components of any ML system are: (1) the representation of\nthe data; and (2) the actual modelling to perform some desired task. Computers, by", "default, have no natural way of representing human language. Modern computers\nare based on the Von Neumann architecture and are essentially very powerful", "calculators, with no natural understanding of what any particular string of\ncharacters means to us humans. Considering the rich complexities of language (e.g.,", "humor, sarcasm, social and cultural references and implications, slang, homonyms,\netc), you can imagine the innate difficulties of appropriately representing", "languages, along with the challenges for computers to then model and\n\u201cunderstand\u201d language.\nThe field of NLP aims to represent words with vectors of floating-point numbers", "(aka word embeddings) such that they capture semantic meaning. More precisely,\nthe degree to which any two words are related in the \u2018real-world\u2019 to us humans", "should be reflected by their corresponding vectors (in terms of their numeric\nvalues). So, words such as \u2018dog\u2019 and \u2018cat\u2019 should be represented by vectors that are", "more similar to one another than, say, \u2018cat\u2019 and \u2018table\u2019 are.\nTo measure how similar any two word embeddings are (in terms of their numeric", "values) it is common to use some similarity as the metric, e.g. the dot-product\nsimilarity we saw in Chapter 8.\nThus, one can imagine plotting every word embedding in -dimensional space and", "observing natural clusters to form, whereby similar words (e.g., synonyms) are\nlocated near each other. The problem of determining how to parse (aka tokenize)", "individual words is known as tokenization. This is an entire topic of its own, so we\nwill not dive into the full details here. However, the high-level idea of tokenization", "is straightforward: the individual inputs of data that are represented and processed\nby a model are referred to as tokens. And, instead of processing each word as a", "whole, words are typically split into smaller, meaningful pieces (akin to syllables).\nFor example, the word \u201cevaluation\u201d may be input into a model as 3 individual\nd\nHow can we define an optimal", "vocabulary of such tokens? How\nmany distinct tokens should we\nhave in our vocabulary? How\nshould we handle digits or other\npunctuation? How does this work\nfor non-English languages, in", "particular, script-based languages\nwhere word boundaries are less\nobvious (e.g., Chinese or\nJapanese)? All of these are open", "tokens (eval + ua + tion). Thus, when we refer to tokens, know that we\u2019re referring\nto these sub-word units. For any given application/model, all of the language data", "must be predefined by a finite vocabulary of valid tokens (typically on the order of\n40,000 distinct tokens).\n9.3 Query, Key, Value, and Attention Output", "Attention mechanisms efficiently process global information by selectively focusing\non the most relevant parts of the input. Given an input sentence, each token is", "processed sequentially to predict subsequent tokens. As more context (previous\ntokens) accumulates, this context ideally becomes increasingly beneficial\u2014provided", "the model can appropriately utilize it. Transformers employ a mechanism known as\nattention, which enables models to identify and prioritize contextually relevant\ntokens.", "tokens.\nFor example, consider the partial sentence: \u201cAnqi forgot ___\u201c. At this point, the\nmodel has processed tokens\u201dAnqi\u201d and \u201cforgot,\u201d and aims to predict the next", "token. Numerous valid completions exist, such as articles (\u201cthe,\u201d \u201can\u201d),\nprepositions (\u201cto,\u201d \u201cabout\u201d), or possessive pronouns (\u201cher,\u201d \u201chis,\u201d \u201ctheir\u201d). A well-", "trained model should assign higher probabilities to contextually relevant tokens,\nsuch as \u201cher,\u201d based on the feminine-associated name \u201cAnqi.\u201d Attention", "mechanisms guide the model to selectively focus on these relevant contextual cues\nusing query, key, and value vectors.", "Our goal is for each input token to learn how much attention it should give to every\nother token in the sequence. To achieve this, each token is assigned a unique query", "vector used to \u201cprobe\u201d or assess other tokens\u2014including itself\u2014to determine\nrelevance.\nA token\u2019s query vector \n is computed by multiplying the input token \n (a -", "(a -\ndimensional vector) by a learnable query weight matrix \n (of dimension \n,\n is a hyperparameter typically chosen such that \n):", "):\nThus, for a sequence of  tokens, we generate  distinct query vectors.\nTo complement query vectors, we introduce key vectors, which tokens use to", "\u201canswer\u201d queries about their relevance. Specifically, when evaluating token \n, its\nquery vector \n is compared to each token\u2019s key vector \n to determine the attention\nweight. Each key vector", "is computed similarly using a learnable key weight\nmatrix \n:\n9.3.1 Query Vectors\nqi\nxi\nd\nWq\nd \u00d7 dk\ndk\ndk < d\nqi = W T\nq xi\nn\nn\n9.3.2 Key Vectors\nx3\nq3\nkj\nki\nWk\nT\nNLP research problems receiving", "increased attention lately.\n The attention mechanism calculates similarity using the dot product, which\nefficiently measures vector similarity:\nThe vector", "The vector \n (softmax\u2019d attention scores) quantifies how much attention token \nshould pay to each token in the sequence, normalized so that elements sum to 1.\nNormalizing by", "Normalizing by \n prevents large dot-product magnitudes, stabilizing training.\nTo incorporate meaningful contributions from attended tokens, we use value\nvectors (", "vectors (\n), providing distinct representations for contribution to attention outputs.\nEach token\u2019s value vector is computed with another learnable matrix \n:", ":\nFinally, attention outputs are computed as weighted sums of value vectors, using\nthe softmax\u2019d attention scores:\nThis vector \n represents token \n\u2019s enriched embedding, incorporating context", "from across the sequence, weighted by learned attention.\n9.4 Self-attention Layer\nSelf-attention is an attention mechanism where the keys, values, and queries are all\ngenerated from the same input.", "At a very high level, typical transformer with self-attention layers maps\n. In particular, the transformer takes in  tokens, each having feature\ndimension", "dimension \n and through many layers of transformation (most important of which\nare self-attention layers); the transformer finally outputs a sequence of  tokens,\neach of which -dimensional still.", "With a self-attention layer, there can be multiple attention head. We start with\nunderstanding a single head.\nA single self-attention head is largely the same as our discussion in Section 9.3. The", "main additional info introduced in this part is a compact matrix form. The layer\nki = W T\nk xi\nai = softmax( [qT\ni k1, qT\ni k2, \u2026 , qT\ni kn]\n\u221adk\n)\nT\n\u2208R1\u00d7n\nai\nqi\n\u221adk\n9.3.3 Value Vectors\nvi\nWv\nvi = W T", "vi\nWv\nvi = W T\nv xi\n9.3.4 Attention Output\nzi =\nn\n\u2211\nj=1\naijvj \u2208Rdk\nzi\nxi\nRn\u00d7d \u27f6Rn\u00d7d\nn\nd,\nn\nd\n9.4.1 A Single Self-attention Head", "takes in  tokens, each having feature dimension . Thus, all tokens can be\ncollectively written as \n, where the -th row of \n stores the -th token,\ndenoted as \n. For each token", ". For each token \n, self-attention computes (via learned\nprojection matrices, discussed in Section 9.3), a query \n, key \n, and\nvalue \n, and overall, we will have  queries,  keys, and  values; all of", "these vectors live in the same dimension in practice, and we often denote all three\nembedding dimension via a unified \n.\nThe self-attention output is calculated as a weighted sum:\nwhere", "where \n is the th element in \n.\nSo far, we\u2019ve discussed self-attention focusing on a single token input-output.\nActually, we can calculate all outputs \n (\n) at the same time using a", "matrix form. For clearness, we first introduce the \n query matrix,\n key matrix, and \n value matrix:\nIt should be straightforward to understand that the \n, \n, \n matrices simply stack\n, \n, and", ", \n, and \n in a row-wise manner, respectively. Now, the the full attention matrix\n is:\nwhich often time is shorten as:\nNote that the Softmax operation is applied in a row-wise manner. The th row  of", "this matrix corresponds to the softmax\u2019d attention scores computed for query \nover all keys (i.e., \n). The full output of the self-attention layer can then be written\ncompactly as:\nn\nd\nX \u2208Rn\u00d7d\ni\nX\ni", "n\nd\nX \u2208Rn\u00d7d\ni\nX\ni\nxi \u2208R1\u00d7d\nxi\nqi \u2208Rdq\nki \u2208Rdk\nvi \u2208Rdv\nn\nn\nn\ndk\nzi =\nn\n\u2211\nj=1\naijvj \u2208Rdk\naij\nj\nai\nzi i = 1, 2, \u2026 , n\nQ \u2208Rn\u00d7dk\nK \u2208Rn\u00d7dk\nV \u2208Rn\u00d7dk\nQ =\n\u2208Rn\u00d7d,\nK =\n\u2208Rn\u00d7d,\nV =\n\u2208Rn\u00d7dv\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1\nq\u22a4\n2\n\u22ee\nq\u22a4\nn\n\u23a4", "q\u22a4\n1\nq\u22a4\n2\n\u22ee\nq\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\nk\u22a4\n1\nk\u22a4\n2\n\u22ee\nk\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\n\u23a1\n\u23a2\n\u23a3\nv\u22a4\n1\nv\u22a4\n2\n\u22ee\nv\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\nQ K V\nqi ki\nvi\nA \u2208Rn\u00d7n\nA =\n\u23a1\n\u23a2\n\u23a3\nsoftmax ([\n]/\u221adk)\nsoftmax ([\n]/\u221adk)\n\u22ee\nsoftmax ([\n]/\u221adk)\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4", "1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n(9.1)\n= softmax ( QK \u22a4\n\u221adk\n)\nA = softmax\n1\n\u221adk\n\u239b\n\u239c\n\u239d\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\n\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1", "\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n\u239e\n\u239f\n\u23a0\ni\nA\nqi\n\u03b1i\n\u22a4\n where \n is the matrix of value vectors stacked row-wise, and \n is", "is\nthe output, whose th row corresponds to the attention output for the th query (i.e.,\n).\nYou will also see this compact notation Attention  in the literature, which is an", "operation of three arguments \n, \n, and \n (and we add an emphasis that the\nsoftmax is performed on each row):\nHuman language can be very nuanced. There are many properties of language that", "collectively contribute to a human\u2019s understanding of any given sentence. For\nexample, words have different tenses (past, present, future, etc), genders,", "abbreviations, slang references, implied words or meanings, cultural references,\nsituational relevance, etc. While the attention mechanism allows us to appropriately", "focus on tokens in the input sentence, it\u2019s unreasonable to expect a single set of\n matrices to fully represent \u2013 and for a model to capture \u2013 the meaning of\na sentence with all of its complexities.", "To address this limitation, the idea of multi-head attention is introduced. Instead of\nrelying on just one attention head (i.e., a single set of \n matrices), the", "matrices), the\nmodel uses multiple attention heads, each with its own independently learned set\nof \n matrices. This allows each head to attend to different parts of the", "input tokens and to model different types of semantic relationships. For instance,\none head might focus on syntactic structure and another on verb tense or sentiment.", "These different \u201cperspectives\u201d are then concatenated and projected to produce a\nricher, more expressive representation of the input.", "Now, we introduce the formal math notations. Let us denote the number of head as\n. For the th head, the input \n is linearly projected into query, key, and", "value matrices using the projection matrices \n, \n, and\n (recall that usually \n):\nThe output of the -th head is \n: \n. After\ncomputing all  heads, we concatenate their outputs and apply a final linear", "Z =\n= AV \u2208Rn\u00d7dk\n\u23a1\n\u23a2\n\u23a3\nz\u22a4\n1\nz\u22a4\n2\n\u22ee\nz\u22a4\nn\n\u23a4\n\u23a5\n\u23a6\nV \u2208Rn\u00d7dk\nZ \u2208Rn\u00d7dk\ni\ni\nzi\nQ K\nV\nAttention(Q, K, V ) = softmaxrow ( QK \u22a4\n\u221adk\n)V\n9.4.2 Multi-head Self-attention\n{Q, K, V }\n{Q, K, V }\n{Q, K, V }\nH\nh\nX \u2208Rn\u00d7d", "H\nh\nX \u2208Rn\u00d7d\nW h\nq \u2208Rd\u00d7dq W h\nk \u2208Rd\u00d7dk\nW h\nv \u2208Rd\u00d7dv\ndq = dk = dv\nQh = XW h\nq\nK h = XW h\nk\nV h = XW h\nv\ni\nZ h Z h = Attention(Qh, K h, V h) \u2208Rn\u00d7dk\nh\n projection:", "h\n projection:\nwhere the concatenation operation concatenates \n horizontally, yielding a matrix\nof size \n, and \n is a final linear projection matrix.\n9.5 Transformers Architecture Details", "An extremely observant reader might have been suspicious of a small but very\nimportant detail that we have not yet discussed: the attention mechanism, as", "introduced so far, does not encode the order of the input tokens. For instance, when\ncomputing softmax\u2019d attention scores and building token representations, the", "model is fundamentally permutation-equivariant \u2014 the same set of tokens, even if\nscrambled into a different order, would result in identical outputs permuted in the", "same order \u2014 Formally, when we fix \n and switch the input \n with \n, then the output \n and \n will be switched. However, natural language is not a bag\nof words: meaning is tied closely to word order.", "To address this, transformers incorporate positional embeddings \u2014 additional\ninformation that encodes the position of each token in the sequence. These", "embeddings are added to the input token embeddings before any attention layers\nare applied, effectively injecting ordering information into the model.", "There are two main strategies for positional embeddings: (i) learned positional\nembeddings, where a trainable vector \n is assigned to each position (i.e.,\ntoken index)", "token index) \n. These vectors are learned alongside all other model\nparameters and allow the model to discover how best to encode position for a given", "task, (ii) fixed positional embeddings, such as sinusoidal positional embedding\nproposed in the original Transformer paper:\nwhere \n is the token index, while \n is the dimension index.", "Namely, this sinusoidal positional embedding uses sine for the even dimension and\ncosine for the odd dimension. Regardless of learnable or fixed positional", "embedding, it will enter the computation of attention at the input place:\n\u200b where \n is the th original input token, and \n is its positional\nembedding. The", "embedding. The \n will now be what we really feed into the attention layer, so that\nthe input to the attention mechanism now carries information about both what the", "token is and where it appears in the sequence.\nMultiHead(X) = Concat(Z 1, \u2026 , Z H)(W O)T\nZ h\nn \u00d7 Hdk\nW O \u2208Rd\u00d7Hdk\n9.5.1 Positional Embeddings\n{Wq, Wk, Wv}\nxi\nxj\nzi\nzj\npi \u2208Rd\ni = 0, 1, 2, . . . , n", "p(i,2k) = sin (\ni\n100002k/d )\np(i,2k+1) = cos (\ni\n100002k/d )\ni = 1, 2, . . , n\nk = 1, 2, . . . , d\nx\u2217\ni = xi + pi ,\nxi\ni\npi\nx\u2217\ni", "xi\ni\npi\nx\u2217\ni\n This simple additive design enables attention layers to leverage both semantic\ncontent and ordering structure when deciding where to focus. In practice, this", "addition occurs at the very first layer of the transformer stack, and all subsequent\nlayers operate on position-aware representations. This is a key design choice that", "allows transformers to work effectively with sequences of text, audio, or even image\npatches (as in Vision Transformers).\nMore generally, a mask may be applied to limit which tokens are used in the", "attention computation. For example, one common mask limits the attention\ncomputation to tokens that occur previously in time to the one being used for the", "query. This prevents the attention mechanism from \u201clooking ahead\u201d in scenarios\nwhere the transformer is being used to generate one token at a time. This causal", "masking is done by introducing a mask matrix \n that restricts attention to\nonly current and previous positions. A typical causal mask is a lower-triangular\nmatrix:", "matrix:\nand we now have the masked attention matrix:\nThe softmax is performed to each row independently. The attention output is still\n. Essentially, the lower-triangular property of", "ensures that the self-\nattention operation for the -th query only considers tokens \n. Note that we\nshould apply the masking before performing softmax, so that the attention matrix", "can be properly normalized (i.e., each row sum to 1).\nEach self-attention stage is trained to have key, value, and query embeddings that", "lead it to pay specific attention to some particular feature of the input. We generally\nwant to pay attention to many different kinds of features in the input; for example,", "in translation one feature might be be the verbs, and another might be objects or\nsubjects. A transformer utilizes multiple instances of self-attention, each known as", "an \u201cattention head,\u201d to allow combinations of attention paid to many different\nfeatures.\n9.5.2 Causal Self-attention\nM \u2208Rn\u00d7n\nM =\n\u23a1\n\u23a2\n\u23a3\n0\n\u2212\u221e\n\u2212\u221e\n\u22ef\n\u2212\u221e\n0\n0\n\u2212\u221e\n\u22ef\n\u2212\u221e\n0\n0\n0\n\u22ef\n\u2212\u221e\n\u22ee\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n0\n0\n0\n\u22ef\n0\n\u23a4\n\u23a5\n\u23a6", "\u22f1\n\u22ee\n0\n0\n0\n\u22ef\n0\n\u23a4\n\u23a5\n\u23a6\nA = softmax\n1\n\u221adk\n+ M\n\u239b\n\u239c\n\u239d\n\u23a1\n\u23a2\n\u23a3\nq\u22a4\n1 k1\nq\u22a4\n1 k2\n\u22ef\nq\u22a4\n1 kn\nq\u22a4\n2 k1\nq\u22a4\n2 k2\n\u22ef\nq\u22a4\n2 kn\n\u22ee\n\u22ee\n\u22f1\n\u22ee\nq\u22a4\nn k1\nq\u22a4\nn k2\n\u22ef\nq\u22a4\nn kn\n\u23a4\n\u23a5\n\u23a6\n\u239e\n\u239f\n\u23a0\nY = AV\nM\nj\n0, 1, . . . , j"]