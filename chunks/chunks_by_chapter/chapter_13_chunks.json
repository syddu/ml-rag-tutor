["What are some conventions for derivatives of matrices and vectors? It will always\nwork to explicitly write all indices and treat everything as scalars, but we", "introduce here some shortcuts that are often faster to use and helpful for\nunderstanding.\nThere are at least two consistent but different systems for describing shapes and", "rules for doing matrix derivatives. In the end, they all are correct, but it is\nimportant to be consistent.\nWe will use what is often called the \u2018Hessian\u2019 or denominator layout, in which we", "say that for\n of size \n and  of size \n, \n is a matrix of size \n with the \nentry \n. This denominator layout convention has been adopted by the field", "of machine learning to ensure that the shape of the gradient is the same as the\nshape of the respective derivative. This is somewhat controversial at large, but alas,", "we shall continue with denominator layout.\nThe discussion below closely follows the Wikipedia on matrix derivatives.\nA.1 The shapes of things\nHere are important special cases of the rule above:", "Scalar-by-scalar: For  of size \n and  of size \n, \n is the (scalar)\npartial derivative of  with respect to .\nScalar-by-vector: For  of size \n and  of size \n, \n (also written", ", \n (also written\n, the gradient of  with respect to ) is a column vector of size \n with\nthe \n entry \n:\nVector-by-scalar: For  of size \n and  of size \n, \n is a row\nvector of size \n with the \n entry", "with the \n entry \n:\nVector-by-vector: For  of size \n and  of size \n, \n is a matrix of\nsize \n with the \n entry \n:\nAppendix A \u2014 Matrix derivative common\ncases\nx\nn \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\nn \u00d7 m\n(i, j)", "n \u00d7 m\n(i, j)\n\u2202yj/\u2202xi\nx\n1 \u00d7 1\ny\n1 \u00d7 1 \u2202y/\u2202x\ny\nx\nx\nn \u00d7 1\ny\n1 \u00d7 1 \u2202y/\u2202x\n\u2207xy\ny\nx\nn \u00d7 1\nith\n\u2202y/\u2202xi\n\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y/\u2202x1\n\u2202y/\u2202x2\n\u22ee\n\u2202y/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nx\n1 \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\n1 \u00d7 m\njth\n\u2202yj/\u2202x\n\u2202y/\u2202x = [\n].\n\u2202y1/\u2202x", "\u2202y/\u2202x = [\n].\n\u2202y1/\u2202x\n\u2202y2/\u2202x\n\u22ef\n\u2202ym/\u2202x\nx\nn \u00d7 1\ny\nm \u00d7 1 \u2202y/\u2202x\nn \u00d7 m\n(i, j)\n\u2202yj/\u2202xi\n\uf461Appendices > A  Matrix derivative common cases\n\uf52a\n1\n Scalar-by-matrix: For \n of size \n and  of size \n, \n (also written", ", \n (also written\n, the gradient of  with respect to \n) is a matrix of size \n with the\n entry \n:\nYou may notice that in this list, we have not included matrix-by-matrix, matrix-by-", "vector, or vector-by-matrix derivatives. This is because, generally, they cannot be\nexpressed nicely in matrix form and require higher order objects (e.g., tensors) to", "represent their derivatives. These cases are beyond the scope of this course.\nAdditionally, notice that for all cases, you can explicitly compute each element of", "the derivative object using (scalar) partial derivatives. You may find it useful to\nwork through some of these by hand as you are reviewing matrix derivatives.\nA.2 Some vector-by-vector identities", "Here are some examples of \n. In each case, assume  is \n,  is \n,  is\na scalar constant,  is a vector that does not depend on  and \n is a matrix that", "is a matrix that\ndoes not depend on ,  and  are scalars that do depend on , and  and  are\nvectors that do depend on . We also have vector-valued functions  and .", "First, we will cover a couple of fundamental cases: suppose that  is an \nvector which is not a function of , an \n vector. Then,\nis an", "is an \n matrix of 0s. This is similar to the scalar case of differentiating a\nconstant. Next, we can consider the case of differentiating a vector with respect to\nitself:\nThis is the", "This is the \n identity matrix, with 1\u2019s along the diagonal and 0\u2019s elsewhere. It\nmakes sense, because \n is 1 for \n and 0 otherwise. This identity is also\nsimilar to the scalar case.\n\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3", "\u2202y/\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y1/\u2202x1\n\u2202y2/\u2202x1\n\u22ef\n\u2202ym/\u2202x1\n\u2202y1/\u2202x2\n\u2202y2/\u2202x2\n\u22ef\n\u2202ym/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202y1/\u2202xn\n\u2202y2/\u2202xn\n\u22ef\n\u2202ym/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nX\nn \u00d7 m\ny\n1 \u00d7 1 \u2202y/\u2202X\n\u2207Xy\ny\nX\nn \u00d7 m\n(i, j)\n\u2202y/\u2202Xi,j\n\u2202y/\u2202X =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202y/\u2202X1,1\n\u22ef\n\u2202y/\u2202X1,m\n\u22ee\n\u22f1", "\u22ef\n\u2202y/\u2202X1,m\n\u22ee\n\u22f1\n\u22ee\n\u2202y/\u2202Xn,1\n\u22ef\n\u2202y/\u2202Xn,m\n\u23a4\n\u23a5\n\u23a6\n\u2202y/\u2202x\nx\nn \u00d7 1 y\nm \u00d7 1 a\na\nx\nA\nx u\nv\nx\nu\nv\nx\nf\ng\nA.2.1 Some fundamental cases\na\nm \u00d7 1\nx\nn \u00d7 1\n\u2202a\n\u2202x = 0,\n(A.1)\nn \u00d7 m\n\u2202x\n\u2202x = I\nn \u00d7 n\n\u2202xj/xi\ni = j", "n \u00d7 n\n\u2202xj/xi\ni = j\n Let the dimensions of \n be \n. Then the object \n is an \n vector. We can\nthen compute the derivative of \n with respect to  as:\nNote that any element of the column vector", "can be written as, for \n:\nThus, computing the \n entry of \n requires computing the partial derivative\nTherefore, the \n entry of \n is the \n entry of \n:\nSimilarly, for objects", "of the same shape, one can obtain,\nSuppose that \n are both vectors of size \n. Then,\nSuppose that  is a scalar constant and  is an \n vector that is a function of .\nThen,", "Then,\nOne can extend the previous identity to vector- and matrix-valued constants.\nSuppose that  is a vector with shape \n and  is a scalar which depends on .\nThen,\nFirst, checking dimensions, \n is", "is \n and  is \n so \n is \n and our\nanswer is \n as it should be. Now, checking a value, element \n of the\nA.2.2 Derivatives involving a constant matrix\nA\nm \u00d7 n\nAx\nm \u00d7 1\nAx\nx\n\u2202Ax\n\u2202x\n=\n\u23a1\n\u23a2\n\u23a3\n\u2202(Ax)1/\u2202x1", "=\n\u23a1\n\u23a2\n\u23a3\n\u2202(Ax)1/\u2202x1\n\u2202(Ax)2/\u2202x1\n\u22ef\n\u2202(Ax)m/\u2202x1\n\u2202(Ax)1/\u2202x2\n\u2202(Ax)2/\u2202x2\n\u22ef\n\u2202(Ax)m/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202(Ax)1/\u2202xn\n\u2202(Ax)2/\u2202xn\n\u22ef\n\u2202(Ax)m/\u2202xn\n\u23a4\n\u23a5\n\u23a6\nAx\nj = 1, \u2026 , m\n(Ax)j =\nn\n\u2211\nk=1\nAj,kxk.\n(i, j)\n\u2202Ax\n\u2202x\n\u2202(Ax)j/\u2202xi :", "\u2202Ax\n\u2202x\n\u2202(Ax)j/\u2202xi :\n\u2202(Ax)j/\u2202xi = \u2202(\nn\n\u2211\nk=1\nAj,kxk)/\u2202xi = Aj,i\n(i, j)\n\u2202Ax\n\u2202x\n(j, i)\nA\n\u2202Ax\n\u2202x\n= AT\n(A.2)\nx, A\n\u2202xTA\n\u2202x\n= A\n(A.3)\nA.2.3 Linearity of derivatives\nu, v\nm \u00d7 1\n\u2202(u + v)\n\u2202x\n= \u2202u\n\u2202x + \u2202v\n\u2202x", "\u2202x\n= \u2202u\n\u2202x + \u2202v\n\u2202x\n(A.4)\na\nu\nm \u00d7 1\nx\n\u2202au\n\u2202x = a \u2202u\n\u2202x\na\nm \u00d7 1\nv\nx\n\u2202va\n\u2202x = \u2202v\n\u2202x aT\n\u2202v/\u2202x\nn \u00d7 1\na\nm \u00d7 1\naT\n1 \u00d7 m\nn \u00d7 m\n(i, j)\n answer is \n = \n which corresponds to element \n of\n.", "of\n.\nSimilarly, suppose that \n is a matrix which does not depend on  and  is a\ncolumn vector which does depend on . Then,", "Suppose that  is a scalar which depends on , while  is a column vector of shape\n and  is a column vector of shape \n. Then,\nOne can see this relationship by expanding the derivative as follows:", "Then, one can use the product rule for scalar-valued functions,\nto obtain the desired result.\nSuppose that  is a vector-valued function with output vector of shape \n, and", ", and\nthe argument to  is a column vector  of shape \n which depends on . Then,\none can obtain the chain rule as,\nFollowing \u201cthe shapes of things,\u201d \n is \n and \n is \n, where\nelement \n is", "element \n is \n. The same chain rule applies for further compositions\nof functions:\nA.3 Some other identities\nYou can get many scalar-by-vector and vector-by-scalar cases as special cases of the", "rules above, making one of the relevant vectors just be 1 x 1. Here are some other\nones that are handy. For more, see the Wikipedia article on Matrix derivatives (for", "consistency, only use the ones in denominator layout).\n\u2202vaj/\u2202xi\n(\u2202v/\u2202xi)aj\n(i, j)\n(\u2202v/\u2202x)aT\nA\nx\nu\nx\n\u2202Au\n\u2202x\n= \u2202u\n\u2202x AT\nA.2.4 Product rule (vector-valued numerator)\nv\nx\nu\nm \u00d7 1\nx\nn \u00d7 1\n\u2202vu\n\u2202x = v \u2202u", "n \u00d7 1\n\u2202vu\n\u2202x = v \u2202u\n\u2202x + \u2202v\n\u2202x uT\n\u2202vu\n\u2202x =\n.\n\u23a1\n\u23a2\n\u23a3\n\u2202(vu1)/\u2202x1\n\u2202(vu2)/\u2202x1\n\u22ef\n\u2202(vum)/\u2202x1\n\u2202(vu1)/\u2202x2\n\u2202(vu2)/\u2202x2\n\u22ef\n\u2202(vum)/\u2202x2\n\u22ee\n\u22ee\n\u22f1\n\u22ee\n\u2202(vu1)/\u2202xn\n\u2202(vu2)/\u2202xn\n\u22ef\n\u2202(vum)/\u2202xn\n\u23a4\n\u23a5\n\u23a6", "\u22ef\n\u2202(vum)/\u2202xn\n\u23a4\n\u23a5\n\u23a6\n\u2202(vuj)/\u2202xi = v(\u2202uj/\u2202xi) + (\u2202v/\u2202xi)uj,\nA.2.5 Chain rule\ng\nm \u00d7 1\ng\nu\nd \u00d7 1\nx\n\u2202g(u)\n\u2202x\n= \u2202u\n\u2202x\n\u2202g(u)\n\u2202u\n\u2202u/\u2202x\nn \u00d7 d\n\u2202g(u)/\u2202u\nd \u00d7 m\n(i, j)\n\u2202g(u)j/\u2202ui\n\u2202f(g(u))\n\u2202x\n= \u2202u\n\u2202x\n\u2202g(u)\n\u2202u\n\u2202f(g)", "\u2202x\n\u2202g(u)\n\u2202u\n\u2202f(g)\n\u2202g\nT\n A.4 Derivation of gradient for linear regression\nRecall here that \n is a matrix of of size \n and \n is an \n vector.", "is an \n vector.\nApplying identities Equation A.3, Equation A.5,Equation A.4, Equation A.2,\nEquation A.1\nA.5 Matrix derivatives using Einstein summation", "You do not have to read or learn this! But you might find it interesting or helpful.\nConsider the objective function for linear regression, written out as products of\nmatrices:\nwhere \n is \n, \n is", "where \n is \n, \n is \n, and  is \n. How does one show, with no\nshortcuts, that\nOne neat way, which is very explicit, is to simply write all the matrices as variables\nwith row and column indices, e.g.,", "is the row , column  entry of the matrix\n. Furthermore, let us use the convention that in any product, all indices which\nappear more than once get summed over; this is a popular convention in", "theoretical physics, and lets us suppress all the summation symbols which would\notherwise clutter the following expresssions. For example, \n would be the", "would be the\nimplicit summation notation giving the element at the \n row of the matrix-vector\nproduct \n.\nUsing implicit summation notation with explicit indices, we can rewrite \n as\n\u2202uTv\n\u2202x\n= \u2202u", "as\n\u2202uTv\n\u2202x\n= \u2202u\n\u2202x v + \u2202v\n\u2202x u\n(A.5)\n\u2202uT\n\u2202x\n= ( \u2202u\n\u2202x )\nT\n(A.6)\nX\nn \u00d7 d\nY\nn \u00d7 1\n\u2202(X\u03b8 \u2212Y)T(X\u03b8 \u2212Y)/n\n\u2202\u03b8\n= 2\nn\n\u2202(X\u03b8 \u2212Y)\n\u2202\u03b8\n(X\u03b8 \u2212Y)\n= 2\nn ( \u2202X\u03b8\n\u2202\u03b8\n\u2212\u2202Y\n\u2202\u03b8 )(X\u03b8 \u2212Y)\n= 2\nn (XT \u22120)(X\u03b8 \u2212Y)\n= 2\nn XT(X\u03b8 \u2212Y)", "= 2\nn XT(X\u03b8 \u2212Y)\nJ(\u03b8) = 1\nn (X\u03b8 \u2212Y )T(X\u03b8 \u2212Y ) ,\nX\nn \u00d7 d Y\nn \u00d7 1\n\u03b8\nd \u00d7 1\n\u2207\u03b8J = 2\nn XT(X\u03b8 \u2212Y ) ?\nXab\na\nb\nX\nXab\u03b8b\nath\nX\u03b8\nJ(\u03b8)\nJ(\u03b8) = 1\nn (Xab\u03b8b \u2212Ya) (Xac\u03b8c \u2212Ya) .", "Note that we no longer need the transpose on the first term, because all that\ntranspose accomplished was to take a dot product between the vector given by the", "left term, and the vector given by the right term. With implicit summation, this is\naccomplished by the two terms sharing the repeated index .\nTaking the derivative of  with respect to the", "element of  thus gives, using the\nchain rule for (ordinary scalar) multiplication:\nwhere the second line follows from the first, with the definition that \n only\nwhen \n (and similarly for", "). And the third line follows from the second by\nrecognizing that the two terms in the second line are identical. Now note that in\nthis implicit summation notation, the", "element of the matrix product of  and\n is \n. That is, ordinary matrix multiplication sums over indices\nwhich are adjacent to each other, because a row of  times a column of \n becomes", "becomes\na scalar number. So the term in the above equation with \n is not a matrix\nproduct of \n with \n. However, taking the transpose \n switches row and column\nindices, so \n. And", "indices, so \n. And \n is a matrix product of \n with \n! Thus, we\nhave that\nwhich is the desired result.\na\nJ\ndth\n\u03b8\ndJ\nd\u03b8d\n=\n1\nn [Xab\u03b4bd (Xac\u03b8c \u2212Ya) + (Xab\u03b8b \u2212Ya)Xac\u03b4cd]\n=\n1", "=\n1\nn [Xad (Xac\u03b8c \u2212Ya) + (Xab\u03b8b \u2212Ya)Xad]\n=\n2\nn Xad (Xab\u03b8b \u2212Ya) ,\n\u03b4bd = 1\nb = d\n\u03b4cd\na, b\nA\nB\n(AB)ac = AabBbc\nA\nB\nXadXab\nX\nX\nXT\nXad = X T\nda\nX T\ndaXab\nXT\nX\ndJ\nd\u03b8d\n= 2\nn X T\nda (Xab\u03b8b \u2212Ya)\n= 2", "da (Xab\u03b8b \u2212Ya)\n= 2\nn [XT (X\u03b8 \u2212Y )]d ,"]