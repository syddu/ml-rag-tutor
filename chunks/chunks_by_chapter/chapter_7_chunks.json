["This page contains all content from the legacy PDF notes; convolutional neural networks\nchapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "So far, we have studied what are called fully connected neural networks, in which all\nof the units at one layer are connected to all of the units in the next layer. This is a", "good arrangement when we don\u2019t know anything about what kind of mapping\nfrom inputs to outputs we will be asking the network to learn to approximate. But if", "we do know something about our problem, it is better to build it into the structure\nof our neural network. Doing so can save computation time and significantly", "diminish the amount of training data required to arrive at a solution that\ngeneralizes robustly.\nOne very important application domain of neural networks, where the methods", "have achieved an enormous amount of success in recent years, is signal processing.\nSignals might be spatial (in two-dimensional camera images or three-dimensional", "depth or CAT scans) or temporal (speech or music). If we know that we are\naddressing a signal-processing problem, we can take advantage of invariant", "properties of that problem. In this chapter, we will focus on two-dimensional spatial\nproblems (images) but use one-dimensional ones as a simple example. In a later", "chapter, we will address temporal problems.\nImagine that you are given the problem of designing and training a neural network", "that takes an image as input, and outputs a classification, which is positive if the\nimage contains a cat and negative if it does not. An image is described as a two-", "dimensional array of pixels, each of which may be represented by three integer\nvalues, encoding intensity levels in red, green, and blue color channels.", "There are two important pieces of prior structural knowledge we can bring to bear\non this problem:\nSpatial locality: The set of pixels we will have to take into consideration to find", "a cat will be near one another in the image.\nTranslation invariance: The pattern of pixels that characterizes a cat is the\nsame no matter where in the image the cat occurs.", "We will design neural network structures that take advantage of these properties.\n7.1 Filters\n7  Convolutional Neural Networks\nNote\nA pixel is a \u201cpicture element.\u201d\nSo, for example, we won\u2019t have to", "consider some combination of\npixels in the four corners of the\nimage, in order to see if they\nencode cat-ness.\nCats don\u2019t look different if they\u2019re\non the left or the right side of the\nimage.", "image.\n\uf4617  Convolutional Neural Networks\n\uf52a\n We begin by discussing image filters.\nAn image filter is a function that takes in a local spatial neighborhood of pixel", "values and detects the presence of some pattern in that data.\nLet\u2019s consider a very simple case to start, in which we have a 1-dimensional binary", "\u201cimage\u201d and a filter  of size two. The filter is a vector of two numbers, which we\nwill move along the image, taking the dot product between the filter values and the", "image values at each step, and aggregating the outputs to produce a new image.\nLet \n be the original image, of size ; then pixel  of the the output image is\nspecified by", "specified by\nTo ensure that the output image is also of dimension , we will generally \u201cpad\u201d the\ninput image with 0 values if we need to access pixels that are beyond the bounds of", "the input image. This process of applying the filter to the image to create a new\nimage is called \u201cconvolution.\u201d\nIf you are already familiar with what a convolution is, you might notice that this", "definition corresponds to what is often called a correlation and not to a convolution.\nIndeed, correlation and convolution refer to different operations in signal", "processing. However, in the neural networks literature, most libraries implement\nthe correlation (as described in this chapter) but call it convolution. The distinction", "is not significant; in principle, if convolution is required to solve the problem, the\nnetwork could learn the necessary weights. For a discussion of the difference", "between convolution and correlation and the conventions used in the literature you\ncan read Section 9.1 in this excellent book: Deep Learning.\nHere is a concrete example. Let the filter", ". Then given the image in\nthe first line below, we can convolve it with filter \n to obtain the second image.\nYou can think of this filter as a detector for \u201cleft edges\u201d in the original image\u2014to see", "this, look at the places where there is a  in the output image, and see what pattern\nexists at that position in the input image. Another interesting filter is", ". The third image (the last line below) shows the result of\nconvolving the first image with \n, where we see that the output pixel \ncorresponds to when the center of \n is aligned at input pixel .", "\u2753 Study Question\nConvince yourself that filter \n can be understood as a detector for isolated\npositive pixels in the binary image.\nF\nX\nd\ni\nYi = F \u22c5(Xi\u22121, Xi) .\nd\nF1 = (\u22121, +1)\nF1\n1\nF2 = (\u22121, +1, \u22121)", "1\nF2 = (\u22121, +1, \u22121)\nF2\ni\nF2\ni\nF2\nUnfortunately in\nAI/ML/CS/Math, the word\n``filter\u2019\u2019 gets used in many ways: in\naddition to the one we describe\nhere, it can describe a temporal", "process (in fact, our moving\naverages are a kind of filter) and\neven a somewhat esoteric algebraic\nstructure.\nAnd filters are also sometimes\ncalled convolutional kernels.\n 0\n0\n1\n1\n1\n0\n1\n0\n0\n0\nImage:", "1\n0\n1\n0\n0\n0\nImage:\nF1:\n-1\n+1\n0\n0\n1\n0\n0\n-1\n1\n-1\n0\n0\nAfter con v olution (with F1):\n0\n-1\n0\n-1\n0\n-2\n1\n-1\n0\n0\nAfter con v olution (with F2):\nF2\n-1\n+1\n-1", "F2\n-1\n+1\n-1\nTwo-dimensional versions of filters like these are thought to be found in the visual\ncortex of all mammalian brains. Similar patterns arise from statistical analysis of", "natural images. Computer vision people used to spend a lot of time hand-designing\nfilter banks. A filter bank is a set of sets of filters, arranged as shown in the diagram\nbelow.\nImage", "below.\nImage\nAll of the filters in the first group are applied to the original image; if there are \nsuch filters, then the result is  new images, which are called channels. Now imagine", "stacking all these new images up so that we have a cube of data, indexed by the\noriginal row and column indices of the image, as well as by the channel. The next", "set of filters in the filter bank will generally be three-dimensional: each one will be\napplied to a sub-range of the row and column indices of the image and to all of the\nchannels.", "channels.\nThese 3D chunks of data are called tensors. The algebra of tensors is fun, and a lot\nlike matrix algebra, but we won\u2019t go into it in any detail.", "Here is a more complex example of two-dimensional filtering. We have two \nfilters in the first layer, \n and \n. You can think of each one as \u201clooking\u201d for three\npixels in a row, \n vertically and", "vertically and \n horizontally. Assuming our input image is\n, then the result of filtering with these two filters is an \n tensor. Now\nk\nk\n3 \u00d7 3\nf1\nf2\nf1\nf2\nn \u00d7 n\nn \u00d7 n \u00d7 2", "f2\nn \u00d7 n\nn \u00d7 n \u00d7 2\nThere are now many useful neural-\nnetwork software packages, such\nas TensorFlow and PyTorch that\nmake operations on tensors easy.", "we apply a tensor filter (hard to draw!) that \u201clooks for\u201d a combination of two\nhorizontal and two vertical bars (now represented by individual pixels in the two", "channels), resulting in a single final \n image.\nWhen we have a color image as input, we treat it as having three channels, and\nhence as an \n tensor.\nf2\nf1\ntensor\n\ufb01lter", "f2\nf1\ntensor\n\ufb01lter\nWe are going to design neural networks that have this structure. Each \u201cbank\u201d of the\nfilter bank will correspond to a neural-network layer. The numbers in the individual", "filters will be the \u201cweights\u201d (plus a single additive bias or offset value for each\nfilter) of the network, that we will train using gradient descent. What makes this", "interesting and powerful (and somewhat confusing at first) is that the same weights\nare used many many times in the computation of each layer. This weight sharing", "means that we can express a transformation on a large image with relatively few\nparameters; it also means we\u2019ll have to take care in figuring out exactly how to train\nit!", "it!\nWe will define a filter layer  formally with:\nnumber of filters \n;\nsize of one filter is \n plus  bias value (for this one filter);\nstride", "stride \n is the spacing at which we apply the filter to the image; in all of our\nexamples so far, we have used a stride of 1, but if we were to \u201cskip\u201d and apply", "the filter only at odd-numbered indices of the image, then it would have a\nstride of two (and produce a resulting image of half the size);\ninput tensor size \npadding:", "padding: \n is how many extra pixels \u2013 typically with value 0 \u2013 we add around\nthe edges of the input. For an input of size \n, our new\neffective input size with padding becomes\n.\nn \u00d7 n\nn \u00d7 n \u00d7 3\nl\nml", "n \u00d7 n \u00d7 3\nl\nml\nkl \u00d7 kl \u00d7 ml\u22121\n1\nsl\nnl\u22121 \u00d7 nl\u22121 \u00d7 ml\u22121\npl\nnl\u22121 \u00d7 nl\u22121 \u00d7 ml\u22121\n(nl\u22121 + 2 \u22c5pl) \u00d7 (nl\u22121 + 2 \u22c5pl) \u00d7 ml\u22121\nFor simplicity, we are assuming\nthat all images and filters are", "square (having the same number of\nrows and columns). That is in no\nway necessary, but is usually fine\nand definitely simplifies our\nnotation.\n This layer will produce an output tensor of size", ", where\n. The weights are the values defining the filter:\nthere will be \n different \n tensors of weight values; plus each filter", "may have a bias term, which means there is one more weight value per filter. A filter\nwith a bias operates just like the filter examples above, except we add the bias to the", "output. For instance, if we incorporated a bias term of 0.5 into the filter \n above,\nthe output would be \n instead of\n.", "instead of\n.\nThis may seem complicated, but we get a rich class of mappings that exploit image\nstructure and have many fewer weights than a fully connected layer would.\n\u2753 Study Question", "\u2753 Study Question\nHow many weights are in a convolutional layer specified as above?\n\u2753 Study Question\nIf we used a fully-connected layer with the same size inputs and outputs, how", "many weights would it have?\n7.2 Max pooling\nIt is typical (both in engineering and in natrure) to structure filter banks into a", "pyramid, in which the image sizes get smaller in successive layers of processing. The\nidea is that we find local patterns, like bits of edges in the early layers, and then", "look for patterns in those patterns, etc. This means that, effectively, we are looking\nfor patterns in larger pieces of the image as we apply successive filters. Having a", "stride greater than one makes the images smaller, but does not necessarily\naggregate information over that spatial range.", "Another common layer type, which accomplishes this aggregation, is max pooling. A\nmax pooling layer operates like a filter, but has no weights. You can think of it as", "purely functional, like a ReLU in a fully connected network. It has a filter size, as in a\nfilter layer, but simply returns the maximum value in its field.", "Usually, we apply max pooling with the following traits:\n, so that the resulting image is smaller than the input image; and\n, so that the whole image is covered.", "As a result of applying a max pooling layer, we don\u2019t keep track of the precise\nlocation of a pattern. This helps our filters to learn to recognize patterns\nnl \u00d7 nl \u00d7 ml", "nl \u00d7 nl \u00d7 ml\nnl = \u2308(nl\u22121 + 2 \u22c5pl \u2212(kl \u22121))/sl\u2309\nml\nkl \u00d7 kl \u00d7 ml\u22121\nF2\n(\u22120.5, 0.5, \u22120.5, 0.5, \u22121.5, 1.5, \u22120.5, 0.5)\n(\u22121, 0, \u22121, 0, \u22122, 1, \u22121, 0)\nstride > 1\nk \u2265stride\nRecall that \n is the function; it", "returns the smallest integer greater\nthan or equal to its input. E.g.,\n and \n.\n\u2308\u22c5\u2309\n\u23082.5\u2309= 3\n\u23083\u2309= 3\nWe sometimes use the term\nreceptive field or just field to mean\nthe area of an input image that a", "filter is being applied to.\n independent of their location.\nConsider a max pooling layer where both the strides and  are set to be 2. This\nwould map a \n image to a \n image. Note that max pooling", "layers do not have additional bias or offset values.\n\u2753 Study Question\nMaximilian Poole thinks it would be a good idea to add two max pooling layers", "of size , one right after the other, to their network. What single layer would be\nequivalent?\nOne potential concern about max-pooling layers is that they actually don\u2019t", "completely preserve translation invariance. If you do max-pooling with a stride\nother than 1 (or just pool over the whole image size), then shifting the pattern you", "are hoping to detect within the image by a small amount can change the output of\nthe max-pooling layer substantially, just because there are discontinuities induced", "by the way the max-pooling window matches up with its input image. Here is an\ninteresting paper that illustrates this phenomenon clearly and suggests that one", "should first do max-pooling with a stride of 1, then do \u201cdownsampling\u201d by\naveraging over a window of outputs.\n7.3 Typical architecture\nHere is the form of a typical convolutional network:", "At the end of each filter layer, we typically apply a ReLU activation function. There\nmay be multiple filter plus ReLU layers. Then we have a max pooling layer. Then", "we have some more filter + ReLU layers. Then we have max pooling again. Once\nthe output is down to a relatively small size, there is typically a last fully-connected", "layer, leading into an activation function such as softmax that produces the final\noutput. The exact design of these structures is an art\u2014there is not currently any", "clear theoretical (or even systematic empirical) understanding of how these various\ndesign choices affect overall performance of the network.", "The critical point for us is that this is all just a big neural network, which takes an\ninput and computes an output. The mapping is a differentiable function of the", "weights, which means we can adjust the weights to decrease the loss by performing\nk\n64 \u00d7 64 \u00d7 3\n32 \u00d7 32 \u00d7 3\nk\nThe \u201cdepth\u201d dimension in the\nlayers shown as cuboids\ncorresponds to the number of", "channels in the output tensor.\n(Figure source: Mathworks)\nWell, technically the derivative\ndoes not exist at every point, both\nbecause of the ReLU and the max", "gradient descent, and we can compute the relevant gradients using back-\npropagation!\n7.4 Backpropagation in a simple CNN", "Let\u2019s work through a very simple example of how back-propagation can work on a\nconvolutional network. The architecture is shown below. Assume we have a one-\ndimensional single-channel image", "of size \n, and a single filter \n of size\n (where we omit the filter bias) for the first convolutional operation\ndenoted \u201cconv\u201d in the figure below. Then we pass the intermediate result", "through a ReLU layer to obtain the activation \n, and finally through a fully-\nconnected layer with weights \n, denoted \u201cfc\u201d below, with no additional\nactivation function, resulting in the output \n.", ".\nX = A0\n0\n0\npad with 0\u2019s \n(to get output \nof same shap e)\nW 1\nZ1\nA1\nZ2 = A2\nW 2\nconv\nReLU\nfc\nFor simplicity assume  is odd, let the input image \n, and assume we are", ", and assume we are\nusing squared loss. Then we can describe the forward pass as follows:\n\u2753 Study Question\nAssuming a stride of \n for a filter of size , how much padding do we need to", "add to the top and bottom of the image? We see one zero at the top and bottom\nin the figure just above; what filter size is implicitly being shown in the figure?", "(Recall the padding is for the sake of getting an output the same size as the\ninput.)\nX\nn \u00d7 1 \u00d7 1\nW 1\nk \u00d7 1 \u00d7 1\nZ 1\nA1\nW 2\nA2\nk\nX = A0\nZ 1\ni = W 1TA0\n[i\u2212\u230ak/2\u230b:i+\u230ak/2\u230b]\nA1 = ReLU(Z 1)", "A1 = ReLU(Z 1)\nA2 = Z 2 = W 2TA1\nLsquare(A2, y) = (A2 \u2212y)2\n1,\nk\n7.4.1 Weight update\npooling operations, but we ignore\nthat fact.\n How do we update the weights in filter \n?\n is the \n matrix such that", "matrix such that \n. So, for\nexample, if \n, which corresponds to column 10 in this matrix, which\nillustrates the dependence of pixel 10 of the output image on the weights, and\nif", "if \n, then the elements in column 10 will be \n.\n is the \n diagonal matrix such that\n, an \n vector\nMultiplying these components yields the desired gradient, of shape \n.", ".\nOne last point is how to handle back-propagation through a max-pooling operation.\nLet\u2019s study this via a simple example. Imagine\nwhere \n and", "where \n and \n are each computed by some network. Consider doing back-\npropagation through the maximum. First consider the case where \n. Then the", ". Then the\nerror value at  is propagated back entirely to the network computing the value \n.\nThe weights in the network computing \n will ultimately be adjusted, and the\nnetwork computing", "network computing \n will be untouched.\n\u2753 Study Question\nWhat is \n ?\nW 1\n\u2202loss\n\u2202W 1 = \u2202Z 1\n\u2202W 1\n\u2202A1\n\u2202Z 1\n\u2202loss\n\u2202A1\n\u2202Z 1/\u2202W 1\nk \u00d7 n\n\u2202Z 1\ni /\u2202W 1\nj = Xi\u2212\u230ak/2\u230b+j\u22121\ni = 10\nk = 5\nX8, X9, X10, X11, X12", "\u2202A1/\u2202Z 1\nn \u00d7 n\n\u2202A1\ni /\u2202Z 1\ni = {1\nif Z 1\ni > 0\n0\notherwise\n\u2202loss/\u2202A1 = (\u2202loss/\u2202A2)(\u2202A2/\u2202A1) = 2(A2 \u2212y)W 2\nn \u00d7 1\nk \u00d7 1\n7.4.2 Max pooling\ny = max(a1, a2) ,\na1\na2\na1 > a2\ny\na1\na1\na2\n\u2207(x,y) max(x, y)"]