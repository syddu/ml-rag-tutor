["This page contains all content from the legacy PDF notes; features chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Linear regression and classification are powerful tools, but in the real world, data\noften exhibit non-linear behavior that cannot immediately be captured by the linear", "models which we have built so far. For example, suppose the true behavior of a\nsystem (with \n) looks like this wavelet:", "Such behavior is actually ubiquitous in physical systems, e.g., in the vibrations of\nthe surface of a drum, or scattering of light through an aperture. However, no single", "hyperplane would be a very good fit to such peaked responses!\nA richer class of hypotheses can be obtained by performing a non-linear feature\ntransformation \n before doing the regression. That is,", "is a linear\nfunction of , but \n is a non-linear function of \n if  is a non-linear\nfunction of .\nThere are many different ways to construct . Some are relatively systematic and", "domain independent. Others are directly related to the semantics (meaning) of the\noriginal features, and we construct them deliberately with our application (goal) in\nmind.", "mind.\n5.1 Gaining intuition about feature\ntransformations\nIn this section, we explore the effects of non-linear feature transformations on\nsimple classification problems, to gain intuition.", "Let\u2019s look at an example data set that starts in 1-D:\n5  Feature Representation\nNote\nd = 2\n\u03d5(x)\n\u03b8Tx + \u03b80\nx\n\u03b8T\u03d5(x) + \u03b80\nx,\n\u03d5\nx\n\u03d5\n\uf4615  Feature Representation\n\uf52a\n x\n0", "\uf52a\n x\n0\nThese points are not linearly separable, but consider the transformation\n. Plotting this transformed data (in two-dimensional space, since", "there are now two features), we see that it is now separable. There are lots of\npossible separators; we have just shown one of them here.\nx\nx2\nseparator", "x\nx2\nseparator\nA linear separator in  space is a nonlinear separator in the original space! Let\u2019s see\nhow this plays out in our simple example. Consider the separator \n(which corresponds to \n and", "and \n in our transformed space), which\nlabels the half-plane \n as positive. What separator does it correspond to in\nthe original 1-D space? We have to ask the question: which  values have the", "property that \n. The answer is \n and \n, so those two points constitute\nour separator, back in the original space. Similarly, by evaluating where \nand where", "and where \n, we can find the regions of 1D space that are labeled positive\nand negative (respectively) by this separator.\nExample\n\u03d5(x) = [x, x2]T\nExample\n\u03d5\nx2 \u22121 = 0\n\u03b8 = [0, 1]T\n\u03b80 = \u22121\nx2 \u22121 > 0\nx", "\u03b80 = \u22121\nx2 \u22121 > 0\nx\nx2 \u22121 = 0\n+1\n\u22121\nx2 \u22121 > 0\nx2 \u22121 < 0\nExample\n x\n0\n1\n-1\n5.2 Systematic feature construction\nHere are two different ways to systematically construct features in a problem", "independent way.\nIf the features in your problem are already naturally numerical, one systematic\nstrategy for constructing a new feature space is to use a polynomial basis. The idea is", "that, if you are using the th-order basis (where  is a positive integer), you include\na feature for every possible product of  different dimensions in your original input.", "Here is a table illustrating the th order polynomial basis for different values of ,\ncalling out the cases when \n and \n:\nOrder\nin general (\n)\n0\n1\n2\n3\n\u22ee\n\u22ee\n\u22ee", ")\n0\n1\n2\n3\n\u22ee\n\u22ee\n\u22ee\nThis transformation can be used in combination with linear regression or logistic\nregression (or any other regression or classification model). When we\u2019re using a", "linear regression or classification model, the key insight is that a linear regressor or\nseparator in the transformed space is a non-linear regressor or separator in the\noriginal space.", "original space.\nTo give a regression example, the wavelet pictured at the start of this chapter can be\nfit much better using a polynomial feature representation up to order \n,", ",\ncompared to just using a simple hyperplane in the original (single-dimensional)\nfeature space:\n5.2.1 Polynomial basis\nk\nk\nk\nk\nk\nd = 1\nd > 1\nd = 1\nd > 1\n[1]\n[1]\n[1, x]T\n[1, x1, \u2026 , xd]T\n[1, x, x2]T", "[1, x, x2]T\n[1, x1, \u2026 , xd, x2\n1, x1x2, \u2026]T\n[1, x, x2, x3]T\n[1, x1, \u2026 , xd, x2\n1, x1x2, \u2026 , x3\n1, x1x2\n2, x1x2x3, \u2026]T\nk = 8\n The raw data (with \n random samples) is plotted on the left, and the", "regression result (curved surface) is on the right.\nNow let\u2019s look at a classification example and see how polynomial feature\ntransformation may help us.", "One well-known example is the \u201cexclusive or\u201d (xor) data set, the drosophila of\nmachine-learning data sets:\nClearly, this data set is not linearly separable. So, what if we try to solve the xor", "classification problem using a polynomial basis as the feature transformation? We\ncan just take our two-dimensional data and transform it into a higher-dimensional", "data set, by applying some feature transformation . Now, we have a classification\nproblem as usual.\nLet\u2019s try it for \n on our xor problem. The feature transformation is\n\u2753 Study Question", "\u2753 Study Question\nIf we train a classifier after performing this feature transformation, would we\nlose any expressive power if we let \n (i.e., trained without offset instead of\nwith offset)?", "with offset)?\nWe might run a classification learning algorithm and find a separator with\ncoefficients \n and \n. This corresponds to\nn = 1000\nExample\n\u03d5\nk = 2\n\u03d5([x1, x2]T) = [1, x1, x2, x2\n1, x1x2, x2", "1, x1x2, x2\n2]T .\n\u03b80 = 0\n\u03b8 = [0, 0, 0, 0, 4, 0]T\n\u03b80 = 0\n2\n2\nD. Melanogaster is a species of\nfruit fly, used as a simple system in\nwhich to study genetics, since 1910.", "and is plotted below, with the gray shaded region classified as negative and the\nwhite region classified as positive:\n\u2753 Study Question", "\u2753 Study Question\nBe sure you understand why this high-dimensional hyperplane is a separator,\nand how it corresponds to the figure.", "For fun, we show some more plots below. Here is another result for a linear\nclassifier on xor generated with logistic regression and gradient descent, using a", "random initial starting point and second-order polynomial basis:\nHere is a harder data set. Logistic regression with gradient descent failed to", "separate it with a second, third, or fourth-order basis feature representation, but\n0 + 0x1 + 0x2 + 0x2\n1 + 4x1x2 + 0x2\n2 + 0 = 0\nExample\nExample", "Example\nExample\n succeeded with a fifth-order basis. Shown below are some results after \ngradient descent iterations (from random starting points) for bases of order 2", "(upper left), 3 (upper right), 4 (lower left), and 5 (lower right).\n\u2753 Study Question\nPercy Eptron has a domain with four numeric input features, \n. He\ndecides to use a representation of the form", "where \n means the vector  concatenated with the vector .\nWhat is the dimension of Percy\u2019s representation? Under what assumptions\nabout the original features is this a reasonable choice?", "Another cool idea is to use the training data itself to construct a feature space. The\nidea works as follows. For any particular point  in the input space \n, we can\n\u223c1000\nExample\n(x1, \u2026 , x4)", "(x1, \u2026 , x4)\n\u03d5(x) = PolyBasis((x1, x2), 3)\u2322PolyBasis((x3, x4), 3)\na\u2322b\na\nb\n5.2.2 (Optional) Radial basis functions\np\nX\n construct a feature \n which takes any element \n and returns a scalar value", "that is related to how far  is from the  we started with.\nLet\u2019s start with the basic case, in which \n. Then we can define\nThis function is maximized when \n and decreases exponentially as  becomes", "more distant from .\nThe parameter  governs how quickly the feature value decays as we move away\nfrom the center point . For large values of , the \n values are nearly 0 almost", "everywhere except right near ; for small values of , the features have a high value\nover a larger part of the space.\nNow, given a dataset \n containing  points, we can make a feature transformation", "that maps points in our original space, \n, into points in a new space, \n. It is\ndefined as follows:\nSo, we represent a new datapoint  in terms of how far it is from each of the", "datapoints in our training set.\nThis idea can be generalized in several ways and is the fundamental concept\nunderlying kernel methods, that are not directly covered in this class but we", "recommend you read about some time. This idea of describing objects in terms of\ntheir similarity to a set of reference objects is very powerful and can be applied to\ncases where", "cases where \n is not a simple vector space, but where the inputs are graphs or\nstrings or other types of objects, as long as there is a distance metric defined on the\ninput space.", "input space.\n5.3 (Optional) Hand-constructing features for real\ndomains\nIn many machine-learning applications, we are given descriptions of the inputs", "with many different types of attributes, including numbers, words, and discrete\nfeatures. An important factor in the success of an ML application is the way that the", "features are chosen to be encoded by the human who is framing the learning\nproblem.\nGetting a good encoding of discrete features is particularly important. You want to", "create \u201copportunities\u201d for the ML system to find the underlying patterns. Although\nthere are machine-learning methods that have special mechanisms for handling", "discrete inputs, most of the methods we consider in this class will assume the input\nfp\nx \u2208X\nx\np\nX = Rd\nfp(x) = e\u2212\u03b2\u2225p\u2212x\u22252 .\np = x\nx\np\n\u03b2\np\n\u03b2\nfp\np\n\u03b2\nD\nn\n\u03d5\nRd\nRn", "fp\np\n\u03b2\nD\nn\n\u03d5\nRd\nRn\n\u03d5(x) = [fx(1)(x), fx(2)(x), \u2026 , fx(n)(x)]T .\nx\nX\n5.3.1 Discrete features\n vectors  are in \n. So, we have to figure out some reasonable strategies for turning", "discrete values into (vectors of) real numbers.\nWe\u2019ll start by listing some encoding strategies, and then work through some", "examples. Let\u2019s assume we have some feature in our raw data that can take on one\nof  discrete values.\nNumeric: Assign each of these values a number, say \n. We", ". We\nmight want to then do some further processing, as described in Section 1.3.3.\nThis is a sensible strategy only when the discrete values really do signify some", "sort of numeric quantity, so that these numerical values are meaningful.\nThermometer code: If your discrete values have a natural ordering, from", ", but not a natural mapping into real numbers, a good strategy is to use\na vector of length  binary variables, where we convert discrete input value\n into a vector in which the first  values are", "and the rest are \n.\nThis does not necessarily imply anything about the spacing or numerical\nquantities of the inputs, but does convey something about ordering.", "Factored code: If your discrete values can sensibly be decomposed into two\nparts (say the \u201cmaker\u201d and \u201cmodel\u201d of a car), then it\u2019s best to treat those as two", "separate features, and choose an appropriate encoding of each one from this\nlist.\nOne-hot code: If there is no obvious numeric, ordering, or factorial structure,", "then the best strategy is to use a vector of length , where we convert discrete\ninput value \n into a vector in which all values are \n, except for the \nth, which is \n.", "th, which is \n.\nBinary code: It might be tempting for the computer scientists among us to use\nsome binary code, which would let us represent  values using a vector of\nlength", "length \n. This is a bad idea! Decoding a binary code takes a lot of work, and\nby encoding your inputs this way, you\u2019d be forcing your system to learn the\ndecoding algorithm.", "decoding algorithm.\nAs an example, imagine that we want to encode blood types, that are drawn from\nthe set \n. There is no obvious linear", "numeric scaling or even ordering to this set. But there is a reasonable factoring, into\ntwo features: \n and \n. And, in fact, we can further reasonably\nfactor the first group into \n,", ", \n. So, here are two plausible\nencodings of the whole set:\nUse a 6-D vector, with two components of the vector each encoding the\ncorresponding factor using a one-hot encoding.", "Use a 3-D vector, with one dimension for each factor, encoding its presence as\n and absence as \n (this is sometimes better than \n). In this case, \nwould be \n and \n would be \n.\nx\nRd\nk", "would be \n.\nx\nRd\nk\n1.0/k, 2.0/k, \u2026 , 1.0\n1, \u2026 , k\nk\n0 < j \u2264k\nj\n1.0\n0.0\nk\n0 < j \u2264k\n0.0\nj\n1.0\nk\nlog k\n{A+, A\u2212, B+, B\u2212, AB+, AB\u2212, O+, O\u2212}\n{A, B, AB, O}\n{+, \u2212}\n{A, notA} {B, notB}\n1.0\n\u22121.0\n0.0\nAB+", "1.0\n\u22121.0\n0.0\nAB+\n[1.0, 1.0, 1.0]T\nO\u2212\n[\u22121.0, \u22121.0, \u22121.0]T\n \u2753 Study Question\nHow would you encode \n in both of these approaches?", "The problem of taking a text (such as a tweet or a product review, or even this\ndocument!) and encoding it as an input for a machine-learning algorithm is", "interesting and complicated. Much later in the class, we\u2019ll study sequential input\nmodels, where, rather than having to encode a text as a fixed-length feature vector,", "we feed it into a hypothesis word by word (or even character by character!).\nThere are some simple encodings that work well for basic applications. One of them", "is the bag of words (bow) model, which can be used to encode documents. The idea is\nto let  be the number of words in our vocabulary (either computed from the", "training set or some other body of text or dictionary). We will then make a binary\nvector (with values \n and \n) of length , where element  has value \n if word \noccurs in the document, and", "otherwise.\nIf some feature is already encoded as a numeric value (heart rate, stock price,\ndistance, etc.) then we should generally keep it as a numeric value. An exception", "might be a situation in which we know there are natural \u201cbreakpoints\u201d in the\nsemantics: for example, encoding someone\u2019s age in the US, we might make an", "explicit distinction between under and over 18 (or 21), depending on what kind of\nthing we are trying to predict. It might make sense to divide into discrete bins", "(possibly spacing them closer together for the very young) and to use a one-hot\nencoding for some sorts of medical situations in which we don\u2019t expect a linear (or", "even monotonic) relationship between age and some physiological features.\n\u2753 Study Question\nConsider using a polynomial basis of order  as a feature transformation  on", "our data. Would increasing  tend to increase or decrease structural error? What\nabout estimation error?\nA+\n5.3.2 Text\nd\n1.0\n0.0\nd\nj\n1.0\nj\n0.0\n5.3.3 Numeric values\nk\n\u03d5\nk"]