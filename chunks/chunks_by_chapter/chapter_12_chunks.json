["This page contains all content from the legacy PDF notes; non-parametric models chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Neural networks have adaptable complexity, in the sense that we can try different\nstructural models and use cross validation to find one that works well on our data.", "Beyond neural networks, we may further broaden the class of models that we can\nfit to our data, for example as illustrated by the techniques introduced in this\nchapter.", "chapter.\nHere, we turn to models that automatically adapt their complexity to the training\ndata. The name non-parametric methods is misleading: it is really a class of methods", "that does not have a fixed parameterization in advance. Rather, the complexity of\nthe parameterization can grow as we acquire more data.", "Some non-parametric models, such as nearest-neighbor, rely directly on the data to\nmake predictions and do not compute a model that summarizes the data. Other", "non-parametric methods, such as decision trees, can be seen as dynamically\nconstructing something that ends up looking like a more traditional parametric", "model, but where the actual training data affects exactly what the form of the model\nwill be.\nThe non-parametric methods we consider here tend to have the form of a\ncomposition of simple models:", "Nearest neighbor models: Section 12.1 where we don\u2019t process data at training\ntime, but do all the work when making predictions, by looking for the closest", "training example(s) to a given new data point.\nTree models: Section 12.2 where we partition the input space and use different", "simple predictions on different regions of the space; the hypothesis space can\nbecome arbitrarily large allowing finer and finer partitions of the input space.", "Ensemble models: Section 12.2.3 in which we train several different classifiers on\nthe whole space and average the answers; this decreases the estimation error. In", "particular, we will look at bootstrap aggregation, or bagging of trees.\nBoosting is a way to construct a model composed of a sequence of component", "models (e.g., a model consisting of a sequence of trees, each subsequent tree\nseeking to correct errors in the previous trees) that decreases both estimation", "and structural error. We won\u2019t consider this in detail in this class.\n12  Non-parametric methods\nNote\n\uf46112  Non-parametric methods\n\uf52a", "\uf52a\n * -means clustering methods, Section 12.3 where we partition data into groups\nbased on similarity without predefined labels, adapting complexity by\nadjusting the number of clusters.", "Why are we studying these methods, in the heyday of complicated models such as\nneural networks ?\nThey are fast to implement and have few or no hyperparameters to tune.", "They often work as well as or better than more complicated methods.\nPredictions from some of these models can be easier to explain to a human", "user: decision trees are fairly directly human-interpretable, and nearest\nneighbor methods can justify their decisions to some extent by showing a few", "training examples that the predictions were based on.\n12.1 Nearest Neighbor\nIn nearest-neighbor models, we don\u2019t do any processing of the data at training time", "\u2013 we just remember it! All the work is done at prediction time.\nInput values  can be from any domain \n (\n, documents, tree-structured objects,\netc.). We just need a distance metric,", ", which satisfies the following,\nfor all \n:\nGiven a data-set \n, our predictor for a new \n is\nthat is, the predicted output associated with the training point that is closest to the", "query point . Tie breaking is typically done at random.\nThis same algorithm works for regression and classification!\nThe nearest neighbor prediction function can be described by dividing the space up", "into regions whose closest point is each individual training point as shown below :\nk\nx\nX Rd\nd : X \u00d7 X \u2192R+\nx, x\u2032, x\u2032\u2032 \u2208X\nd(x, x) = 0\nd(x, x\u2032) = d(x\u2032, x)\nd(x, x\u2032\u2032) \u2264d(x, x\u2032) + d(x\u2032, x\u2032\u2032)", "D = {(x(i), y(i))}n\ni=1\nx \u2208X\nh(x) = y(i)\nwhere\ni = arg min\ni\nd(x, x(i)) ,\nx\n In each region, we predict the associated  value.", "There are several useful variations on this method. In -nearest-neighbors, we find\nthe  training points nearest to the query point  and output the majority  value", "for classification or the average for regression. We can also do locally weighted\nregression in which we fit locally linear regression models to the  nearest points,", "possibly giving less weight to those that are farther away. In large data-sets, it is\nimportant to use good data structures (e.g., ball trees) to perform the nearest-", "neighbor look-ups efficiently (without looking at all the data points each time).\n12.2 Tree Models\nThe idea here is that we would like to find a partition of the input space and then fit", "very simple models to predict the output in each piece. The partition is described\nusing a (typically binary) \u201ctree\u201d that recursively splits the space.\nTree methods differ by:", "The class of possible ways to split the space at each node; these are typically\nlinear splits, either aligned with the axes of the space, or sometimes using more\ngeneral classifiers.", "The class of predictors within the partitions; these are often simply constants,\nbut may be more general classification or regression models.", "The way in which we control the complexity of the hypothesis: it would be\nwithin the capacity of these methods to have a separate partition element for\neach individual training example.\ny\nk\nk\nx\ny\nk", "y\nk\nk\nx\ny\nk\n The algorithm for making the partitions and fitting the models.\nOne advantage of tree models is that they are easily interpretable by humans. This", "is important in application domains, such as medicine, where there are human\nexperts who often ultimately make critical decisions and who need to feel confident", "in their understanding of recommendations made by an algorithm. Below is an\nexample decision tree, illustrating how one might be able to understand the\ndecisions made by the tree.", "#Example Here is a sample tree (reproduced from Breiman, Friedman, Olshen, Stone\n(1984)):\nThese methods are most appropriate for domains where the input space is not very", "high-dimensional and where the individual input features have some substantially\nuseful information individually or in small groups. Trees would not be good for", "image input, but might be good in cases with, for example, a set of meaningful\nmeasurements of the condition of a patient in the hospital, as in the example above.", "We\u2019ll concentrate on the CART/ID3 (\u201cclassification and regression trees\u201d and\n\u201citerative dichotomizer 3\u201d, respectively) family of algorithms, which were invented", "independently in the statistics and the artificial intelligence communities. They\nwork by greedily constructing a partition, where the splits are axis aligned and by", "fitting a constant model in the leaves. The interesting questions are how to select the\nsplits and how to control complexity. The regression and classification versions are\nvery similar.", "very similar.\nAs a concrete example, consider the following images:\nNote\n  \nThe left image depicts a set of labeled data points in a two-dimensional feature", "space. The right shows a partition into regions by a decision tree, in this case having\nno classification errors in the final partitions.\nThe predictor is made up of", "a partition function, , mapping elements of the input space into exactly one of\n regions, \n, and\na collection of \n output values, \n, one for each region.", "If we already knew a division of the space into regions, we would set \n, the\nconstant output for region \n, to be the average of the training output values in\nthat region. For a training data set", ", we let  be an\nindicator set of all of the elements within \n, so that \n for our whole\ndata set. We can define \n as the subset of data set samples that are in region \n, so\nthat \n. Then", ", so\nthat \n. Then\nWe can define the error in a region as \n. For example, \n as the sum of squared\nerror would be expressed as\nIdeally, we should select the partition to minimize", "for some regularization constant . It is enough to search over all partitions of the\ntraining data (not all partitions of the input space!) to optimize this, but the problem\nis NP-complete.", "is NP-complete.\n12.2.1 Regression\n\u03c0\nM\nR1, \u2026 , RM\nM\nOm\nOm\nRm\nD = {(x(i), y(i))}, i = 1, \u2026 n\nI\nD\nI = {1, \u2026 , n}\nIm\nRm\nIm = {i \u2223x(i) \u2208Rm}\nOm = averagei\u2208Im y(i) .\nEm\nEm\nEm = \u2211\ni\u2208Im\n(y(i) \u2212Om)2 .\n\u03bbM +\nM\n\u2211", "\u03bbM +\nM\n\u2211\nm=1\nEm ,\n\u03bb\n12.2.1.1 Building a tree\n So, we\u2019ll be greedy. We establish a criterion, given a set of data, for finding the best", "single split of that data, and then apply it recursively to partition the space. For the\ndiscussion below, we will select the partition of the data that minimizes the sum of the", "sum of squared errors of each partition element. Then later, we will consider other\nsplitting criteria.\nGiven a data set \n, we now consider  to be an\nindicator of the subset of elements within", "that we wish to build a tree (or subtree)\nfor. That is,  may already indicate a subset of data set \n, based on prior splits in\nconstructing our overall tree. We define terms as follows:", "indicates the set of examples (subset of ) whose feature value in dimension\n is greater than or equal to split point ;\n indicates the set of examples (subset of ) whose feature value in dimension", "is less than ;\n is the average  value of the data points indicated by set \n; and\n is the average  value of the data points indicated by set \n.", ".\nHere is the pseudocode. In what follows,  is the largest leaf size that we will allow\nin the tree, and is a hyperparameter of the algorithm.\nprocedure BuildTree(\n)\nif \n then\nreturn \nelse", "then\nreturn \nelse\nfor all split dimension , split value  do\nend for\nreturn \nend if\nend procedure\nIn practice, we typically start by calling BuildTree  with the first input equal to our", "whole data set (that is, with \n). But then that call of BuildTree  can\nrecursively lead to many other calls of BuildTree .\nLet\u2019s think about how long each call of BuildTree  takes to run. We have to", "consider all possible splits. So we consider a split in each of the  dimensions. In\neach dimension, we only need to consider splits between two data points (any other\nD = {(x(i), y(i))}, i = 1, \u2026 n\nI", "I\nD\nI\nD\nI +\nj,s\nI\nj\ns\nI \u2212\nj,s\nI\nj\ns\n^y+\nj,s\ny\nI +\nj,s\n^y\u2212\nj,s\ny\nI \u2212\nj,s\nk\n1:\nI, k\n2:\n|I| \u2264k\n3:\n^y \u2190\n1\n|I| \u2211i\u2208I y(i)\n4:\nLeaf(value = ^y)\n5:\n6:\nj\ns\n7:\nI +\nj,s \u2190{i \u2208I \u2223x(i)\nj\n\u2265s}\n8:\nI \u2212\nj,s \u2190{i \u2208I \u2223x(i)", "j,s \u2190{i \u2208I \u2223x(i)\nj\n< s}\n9:\n^y+\nj,s \u2190\n1\n|I +\nj,s| \u2211i\u2208I +\nj,s y(i)\n10:\n^y\u2212\nj,s \u2190\n1\n|I \u2212\nj,s| \u2211i\u2208I \u2212\nj,s y(i)\n11:\nEj,s \u2190\u2211i\u2208I +\nj,s(y(i) \u2212^y+\nj,s)2 + \u2211i\u2208I \u2212\nj,s(y(i) \u2212^y\u2212\nj,s)2\n12:\n13:", "j,s)2\n12:\n13:\n(j\u2217, s\u2217) \u2190arg minj,s Ej,s\n14:\n15:\nNode(j\u2217, s\u2217, BuildTree(I \u2212\nj\u2217,s\u2217, k), BuildTree(I +\nj\u2217,s\u2217, k))\n16:\n17:\nI = {1, \u2026 , n}\nd", "I = {1, \u2026 , n}\nd\n split will give the same error on the training data). So, in total, we consider \nsplits in each call to BuildTree .", "It might be tempting to regularize by using a somewhat large value of , or by\nstopping when splitting a node does not significantly decrease the error. One", "problem with short-sighted stopping criteria is that they might not see the value of\na split that will require one more split before it seems useful. So, we will tend to", "build a tree that is too large, and then prune it back.\nWe define cost complexity of a tree , where \n ranges over its leaves, as\nand", "and \n is the number of leaves. For a fixed , we can find a  that (approximately)\nminimizes \n by \u201cweakest-link\u201d pruning:\nCreate a sequence of trees by successively removing the bottom-level split that", "minimizes the increase in overall error, until the root is reached.\nReturn the  in the sequence that minimizes the cost complexity.\nWe can choose an appropriate  using cross validation.", "The strategy for building and pruning classification trees is very similar to the\nstrategy for regression trees.\nGiven a region \n corresponding to a leaf of the tree, we would pick the output", "class  to be the value that exists most frequently (the majority value) in the data\npoints whose  values are in that region, i.e., data points indicated by \n:", ":\nLet\u2019s now define the error in a region as the number of data points that do not have\nthe value \n:\nWe define the empirical probability of an item from class  occurring in region \n as:\nwhere", "as:\nwhere \n is the number of training points in region \n; that is, \n For later\nuse, we\u2019ll also define the empirical probabilities of split values, \n, as the", ", as the\nfraction of points with dimension  in split  occurring in region \n (one branch of\nO(dn)\n12.2.1.2 Pruning\nk\nT\nm\nC\u03b1(T) =\n|T|\n\u2211\nm=1\nEm(T) + \u03b1|T| ,\n|T|\n\u03b1\nT\nC\u03b1(T)\nT\n\u03b1\n12.2.2 Classification\nRm\ny\nx", "Rm\ny\nx\nIm\nOm = majorityi\u2208Im y(i) .\nOm\nEm = {i \u2223i \u2208Im and y(i) \u2260Om}\n.\n\u2223\u2223\nk\nm\n^Pm,k = ^P(Im, k) =\n{i \u2223i \u2208Im and y(i) = k}\nNm\n,\n\u2223\u2223\nNm\nm\nNm = |Im|.\n^Pm,j,s\nj\ns\nm\n the tree), and", "m\n the tree), and \n as the complement (the fraction of points in the other\nbranch).\nIn our greedy algorithm, we need a way to decide which split to make next. There", "are many criteria that express some measure of the \u201cimpurity\u201d in child nodes. Some\nmeasures include:\nMisclassification error:\nGini index:\nEntropy:\nSo that the entropy \n is well-defined when", ", we will stipulate that\n.\nThese splitting criteria are very similar, and it\u2019s not entirely obvious which one is\nbetter. We will focus on entropy, just to be concrete.", "Analogous to how for regression we choose the dimension  and split  that\nminimizes the sum of squared error \n, for classification, we choose the dimension", "and split  that minimizes the weighted average entropy over the \u201cchild\u201d data\npoints in each of the two corresponding splits, \n and \n. We calculate the entropy", "in each split based on the empirical probabilities of class memberships in the split,\nand then calculate the weighted average entropy \n as", "as\nChoosing the split that minimizes the entropy of the children is equivalent to\nmaximizing the information gain of the test \n, defined by", ", defined by\nIn the two-class case (with labels 0 and 1), all of the splitting criteria mentioned\nabove have the values\n1 \u2212^Pm,j,s\nSplitting criteria\nQm(T) = Em\nNm\n= 1 \u2212^Pm,Om\nQm(T) = \u2211\nk", "Qm(T) = \u2211\nk\n^Pm,k(1 \u2212^Pm,k)\nQm(T) = H(Im) = \u2212\u2211\nk\n^Pm,k log2 ^Pm,k\nH\n^P = 0\n0 log2 0 = 0\nj\ns\nEj,s\nj\ns\nI +\nj,s\nI \u2212\nj,s\n^H\n^H = (fraction of points in left data set) \u22c5H(I \u2212\nj,s)", "j,s)\n+(fraction of points in right data set) \u22c5H(I +\nj,s)\n= (1 \u2212^Pm,j,s)H(I \u2212\nj,s) + ^Pm,j,sH(I +\nj,s)\n=\n|I \u2212\nj,s|\nNm\n\u22c5H(I \u2212\nj,s) +\n|I +\nj,s|\nNm\n\u22c5H(I +\nj,s) .\nxj = s\ninfoGain(xj = s, Im) =\nH(Im) \u2212(", "H(Im) \u2212(\n|I \u2212\nj,s|\nNm\n\u22c5H(I \u2212\nj,s) +\n|I +\nj,s|\nNm\n\u22c5H(I +\nj,s))\n{\n The respective impurity curves are shown below, where \n; the vertical axis\nplots \n for each of the three criteria.", "There used to be endless haggling about which impurity function one should use. It\nseems to be traditional to use entropy to select which node to split while growing the", "tree, and misclassification error in the pruning criterion.\nOne important limitation or drawback in conventional trees is that they can have", "high estimation error: small changes in the data can result in very big changes in the\nresulting tree.\nBootstrap aggregation is a technique for reducing the estimation error of a non-linear", "predictor, or one that is adaptive to the data. The key idea applied to trees, is to\nbuild multiple trees with different subsets of the data, and then create an ensemble", "model that combines the results from multiple trees to make a prediction.\nConstruct \n new data sets of size . Each data set is constructed by sampling \ndata points with replacement from", ". A single data set is called bootstrap sample\nof \n.\nTrain a predictor \n on each bootstrap sample.\nRegression case: bagged predictor is\nClassification case: Let", "be the number of classes. We find a majority bagged\npredictor as follows. We let \n be a \u201cone-hot\u201d vector with a single 1 and\n{\n.\n0.0\nwhen  ^Pm,0 = 0.0\n0.0\nwhen  ^Pm,0 = 1.0\np = ^Pm,0\nQm(T)", "p = ^Pm,0\nQm(T)\n12.2.3 Bagging\nB\nn\nn\nD\nD\n^f b(x)\n^fbag(x) = 1\nB\nB\n\u2211\nb=1\n^f b(x) .\nK\n^f b(x)\n  zeros, and define the predicted output  for predictor \n as\n. Then", "as\n. Then\nwhich is a vector containing the proportion of classifiers that predicted each\nclass  for input . Then the overall predicted output is", "There are theoretical arguments showing that bagging does, in fact, reduce\nestimation error. However, when we bag a model, any simple intrepetability is lost.", "Random forests are collections of trees that are constructed to be de-correlated, so\nthat using them to vote gives maximal advantage. In competitions, they often have", "excellent classification performance among large collections of much fancier\nmethods.\nIn what follows, \n, \n, and  are hyperparameters of the algorithm.\nprocedure RandomForest(\n)\nfor \n to  do", ")\nfor \n to  do\nDraw a bootstrap sample \n of size  from \nGrow tree \n on \n:\nwhile there are splittable nodes do\nSelect \n variables at random from the  total variables", "Pick the best variable and split point among those \nSplit the current node\nend while\nend for\nreturn \nend procedure\nGiven the ensemble of trees, vote to make a prediction on a new .", "There are many variations on the tree theme. One is to employ different regression\nor classification methods in each leaf. For example, a linear regression might be", "used to model the examples in each leaf, rather than using a constant value.\nIn the relatively simple trees that we\u2019ve considered, splits have been based on only", "a single feature at a time, and with the resulting splits being axis-parallel. Other\nmethods for splitting are possible, including consideration of multiple features and", "linear classifiers based on those, potentially resulting in non-axis-parallel splits.\nComplexity is a concern in such cases, as many possible combinations of features\nK \u22121\n^y\nf b", "K \u22121\n^y\nf b\n^yb(x) = arg maxk ^f b(x)k\n^fbag(x) = 1\nB\nB\n\u2211\nb=1\n^f b(x),\nk\nx\n^ybag(x) = arg max\nk\n^fbag(x)k .\n12.2.4 Random Forests\nB m\nn\n1:\nB, m, n\n2:\nb = 1\nB\n3:\nDb\nn\nD\n4:\nTb\nDb\n5:\n6:\nm\nd\n7:\nm\n8:\n9:", "6:\nm\nd\n7:\nm\n8:\n9:\n10:\n11:\n12:\n{Tb}B\nb=1\n13:\nx\n12.2.5 Tree variants and tradeoffs\n may need to be considered, to select the best variable combination (rather than a\nsingle split variable).", "Another generalization is a hierarchical mixture of experts, where we make a \u201csoft\u201d\nversion of trees, in which the splits are probabilistic (so every point has some degree", "of membership in every leaf). Such trees can be trained using a form of gradient\ndescent. Combinations of bagging, boosting, and mixture tree approaches (e.g.,", "gradient boosted trees) and implementations are readily available (e.g., XGBoost).\nTrees have a number of strengths, and remain a valuable tool in the machine", "learning toolkit. Some benefits include being relatively easy to interpret, fast to\ntrain, and ability to handle multi-class classification in a natural way. Trees can", "easily handle different loss functions; one just needs to change the predictor and\nloss being applied in the leaves. Methods also exist to identify which features are", "particularly important or influential in forming the tree, which can aid in human\nunderstanding of the data set. Finally, in many situations, trees perform", "surprisingly well, often comparable to more complicated regression or classification\nmodels. Indeed, in some settings it is considered good practice to start with trees", "(especially random forest or boosted trees) as a \u201cbaseline\u201d machine learning model,\nagainst which one can evaluate performance of more sophisticated models.", "While tree-based methods excel at supervised learning tasks, we now turn to\nanother important class of non-parametric methods that focus on discovering", "structure in unlabeled data. These clustering methods share some conceptual\nsimilarities with tree-based approaches - both aim to partition the input space into", "meaningful regions - but clustering methods operate without supervision, making\nthem particularly valuable for exploratory data analysis and pattern discovery.\n12.3 -means Clustering", "Clustering is an unsupervised learning method where we aim to discover\nmeaningful groupings or categories in a dataset based on patterns or similarities", "within the data itself, without relying on pre-assigned labels. It is widely used for\nexploratory data analysis, pattern recognition, and segmentation tasks, allowing us", "to interpret and manage complex datasets by uncovering hidden structures and\nrelationships.\nOftentimes a dataset can be partitioned into different categories. A doctor may", "notice that their patients come in cohorts and different cohorts respond to different\ntreatments. A biologist may gain insight by identifying that bats and whales,", "despite outward appearances, have some underlying similarity, and both should be\nconsidered members of the same category, i.e., \u201cmammal\u201d. The problem of", "automatically identifying meaningful groupings in datasets is called clustering.\nOnce these groupings are found, they can be leveraged toward interpreting the data", "and making optimal decisions for each group.\nk\n Mathematically, clustering looks a bit like classification: we wish to find a mapping", "from datapoints, , to categories, . However, rather than the categories being\npredefined labels, the categories in clustering are automatically discovered partitions\nof an unlabeled dataset.", "Because clustering does not learn from labeled examples, it is an example of an\nunsupervised learning algorithm. Instead of mimicking the mapping implicit in\nsupervised training pairs", ", clustering assigns datapoints to categories\nbased on how the unlabeled data \n is distributed in data space.\nIntuitively, a \u201ccluster\u201d is a group of datapoints that are all nearby to each other and", "far away from other clusters. Let\u2019s consider the following scatter plot. How many\nclusters do you think there are?\nThere seem to be about five clumps of datapoints and those clumps are what we", "would like to call clusters. If we assign all datapoints in each clump to a cluster\ncorresponding to that clump, then we might desire that nearby datapoints are", "assigned to the same cluster, while far apart datapoints are assigned to different\nclusters.\nIn designing clustering algorithms, three critical things we need to decide are:", "How do we measure distance between datapoints? What counts as \u201cnearby\u201d\nand \u201cfar apart\u201d?\nHow many clusters should we look for?\nHow do we evaluate how good a clustering is?", "We will see how to begin making these decisions as we work through a concrete\nclustering algorithm in the next section.\nOne of the simplest and most commonly used clustering algorithms is called k-", "means. The goal of the k-means algorithm is to assign datapoints to  clusters in\nsuch a way that the variance within clusters is as small as possible. Notice that this\n12.3.1 Clustering formalisms\nx", "x\ny\n{x(i), y(i)}n\ni=1\n{x(i)}n\ni=1\n12.3.2 The k-means formulation\nk\nFigure 12.1: A dataset we would\nlike to cluster. How many clusters\ndo you think there are?", "matches our intuitive idea that a cluster should be a tightly packed set of\ndatapoints.\nSimilar to the way we showed that supervised learning could be formalized", "mathematically as the minimization of an objective function (loss function +\nregularization), we will show how unsupervised learning can also be formalized as", "minimizing an objective function. Let us denote the cluster assignment for a\ndatapoint \n as \n, i.e., \n means we are assigning datapoint", "to cluster number 1. Then the k-means objective can be quantified with the\nfollowing objective function (which we also call the \u201ck-means loss\u201d):\nwhere \n and \n, so that \n is the", ", so that \n is the\nmean of all datapoints in cluster , and using \n to denote the indicator function\n(which takes on value of 1 if its argument is true and 0 otherwise). The inner sum", "(over data points) of the loss is the variance of datapoints within cluster . We sum\nup the variance of all  clusters to get our overall loss.", "The k-means algorithm minimizes this loss by alternating between two steps: given\nsome initial cluster assignments: 1) compute the mean of all data in each cluster and", "assign this as the \u201ccluster mean\u201d, and 2) reassign each datapoint to the cluster with\nnearest cluster mean. Figure 12.2 shows what happens when we repeat these steps\non the dataset from above.", "Each time we reassign the data to the nearest cluster mean, the k-means loss\ndecreases (the datapoints end up closer to their assigned cluster mean), or stays the", "same. And each time we recompute the cluster means the loss also decreases (the\nmeans end up closer to their assigned datapoints) or stays the same. Overall then,", "the clustering gets better and better, according to our objective \u2013 until it stops\nimproving.\nAfter four iterations of cluster assignment + update means in our example, the k-", "means algorithm stops improving. We say it has converged, and its final solution is\nshown in Figure 12.3.\nx(i)\ny(i) \u2208{1, 2, \u2026 , k}\ny(i) = 1\nx(i)\nk\n\u2211\nj=1\nn\n\u2211\ni=1\n\ud835\udfd9(y(i) = j) x(i) \u2212\u03bc(j)\n2 ,\n\u2225\u2225\n(12.1)", "2 ,\n\u2225\u2225\n(12.1)\n\u03bc(j) =\n1\nNj \u2211n\ni=1 \ud835\udfd9(y(i) = j)x(i)\nNj = \u2211n\ni=1 \ud835\udfd9(y(i) = j)\n\u03bc(j)\nj\n\ud835\udfd9(\u22c5)\nj\nk\n12.3.2.0.0.1 K-means algorithm\nFigure 12.2: The first three steps of\nrunning the k-means algorithm on", "this data. Datapoints are colored\naccording to the cluster to which\nthey are assigned. Cluster means\nare the larger X\u2019s with black\noutlines.", "outlines.\n It seems to converge to something reasonable! Now let\u2019s write out the algorithm in\ncomplete detail:\nprocedure KMeans(\n)\nInitialize centroids \n and assignments \n randomly\nfor \n to  do\nfor", "for \n to  do\nfor \n to  do\nend for\nfor \n to  do\nend for\nif \n then\nbreak//convergence\nend if\nend for\nreturn \nend procedure", "end procedure\nThe for-loop over the  datapoints assigns each datapoint to the nearest cluster\ncenter. The for-loop over the k clusters updates the cluster center to be the mean of", "all datapoints currently assigned to that cluster. As suggested above, it can be\nshown that this algorithm reduces the loss in Equation 12.1 on each iteration, until it", "converges to a local minimum of the loss.\nIt\u2019s like classification except it picked what the classes are rather than being given\nexamples of what the classes are.", "We can also use gradient descent to optimize the k-means objective. To show how to\napply gradient descent, we first rewrite the objective as a differentiable function\nonly of :", "only of :\n is the value of the k-means loss given that we pick the optimal assignments of\nthe datapoints to cluster means (that\u2019s what the \n does). Now we can use the\ngradient", "gradient \n to find the values for  that achieve minimum loss when cluster\n1:\nk, \u03c4, {x(i)}n\ni=1\n2:\n\u03bc(1), \u2026 , \u03bc(k)\ny(1), \u2026 , y(n)\n3:\nt = 1\n\u03c4\n4:\nyold \u2190y\n5:\ni = 1\nn\n6:\ny(i) \u2190arg minj\u2208{1,\u2026,k} x(i) \u2212\u03bc(j)\n2", "2\n\u2225\u2225\n7:\n8:\nj = 1\nk\n9:\nNj \u2190\u2211n\ni=1 \ud835\udfd9(y(i) = j)\n10:\n\u03bc(j) \u2190\n1\nNj \u2211n\ni=1 \ud835\udfd9(y(i) = j) x(i)\n11:\n12:\ny = yold\n13:\n14:\n15:\n16:\n17:\n\u03bc, y\n18:\nn\n12.3.2.0.0.2 Using gradient descent to minimize k-means objective", "\u03bc\nL(\u03bc) =\nn\n\u2211\ni=1\nmin\nj\nx(i) \u2212\u03bc(j)\n2 .\n\u2225\u2225\nL(\u03bc)\nminj\n\u2202L(\u03bc)\n\u2202\u03bc\n\u03bc\nFigure 12.3: Converged result.\n assignments are optimal. Finally, we read off the optimal cluster assignments, given", "the optimized , just by assigning datapoints to their nearest cluster mean:\nThis procedure yields a local minimum of Equation 12.1, as does the standard k-", "means algorithm we presented (though they might arrive at different solutions). It\nmight not be the global optimum since the objective is not convex (due to \n, as", ", as\nthe minimum of multiple convex functions is not necessarily convex).\nThe standard k-means algorithm, as well as the variant that uses gradient descent,", "both are only guaranteed to converge to a local minimum, not necessarily the global\nminimum of the loss. Thus the answer we get out depends on how we initialize the", "cluster means. Figure 12.4 is an example of a different initialization on our toy data,\nwhich results in a worse converged clustering:", "A variety of methods have been developed to pick good initializations (see, for\nexample, the k-means++ algorithm). One simple option is to run the standard k-", "means algorithm multiple times, with different random initial conditions, and then\npick from these the clustering that achieves the lowest k-means loss.", "A very important parameter in cluster algorithms is the number of clusters we are\nlooking for. Some advanced algorithms can automatically infer a suitable number of", "clusters, but most of the time, like with k-means, we will have to pick  \u2013 it\u2019s a\nhyperparameter of the algorithm.\nFigure 12.5 shows an example of the effect. Which result looks more correct? It can", "be hard to say! Using higher k we get more clusters, and with more clusters we can\nachieve lower within-cluster variance \u2013 the k-means objective will never increase,\n\u03bc\ny(i) = arg min\nj\nx(i) \u2212\u03bc(j)\n2 .", "j\nx(i) \u2212\u03bc(j)\n2 .\n\u2225\u2225\nminj\n12.3.2.0.0.3 Importance of initialization\n12.3.2.0.0.4 Importance of k\nk\nFigure 12.4: With the initialization\nof the means to the left, the yellow", "and red means end up splitting\nwhat perhaps should be one cluster\nin half.\nFigure 12.5: Example of k-means\nrun on our toy data, with two\ndifferent values of k. Setting k=4,", "on the left, results in one cluster\nbeing merged, compared to setting\nk=5, on the right. Which clustering\ndo you think is better? How could\nyou decide?", "you decide?\n and will typically strictly decrease as we increase k. Eventually, we can increase k to\nequal the total number of datapoints, so that each datapoint is assigned to its own", "cluster. Then the k-means objective is zero, but the clustering reveals nothing.\nClearly, then, we cannot use the k-means objective itself to choose the best value for", "k. In Section 1.3, we will discuss some ways of evaluating the success of clustering\nbeyond its ability to minimize the k-means objective, and it\u2019s with these sorts of", "methods that we might decide on a proper value of k.\nAlternatively, you may be wondering: why bother picking a single k? Wouldn\u2019t it be", "nice to reveal a hierarchy of clusterings of our data, showing both coarse and fine\ngroupings? Indeed hierarchical clustering is another important class of clustering", "algorithms, beyond k-means. These methods can be useful for discovering tree-like\nstructure in data, and they work a bit like this: initially a coarse split/clustering of", "the data is applied at the root of the tree, and then as we descend the tree we split\nand cluster the data in ever more fine-grained ways. A prototypical example of", "hierarchical clustering is to discover a taxonomy of life, where creatures may be\ngrouped at multiple granularities, from species to families to kingdoms. You may", "find a suite of clustering algorithms in SKLEARN\u2019s cluster module.\nClustering algorithms group data based on a notion of similarity, and thus we need", "to define a distance metric between datapoints. This notion will also be useful in\nother machine learning approaches, such as nearest-neighbor methods that we see", "in Chapter 12. In k-means and other methods, our choice of distance metric can\nhave a big impact on the results we will find.\nOur k-means algorithm uses the Euclidean distance, i.e., \n, with a loss", ", with a loss\nfunction that is the square of this distance. We can modify k-means to use different\ndistance metrics, but a more common trick is to stick with Euclidean distance but", "measured in a feature space. Just like we did for regression and classification\nproblems, we can define a feature map from the data to a nicer feature\nrepresentation,", "representation, \n, and then apply k-means to cluster the data in the feature\nspace.\nAs a simple example, suppose we have two-dimensional data that is very stretched", "out in the first dimension and has less dynamic range in the second dimension.\nThen we may want to scale the dimensions so that each has similar dynamic range,", "prior to clustering. We could use standardization, like we did in Chapter 5.\nIf we want to cluster more complex data, like images, music, chemical compounds,", "etc., then we will usually need more sophisticated feature representations. One\ncommon practice these days is to use feature representations learned with a neural", "network. For example, we can use an autoencoder to compress images into feature\nvectors, then cluster those feature vectors.\n12.3.2.0.0.5 k-means in feature space\nx(i) \u2212\u03bc(j)\n\u2225\u2225\n\u03d5(x)", "x(i) \u2212\u03bc(j)\n\u2225\u2225\n\u03d5(x)\n12.3.3 How to evaluate clustering algorithms\n One of the hardest aspects of clustering is knowing how to evaluate it. This is", "actually a big issue for all unsupervised learning methods, since we are just looking\nfor patterns in the data, rather than explicitly trying to predict target values (which", "was the case with supervised learning).\nRemember, evaluation metrics are not the same as loss functions, so we can\u2019t just", "measure success by looking at the k-means loss. In prediction problems, it is critical\nthat the evaluation is on a held-out test set, while the loss is computed over training", "data. If we evaluate on training data we cannot detect overfitting. Something\nsimilar is going on with the example in Section 12.3.2.0.0.4 where setting k to be too", "large can precisely \u201cfit\u201d the data (minimize the loss), but yields no general insight.\nOne way to evaluate our clusters is to look at the consistency with which they are", "found when we run on different subsamples of our training data, or with different\nhyperparameters of our clustering algorithm (e.g., initializations). For example, if", "running on several bootstrapped samples (random subsets of our data) results in\nvery different clusters, it should call into question the validity of any of the\nindividual results.", "individual results.\nIf we have some notion of what ground truth clusters should be, e.g., a few data\npoints that we know should be in the same cluster, then we can measure whether or", "not our discovered clusters group these examples correctly.\nClustering is often used for visualization and interpretability, to make it easier for", "humans to understand the data. Here, human judgment may guide the choice of\nclustering algorithm. More quantitatively, discovered clusters may be used as input", "to downstream tasks. For example, as we saw in the lab, we may fit a different\nregression function on the data within each cluster. Figure 12.6 gives an example", "where this might be useful. In cases like this, the success of a clustering algorithm\ncan be indirectly measured based on the success of the downstream application", "(e.g., does it make the downstream predictions more accurate).\nFigure 12.6: Averaged across the\nwhole population, risk of heart\ndisease positively correlates with\nhours of exercise. However, if we", "cluster the data, we can observe\nthat there are four subgroups of the\npopulation which correspond to\ndifferent age groups, and within\neach subgroup the correlation is\nnegative. We can make better", "predictions, and better capture the\npresumed true effect, if we cluster\nthis data and then model the trend\nin each cluster separately."]