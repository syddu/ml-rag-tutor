["This page contains all content from the legacy PDF notes; gradient descent chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "In the previous chapter, we showed how to describe an interesting objective\nfunction for machine learning, but we need a way to find the optimal", ", particularly when the objective function is not amenable to\nanalytical optimization. For example, this can be the case when \n involves a", "involves a\nmore complex loss function, or more general forms of regularization. It can also be\nthe case when there are simply too many parameters to learn for it to be\ncomputationally feasible.", "There is an enormous and fascinating literature on the mathematical and\nalgorithmic foundations of optimization, but for this class, we will consider one of", "the simplest methods, called gradient descent.\nIntuitively, in one or two dimensions, we can easily think of \n as defining a\nsurface over", "surface over \n; that same idea extends to higher dimensions. Now, our objective is\nto find the \n value at the lowest point on that surface. One way to think about", "gradient descent is that you start at some arbitrary point on the surface, look to see\nin which direction the \u201chill\u201d goes down most steeply, take a small step in that", "direction, determine the direction of steepest descent from where you are, take\nanother small step, etc.\nBelow, we explicitly give gradient descent algorithms for one and multidimensional", "objective functions (Section 3.1 and Section 3.2). We then illustrate the application of\ngradient descent to a loss function which is not merely mean squared loss", "(Section 3.3). And we present an important method known as stochastic gradient\ndescent (Section 3.4), which is especially useful when datasets are too large for", "descent in a single batch, and has some important behaviors of its own.\n3.1 Gradient descent in one dimension\nWe start by considering gradient descent in one dimension. Assume \n, and", ", and\nthat we know both \n and its first derivative with respect to \n, \n. Here is\npseudo-code for gradient descent on an arbitrary function . Along with  and its\ngradient", "gradient \n (which, in the case of a scalar \n, is the same as its derivative \n), we\nhave to specify some hyper-parameters. These hyper-parameters include the initial\nvalue for parameter", ", a step-size hyper-parameter , and an accuracy hyper-\nparameter  .\n3  Gradient Descent\nNote\n\u0398\u2217= arg min\u0398 J(\u0398)\nJ(\u0398)\nJ(\u0398)\n\u0398\n\u0398\n\u0398 \u2208R\nJ(\u0398)\n\u0398 J \u2032(\u0398)\nf\nf\n\u2207\u0398f\n\u0398\nf \u2032\n\u0398\n\u03b7\n\u03f5\nYou might want to consider", "studying optimization some day!\nIt\u2019s one of the fundamental tools\nenabling machine learning, and it\u2019s\na beautiful and deep field.\n\uf4613  Gradient Descent\n\uf52a", "\uf52a\n The hyper-parameter  is often called learning rate when gradient descent is applied\nin machine learning. For simplicity,  may be taken as a constant, as is the case in", "the pseudo-code below; and we\u2019ll see adaptive (non-constant) step-sizes soon.\nWhat\u2019s important to notice though, is that even when  is constant, the actual\nmagnitude of the change to", "may not be constant, as that change depends on the\nmagnitude of the gradient itself too.\nprocedure 1D-Gradient-Descent(\n)\nrepeat\nuntil \nreturn \nend procedure", "end procedure\nNote that this algorithm terminates when the derivative of the function  is\nsufficiently small. There are many other reasonable ways to decide to terminate,\nincluding:", "including:\nStop after a fixed number of iterations , i.e., when \n. Practically, this is the\nmost common choice.\nStop when the change in the value of the parameter \n is sufficiently small,\ni.e., when", "i.e., when \n.\n\u2753 Study Question\nConsider all of the potential stopping criteria for 1D-Gradient-Descent , both\nin the algorithm as it appears and listed separately later. Can you think of ways", "that any two of the criteria relate to each other?\nTheorem 3.1 Choose any small distance \n. If we assume that  has a minimum, is", "sufficiently \u201csmooth\u201d and convex, and if the learning rate  is sufficiently small, gradient\ndescent will reach a point within  of a global optimum point \n.", ".\nHowever, we must be careful when choosing the learning rate to prevent slow\nconvergence, non-converging oscillation around the minimum, or divergence.", "The following plot illustrates a convex function \n, starting gradient\ndescent at \n with a step-size of \n. It is very well-behaved!\n\u03b7\n\u03b7\n\u03b7\n\u0398\n1:\n\u0398init, \u03b7, f, f \u2032, \u03f5\n2:\n\u0398(0) \u2190\u0398init\n3:\nt \u21900\n4:\n5:\nt \u2190t + 1", "t \u21900\n4:\n5:\nt \u2190t + 1\n6:\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7 f \u2032(\u0398(t\u22121))\n7:\n|f \u2032(\u0398(t))| < \u03f5\n8:\n\u0398(t)\n9:\nf\nT\nt = T\n\u0398\n\u0398(t) \u2212\u0398(t\u22121) < \u03f5\n\u2223\u2223\n~\u03f5 > 0\nf\n\u03b7\n~\u03f5\n\u0398\nf(x) = (x \u22122)2\nxinit = 4.0\n1/2\n \u22121\n1\n2\n3\n4\n5\n6\n2\n4\nx\nf(x)", "3\n4\n5\n6\n2\n4\nx\nf(x)\nIf  is non-convex, where gradient descent converges to depends on \n. First, let\u2019s\nestablish some definitions. Let  be a real-valued function defined over some\ndomain \n. A point", "domain \n. A point \n is called a global minimum point of  if \n for\nall other \n. A point \n is instead called a local minimum point of a function\n if there exists some constant", "such that for all  within the interval defined\nby \n \n, where  is some distance metric, e.g.,\n A global minimum point is also a local minimum point, but a", "local minimum point does not have to be a global minimum point.\n\u2753 Study Question\nWhat happens in this example with very small ? With very big ?", "If  is non-convex (and sufficiently smooth), one expects that gradient descent (run\nlong enough with small enough learning rate) will get very close to a point at which", "the gradient is zero, though we cannot guarantee that it will converge to a global\nminimum point.\nThere are two notable exceptions to this common sense expectation: First, gradient", "descent can get stagnated while approaching a point  which is not a local\nminimum or maximum, but satisfies \n. For example, for \n, starting\ngradient descent from the initial guess", ", while using learning rate \nwill lead to \n converging to zero as \n. Second, there are functions (even\nconvex ones) with no minimum points, like \n, for which gradient", "descent with a positive learning rate converges to \n.\nThe plot below shows two different \n, and how gradient descent started from\neach point heads toward two different local optimum points.\nf\nxinit\nf", "f\nxinit\nf\nD\nx0 \u2208D\nf\nf(x0) \u2264f(x)\nx \u2208D\nx0 \u2208D\nf\n\u03f5 > 0\nx\nd(x, x0) < \u03f5, f(x0) \u2264f(x)\nd\nd(x, x0) = ||x \u2212x0||.\n\u03b7\n\u03b7\nf\nx\nf \u2032(x) = 0\nf(x) = x3\nxinit = 1\n\u03b7 < 1/3\nx(k)\nk \u2192\u221e\nf(x) = exp(\u2212x)\n+\u221e\nxinit\n \u22122\n\u22121\n1\n2\n3\n4", "\u22122\n\u22121\n1\n2\n3\n4\n4\n6\n8\n10\nx\nf(x)\n3.2 Multiple dimensions\nThe extension to the case of multi-dimensional \n is straightforward. Let\u2019s assume\n, so \n.\nThe gradient of  with respect to \n is", "is\nThe algorithm remains the same, except that the update step in line 5 becomes\nand any termination criteria that depended on the dimensionality of \n would have", "would have\nto change. The easiest thing is to keep the test in line 6 as \n,\nwhich is sensible no matter the dimensionality of \n.\n\u2753 Study Question", ".\n\u2753 Study Question\nWhich termination criteria from the 1D case were defined in a way that assumes\n is one dimensional?\n3.3 Application to regression\n\u0398\n\u0398 \u2208Rm\nf : Rm \u2192R\nf\n\u0398\n\u2207\u0398f =\n\u23a1\n\u23a2\n\u23a3\n\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m", "\u23a2\n\u23a3\n\u2202f/\u2202\u03981\n\u22ee\n\u2202f/\u2202\u0398m\n\u23a4\n\u23a5\n\u23a6\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7\u2207\u0398f(\u0398(t\u22121))\n\u0398\nf(\u0398(t)) \u2212f(\u0398(t\u22121)) < \u03f5\n\u2223\u2223\n\u0398\n\u0398\n Recall from the previous chapter that choosing a loss function is the first step in", "formulating a machine-learning problem as an optimization problem, and for\nregression we studied the mean square loss, which captures losws as\n. This leads to the ordinary least squares objective", "We use the gradient of the objective with respect to the parameters,\nto obtain an analytical solution to the linear regression problem. Gradient descent", "could also be applied to numerically compute a solution, using the update rule\nNow, let\u2019s add in the regularization term, to get the ridge-regression objective:", "Recall that in ordinary least squares, we finessed handling \n by adding an extra\ndimension of all 1\u2019s. In ridge regression, we really do need to separate the\nparameter vector  from the offset", ", and so, from the perspective of our general-\npurpose gradient descent method, our whole parameter set \n is defined to be\n. We will go ahead and find the gradients separately for each one:", "Note that \n will be of shape \n and \n will be a scalar since we\nhave separated \n from  here.\n\u2753 Study Question\n(guess \u2212actual)2\nJ(\u03b8) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) \u2212y(i))\n2\n.\n\u2207\u03b8J = 2\nn XT\nd\u00d7n\n(X\u03b8 \u2212Y )\nn\u00d71\n,\n\ue152\n\ue154", "(X\u03b8 \u2212Y )\nn\u00d71\n,\n\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\ue152\n\ue154\n\ue151\ue150\n\ue154\n\ue153\n(3.1)\n\u03b8(t) = \u03b8(t\u22121) \u2212\u03b7 2\nn\nn\n\u2211\ni=1\n([\u03b8(t\u22121)]\nT\nx(i) \u2212y(i))x(i) .\n3.3.1 Ridge regression\nJridge(\u03b8, \u03b80) = 1\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))\n2\n+ \u03bb\u2225\u03b8\u22252 .\n\u03b80\n\u03b8\n\u03b80\n\u0398", "+ \u03bb\u2225\u03b8\u22252 .\n\u03b80\n\u03b8\n\u03b80\n\u0398\n\u0398 = (\u03b8, \u03b80)\n\u2207\u03b8Jridge(\u03b8, \u03b80) = 2\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i))x(i) + 2\u03bb\u03b8\n\u2202Jridge(\u03b8, \u03b80)\n\u2202\u03b80\n= 2\nn\nn\n\u2211\ni=1\n(\u03b8Tx(i) + \u03b80 \u2212y(i)) .\n\u2207\u03b8Jridge\nd \u00d7 1\n\u2202Jridge/\u2202\u03b80\n\u03b80\n\u03b8", "\u2202Jridge/\u2202\u03b80\n\u03b80\n\u03b8\n Convince yourself that the dimensions of all these quantities are correct, under\nthe assumption that  is \n. How does  relate to \n as discussed for \n in the\nprevious section?", "previous section?\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives\n. What is the shape of \n?\n\u2753 Study Question\nCompute \n by finding the vector of partial derivatives\n.", ".\n\u2753 Study Question\nUse these last two results to verify our derivation above.\nPutting everything together, our gradient descent algorithm for ridge regression\nbecomes\nprocedure RR-Gradient-Descent(\n)", ")\nrepeat\nuntil \nreturn \nend procedure\n\u2753 Study Question\nIs it okay that  doesn\u2019t appear in line 8?\n\u2753 Study Question\nIs it okay that the 2\u2019s from the gradient definitions don\u2019t appear in the\nalgorithm?", "algorithm?\n\u03b8\nd \u00d7 1\nd\nm\n\u0398\n\u2207\u03b8||\u03b8||2\n(\u2202||\u03b8||2/\u2202\u03b81, \u2026 , \u2202||\u03b8||2/\u2202\u03b8d)\n\u2207\u03b8||\u03b8||2\n\u2207\u03b8Jridge(\u03b8Tx + \u03b80, y)\n(\u2202Jridge(\u03b8Tx + \u03b80, y)/\u2202\u03b81, \u2026 , \u2202Jridge(\u03b8Tx + \u03b80, y)/\u2202\u03b8d)\n1:\n\u03b8init, \u03b80init, \u03b7, \u03f5\n2:\n\u03b8(0) \u2190\u03b8init\n3:\n\u03b8(0)", "\u03b8(0) \u2190\u03b8init\n3:\n\u03b8(0)\n0\n\u2190\u03b80init\n4:\nt \u21900\n5:\n6:\nt \u2190t + 1\n7:\n\u03b8(t) = \u03b8(t\u22121) \u2212\u03b7 ( 1\nn \u2211n\ni=1 (\u03b8(t\u22121)Tx(i) + \u03b80\n(t\u22121) \u2212y(i))x(i) + \u03bb\u03b8(t\u22121))\n8:\n\u03b8(t)\n0 = \u03b8(t\u22121)\n0\n\u2212\u03b7 ( 1\nn \u2211n\ni=1 (\u03b8(t\u22121)Tx(i) + \u03b80(t\u22121) \u2212y(i)))", "9:\nJridge(\u03b8(t), \u03b8(t)\n0 ) \u2212Jridge(\u03b8(t\u22121), \u03b8(t\u22121)\n0\n) < \u03f5\n\u2223\u2223\n10:\n\u03b8(t), \u03b8(t)\n0\n11:\n\u03bb\nBeware double superscripts! \n is\nthe transpose of the vector .\n[\u03b8]T\n\u03b8\n 3.4 Stochastic gradient descent", "When the form of the gradient is a sum, rather than take one big(ish) step in the\ndirection of the gradient, we can, instead, randomly select one term of the sum, and", "take a very small step in that direction. This seems sort of crazy, but remember that\nall the little steps would average out to the same direction as the big step if you", "were to stay in one place. Of course, you\u2019re not staying in that place, so you move,\nin expectation, in the direction of the gradient.", "Most objective functions in machine learning can end up being written as an\naverage over data points, in which case, stochastic gradient descent (sgd) is", "implemented by picking a data point randomly out of the data set, computing the\ngradient as if there were only that one point in the data set, and taking a small step\nin the negative direction.", "Let\u2019s assume our objective has the form\nwhere  is the number of data points used in the objective (and this may be\ndifferent from the number of points available in the whole data set).", "Here is pseudocode for applying sgd to such an objective ; it assumes we know the\nform of \n for all  in \n:\nprocedure Stochastic-Gradient-Descent(\n)\nfor \n do\nrandomly select \nend for\nend procedure", "end procedure\nNote that now instead of a fixed value of ,  is indexed by the iteration of the\nalgorithm, . Choosing a good stopping criterion can be a little trickier for sgd than", "traditional gradient descent. Here we\u2019ve just chosen to stop after a fixed number of\niterations .\nFor sgd to converge to a local optimum point as  increases, the learning rate has to", "decrease as a function of time. The next result shows one learning rate sequence that\nworks.\nTheorem 3.2 If  is convex, and \n is a sequence satisfying\nf(\u0398) = 1\nn\nn\n\u2211\ni=1\nfi(\u0398) ,\nn\nf\n\u2207\u0398fi\ni\n1 \u2026 n\n1:", "n\nf\n\u2207\u0398fi\ni\n1 \u2026 n\n1:\n\u0398init, \u03b7, f, \u2207\u0398f1, . . . , \u2207\u0398fn, T\n2:\n\u0398(0) \u2190\u0398init\n3:\nt \u21901\n4:\ni \u2208{1, 2, \u2026 , n}\n5:\n\u0398(t) = \u0398(t\u22121) \u2212\u03b7(t) \u2207\u0398fi(\u0398(t\u22121))\n6:\n7:\n\u03b7 \u03b7\nt\nT\nt\nf\n\u03b7(t)\n\u221e\n\u2211\nt=1\n\u03b7(t) = \u221eand\n\u221e\n\u2211\nt=1\n\u03b7(t)2 < \u221e,", "\u221e\n\u2211\nt=1\n\u03b7(t)2 < \u221e,\nSometimes you will see that the\nobjective being written as a sum,\ninstead of an average. In the \u201csum\u201d\nconvention, the \n normalizing\nconstant is getting \u201cabsorbed\u201d into\nindividual", "individual \n.\n1\nn\nfi\nf(\u0398) =\nn\n\u2211\ni=1\nfi(\u0398) .\n then SGD converges with probability one* to the optimal \n.*\nWhy these two conditions? The intuition is that the first condition, on \n, is", ", is\nneeded to allow for the possibility of an unbounded potential range of exploration,\nwhile the second condition, on \n, ensures that the learning rates get smaller\nand smaller as  increases.", "One \u201clegal\u201d way of setting the learning rate is to make \n but people often\nuse rules that decrease more slowly, and so don\u2019t strictly satisfy the criteria for\nconvergence.\n\u2753 Study Question", "\u2753 Study Question\nIf you start a long way from the optimum, would making \n decrease more\nslowly tend to make you move more quickly or more slowly to the optimum?", "There are multiple intuitions for why sgd might be a better choice algorithmically\nthan regular gd (which is sometimes called batch gd (bgd)):", "bgd typically requires computing some quantity over every data point in a data\nset. sgd may perform well after visiting only some of the data. This behavior", "can be useful for very large data sets \u2013 in runtime and memory savings.\nIf your  is actually non-convex, but has many shallow local optimum points", "that might trap bgd, then taking samples from the gradient at some point \nmight \u201cbounce\u201d you around the landscape and away from the local optimum\npoints.", "points.\nSometimes, optimizing  really well is not what we want to do, because it\nmight overfit the training set; so, in fact, although sgd might not get lower", "training error than bgd, it might result in lower test error.\n\u0398\n\u2211\u03b7(t)\n\u2211\u03b7(t)2\nt\n\u03b7(t) = 1/t\n\u03b7(t)\nf\n\u0398\nf"]