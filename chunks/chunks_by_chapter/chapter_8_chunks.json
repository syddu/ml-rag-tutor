["This page contains all content from the legacy PDF notes; autoencoders chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "In previous chapters, we have largely focused on classification and regression\nproblems, where we use supervised learning with training samples that have both", "features/inputs and corresponding outputs or labels, to learn hypotheses or models\nthat can then be used to predict labels for new data.", "In contrast to supervised learning paradigm, we can also have an unsupervised\nlearning setting, where we only have features but no corresponding outputs or", "labels for our dataset. On natural question aries then: if there are no labels, what are\nwe learning?\nOne canonical example of unsupervised learning is clustering, which is discussed in", "Section 12.3. In clustering, the goal is to develop algorithms that can reason about\n\u201csimilarity\u201d among data points\u2019s features, and group the data points into clusters.", "Autoencoders are another family of unsupervised learning algorithms, in this case\nseeking to obtain insights about our data by learning compressed versions of the", "original data, or, in other words, by finding a good lower-dimensional feature\nrepresentations of the same data set. Such insights might help us to discover and", "characterize underlying factors of variation in data, which can aid in scientific\ndiscovery; to compress data for efficient storage or communication; or to pre-", "process our data prior to supervised learning, perhaps to reduce the amount of data\nthat is needed to learn a good classifier or regressor.\n8.1 Autoencoder structure\nAssume that we have input data", ", where \n. We seek to\nlearn an autoencoder that will output a new dataset \n, where\n with \n. We can think about \n as the new representation of data point", ". For example, in Figure 8.1 we show the learned representations of a dataset of\nMNIST digits with \n. We see, after inspecting the individual data points, that", "unsupervised learning has found a compressed (or latent) representation where\nimages of the same digit are close to each other, potentially greatly aiding", "subsequent clustering or classification tasks.\n8  Representation Learning (Autoencoders)\nNote\nD = {x(1), \u2026 , x(n)}\nx(i) \u2208Rd\nDout = {a(1), \u2026 , a(n)}\na(i) \u2208Rk\nk < d\na(i)\nx(i)\nk = 2", "a(i)\nx(i)\nk = 2\n\uf4618  Representation Learning (Autoencoders)\n\uf52a\n Formally, an autoencoder consists of two functions, a vector-valued encoder", "that deterministically maps the data to the representation space \n, and a decoder \n that maps the representation space back into the\noriginal data space.", "In general, the encoder and decoder functions might be any functions appropriate\nto the domain. Here, we are particularly interested in neural network embodiments", "of encoders and decoders. The basic architecture of one such autoencoder,\nconsisting of only a single layer neural network in each of the encoder and decoder,", "is shown in Figure 8.2; note that bias terms \n and \n into the summation nodes\nexist, but are omitted for clarity in the figure. In this example, the original -\ndimensional input is compressed into", "dimensions via the encoder\n with \n and \n, and where the\nnon-linearity \n is applied to each dimension of the vector. To recover (an\napproximation to) the original instance, we then apply the decoder", ", where \n denotes a different non-linearity\n(activation function). In general, both the decoder and the encoder could involve", "multiple layers, as opposed to the single layer shown here. Learning seeks\nparameters \n and \n such that the reconstructed instances,\n, are close to the original input \n.\n8.2 Autoencoder Learning", "g : Rd \u2192Rk\na \u2208Rk\nh : Rk \u2192Rd\nW 1\n0\nW 2\n0\nd\nk = 3\ng(x; W 1, W 1\n0 ) = f1(W 1Tx + W 1\n0 )\nW 1 \u2208Rd\u00d7k\nW 1\n0 \u2208Rk\nf1\nh(a; W 2, W 2\n0 ) = f2(W 2Ta + W 2\n0 )\nf2\nW 1, W 1\n0\nW 2, W 2\n0\nh(g(x(i); W 1, W 1", "h(g(x(i); W 1, W 1\n0 ); W 2, W 2\n0 )\nx(i)\nFigure 8.1: Compression of digits\ndataset into two dimensions. The\ninput \n, an image of a\nhandwritten digit, is shown at the\nnew low-dimensional", "new low-dimensional\nrepresentation \n.\nx(i)\n(a1, a2)\nFigure 8.2: Autoencoder structure,\nshowing the encoder (left half,\nlight green), and the decoder (right\nhalf, light blue), encoding inputs", "to the representation , and\ndecoding the representation to\nproduce , the reconstruction. In\nthis specific example, the\nrepresentation (\n, \n, \n) only has\nthree dimensions.\nx\na\n~x\na1 a2 a3", "x\na\n~x\na1 a2 a3\n We learn the weights in an autoencoder using the same tools that we previously\nused for supervised learning, namely (stochastic) gradient descent of a multi-layer", "neural network to minimize a loss function. All that remains is to specify the loss\nfunction \n, which tells us how to measure the discrepancy between the\nreconstruction \n and the original input . For", "example, for continuous-valued  it might make sense to use squared loss, i.e.,\n.\nLearning then seeks to optimize the parameters of  and  so as to minimize the", "reconstruction error, measured according to this loss function:\n8.3 Evaluating an autoencoder\nWhat makes a good learned representation in an autoencoder? Notice that, without", "further constraints, it is always possible to perfectly reconstruct the input. For\nexample, we could let \n and  and  be the identity functions. In this case, we", "would not obtain any compression of the data.\nTo learn something useful, we must create a bottleneck by making  to be smaller\n(often much smaller) than . This forces the learning algorithm to seek", "transformations that describe the original data using as simple a description as\npossible. Thinking back to the digits dataset, for example, an example of a", "compressed representation might be the digit label (i.e., 0\u20139), rotation, and stroke\nthickness. Of course, there is no guarantee that the learning algorithm will discover", "precisely this representation. After learning, we can inspect the learned\nrepresentations, such as by artificially increasing or decreasing one of the\ndimensions (e.g.,", "dimensions (e.g., \n) and seeing how it affects the output \n, to try to better\nunderstand what it has learned.\nAs with clustering, autoencoders can be a preliminary step toward building other", "models, such as a regressor or classifier. For example, once a good encoder has been\nlearned, the decoder might be replaced with another neural network that is then", "trained with supervised learning (perhaps using a smaller dataset that does include\nlabels).\n8.4 Linear encoders and decoders\nWe close by mentioning that even linear encoders and decoders can be very", "powerful. In this case, rather than minimizing the above objective with gradient\ndescent, a technique called principal components analysis (PCA) can be used to obtain\nL(~x, x)\n~x = h(g(x; W 1, W 1", "0 ); W 2, W 2\n0 )\nx\nx\nLSE(~x, x) = \u2211d\nj=1(xj \u2212~xj)2\nh\ng\nmin\nW 1,W 1\n0 ,W 2,W 2\n0\nn\n\u2211\ni=1\nLSE (h(g(x(i); W 1, W 1\n0 ); W 2, W 2\n0 ), x(i))\nk = d\nh\ng\nk\nd\na1\nh(a)\nAlternatively, you could think of", "this as multi-task learning, where\nthe goal is to predict each\ndimension of . One can mix-and-\nmatch loss functions as appropriate\nfor each dimension\u2019s data type.\nx", "x\n a closed-form solution to the optimization problem using a singular value\ndecomposition (SVD). Just as a multilayer neural network with nonlinear", "activations for regression (learned by gradient descent) can be thought of as a\nnonlinear generalization of a linear regressor (fit by matrix algebraic operations),", "the neural network based autoencoders discussed above (and learned with gradient\ndescent) can be thought of as a generalization of linear PCA (as solved with matrix\nalgebra by SVD).", "algebra by SVD).\n8.5 Advanced encoders and decoders\nAdvanced neural networks built on encoder-decoder architectures have become", "increasingly powerful. One prominent example is generative networks, designed to\ncreate new outputs that resemble\u2014but differ from\u2014existing training examples. A", "notable type, variational autoencoders, learns a compressed representation\ncapturing statistical properties (such as mean and variance) of training data. These", "latent representations can then be sampled to generate novel outputs using the\ndecoder.\nAnother influential encoder-decoder architecture is the Transformer, covered in", "Chapter 9. Transformers consist of multiple encoder and decoder layers combined\nwith self-attention mechanisms, which excel at predicting sequential data, such as", "words and sentences in natural language processing (NLP).\nCentral to autoencoders and Transformers is the idea of learning representations.", "Autoencoders compress data into efficient, informative representations, while NLP\nmodels encode language\u2014words, phrases, sentences\u2014into numerical forms. This", "numerical encoding leads us to the concept of vector embeddings.\n8.6 Embeddings\nIn NLP, words are represented as vectors, commonly known as word embeddings. A", "key property of good embeddings is that their numerical closeness mirrors semantic\nsimilarity. For instance, semantically related words such as \u201cdog\u201d and \u201ccat\u201d should", "have vectors close together, while unrelated words like \u201ccat\u201d and \u201ctable\u201d should be\nfarther apart.\nSimilarity between embeddings is frequently measured using the inner product:", "The inner product indicates how aligned two vectors are: highly positive values\nimply strong similarity, negative values indicate opposition, and values near zero", "suggest no similarity (up to a scaling factor related to the magnitude).\naTb = a \u22c5b\n A groundbreaking embedding method, word2vec (2012), significantly advanced NLP", "by producing embeddings where vector arithmetic corresponded to real-world\nsemantic relationships. For instance:\nSuch embeddings revealed meaningful semantic relationships like analogies across", "diverse vocabulary (e.g., uncle \u2013 man + woman \u2248 aunt).\nImportantly, embeddings don\u2019t need exact coordinates\u2014it\u2019s their relative", "positioning within the vector space that matters. Embeddings are considered\neffective if they facilitate downstream NLP tasks, such as predicting missing words,", "classifying texts, or language translation.\nFor example, effective embeddings allow models to accurately predict a missing\nword in a sentence:\nAfter the rain, the grass was ____.", "Or a model could be built that tries to correctly predict words in the middle of\nsentences:\nThe child fell __ __ during the long car ride", "This task exemplifies self-supervision, a training approach where models generate\nlabels directly from the data itself, eliminating the need for manual labeling.", "Training neural networks through self-supervision involves optimizing their ability\nto predict words accurately from large text corpora (e.g., Wikipedia). Through such", "optimization, embeddings capture subtle semantic and syntactic nuances, greatly\nenhancing NLP capabilities.\nThe idea of good embeddings will play a central role when we discuss attention", "mechanisms in Chapter 9, where embeddings dynamically adjust based on context\n(via the so-called attention mechanism), enabling a more nuanced understanding of\nlanguage.", "language.\nembeddingparis \u2212embeddingfrance + embeddingitaly \u2248embeddingrome"]