["In which we try to describe the outlines of the \u201clifecycle\u201d of supervised learning,\nincluding hyperparameter tuning and evaluation of the final product.\nC.1 General case", "C.1 General case\nWe start with a very generic setting.\nGiven: - Space of inputs (X) - Space of outputs (y) - Space of possible hypotheses ()", "such that each (h ) is a function (h: x y) - Loss function (: y y ) a supervised learning\nalgorithm () takes as input a data set of the form\nwhere \n and \n and returns an \n.", "and returns an \n.\nGiven a problem specification and a set of data \n, we evaluate hypothesis \naccording to average loss, or error,", "If the data used for evaluation were not used during learning of the hypothesis then this\nis a reasonable estimate of how well the hypothesis will make additional", "predictions on new data from the same source.\nA validation strategy  takes an algorithm \n, a loss function , and a data source \nand produces a real number which measures how well", "performs on data from\nthat distribution.\nIn the simplest case, we can divide \n into two sets, \n and \n, train on the\nfirst, and then evaluate the resulting hypothesis on the second. In that case,", "Appendix C \u2014 Supervised learning in a\nnutshell\nC.1.1 Minimal problem specification \ue9cb\nD = {(x(1), y(1)), \u2026 , (x(n), y(n))}\nx(i) \u2208X\ny(i) \u2208y\nh \u2208H\nC.1.2 Evaluating a hypothesis\nD\nh\nE(h, L, D) =\n1\n|D|\nD\n\u2211", "1\n|D|\nD\n\u2211\ni=1\nL (h (x(i)), y(i))\nC.1.3 Evaluating a supervised learning algorithm\nV\nA\nL\nD\nA\nC.1.3.1 Using a validation set\nD\nDtrain \nDval \nV(A, L, D) = E (A (Dtrain ), L, Dval )", "\uf461Appendices > C  Supervised learning in a nutshell\n\uf52a\n We can\u2019t reliably evaluate an algorithm based on a single application to a single", "training and test set, because there are many aspects of the training and testing\ndata, as well as, sometimes, randomness in the algorithm itself, that cause variance", "in the performance of the algorithm. To get a good idea of how well an algorithm\nperforms, we need to, multiple times, train it and evaluate the resulting hypothesis,\nand report the average over", "executions of the algorithm of the error of the\nhypothesis it produced each time.\nWe divide the data into 2 K random non-overlapping subsets:\n.\nThen,", ".\nThen,\nIn cross validation, we do a similar computation, but allow data to be re-used in the\n different iterations of training and testing the algorithm (but never share training", "and testing data for a single iteration!). See Section 2.8.2.2 for details.\nNow, if we have two different algorithms \n and \n, we might be interested in", "knowing which one will produce hypotheses that generalize the best, using data\nfrom a particular source. We could compute \n and \n, and", "and \n, and\nprefer the algorithm with lower validation error. More generally, given algorithms\n, we would prefer\nNow what? We have to deliver a hypothesis to our customer. We now know how to", "find the algorithm, \n, that works best for our type of data. We can apply it to all of\nour data to get the best hypothesis we know how to create, which would be", "and deliver this resulting hypothesis as our best product.\nA majority of learning algorithms have the form of optimizing some objective\ninvolving the training data and a loss function.", "C.1.3.2 Using multiple training/evaluation runs\nK\nDtrain \n1\n, Dval \n1 , \u2026 , Dtrain \nK\n, Dval \nK\nV(A, L, D) = 1\nK\nK\n\u2211\nk=1\nE(A(Dtrain\nk\n), L, Dval\nk ) .\nC.1.3.3 Cross validation\nK", "K\nC.1.4 Comparing supervised learning algorithms\nA1\nA2\nV (A1, L, D)\nV (A\u2208, L, D)\nA1, \u2026 , AM\nA\u2217= arg min\nm V (AM, L, D)\nC.1.5 Fielding a hypothesis\nA\u2217\nh\u2217= A\u2217(D)\nC.1.6 Learning algorithms as optimizers", "Interestingly, this loss function is\nnot always the same as the loss\nfunction that is used for\n So for example, (assuming a perfect optimizer which doesn\u2019t, of course, exist) we", "might say our algorithm is to solve an optimization problem:\nOur objective often has the form\nwhere  is a loss to be minimized during training and \n is a regularization term.", "Often, rather than comparing an arbitrary collection of learning algorithms, we\nthink of our learning algorithm as having some parameters that affect the way it", "maps data to a hypothesis. These are not parameters of the hypothesis itself, but\nrather parameters of the algorithm. We call these hyperparameters. A classic example", "would be to use a hyperparameter  to govern the weight of a regularization term\non an objective to be optimized:\nThen we could think of our algorithm as \n. Picking a good value of  is the", "same as comparing different supervised learning algorithms, which is accomplished\nby validating them and picking the best one!\nC.2 Concrete case: linear regression", "In linear regression the problem formulation is this:\n for values of parameters \n and \n.\nOur learning algorithm has hyperparameter  and can be written as:", "Our learning algorithm has hyperparameter $ $ and can be written as:\nFor a particular training data set and parameter , it finds the best hypothesis on\nthis data, specified with parameters", ", written \n.\nA(D) = arg min\nh\u2208H J (h; D).\nJ (h; D) = E(h, L, D) + R(h),\nL\nR\nC.1.7 Hyperparameters\n\u03bb\nJ (h; D) = E(h, L, D) + \u03bbR(h).\nA(D; \u03bb)\n\u03bb\nx = Rd\ny = R\nH = {\u03b8\u22a4x + \u03b80}\n\u03b8 \u2208Rd\n\u03b80 \u2208R\nL(g, y) = (g \u2212y)2", "L(g, y) = (g \u2212y)2\n\u03bb\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2225\u03b8\u22252\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2225\u03b8\u22252.\n\u03bb\n\u0398 = (\u03b8, \u03b80)\n\u0398\u2217(\u03bb, D)", "\u0398\u2217(\u03bb, D)\nevaluation! We will see this in\nlogistic regression.\n Picking the best value of the hyperparameter is choosing among learning", "algorithms. We could, most simply, optimize using a single training / validation\nsplit, so \n \n, and\nIt would be much better to select the best  using multiple runs or cross-validation;", "that would just be a different choices of the validation procedure  in the top line.\nNote that we don\u2019t use regularization here because we just want to measure how", "good the output of the algorithm is at predicting values of new points, and so that\u2019s\nwhat we measure. We use the regularizer during training when we don\u2019t want to", "focus only on optimizing predictions on the training data.\nFinally! To make a predictor to ship out into the world, we would use all the data\nwe have,", "we have, \n, to train, using the best hyperparameters we know, and return\nFinally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, as", "Here are the same ideas, written out in informal pseudocode:\nD = Dtrain \u222aDval \n\u03bb\u2217= arg min\n\u03bb V (A\u03bb, L, Dval )\n= arg min\n\u03bb E (\u0398\u2217(\u03bb, Dtrain ),  mse, Dval )\n= arg min\n\u03bb\n1\n|Dval |\n\u2211\n(x,y)\u2208Dval", "\u2211\n(x,y)\u2208Dval \n(\u03b8\u2217(\u03bb, Dtrain )\n\u22a4x + \u03b8\u2217\n0 (\u03bb, Dtrain ) \u2212y)\n2\n\u03bb\nV\nD\n\u0398\u2217= A (D; \u03bb\u2217)\n= \u0398\u2217(\u03bb\u2217, D)\n= arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\n(\u03b8\u22a4x + \u03b80 \u2212y)\n2 + \u03bb\u2217\u2225\u03b8\u22252\nE test  = E (\u0398\u2217,  mse , Dtest )\n=\n1\n|Dtest |\n\u2211", "=\n1\n|Dtest |\n\u2211\n(x,y)\u2208Dtot \n(\u03b8\u2217Tx + \u03b8\u2217\n0 \u2212y)\n2\n# returns theta_best(D, lambda)\ndefine train(D, lambda):\n    return minimize(mse(theta, D) + lambda * norm(theta)**2, theta)", "# returns lambda_best using very simple validation\ndefine simple_tune(D_train, D_val, possible_lambda_vals):\n    scores = [mse(train(D_train, lambda), D_val) for lambda in \npossible_lambda_vals]", "return possible_lambda_vals[least_index[scores]]\n# returns theta_best overall\ndefine theta_best(D_train, D_val, possible_lambda_vals):", "return train(D_train + D_val, simple_tune(D_train, D_val, \npossible_lambda_vals))\n# customer evaluation of the theta delivered to them\n C.3 Concrete case: logistic regression", "In binary logistic regression the problem formulation is as follows. We are writing\nthe class labels as 1 and 0.\n for values of parameters \n and \n.\nProxy loss \n Our learning algorithm", "has hyperparameter  and can be written as:\nFor a particular training data set and parameter , it finds the best hypothesis on\nthis data, specified with parameters \n, written \n according to the", "according to the\nproxy loss \n.\nPicking the best value of the hyperparameter is choosing among learning\nalgorithms based on their actual predictions. We could, most simply, optimize using", "a single training / validation split, so \n, and we use the real 01 loss:\nIt would be much better to select the best  using multiple runs or cross-validation;", "that would just be a different choices of the validation procedure  in the top line.\nFinally! To make a predictor to ship out into the world, we would use all the data\nwe have,", "we have, \n, to train, using the best hyperparameters we know, and return\n\u2753 Study Question\nWhat loss function is being optimized inside this algorithm?", "Finally, a customer might evaluate this hypothesis on their data, which we have\nnever seen during training or validation, as\ndefine customer_val(theta):\n    return mse(theta, D_test)\nX = Rd", "X = Rd\ny = {+1, 0}\nH = {\u03c3 (\u03b8\u22a4x + \u03b80)}\n\u03b8 \u2208Rd\n\u03b80 \u2208R\nL(g, y) = L01( g,  h)\nLnll(g, y) = \u2212(y log(g) + (1 \u2212y) log(1 \u2212g))\n\u03bb\nA(D; \u03bb) = \u0398\u2217(\u03bb, D) = arg min\n\u03b8,\u03b80\n1\n|D|\n\u2211\n(x,y)\u2208D\nLnll (\u03c3 (\u03b8\u22a4x + \u03b80), y) + \u03bb\u2225\u03b8\u22252", "\u03bb\n\u0398 = (\u03b8, \u03b80)\n\u0398\u2217(\u03bb, D)\nLnll \nD = Dtrain  \u222aDval\n\u03bb\u2217= arg min\n\u03bb V (A\u03bb, L01, Dval )\n= arg min\n\u03bb E (\u0398\u2217(\u03bb, Dtrain ), L01, Dval )\n= arg min\n\u03bb\n1\n|Dval |\n\u2211\n(x,y)\u2208Dval \nL01 (\u03c3 (\u03b8\u2217(\u03bb, Dtrain )\n\u22a4x + \u03b8\u2217", "\u22a4x + \u03b8\u2217\n0 (\u03bb, Dtrain )), y)\n\u03bb\nV\nD\n\u0398\u2217= A(D; \u03bb\u2217)\nE test = E(\u0398\u2217, L01, Dtest)\n The customer just wants to buy the right stocks! So we use the real \n here for\nvalidation.\nL01"]