["This page contains all content from the legacy PDF notes; reinforcement learning chapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Reinforcement learning (RL) is a type of machine learning where an agent learns to\nmake decisions by interacting with an environment. Unlike other learning", "paradigms, RL has several distinctive characteristics:\nThe agent interacts directly with an environment, receiving feedback in the\nform of rewards or penalties", "The agent can choose actions that influences what information it gains from the\nenvironment\nThe agent updates its decision-making strategy incrementally as it gains more\nexperience", "experience\nIn a reinforcement learning problem, the interaction between the agent and\nenvironment follows a specific pattern:\nLearner\nEnvironmen t\nreward\nstate\naction", "reward\nstate\naction\nThe interaction cycle proceeds as follows:\n1. Agent observes the current state \n2. Agent selects and executes an action \n3. Agent receives a reward \n from the environment", "4. Agent observes the new state \n5. Agent selects and executes a new action \n6. Agent receives a new reward \n7. This cycle continues\u2026", "Similar to MDP Chapter 10, in an RL problem, the agent\u2019s goal is to learn a policy - a\nmapping from states to actions - that maximizes its expected cumulative reward", "over time. This policy guides the agent\u2019s decision-making process, helping it choose\nactions that lead to the most favorable outcomes.\n11  Reinforcement Learning\nNote\ns(i)\na(i)\nr(i)\ns(i+1)\na(i+1)", "r(i)\ns(i+1)\na(i+1)\nr(i+1)\n\uf46111  Reinforcement Learning\n\uf52a\n 11.1 Reinforcement learning algorithms overview\nApproaches to reinforcement learning differ significantly according to what kind of", "hypothesis or model is being learned. Roughly speaking, RL methods can be\ncategorized into model-free methods and model-based methods. The main", "distinction is that model-based methods explicitly learn the transition and reward\nmodels to assist the end-goal of learning a policy; model-free methods do not. We", "will start our discussion with the model-free methods, and introduce two of the\narguably most popular types of algorithms, Q-learning Section 11.1.2 and policy", "gradient Section 11.3. We then describe model-based methods Section 11.4. Finally,\nwe briefly consider \u201cbandit\u201d problems Section 11.5, which differ from our MDP", "learning context by having probabilistic rewards.\nModel-free methods are methods that do not explicitly learn transition and reward", "models. Depending on what is explicitly being learned, model-free methods are\nsometimes further categorized into value-based methods (where the goal is to", "learn/estimate a value function) and policy-based methods (where the goal is to\ndirectly learn an optimal policy). It\u2019s important to note that such categorization is", "approximate and the boundaries are blurry. In fact, current RL research tends to\ncombine the learning of value functions, policies, and transition and reward models", "all into a complex learning algorithm, in an attempt to combine the strengths of\neach approach.\nQ-learning is a frequently used class of RL algorithms that concentrates on learning", "(estimating) the state-action value function, i.e., the \n function. Specifically, recall\nthe MDP value-iteration update:", "The Q-learning algorithm below adapts this value-iteration idea to the RL scenario,\nwhere we do not know the transition function  or reward function \n, and instead", ", and instead\nrely on samples to perform the updates.\nprocedure Q-Learning(\n)\nfor all \n do\nend for\nwhile \n do\n11.1.1 Model-free methods\n11.1.2 Q-learning\nQ\nQ(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQ(s\u2032, a\u2032)\nT\nR\n1:\nS, A, \u03b3, \u03b1, s0, max_iter\n2:\ni \u21900\n3:\ns \u2208S, a \u2208A\n4:\nQold(s, a) \u21900\n5:\n6:\ns \u2190s0\n7:\ni < max_iter\nThe thing that most students seem\nto get confused about is when we", "do value iteration and when we do\nQ-learning. Value iteration\nassumes you know  and \n and\njust need to compute \n. In Q-\nlearning, we don\u2019t know or even\ndirectly estimate  and \n: we\nestimate", ": we\nestimate \n directly from\nexperience!\nT\nR\nQ\nT\nR\nQ\n end while\nreturn \nend procedure\nWith the pseudo\u2011code provided for Q\u2011Learning, there are a few key things to note.", "First, we must determine which state to initialize the learning from. In the context of\na game, this initial state may be well defined. In the context of a robot navigating an", "environment, one may consider sampling the initial state at random. In any case,\nthe initial state is necessary to determine the trajectory the agent will experience as\nit navigates the environment.", "Second, different contexts will influence how we want to choose when to stop\niterating through the while loop. Again, in some games there may be a clear", "terminating state based on the rules of how it is played. On the other hand, a robot\nmay be allowed to explore an environment ad infinitum. In such a case, one may", "consider either setting a fixed number of transitions (as done explicitly in the\npseudo\u2011code) to take; or we may want to stop iterating in the example once the", "values in the Q\u2011table are not changing, after the algorithm has been running for a\nwhile.\nFinally, a single trajectory through the environment may not be sufficient to", "adequately explore all state\u2011action pairs. In these instances, it becomes necessary to\nrun through a number of iterations of the Q\u2011Learning algorithm, potentially with", "different choices of initial state \n.\nOf course, we would then want to modify Q\u2011Learning such that the Q table is not\nreset with each call.\nNow, let\u2019s dig into what is happening in Q\u2011Learning. Here,", "represents the\nlearning rate, which needs to decay for convergence purposes, but in practice is often\nset to a constant. It\u2019s also worth mentioning that Q-learning assumes a discrete state", "and action space where states and actions take on discrete values like \n etc.\nIn contrast, a continuous state space would allow the state to take values from, say,", "a continuous range of numbers; for example, the state could be any real number in\nthe interval \n. Similarly, a continuous action space would allow the action to be", "drawn from, e.g., a continuous range of numbers. There are now many extensions\ndeveloped based on Q-learning that can handle continuous state and action spaces", "(we\u2019ll look at one soon), and therefore the algorithm above is also sometimes\nreferred to more specifically as tabular Q-learning.\nIn the Q-learning update rule\n8:\na \u2190select_action(s, Qold(s, a))\n9:", "9:\n(r, s\u2032) \u2190execute(a)\n10:\nQnew(s, a) \u2190(1 \u2212\u03b1) Qold(s, a) + \u03b1(r + \u03b3 maxa\u2032 Qold(s\u2032, a\u2032))\n11:\ns \u2190s\u2032\n12:\ni \u2190i + 1\n13:\nQold \u2190Qnew\n14:\n15:\nQnew\n16:\ns0\n\u03b1 \u2208(0, 1]\n1, 2, 3, \u2026\n[1, 3]", "1, 2, 3, \u2026\n[1, 3]\nQ[s, a] \u2190(1 \u2212\u03b1)Q[s, a] + \u03b1(r + \u03b3 max\na\u2032\nQ[s\u2032, a\u2032])\n(11.1)\nThis notion of running a number of\ninstances of Q\u2011Learning is often\nreferred to as experiencing\nmultiple episodes.", "multiple episodes.\n the term \n is often referred to as the one-step look-ahead target.\nThe update can be viewed as a combination of two different iterative processes that", "we have already seen: the combination of an old estimate with the target using a\nrunning average with a learning rate \nEquation 11.1 can also be equivalently rewritten as", "which allows us to interpret Q-learning in yet another way: we make an update (or\ncorrection) based on the temporal difference between the target and the current\nestimated value", "estimated value \nThe Q-learning algorithm above includes a procedure called select_action , that,\ngiven the current state  and current \n function, has to decide which action to take.\nIf the", "If the \n value is estimated very accurately and the agent is deployed to \u201cbehave\u201d in\nthe world (as opposed to \u201clearn\u201d in the world), then generally we would want to", "choose the apparently optimal action \n.\nBut, during learning, the \n value estimates won\u2019t be very good and exploration is", "important. However, exploring completely at random is also usually not the best\nstrategy while learning, because it is good to focus your attention on the parts of the", "state space that are likely to be visited when executing a good policy (not a bad or\nrandom one).\nA typical action-selection strategy that attempts to address this exploration versus", "exploitation dilemma is the so-called -greedy strategy:\nwith probability \n, choose \n;\nwith probability , choose the action \n uniformly at random.", "where the  probability of choosing a random action helps the agent to explore and\ntry out actions that might not seem so desirable at the moment.", "Q-learning has the surprising property that it is guaranteed to converge to the actual\noptimal \n function! The conditions specified in the theorem are: visit every state-", "action pair infinitely often, and the learning rate  satisfies a scheduling condition.\nThis implies that for exploration strategy specifically, any strategy is okay as long as", "it tries every state-action infinitely often on an infinite run (so that it doesn\u2019t\nconverge prematurely to a bad action choice).", "Q-learning can be very inefficient. Imagine a robot that has a choice between\nmoving to the left and getting a reward of 1, then returning to its initial state, or", "moving to the right and walking down a 10-step hallway in order to get a reward of\n1000, then returning to its initial state.\n(r + \u03b3 maxa\u2032 Q[s\u2032, a\u2032])\n\u03b1.\nQ[s, a] \u2190Q[s, a] + \u03b1((r + \u03b3 max\na\u2032", "a\u2032\nQ[s\u2032, a\u2032]) \u2212Q[s, a]),\n(11.2)\nQ[s, a].\ns\nQ\nQ\narg maxa\u2208A Q(s, a)\nQ\n\u03f5\n1 \u2212\u03f5\narg maxa\u2208A Q(s, a)\n\u03f5\na \u2208A\n\u03f5\nQ\n\u03b1\n robot\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n+1000\n+1\n-1", "8\n9\n10\n+1000\n+1\n-1\nThe first time the robot moves to the right and goes down the hallway, it will\nupdate the \n value just for state 9 on the hallway and action ``right\u2019\u2019 to have a high", "value, but it won\u2019t yet understand that moving to the right in the earlier steps was a\ngood choice. The next time it moves down the hallway it updates the value of the", "state before the last one, and so on. After 10 trips down the hallway, it now can see\nthat it is better to move to the right than to the left.\nMore concretely, consider the vector of Q values", ", representing\nthe Q values for moving right at each of the positions \n. Position index 0\nis the starting position of the robot as pictured above.\nThen, for \n and \n, Equation 11.2 becomes", "Starting with Q values of 0,\nSince the only nonzero reward from moving right is \n, after our\nrobot makes it down the hallway once, our new Q vector is\nAfter making its way down the hallway again,", "updates:\nSimilarly,\nQ\nQ(i = 0, \u2026 , 9; right)\ni = 0, \u2026 , 9\n\u03b1 = 1\n\u03b3 = 0.9\nQ(i, right) = R(i, right) + 0.9 max\na\nQ(i + 1, a) .\nQ(0)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nR(9, right) = 1000", "R(9, right) = 1000\nQ(1)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1000\nQ(8, right) = 0 + 0.9 Q(9, right) = 900\nQ(2)(i = 0, \u2026 , 9; right) = [\n] .\n0\n0\n0\n0\n0\n0\n0\n0\n900\n1000", "0\n0\n0\n0\n0\n900\n1000\nQ(3)(i = 0, \u2026 , 9; right) = [\n] ,\n0\n0\n0\n0\n0\n0\n0\n810\n900\n1000\nQ(4)(i = 0, \u2026 , 9; right) = [\n] ,\n0\n0\n0\n0\n0\n0\n729\n810\n900\n1000\nWe are violating our usual", "notational conventions here, and\nwriting \n to mean the Q value\nfunction that results after the robot\nruns all the way to the end of the\nhallway, when executing the policy", "that always moves to the right.\nQi\n and the robot finally sees the value of moving right from position 0.\n\u2753 Study Question\nDetermine the Q value functions that result from always executing the \u201cmove", "left\u201d policy.\n11.2 Function approximation: Deep Q learning\nIn our Q-learning algorithm above, we essentially keep track of each \n value in a\ntable, indexed by  and . What do we do if  and/or", "are large (or continuous)?\nWe can use a function approximator like a neural network to store Q values. For\nexample, we could design a neural network that takes inputs  and , and outputs", ". We can treat this as a regression problem, optimizing this loss:\nwhere \n is now the output of the neural network.\nThere are several different architectural choices for using a neural network to", "approximate \n values:\nOne network for each action , that takes  as input and produces \n as\noutput;\nOne single network that takes  as input and produces a vector \n,\nconsisting of the", "consisting of the \n values for each action; or\nOne single network that takes \n concatenated into a vector (if  is discrete,", "we would probably use a one-hot encoding, unless it had some useful internal\nstructure) and produces \n as output.", "as output.\nThe first two choices are only suitable for discrete (and not too big) action sets. The\nlast choice can be applied for continuous actions, but then it is difficult to find\n.", ".\nThere are not many theoretical guarantees about Q-learning with function\napproximation and, indeed, it can sometimes be fairly unstable (learning to perform", "well for a while, and then suddenly getting worse, for example). But neural\nnetwork Q-learning has also had some significant successes.\n\u2026\nQ(10)(i = 0, \u2026 , 9; right) = [\n] ,\n387.4\n420.5\n478.3\n531.4", "420.5\n478.3\n531.4\n590.5\n656.1\n729\n810\n900\n1000\nQ\ns\na\nS\nA\ns\na\nQ(s, a)\n(Q(s, a) \u2212(r + \u03b3 max\na\u2032\nQ(s\u2032, a\u2032)))\n2\nQ(s, a)\nQ\na\ns\nQ(s, a)\ns\nQ(s, \u22c5)\nQ\ns, a\na\nQ(s, a)\narg maxa\u2208A Q(s, a)\nHere, we can see the", "exploration/exploitation dilemma\nin action: from the perspective of\n, it will seem that getting the\nimmediate reward of  is a better\nstrategy without exploring the long\nhallway.\ns0 = 0\n1", "hallway.\ns0 = 0\n1\nThis is the so-called squared\nBellman error; as the name\nsuggests, it\u2019s closely related to the\nBellman equation we saw in MDPs\nin Chapter Chapter 10. Roughly", "speaking, this error measures how\nmuch the Bellman equality is\nviolated.\nFor continuous action spaces, it is\npopular to use a class of methods\ncalled actor-critic methods, which", "combine policy and value-function\nlearning. We won\u2019t get into them in\ndetail here, though.\n One form of instability that we do know how to guard against is catastrophic", "forgetting. In standard supervised learning, we expect that the training  values\nwere drawn independently from some distribution.", "But when a learning agent, such as a robot, is moving through an environment, the\nsequence of states it encounters will be temporally correlated. For example, the", "robot might spend 12 hours in a dark environment and then 12 in a light one. This\ncan mean that while it is in the dark, the neural-network weight-updates will make\nthe", "the \n function \"forget\" the value function for when it\u2019s light.\nOne way to handle this is to use experience replay, where we save our", "experiences in a replay buffer. Whenever we take a step in the world, we add the\n to the replay buffer and use it to do a Q-learning update. Then we also", "randomly select some number of tuples from the replay buffer, and do Q-learning\nupdates based on them as well. In general, it may help to keep a sliding window of", "just the 1000 most recent experiences in the replay buffer. (A larger buffer will be\nnecessary for situations when the optimal policy might visit a large part of the state", "space, but we like to keep the buffer size small for memory reasons and also so that\nwe don\u2019t focus on parts of the state space that are irrelevant for the optimal policy.)", "The idea is that it will help us propagate reward values through our state space\nmore efficiently if we do these updates. We can see it as doing something like value", "iteration, but using samples of experience rather than a known model.\nAn alternative strategy for learning the \n function that is somewhat more robust\nthan the standard", "than the standard \n-learning algorithm is a method called fitted Q.\nprocedure Fitted-Q-Learning(\n)\n//e.g., \n can be drawn randomly from \ninitialize neural-network representation of \nwhile True do", "while True do\n experience from executing -greedy policy based on  for \n steps\n represented as tuples \nfor each tuple \n do\nend for\nre-initialize neural-network representation of \nend while", "end while\nend procedure\nHere, we alternate between using the policy induced by the current \n function to\ngather a batch of data \n, adding it to our overall data set \n, and then using", ", and then using\nsupervised neural-network training to learn a representation of the \n value\nfunction on the whole data set. This method does not mix the dynamic-\nx\nQ\n(s, a, s\u2032, r)\n(s, a, s\u2032, r)", "(s, a, s\u2032, r)\n11.2.1 Fitted Q-learning\nQ\nQ\n1:\nA, s0, \u03b3, \u03b1, \u03f5, m\n2:\ns \u2190s0\ns0\nS\n3:\nD \u2190\u2205\n4:\nQ\n5:\n6:\nDnew \u2190\n\u03f5\nQ\nm\n7:\nD \u2190D \u222aDnew\n(s, a, s\u2032, r)\n8:\nDsupervised \u2190\u2205\n9:\n(s, a, s\u2032, r) \u2208D\n10:\nx \u2190(s, a)\n11:", "10:\nx \u2190(s, a)\n11:\ny \u2190r + \u03b3 maxa\u2032\u2208A Q(s\u2032, a\u2032)\n12:\nDsupervised \u2190Dsupervised \u222a{(x, y)}\n13:\n14:\nQ\n15:\nQ \u2190supervised-NN-regression(Dsupervised)\n16:\n17:\nQ\nDnew\nD\nQ\nAnd, in fact, we routinely shuffle", "their order in the data file, anyway.\n programming phase (computing new \n values based on old ones) with the function\napproximation phase (supervised training of the neural network) and avoids", "catastrophic forgetting. The regression training in line 10 typically uses squared\nerror as a loss function and would be trained until the fit is good (possibly\nmeasured on held-out data).", "11.3 Policy gradient\nA different model-free strategy is to search directly for a good policy. The strategy\nhere is to define a functional form \n for the policy, where  represents the", "parameters we learn from experience. We choose  to be differentiable, and often\ndefine\n, a conditional probability distribution over our possible actions.", "Now, we can train the policy parameters using gradient descent:\nWhen  has relatively low dimension, we can compute a numeric estimate of", "the gradient by running the policy multiple times for different values of , and\ncomputing the resulting rewards.\nWhen  has higher dimensions (e.g., it represents the set of parameters in a", "complicated neural network), there are more clever algorithms, e.g., one called\nREINFORCE, but they can often be difficult to get to work reliably.", "Policy search is a good choice when the policy has a simple known form, but the\nMDP would be much more complicated to estimate.\n11.4 Model-based RL", "11.4 Model-based RL\nThe conceptually simplest approach to RL is to model \n and  from the data we\nhave gotten so far, and then use those models, together with an algorithm for", "solving MDPs (such as value iteration) to find a policy that is near-optimal given\nthe current models.\nAssume that we have had some set of interactions with the environment, which can", "be characterized as a set of tuples of the form \n.\nBecause the transition function \n specifies probabilities, multiple\nobservations of \n may be needed to model the transition function. One", "approach to building a model \n for the true \n is to estimate it using\na simple counting strategy:\nQ\nf(s; \u03b8) = a\n\u03b8\nf\nf(s, a; \u03b8) = Pr(a|s)\n\u03b8\n\u03b8\n\u03b8\nR\nT\n(s(t), a(t), s(t+1), r(t))\nT(s, a, s\u2032)\n(s, a, s\u2032)", "(s, a, s\u2032)\n^T(s, a, s\u2032)\nT(s, a, s\u2032)\n^T(s, a, s\u2032) = #(s, a, s\u2032) + 1\n#(s, a) + |S| .\nThis means the chance of choosing\nan action depends on which state\nthe agent is in. Suppose, e.g., a", "robot is trying to get to a goal and\ncan go left or right. An\nunconditional policy can say: I go\nleft 99% of the time; a conditional\npolicy can consider the robot\u2019s", "state, and say: if I\u2019m to the right of\nthe goal, I go left 99% of the time.\n Here, \n represents the number of times in our data set we have the\nsituation where \n, \n, \n, and \n represents the number of", "times in our data set we have the situation where \n, \n.\nAdding 1 and \n to the numerator and denominator, respectively, is a form of", "smoothing called the Laplace correction. It ensures that we never estimate that a\nprobability is 0, and keeps us from dividing by 0. As the amount of data we gather", "increases, the influence of this correction fades away.\nIn contrast, the reward function \n is a deterministic function, such that\nknowing the reward  for a given", "is sufficient to fully determine the function\nat that point. Our model \n can simply be a record of observed rewards, such that\n.\nGiven empirical models  and", "for the transition and reward functions, we can\nnow solve the MDP \n to find an optimal policy using value iteration, or\nuse a search algorithm to find an action to take for a particular state.", "This approach is effective for problems with small state and action spaces, where it\nis not too hard to get enough experience to model  and \n well; but it is difficult to", "generalize this method to handle continuous (or very large discrete) state spaces,\nand is a topic of current research.\n11.5 Bandit problems", "Bandit problems are a subset of reinforcement learning problems. A basic bandit\nproblem is given by:\nA set of actions \n;\nA set of reward values \n; and\nA probabilistic reward function \n, i.e.,", ", i.e., \n is a function that\ntakes an action and a reward and returns the probability of getting that reward\nconditioned on that action being taken,\n. Each time the agent takes an action, a", "new value is drawn from this distribution.\nThe most typical bandit problem has \n and \n. This is called a -\narmed bandit problem, where the decision is which \u201carm\u201d (action ) to select, and the", "reward is either getting a payoff ( ) or not ( ).\nThe important question is usually one of exploration versus exploitation. Imagine you\nhave tried each action 10 times, and now you have estimates", "for the\nprobabilities \n. Which arm should you pick next? You could:\nexploit your knowledge, choosing the arm with the highest value of expected\nreward; or\n#(s, a, s\u2032)\ns(t) = s a(t) = a s(t+1) = s\u2032", "#(s, a)\ns(t) = s a(t) = a\n|S|\nR(s, a)\nr\n(s, a)\n^R\n^R(s, a) = r = R(s, a)\n^T\n^R\n(S, A, ^T, ^R)\nT\nR\nA\nR\nRp : A \u00d7 R \u2192R\nRp\nRp(a, r) = Pr(reward = r \u2223action = a)\nR = {0, 1}\n|A| = k\nk\na\n1\n0\n^Rp(a, r)", "k\na\n1\n0\n^Rp(a, r)\nRp(a, r)\nConceptually, this is similar to\nhaving \u201cinitialized\u201d our estimate\nfor the transition function with\nuniform random probabilities\nbefore making any observations.", "Notice that this probablistic\nrewards set up in bandits differs\nfrom the \u201crewards are\ndeterministic\u201d assumptions we\nmade so far.\nWhy \u201cbandit\u201d? In English slang,\n\u201cone-armed bandit\u201d refers to a slot", "machine because it has one arm\nand takes your money! Here, we\nhave a similar machine but with \narms.\nk\n explore further, trying some or all actions more times to get better estimates of\nthe \n values.", "the \n values.\nThe theory ultimately tells us that, the longer our horizon  (or similarly, closer to 1\nour discount factor), the more time we should spend exploring, so that we don\u2019t", "converge prematurely on a bad choice of action.\nBandit problems are reinforcement learning problems (and very different from\nbatch supervised learning) in that:", "The agent gets to influence what data it obtains (selecting  gives it another\nsample from \n), and\nThe agent is penalized for mistakes it makes while it is learning (trying to", "maximize the expected reward it gets while behaving).\nIn a contextual bandit problem, you have multiple possible states from some set ,\nand a separate bandit problem associated with each one.", "Bandit problems are an essential subset of reinforcement learning. It\u2019s important to\nbe aware of the issues, but we will not study solutions to them in this class.\nRp(a, r)\nh\na\nR(a, r)\nS"]