["This page contains all content from the legacy PDF notes; markov decision processes\nchapter.\nAs we phase out the PDF, this page may receive updates not reflected in the static PDF.", "Consider a robot learning to navigate through a maze, a game-playing AI\ndeveloping strategies through self-play, or a self-driving car making driving", "decisions in real-time. These problems share a common challenge: the agent must\nmake a sequence of decisions where each choice affects future possibilities and", "rewards. Unlike static prediction tasks where we learn a one-time mapping from\ninputs to outputs, these problems require reasoning about the consequences of\nactions over time.", "actions over time.\nThis sequential and dynamical nature demands mathematical tools beyond the\nmore static supervised or unsupervised learning approaches. The most general", "framework for such problems is reinforcement learning (RL), where an agent learns to\ntake actions in an unknown environment to maximize cumulative rewards over\ntime.", "time.\nIn this chapter, we\u2019ll first study Markov decision processes (MDPs), which provide the\nmathematical foundation for understanding and solving sequential decision", "making problems like RL. MDPs formalize the interaction between an agent and its\nenvironment, capturing the key elements of states, actions, rewards, and transitions.", "10.1 Definition and value functions\nFormally, a Markov decision process is \n where  is the state space, \nis the action space, and:\n is a transition model, where", "specifying a conditional probability distribution;\n is a reward function, where \n specifies an immediate\nreward for taking action  when in state ; and", "is a discount factor, which we\u2019ll discuss in Section 10.1.2.\nIn this class, we assume the rewards are deterministic functions. Further, in this", "MDP chapter, we assume the state space and action space are discrete and finite.\n10  Markov Decision Processes\nNote\n\u27e8S, A, T, R, \u03b3\u27e9\nS\nA\nT : S \u00d7 A \u00d7 S \u2192R\nT(s, a, s\u2032) = Pr(St = s\u2032|St\u22121 = s, At\u22121 = a) ,", "R : S \u00d7 A \u2192R\nR(s, a)\na\ns\n\u03b3 \u2208[0, 1]\nThe notation \n uses a capital\nletter  to stand for a random\nvariable, and small letter  to stand\nfor a concrete value. So \n here is a", "here is a\nrandom variable that can take on\nelements of  as values.\nSt = s\u2032\nS\ns\nSt\nS\n\uf46110  Markov Decision Processes\n\uf52a", "\uf52a\n The following description of a simple machine as Markov decision process provides a\nconcrete example of an MDP.", "The machine has three possible operations (actions): wash , paint , and eject  (each with\na corresponding button). Objects are put into the machine, and each time you push a", "button, something is done to the object. However, it\u2019s an old machine, so it\u2019s not very\nreliable. The machine has a camera inside that can clearly detect what is going on with the", "object and will output the state of the object: dirty , clean , painted , or ejected .\nFor each action, this is what is done to the object:\nWash", "Wash\nIf you perform the wash  operation on any object\u2014whether it\u2019s dirty, clean, or\npainted\u2014it will end up clean  with probability 0.9 and dirty  otherwise.\nPaint", "Paint\nIf you perform the paint  operation on a clean object, it will become nicely painted\nwith probability 0.8. With probability 0.1, the paint misses but the object stays clean,", "and with probability 0.1, the machine dumps rusty dust all over the object, making it\ndirty .\nIf you perform the paint  operation on a painted  object, it stays painted  with\nprobability 1.0.", "probability 1.0.\nIf you perform the paint  operation on a dirty  object, it stays dirty  with\nprobability 1.0.\nEject\nIf you perform an eject  operation on any object, the object comes out of the", "machine and the process is terminated. The object remains ejected  regardless of\nany further actions.\nThese descriptions specify the transition model , and the transition function for each", "action can be depicted as a state machine diagram. For example, here is the diagram for\nwash :\nExample\nT\n dirty\nclean\npainted\nejected\n0.1\n0.9\n0.9\n0.1\n0.1\n0.9\n1.0", "0.9\n0.1\n0.1\n0.9\n1.0\nYou get reward +10 for ejecting a painted object, reward 0 for ejecting a non-painted\nobject, reward 0 for any action on an \u201cejected\u201d object, and reward -3 otherwise. The MDP", "description would be completed by also specifying a discount factor.\nA policy is a function  that specifies what action to take in each state. The policy is", "what we will want to learn; it is akin to the strategy that a player employs to win a\ngiven game. Below, we take just the initial steps towards this eventual goal. We", "describe how to evaluate how good a policy is, first in the finite horizon case\nSection 10.1.1 when the total number of transition steps is finite. In the finite", "horizon case, we typically denote the policy as \n, where  is a non-negative\ninteger denoting the number of steps remaining and \n is the current state. Then", "we consider the infinite horizon case Section 10.1.2, when you don\u2019t know when the\ngame will be over.\nThe goal of a policy is to maximize the expected total reward, averaged over the", "stochastic transitions that the domain makes. Let\u2019s first consider the case where\nthere is a finite horizon , indicating the total number of steps of interaction that the", "agent will have with the MDP.\nWe seek to measure the goodness of a policy. We do so by defining for a given\nhorizon  and MDP policy \n, the \u201chorizon  value\u201d of a state, \n. We do this by", ". We do this by\ninduction on the horizon, which is the number of steps left to go.\nThe base case is when there are no steps remaining, in which case, no matter what\nstate we\u2019re in, the value is 0, so", "Then, the value of a policy in state  at horizon \n is equal to the reward it will\nget in state  plus the next state\u2019s expected horizon  value, discounted by a factor \n\u03c0\n\u03c0h(s)\nh\ns \u2208S", "\u03c0\n\u03c0h(s)\nh\ns \u2208S\n10.1.1 Finite-horizon value functions\nh\nh\n\u03c0h\nh\nV\u03c0\nh(s)\nV\u03c0\n0(s) = 0 .\n(10.1)\ns\nh + 1\ns\nh\n\u03b3\n . So, starting with horizons 1 and 2, and then moving to the general case, we have:", "The sum over  is an expectation: it considers all possible next states , and\ncomputes an average of their \n-horizon values, weighted by the probability", "that the transition function from state  with the action chosen by the policy \nassigns to arriving in state , and discounted by .\n\u2753 Study Question\nWhat is the value of", "for any given state\u2013action pair \n?\n\u2753 Study Question\nConvince yourself that the definitions in Equation 10.1 and Equation 10.3 are\nspecial cases of the more general formulation in Equation 10.4.", "Then we can say that a policy  is better than policy  for horizon  if and only if\nfor all \n, \n and there exists at least one \n such that\n.", "such that\n.\nMore typically, the actual finite horizon is not known, i.e., when you don\u2019t know\nwhen the game will be over! This is called the infinite horizon version of the problem.", "How does one evaluate the goodness of a policy in the infinite horizon case?\nIf we tried to simply take our definitions above and use them for an infinite", "horizon, we could get in trouble. Imagine we get a reward of 1 at each step under\none policy and a reward of 2 at each step under a different policy. Then the reward", "as the number of steps grows in each case keeps growing to become infinite in the\nlimit of more and more steps. Even though it seems intuitive that the second policy", "should be better, we can\u2019t justify that by saying \n.\nV\u03c0\n1(s) = R(s, \u03c01(s)) + 0\n(10.2)\nV\u03c0\n2(s) = R(s, \u03c02(s)) + \u03b3 \u2211\ns\u2032\nT(s, \u03c02(s), s\u2032)V\u03c0\n1(s\u2032)\n(10.3)\n\u22ee\nV\u03c0\nh(s) = R(s, \u03c0h(s)) + \u03b3 \u2211\ns\u2032\nT(s, \u03c0h(s), s\u2032)V\u03c0", "T(s, \u03c0h(s), s\u2032)V\u03c0\nh\u22121(s\u2032)\n(10.4)\ns\u2032\ns\u2032\n(h \u22121)\ns\n\u03c0h(s)\ns\u2032\n\u03b3\n\u2211\ns\u2032\nT(s, a, s\u2032)\n(s, a)\n\u03c0\n\u00af\u03c0\nh\ns \u2208S V\u03c0\nh(s) \u2265V\u00af\u03c0\nh(s)\ns \u2208S\nV\u03c0\nh(s) > V\u00af\u03c0\nh(s)\n10.1.2 Infinite-horizon value functions\n\u221e< \u221e", "\u221e< \u221e\n One standard approach to deal with this problem is to consider the discounted\ninfinite horizon. We will generalize from the finite-horizon case by adding a\ndiscount factor.", "discount factor.\nIn the finite-horizon case, we valued a policy based on an expected finite-horizon\nvalue:\nwhere \n is the reward received at time .\nWhat is", "What is \n? This mathematical notation indicates an expectation, i.e., an average taken\nover all the random possibilities which may occur for the argument. Here, the expectation", "is taken over the conditional probability \n, where \n is the random variable\nfor the reward, subject to the policy being  and the state being \n. Since  is a function,", "this notation is shorthand for conditioning on all of the random variables implied by\npolicy  and the stochastic transitions of the MDP.\nA very important point is that", "is always deterministic (in this class) for any given\n and . Here \n represents the set of all possible \n at time step ; this \n is a", "is a\nrandom variable because the state we\u2019re in at step  is itself a random variable, due to\nprior stochastic state transitions up to but not including at step  and prior (deterministic)", "actions dictated by policy \nNow, for the infinite-horizon case, we select a discount factor \n, and\nevaluate a policy based on its expected infinite horizon value:", "Note that the  indices here are not the number of steps to go, but actually the\nnumber of steps forward from the starting state (there is no sensible notion of \u201csteps", "to go\u201d in the infinite horizon case).\nEquation 10.5 and Equation 10.6 are a conceptual stepping stone. Our main objective is to", "get to Equation 10.8, which can also be viewed as including  in Equation 10.4, with the\nappropriate definition of the infinite-horizon value.", "There are two good intuitive motivations for discounting. One is related to\neconomic theory and the present value of money: you\u2019d generally rather have some", "money today than that same amount of money next week (because you could use it\nnow or invest it). The other is to think of the whole process terminating, with\nprobability", "probability \n on each step of the interaction. (At every step, your expected\nfuture lifetime, given that you have survived until now, is \n.) This value is", ".) This value is\nthe expected amount of reward the agent would gain under this terminating model.\nE [\nh\u22121\n\u2211\nt=0\n\u03b3 tRt \u2223\u03c0, s0] ,\n(10.5)\nRt\nt\nNote\nE [\u22c5]\nPr(Rt = r \u2223\u03c0, s0)\nRt\n\u03c0\ns0\n\u03c0\n\u03c0\nR(s, a)\ns\na\nRt", "\u03c0\n\u03c0\nR(s, a)\ns\na\nRt\nR(st, a)\nt\nRt\nt\nt\n\u03c0.\n0 \u2264\u03b3 \u22641\nE [\n\u221e\n\u2211\nt=0\n\u03b3 tRt \u2223\u03c0, s0] = E [R0 + \u03b3R1 + \u03b3 2R2 + \u2026 \u2223\u03c0, s0] .\n(10.6)\nt\nNote\n\u03b3\n1 \u2212\u03b3\n1/(1 \u2212\u03b3)\n \u2753 Study Question", "\u2753 Study Question\nVerify this fact: if, on every day you wake up, there is a probability of \n that\ntoday will be your last day, then your expected lifetime is \n days.", "days.\nLet us now evaluate a policy in terms of the expected discounted infinite-horizon\nvalue that the agent will get in the MDP if it executes that policy. We define the", "infinite-horizon value of a state  under policy  as\nBecause the expectation of a linear combination of random variables is the linear\ncombination of the expectations, we have", "The equation defined in Equation 10.8 is known as the Bellman Equation, which\nbreaks down the value function into the immediate reward and the (discounted)", "future value function. You could write down one of these equations for each of the\n states. There are  unknowns \n. These are linear equations, and", "standard software (e.g., using Gaussian elimination or other linear algebraic\nmethods) will, in most cases, enable us to find the value of each state under this\npolicy.\n10.2 Finding policies for MDPs", "Given an MDP, our goal is typically to find a policy that is optimal in the sense that\nit gets as much total reward as possible, in expectation over the stochastic", "transitions that the domain makes. We build on what we have learned about\nevaluating the goodness of a policy (Section 10.1.2), and find optimal policies for the", "finite horizon case (Section 10.2.1), then the infinite horizon case (Section 10.2.2).\nHow can we go about finding an optimal policy for an MDP? We could imagine", "enumerating all possible policies and calculating their value functions as in the\nprevious section and picking the best one \u2013 but that\u2019s too much work!", "The first observation to make is that, in a finite-horizon problem, the best action to\ntake depends on the current state, but also on the horizon: imagine that you are in a", "situation where you could reach a state with reward 5 in one step or a state with\nreward 100 in two steps. If you have at least two steps to go, then you\u2019d move\n1 \u2212\u03b3\n1/(1 \u2212\u03b3)\ns\n\u03c0\nV\u03c0", "1/(1 \u2212\u03b3)\ns\n\u03c0\nV\u03c0\n\u221e(s) = E[R0 + \u03b3R1 + \u03b3 2R2 + \u22ef\u2223\u03c0, S0 = s]\n= E[R0 + \u03b3(R1 + \u03b3(R2 + \u03b3 \u2026))) \u2223\u03c0, S0 = s] .\n(10.7)\nV\u03c0\n\u221e(s) = E[R0 \u2223\u03c0, S0 = s] + \u03b3E[R1 + \u03b3(R2 + \u03b3 \u2026))) \u2223\u03c0, S0 = s]\n= R(s, \u03c0(s)) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, \u03c0(s), s\u2032)V\u03c0\n\u221e(s\u2032) .\n(10.8)\nn = |S|\nn\nV\u03c0(s)\n10.2.1 Finding optimal finite-horizon policies\n toward the reward 100 state, but if you only have one step left to go, you should go", "in the direction that will allow you to gain 5!\nFor the finite-horizon case, we define \n to be the expected value of\nstarting in state ,\nexecuting action , and\ncontinuing for", "continuing for \n more steps executing an optimal policy for the\nappropriate horizon on each step.\nSimilar to our definition of \n for evaluating a policy, we define the \n function", "function\nrecursively according to the horizon. The only difference is that, on each step with\nhorizon , rather than selecting an action specified by a given policy, we select the", "value of  that will maximize the expected \n value of the next state.\nwhere \n denotes the next time-step state/action pair. We can solve for the\nvalues of", "values of \n with a simple recursive algorithm called finite-horizon value iteration\nthat just computes \n starting from horizon 0 and working backward to the desired\nhorizon . Given \n, an optimal", ", an optimal \n can be found as follows:\nwhich gives the immediate best action(s) to take when there are  steps left; then\n gives the best action(s) when there are \n steps left, and so on. In the", "case where there are multiple best actions, we typically can break ties randomly.\nAdditionally, it is worth noting that in order for such an optimal policy to be", "computed, we assume that the reward function \n is bounded on the set of all\npossible (state, action) pairs. Furthermore, we will assume that the set of all possible\nactions is finite.", "actions is finite.\n\u2753 Study Question\nThe optimal value function is unique, but the optimal policy is not. Think of a\nsituation in which there is more than one optimal policy.\nQ\u2217\nh(s, a)\ns\na\nh \u22121\nV\u2217\nh", "s\na\nh \u22121\nV\u2217\nh\nQ\u2217\nh\nh\na\nQ\u2217\nh\nQ\u2217\n0(s, a) = 0\nQ\u2217\n1(s, a) = R(s, a) + 0\nQ\u2217\n2(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\n1(s\u2032, a\u2032)\n\u22ee\nQ\u2217\nh(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\nh\u22121(s\u2032, a\u2032)", "a\u2032\nQ\u2217\nh\u22121(s\u2032, a\u2032)\n(s\u2032, a\u2032)\nQ\u2217\nh\nQ\u2217\nh\nh\nQ\u2217\nh\n\u03c0\u2217\nh\n\u03c0\u2217\nh(s) = arg max\na\nQ\u2217\nh(s, a) .\nh\n\u03c0\u2217\nh\u22121(s)\n(h \u22121)\nR(s, a)\n10.2.2 Finding optimal infinite-horizon policies\nWe can also define the action-value", "function for a fixed policy ,\ndenoted by \n. This quantity\nrepresents the expected sum of\ndiscounted rewards obtained by\ntaking action  in state  and\nthereafter following the policy", "over the remaining horizon of\n steps.\nSimilar to \n, \n satisfies\nthe Bellman recursion/equations\nintroduced earlier. In fact, for a\ndeterministic policy :\nHowever, since our primary goal in", "dealing with action values is\ntypically to identify an optimal\npolicy, we will not dwell\nextensively on (\n). Instead,\nwe will place more emphasis on\nthe optimal action-value functions\n.\n\u03c0\nQ\u03c0\nh(s, a)", ".\n\u03c0\nQ\u03c0\nh(s, a)\na\ns\n\u03c0\nh \u22121\nV\u03c0\nh(s) Q\u03c0\nh(s, a)\n\u03c0\nQ\u03c0\nh(s, \u03c0(s)) = V\u03c0\nh(s).\nQ\u03c0\nh(s, a)\nQ\u2217\nh(s, a)\n In contrast to the finite-horizon case, the best way of behaving in an infinite-horizon", "discounted MDP is not time-dependent. That is, the decisions you make at time\n looking forward to infinity, will be the same decisions that you make at time", "for any positive , also looking forward to infinity.\nAn important theorem about MDPs is: in the infinite-horizon case, there exists a\nstationary optimal policy", "(there may be more than one) such that for all \nand all other policies , we have\nThere are many methods for finding an optimal policy for an MDP. We have already", "seen the finite-horizon value iteration case. Here we will study a very popular and\nuseful method for the infinite-horizon case, infinite-horizon value iteration. It is also", "important to us, because it is the basis of many reinforcement-learning methods.\nWe will again assume that the reward function \n is bounded on the set of all", "possible (state, action) pairs and additionally that the number of actions in the\naction space is finite. Define \n to be the expected infinite-horizon value of", "being in state , executing action , and executing an optimal policy \n thereafter.\nUsing similar reasoning to the recursive definition of \n we can express this value\nrecursively as", "recursively as\nThis is also a set of equations, one for each \n pair. This time, though, they are\nnot linear (due to the \n operation), and so they are not easy to solve. But there is", "a theorem that says they have a unique solution!\nOnce we know the optimal action-value function \n, then we can extract an\noptimal policy \n as\nWe can iteratively solve for the", "values with the infinite-horizon value iteration\nalgorithm, shown below:\nAlgorithm 10.1 Infinite-Horizon-Value-Iteration\nRequire: , \n, , , , \nInitialization:\nfor each \n and \n do\nend for", "and \n do\nend for\nwhile not converged do\nfor each \n and \n do\nend for\nif \n then\nreturn \nend if\nt = 0\nt = T\nT\n\u03c0\u2217\ns \u2208S\n\u03c0\nV\u03c0(s) \u2264V\u03c0\u2217(s) .\nR(s, a)\nQ\u2217\n\u221e(s, a)\ns\na\n\u03c0\u2217\nV\u03c0,\nQ\u2217\n\u221e(s, a) = R(s, a) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQ\u2217\n\u221e(s\u2032, a\u2032) .\n(s, a)\nmax\nQ\u2217\n\u221e(s, a)\n\u03c0\u2217\n\u03c0\u2217(s) = arg max\na\nQ\u2217\n\u221e(s, a)\nQ\u2217\nS A T R \u03b3 \u03f5\n1:\n2:\ns \u2208S\na \u2208A\n3:\nQold(s, a) \u21900\n4:\n5:\n6:\ns \u2208S\na \u2208A\n7:\nQnew(s, a) \u2190R(s, a) + \u03b3 \u2211\ns\u2032", "s\u2032\nT(s, a, s\u2032) max\na\u2032\nQold(s\u2032, a\u2032)\n8:\n9:\nmax\ns,a |Qold(s, a) \u2212Qnew(s, a)| < \u03f5\n10:\nQnew\n11:\n end while\nThere are a lot of nice theoretical results about infinite-horizon value iteration. For", "some given (not necessarily optimal) \n function, define \n.\nAfter executing infinite-horizon value iteration with convergence hyper-\nparameter ,\nThere is a value of  such that", "As the algorithm executes, \n decreases monotonically on each\niteration.\nThe algorithm can be executed asynchronously, in parallel: as long as all", "pairs are updated infinitely often in an infinite run, it still converges to the\noptimal value.\n12:\nQold \u2190Qnew\n13:\nTheory\nQ\n\u03c0Q(s) = arg maxa Q(s, a)\n\u03f5\n\u2225V\u03c0Qnew \u2212V\u03c0\u2217\u2225max < \u03f5 .\n\u03f5", "\u03f5\n\u2225Qold \u2212Qnew\u2225max < \u03f5 \u27f9\u03c0Qnew = \u03c0\u2217\n\u2225V\u03c0Qnew \u2212V\u03c0\u2217\u2225max\n(s, a)"]